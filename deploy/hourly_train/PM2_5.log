nohup: ignoring input
北京市
1005A
1008A
1009A
1010A
1012A
3417A
3418A
3671A
3672A
3673A
3674A
3675A
3695A
3696A
3697A
[flaml.automl: 09-16 04:30:26] {2390} INFO - task = regression
[flaml.automl: 09-16 04:30:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 04:30:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 04:30:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 04:30:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 04:30:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 04:30:28] {3025} INFO - Estimated sufficient time budget=288172s. Estimated necessary time budget=288s.
[flaml.automl: 09-16 04:30:28] {3072} INFO -  at 2.8s,	estimator xgboost's best error=24.3293,	best estimator xgboost's best error=24.3293
[flaml.automl: 09-16 04:30:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 04:30:30] {3072} INFO -  at 4.3s,	estimator xgboost's best error=23.1797,	best estimator xgboost's best error=23.1797
[flaml.automl: 09-16 04:30:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 04:30:31] {3072} INFO -  at 6.0s,	estimator xgboost's best error=23.1797,	best estimator xgboost's best error=23.1797
[flaml.automl: 09-16 04:30:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 04:30:34] {3072} INFO -  at 8.6s,	estimator xgboost's best error=23.1797,	best estimator xgboost's best error=23.1797
[flaml.automl: 09-16 04:30:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 04:30:35] {3072} INFO -  at 9.8s,	estimator xgboost's best error=15.4613,	best estimator xgboost's best error=15.4613
[flaml.automl: 09-16 04:30:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 04:30:37] {3072} INFO -  at 11.7s,	estimator xgboost's best error=12.9188,	best estimator xgboost's best error=12.9188
[flaml.automl: 09-16 04:30:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 04:30:38] {3072} INFO -  at 13.2s,	estimator xgboost's best error=12.9188,	best estimator xgboost's best error=12.9188
[flaml.automl: 09-16 04:30:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 04:30:41] {3072} INFO -  at 16.0s,	estimator xgboost's best error=12.9188,	best estimator xgboost's best error=12.9188
[flaml.automl: 09-16 04:30:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 04:30:43] {3072} INFO -  at 17.5s,	estimator xgboost's best error=12.9188,	best estimator xgboost's best error=12.9188
[flaml.automl: 09-16 04:30:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 04:30:46] {3072} INFO -  at 20.4s,	estimator xgboost's best error=12.9188,	best estimator xgboost's best error=12.9188
[flaml.automl: 09-16 04:30:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 04:30:49] {3072} INFO -  at 23.7s,	estimator xgboost's best error=9.3140,	best estimator xgboost's best error=9.3140
[flaml.automl: 09-16 04:30:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 04:30:53] {3072} INFO -  at 27.9s,	estimator xgboost's best error=9.3140,	best estimator xgboost's best error=9.3140
[flaml.automl: 09-16 04:30:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 04:30:57] {3072} INFO -  at 31.4s,	estimator xgboost's best error=4.7766,	best estimator xgboost's best error=4.7766
[flaml.automl: 09-16 04:30:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 04:30:59] {3072} INFO -  at 33.7s,	estimator xgboost's best error=4.7766,	best estimator xgboost's best error=4.7766
[flaml.automl: 09-16 04:30:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 04:31:04] {3072} INFO -  at 38.5s,	estimator xgboost's best error=3.5471,	best estimator xgboost's best error=3.5471
[flaml.automl: 09-16 04:31:04] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 04:31:11] {3072} INFO -  at 46.0s,	estimator xgboost's best error=3.5471,	best estimator xgboost's best error=3.5471
[flaml.automl: 09-16 04:31:11] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 04:31:14] {3072} INFO -  at 48.9s,	estimator xgboost's best error=3.5471,	best estimator xgboost's best error=3.5471
[flaml.automl: 09-16 04:31:14] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 04:31:16] {3072} INFO -  at 50.7s,	estimator xgboost's best error=3.5471,	best estimator xgboost's best error=3.5471
[flaml.automl: 09-16 04:31:16] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 04:31:22] {3072} INFO -  at 56.7s,	estimator xgboost's best error=3.5471,	best estimator xgboost's best error=3.5471
[flaml.automl: 09-16 04:31:29] {3335} INFO - retrain xgboost for 6.7s
[flaml.automl: 09-16 04:31:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7164723828648284, colsample_bynode=1,
             colsample_bytree=0.5372780014279438, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.4863979073627235,
             missing=nan, monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.013677698859125073, reg_lambda=9.040735251848712,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 04:31:29] {2636} INFO - fit succeeded
[flaml.automl: 09-16 04:31:29] {2637} INFO - Time taken to find the best model: 38.54177713394165
PM2.5(0)最佳参数：{'n_estimators': 4, 'max_leaves': 18, 'min_child_weight': 0.4863979073627235, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7164723828648284, 'colsample_bytree': 0.5372780014279438, 'reg_alpha': 0.013677698859125073, 'reg_lambda': 9.040735251848712, 'FLAML_sample_size': 40000}
PM2.5(0)最佳损失：-2.5471474198631316
PM2.5(0)最好结果：{'pred_time': 9.701723971874455e-06, 'wall_clock_time': 38.54177713394165, 'metric_for_logging': {'pred_time': 9.701723971874455e-06}, 'val_loss': 3.5471474198631316, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 18, 'min_child_weight': 0.4863979073627235, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7164723828648284, 'colsample_bytree': 0.5372780014279438, 'reg_alpha': 0.013677698859125073, 'reg_lambda': 9.040735251848712, 'FLAML_sample_size': 40000}, 'config/n_estimators': 4, 'config/max_leaves': 18, 'config/min_child_weight': 0.4863979073627235, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7164723828648284, 'config/colsample_bytree': 0.5372780014279438, 'config/reg_alpha': 0.013677698859125073, 'config/reg_lambda': 9.040735251848712, 'config/FLAML_sample_size': 40000, 'experiment_tag': 'exp', 'time_total_s': 4.820864677429199}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7164723828648284, colsample_bynode=1,
             colsample_bytree=0.5372780014279438, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.4863979073627235,
             missing=nan, monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.013677698859125073, reg_lambda=9.040735251848712,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9470698218988403
PM2.5(0)的mse=36.29644000924417
PM2.5(0)的mae=3.5524360509344146
PM2.5(0)的mar=0.21915440580385104
总共花费的时间为：67.20
天津市
1015A
1019A
2859A
2860A
2922A
3325A
3326A
3327A
3460A
3461A
3462A
[flaml.automl: 09-16 05:03:04] {2390} INFO - task = regression
[flaml.automl: 09-16 05:03:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:03:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:03:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:03:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:03:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:03:06] {3025} INFO - Estimated sufficient time budget=241865s. Estimated necessary time budget=242s.
[flaml.automl: 09-16 05:03:07] {3072} INFO -  at 2.7s,	estimator xgboost's best error=23.2587,	best estimator xgboost's best error=23.2587
[flaml.automl: 09-16 05:03:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:03:09] {3072} INFO -  at 4.8s,	estimator xgboost's best error=19.3752,	best estimator xgboost's best error=19.3752
[flaml.automl: 09-16 05:03:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:03:10] {3072} INFO -  at 6.6s,	estimator xgboost's best error=19.3752,	best estimator xgboost's best error=19.3752
[flaml.automl: 09-16 05:03:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:03:12] {3072} INFO -  at 8.5s,	estimator xgboost's best error=19.3752,	best estimator xgboost's best error=19.3752
[flaml.automl: 09-16 05:03:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:03:14] {3072} INFO -  at 10.2s,	estimator xgboost's best error=8.4004,	best estimator xgboost's best error=8.4004
[flaml.automl: 09-16 05:03:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:03:16] {3072} INFO -  at 12.0s,	estimator xgboost's best error=8.4004,	best estimator xgboost's best error=8.4004
[flaml.automl: 09-16 05:03:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:03:17] {3072} INFO -  at 13.5s,	estimator xgboost's best error=8.4004,	best estimator xgboost's best error=8.4004
[flaml.automl: 09-16 05:03:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:03:19] {3072} INFO -  at 15.0s,	estimator xgboost's best error=8.4004,	best estimator xgboost's best error=8.4004
[flaml.automl: 09-16 05:03:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:03:20] {3072} INFO -  at 16.5s,	estimator xgboost's best error=7.7649,	best estimator xgboost's best error=7.7649
[flaml.automl: 09-16 05:03:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:03:22] {3072} INFO -  at 17.9s,	estimator xgboost's best error=7.7649,	best estimator xgboost's best error=7.7649
[flaml.automl: 09-16 05:03:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:03:23] {3072} INFO -  at 19.0s,	estimator xgboost's best error=7.7649,	best estimator xgboost's best error=7.7649
[flaml.automl: 09-16 05:03:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:03:24] {3072} INFO -  at 20.3s,	estimator xgboost's best error=7.7649,	best estimator xgboost's best error=7.7649
[flaml.automl: 09-16 05:03:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:03:25] {3072} INFO -  at 21.4s,	estimator xgboost's best error=5.9006,	best estimator xgboost's best error=5.9006
[flaml.automl: 09-16 05:03:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:03:27] {3072} INFO -  at 23.0s,	estimator xgboost's best error=5.9006,	best estimator xgboost's best error=5.9006
[flaml.automl: 09-16 05:03:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 05:03:28] {3072} INFO -  at 24.5s,	estimator xgboost's best error=5.9006,	best estimator xgboost's best error=5.9006
[flaml.automl: 09-16 05:03:28] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 05:03:30] {3072} INFO -  at 26.1s,	estimator xgboost's best error=5.9006,	best estimator xgboost's best error=5.9006
[flaml.automl: 09-16 05:03:30] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 05:03:31] {3072} INFO -  at 26.9s,	estimator xgboost's best error=5.9006,	best estimator xgboost's best error=5.9006
[flaml.automl: 09-16 05:03:31] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 05:03:38] {3072} INFO -  at 33.8s,	estimator xgboost's best error=4.9376,	best estimator xgboost's best error=4.9376
[flaml.automl: 09-16 05:03:38] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 05:03:41] {3072} INFO -  at 37.0s,	estimator xgboost's best error=4.9376,	best estimator xgboost's best error=4.9376
[flaml.automl: 09-16 05:03:41] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 05:03:59] {3072} INFO -  at 55.3s,	estimator xgboost's best error=4.9376,	best estimator xgboost's best error=4.9376
[flaml.automl: 09-16 05:04:06] {3335} INFO - retrain xgboost for 7.2s
[flaml.automl: 09-16 05:04:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 05:04:06] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:04:06] {2637} INFO - Time taken to find the best model: 33.761184215545654
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 120161}
PM2.5(0)最佳损失：-3.9375731391311053
PM2.5(0)最好结果：{'pred_time': 6.852034253942673e-06, 'wall_clock_time': 33.761184215545654, 'metric_for_logging': {'pred_time': 6.852034253942673e-06}, 'val_loss': 4.937573139131105, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 120161}, 'config/n_estimators': 8, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609401, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466859, 'config/FLAML_sample_size': 120161, 'experiment_tag': 'exp', 'time_total_s': 6.863032341003418}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9334618882349046
PM2.5(0)的mse=58.057168923650565
PM2.5(0)的mae=4.92799414390661
PM2.5(0)的mar=0.2281909580254316
总共花费的时间为：64.43
石家庄市
1030A
1032A
1033A
1035A
2862A
3456A
3572A
3573A
3574A
[flaml.automl: 09-16 05:30:13] {2390} INFO - task = regression
[flaml.automl: 09-16 05:30:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:30:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:30:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:30:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:30:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:30:15] {3025} INFO - Estimated sufficient time budget=215606s. Estimated necessary time budget=216s.
[flaml.automl: 09-16 05:30:15] {3072} INFO -  at 2.7s,	estimator xgboost's best error=26.6488,	best estimator xgboost's best error=26.6488
[flaml.automl: 09-16 05:30:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:30:18] {3072} INFO -  at 5.3s,	estimator xgboost's best error=19.2101,	best estimator xgboost's best error=19.2101
[flaml.automl: 09-16 05:30:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:30:20] {3072} INFO -  at 7.6s,	estimator xgboost's best error=19.2101,	best estimator xgboost's best error=19.2101
[flaml.automl: 09-16 05:30:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:30:22] {3072} INFO -  at 9.8s,	estimator xgboost's best error=19.2101,	best estimator xgboost's best error=19.2101
[flaml.automl: 09-16 05:30:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:30:24] {3072} INFO -  at 11.9s,	estimator xgboost's best error=9.3966,	best estimator xgboost's best error=9.3966
[flaml.automl: 09-16 05:30:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:30:27] {3072} INFO -  at 14.3s,	estimator xgboost's best error=9.3966,	best estimator xgboost's best error=9.3966
[flaml.automl: 09-16 05:30:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:30:28] {3072} INFO -  at 16.2s,	estimator xgboost's best error=7.3626,	best estimator xgboost's best error=7.3626
[flaml.automl: 09-16 05:30:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:30:31] {3072} INFO -  at 18.4s,	estimator xgboost's best error=7.3626,	best estimator xgboost's best error=7.3626
[flaml.automl: 09-16 05:30:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:30:32] {3072} INFO -  at 20.0s,	estimator xgboost's best error=7.3626,	best estimator xgboost's best error=7.3626
[flaml.automl: 09-16 05:30:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:30:34] {3072} INFO -  at 21.8s,	estimator xgboost's best error=7.3626,	best estimator xgboost's best error=7.3626
[flaml.automl: 09-16 05:30:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:30:36] {3072} INFO -  at 23.3s,	estimator xgboost's best error=6.5950,	best estimator xgboost's best error=6.5950
[flaml.automl: 09-16 05:30:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:30:37] {3072} INFO -  at 24.4s,	estimator xgboost's best error=6.5950,	best estimator xgboost's best error=6.5950
[flaml.automl: 09-16 05:30:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:30:38] {3072} INFO -  at 26.2s,	estimator xgboost's best error=6.2972,	best estimator xgboost's best error=6.2972
[flaml.automl: 09-16 05:30:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:30:40] {3072} INFO -  at 27.6s,	estimator xgboost's best error=6.2972,	best estimator xgboost's best error=6.2972
[flaml.automl: 09-16 05:30:40] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 05:30:41] {3072} INFO -  at 29.1s,	estimator xgboost's best error=6.2972,	best estimator xgboost's best error=6.2972
[flaml.automl: 09-16 05:30:41] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 05:30:42] {3072} INFO -  at 30.2s,	estimator xgboost's best error=6.2972,	best estimator xgboost's best error=6.2972
[flaml.automl: 09-16 05:30:42] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 05:30:44] {3072} INFO -  at 31.5s,	estimator xgboost's best error=6.1723,	best estimator xgboost's best error=6.1723
[flaml.automl: 09-16 05:30:44] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 05:30:45] {3072} INFO -  at 32.9s,	estimator xgboost's best error=6.1723,	best estimator xgboost's best error=6.1723
[flaml.automl: 09-16 05:30:45] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 05:30:46] {3072} INFO -  at 33.7s,	estimator xgboost's best error=6.1723,	best estimator xgboost's best error=6.1723
[flaml.automl: 09-16 05:30:46] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 05:30:47] {3072} INFO -  at 34.9s,	estimator xgboost's best error=6.1723,	best estimator xgboost's best error=6.1723
[flaml.automl: 09-16 05:30:47] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 05:30:48] {3072} INFO -  at 35.6s,	estimator xgboost's best error=6.1723,	best estimator xgboost's best error=6.1723
[flaml.automl: 09-16 05:30:48] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 05:30:51] {3072} INFO -  at 39.1s,	estimator xgboost's best error=5.7670,	best estimator xgboost's best error=5.7670
[flaml.automl: 09-16 05:30:51] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 05:31:09] {3072} INFO -  at 56.3s,	estimator xgboost's best error=5.6094,	best estimator xgboost's best error=5.6094
[flaml.automl: 09-16 05:31:33] {3335} INFO - retrain xgboost for 24.0s
[flaml.automl: 09-16 05:31:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8829869111543351, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=7.2980390605760626, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003039849851660253, scale_pos_weight=1,
             subsample=0.9753451852946031, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 05:31:33] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:31:33] {2637} INFO - Time taken to find the best model: 56.27882981300354
[flaml.automl: 09-16 05:31:33] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 28, 'max_leaves': 9, 'min_child_weight': 7.2980390605760626, 'learning_rate': 0.6023269513573992, 'subsample': 0.9753451852946031, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8829869111543351, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003039849851660253, 'FLAML_sample_size': 93752}
PM2.5(0)最佳损失：-4.609375735740006
PM2.5(0)最好结果：{'pred_time': 8.493258489165848e-06, 'wall_clock_time': 56.27882981300354, 'metric_for_logging': {'pred_time': 8.493258489165848e-06}, 'val_loss': 5.609375735740006, 'training_iteration': 1, 'config': {'n_estimators': 28, 'max_leaves': 9, 'min_child_weight': 7.2980390605760626, 'learning_rate': 0.6023269513573992, 'subsample': 0.9753451852946031, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8829869111543351, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003039849851660253, 'FLAML_sample_size': 93752}, 'config/n_estimators': 28, 'config/max_leaves': 9, 'config/min_child_weight': 7.2980390605760626, 'config/learning_rate': 0.6023269513573992, 'config/subsample': 0.9753451852946031, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8829869111543351, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.003039849851660253, 'config/FLAML_sample_size': 93752, 'experiment_tag': 'exp', 'time_total_s': 17.14250612258911}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8829869111543351, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=7.2980390605760626, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003039849851660253, scale_pos_weight=1,
             subsample=0.9753451852946031, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9137651050294117
PM2.5(0)的mse=72.35742029644074
PM2.5(0)的mae=5.7115364790889664
PM2.5(0)的mar=0.2130961732481735
总共花费的时间为：81.86
唐山市
1041A
3575A
3576A
3577A
3578A
3692A
[flaml.automl: 09-16 05:49:04] {2390} INFO - task = regression
[flaml.automl: 09-16 05:49:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:49:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:49:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:49:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:49:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:49:06] {3025} INFO - Estimated sufficient time budget=123075s. Estimated necessary time budget=123s.
[flaml.automl: 09-16 05:49:06] {3072} INFO -  at 2.3s,	estimator xgboost's best error=23.6595,	best estimator xgboost's best error=23.6595
[flaml.automl: 09-16 05:49:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:49:10] {3072} INFO -  at 6.2s,	estimator xgboost's best error=11.9492,	best estimator xgboost's best error=11.9492
[flaml.automl: 09-16 05:49:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:49:12] {3072} INFO -  at 8.4s,	estimator xgboost's best error=11.9492,	best estimator xgboost's best error=11.9492
[flaml.automl: 09-16 05:49:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:49:16] {3072} INFO -  at 12.5s,	estimator xgboost's best error=11.9492,	best estimator xgboost's best error=11.9492
[flaml.automl: 09-16 05:49:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:49:18] {3072} INFO -  at 14.6s,	estimator xgboost's best error=8.9336,	best estimator xgboost's best error=8.9336
[flaml.automl: 09-16 05:49:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:49:21] {3072} INFO -  at 17.5s,	estimator xgboost's best error=8.9336,	best estimator xgboost's best error=8.9336
[flaml.automl: 09-16 05:49:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:49:24] {3072} INFO -  at 20.7s,	estimator xgboost's best error=7.0189,	best estimator xgboost's best error=7.0189
[flaml.automl: 09-16 05:49:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:49:27] {3072} INFO -  at 23.3s,	estimator xgboost's best error=7.0189,	best estimator xgboost's best error=7.0189
[flaml.automl: 09-16 05:49:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:49:30] {3072} INFO -  at 26.2s,	estimator xgboost's best error=7.0189,	best estimator xgboost's best error=7.0189
[flaml.automl: 09-16 05:49:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:49:33] {3072} INFO -  at 28.8s,	estimator xgboost's best error=7.0189,	best estimator xgboost's best error=7.0189
[flaml.automl: 09-16 05:49:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:49:35] {3072} INFO -  at 30.9s,	estimator xgboost's best error=7.0189,	best estimator xgboost's best error=7.0189
[flaml.automl: 09-16 05:49:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:49:38] {3072} INFO -  at 34.1s,	estimator xgboost's best error=7.0189,	best estimator xgboost's best error=7.0189
[flaml.automl: 09-16 05:49:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:49:40] {3072} INFO -  at 36.2s,	estimator xgboost's best error=7.0189,	best estimator xgboost's best error=7.0189
[flaml.automl: 09-16 05:49:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:49:53] {3072} INFO -  at 49.3s,	estimator xgboost's best error=6.3727,	best estimator xgboost's best error=6.3727
[flaml.automl: 09-16 05:50:04] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-16 05:50:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 05:50:04] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:50:04] {2637} INFO - Time taken to find the best model: 49.32592558860779
[flaml.automl: 09-16 05:50:04] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 58451}
PM2.5(0)最佳损失：-5.372677963288405
PM2.5(0)最好结果：{'pred_time': 1.2204204732588019e-05, 'wall_clock_time': 49.32592558860779, 'metric_for_logging': {'pred_time': 1.2204204732588019e-05}, 'val_loss': 6.372677963288405, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 58451}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 58451, 'experiment_tag': 'exp', 'time_total_s': 13.08029580116272}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8842805928008421
PM2.5(0)的mse=91.272995071173
PM2.5(0)的mae=6.606301576126245
PM2.5(0)的mar=0.3238908261351211
总共花费的时间为：60.98
秦皇岛市
1043A
1044A
3132A
[flaml.automl: 09-16 06:00:37] {2390} INFO - task = regression
[flaml.automl: 09-16 06:00:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:00:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:00:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:00:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:00:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:00:39] {3025} INFO - Estimated sufficient time budget=21991s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 06:00:39] {3072} INFO -  at 2.4s,	estimator xgboost's best error=19.3612,	best estimator xgboost's best error=19.3612
[flaml.automl: 09-16 06:00:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:00:43] {3072} INFO -  at 6.0s,	estimator xgboost's best error=9.6204,	best estimator xgboost's best error=9.6204
[flaml.automl: 09-16 06:00:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:00:45] {3072} INFO -  at 8.1s,	estimator xgboost's best error=9.6204,	best estimator xgboost's best error=9.6204
[flaml.automl: 09-16 06:00:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:01:03] {3072} INFO -  at 26.5s,	estimator xgboost's best error=9.6204,	best estimator xgboost's best error=9.6204
[flaml.automl: 09-16 06:01:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:01:05] {3072} INFO -  at 28.5s,	estimator xgboost's best error=7.1434,	best estimator xgboost's best error=7.1434
[flaml.automl: 09-16 06:01:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:01:08] {3072} INFO -  at 31.5s,	estimator xgboost's best error=7.1434,	best estimator xgboost's best error=7.1434
[flaml.automl: 09-16 06:01:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:01:12] {3072} INFO -  at 34.7s,	estimator xgboost's best error=5.4356,	best estimator xgboost's best error=5.4356
[flaml.automl: 09-16 06:01:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:01:17] {3072} INFO -  at 39.7s,	estimator xgboost's best error=5.4356,	best estimator xgboost's best error=5.4356
[flaml.automl: 09-16 06:01:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:01:20] {3072} INFO -  at 42.8s,	estimator xgboost's best error=5.4356,	best estimator xgboost's best error=5.4356
[flaml.automl: 09-16 06:01:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:01:25] {3072} INFO -  at 48.4s,	estimator xgboost's best error=5.4356,	best estimator xgboost's best error=5.4356
[flaml.automl: 09-16 06:01:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:01:27] {3072} INFO -  at 50.6s,	estimator xgboost's best error=5.4356,	best estimator xgboost's best error=5.4356
[flaml.automl: 09-16 06:01:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:01:29] {3072} INFO -  at 51.7s,	estimator xgboost's best error=5.4356,	best estimator xgboost's best error=5.4356
[flaml.automl: 09-16 06:01:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:01:36] {3072} INFO -  at 58.7s,	estimator xgboost's best error=4.9560,	best estimator xgboost's best error=4.9560
[flaml.automl: 09-16 06:01:43] {3335} INFO - retrain xgboost for 7.0s
[flaml.automl: 09-16 06:01:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:01:43] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:01:43] {2637} INFO - Time taken to find the best model: 58.73114371299744
[flaml.automl: 09-16 06:01:43] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
PM2.5(0)最佳损失：-3.9560228999995264
PM2.5(0)最好结果：{'pred_time': 1.1289180923174669e-05, 'wall_clock_time': 58.73114371299744, 'metric_for_logging': {'pred_time': 1.1289180923174669e-05}, 'val_loss': 4.956022899999526, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 7.025976181030273}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.88793064132218
PM2.5(0)的mse=61.88171301083593
PM2.5(0)的mae=4.967052831855704
PM2.5(0)的mar=0.2598091522836257
总共花费的时间为：66.34
邯郸市
1050A
3579A
3580A
[flaml.automl: 09-16 06:11:19] {2390} INFO - task = regression
[flaml.automl: 09-16 06:11:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:11:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:11:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:11:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:11:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:11:23] {3025} INFO - Estimated sufficient time budget=34398s. Estimated necessary time budget=34s.
[flaml.automl: 09-16 06:11:23] {3072} INFO -  at 3.6s,	estimator xgboost's best error=27.5796,	best estimator xgboost's best error=27.5796
[flaml.automl: 09-16 06:11:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:11:29] {3072} INFO -  at 9.7s,	estimator xgboost's best error=13.3200,	best estimator xgboost's best error=13.3200
[flaml.automl: 09-16 06:11:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:11:32] {3072} INFO -  at 13.1s,	estimator xgboost's best error=13.3200,	best estimator xgboost's best error=13.3200
[flaml.automl: 09-16 06:11:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:11:58] {3072} INFO -  at 39.0s,	estimator xgboost's best error=13.3200,	best estimator xgboost's best error=13.3200
[flaml.automl: 09-16 06:11:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:12:01] {3072} INFO -  at 41.7s,	estimator xgboost's best error=10.0596,	best estimator xgboost's best error=10.0596
[flaml.automl: 09-16 06:12:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:12:04] {3072} INFO -  at 45.0s,	estimator xgboost's best error=8.5457,	best estimator xgboost's best error=8.5457
[flaml.automl: 09-16 06:12:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:12:07] {3072} INFO -  at 47.8s,	estimator xgboost's best error=7.6570,	best estimator xgboost's best error=7.6570
[flaml.automl: 09-16 06:12:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:12:13] {3072} INFO -  at 54.1s,	estimator xgboost's best error=7.6570,	best estimator xgboost's best error=7.6570
[flaml.automl: 09-16 06:12:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:12:18] {3072} INFO -  at 58.8s,	estimator xgboost's best error=7.4819,	best estimator xgboost's best error=7.4819
[flaml.automl: 09-16 06:12:23] {3335} INFO - retrain xgboost for 4.5s
[flaml.automl: 09-16 06:12:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:12:23] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:12:23] {2637} INFO - Time taken to find the best model: 58.82958126068115
[flaml.automl: 09-16 06:12:23] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}
PM2.5(0)最佳损失：-6.481936071820828
PM2.5(0)最好结果：{'pred_time': 3.2142585354696834e-05, 'wall_clock_time': 58.82958126068115, 'metric_for_logging': {'pred_time': 3.2142585354696834e-05}, 'val_loss': 7.481936071820828, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 1.1765273389893034, 'config/learning_rate': 0.8338182546732649, 'config/subsample': 0.9114052575858262, 'config/colsample_bylevel': 0.7232930776213671, 'config/colsample_bytree': 0.8046603913292237, 'config/reg_alpha': 0.00473280911734172, 'config/reg_lambda': 15.090081382165623, 'experiment_tag': 'exp', 'time_total_s': 4.7316296100616455}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9025879845451324
PM2.5(0)的mse=86.62467768751954
PM2.5(0)的mae=6.857850790085554
PM2.5(0)的mar=0.2983518262609452
总共花费的时间为：64.05
保定市
1051A
1053A
1054A
1055A
1056A
3581A
3582A
3583A
[flaml.automl: 09-16 06:35:35] {2390} INFO - task = regression
[flaml.automl: 09-16 06:35:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:35:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:35:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:35:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:35:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:35:37] {3025} INFO - Estimated sufficient time budget=191281s. Estimated necessary time budget=191s.
[flaml.automl: 09-16 06:35:37] {3072} INFO -  at 2.8s,	estimator xgboost's best error=25.5064,	best estimator xgboost's best error=25.5064
[flaml.automl: 09-16 06:35:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:35:40] {3072} INFO -  at 5.8s,	estimator xgboost's best error=16.1069,	best estimator xgboost's best error=16.1069
[flaml.automl: 09-16 06:35:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:35:42] {3072} INFO -  at 8.1s,	estimator xgboost's best error=16.1069,	best estimator xgboost's best error=16.1069
[flaml.automl: 09-16 06:35:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:35:44] {3072} INFO -  at 10.3s,	estimator xgboost's best error=16.1069,	best estimator xgboost's best error=16.1069
[flaml.automl: 09-16 06:35:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:35:47] {3072} INFO -  at 12.5s,	estimator xgboost's best error=9.5307,	best estimator xgboost's best error=9.5307
[flaml.automl: 09-16 06:35:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:35:49] {3072} INFO -  at 14.7s,	estimator xgboost's best error=9.5307,	best estimator xgboost's best error=9.5307
[flaml.automl: 09-16 06:35:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:35:51] {3072} INFO -  at 17.0s,	estimator xgboost's best error=8.7763,	best estimator xgboost's best error=8.7763
[flaml.automl: 09-16 06:35:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:35:53] {3072} INFO -  at 18.8s,	estimator xgboost's best error=8.7763,	best estimator xgboost's best error=8.7763
[flaml.automl: 09-16 06:35:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:35:55] {3072} INFO -  at 21.0s,	estimator xgboost's best error=8.7763,	best estimator xgboost's best error=8.7763
[flaml.automl: 09-16 06:35:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:35:57] {3072} INFO -  at 22.8s,	estimator xgboost's best error=8.7763,	best estimator xgboost's best error=8.7763
[flaml.automl: 09-16 06:35:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:35:59] {3072} INFO -  at 24.6s,	estimator xgboost's best error=8.7763,	best estimator xgboost's best error=8.7763
[flaml.automl: 09-16 06:35:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:36:02] {3072} INFO -  at 27.8s,	estimator xgboost's best error=6.3768,	best estimator xgboost's best error=6.3768
[flaml.automl: 09-16 06:36:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:36:04] {3072} INFO -  at 30.1s,	estimator xgboost's best error=6.3768,	best estimator xgboost's best error=6.3768
[flaml.automl: 09-16 06:36:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 06:36:13] {3072} INFO -  at 38.5s,	estimator xgboost's best error=5.5776,	best estimator xgboost's best error=5.5776
[flaml.automl: 09-16 06:36:13] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 06:36:25] {3072} INFO -  at 51.4s,	estimator xgboost's best error=5.3885,	best estimator xgboost's best error=5.3885
[flaml.automl: 09-16 06:36:38] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-16 06:36:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:36:38] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:36:38] {2637} INFO - Time taken to find the best model: 51.37626552581787
[flaml.automl: 09-16 06:36:38] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 84179}
PM2.5(0)最佳损失：-4.388495931175241
PM2.5(0)最好结果：{'pred_time': 4.281619432672415e-06, 'wall_clock_time': 51.37626552581787, 'metric_for_logging': {'pred_time': 4.281619432672415e-06}, 'val_loss': 5.388495931175241, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 84179}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 84179, 'experiment_tag': 'exp', 'time_total_s': 12.82485318183899}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.921494310656577
PM2.5(0)的mse=62.11363048756012
PM2.5(0)的mae=5.323701018538338
PM2.5(0)的mar=0.2263606399779202
总共花费的时间为：65.55
张家口市
1060A
1061A
3131A
3323A
[flaml.automl: 09-16 06:48:52] {2390} INFO - task = regression
[flaml.automl: 09-16 06:48:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:48:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:48:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:48:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:48:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:48:55] {3025} INFO - Estimated sufficient time budget=138524s. Estimated necessary time budget=139s.
[flaml.automl: 09-16 06:48:55] {3072} INFO -  at 3.5s,	estimator xgboost's best error=14.3944,	best estimator xgboost's best error=14.3944
[flaml.automl: 09-16 06:48:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:49:01] {3072} INFO -  at 9.4s,	estimator xgboost's best error=7.6577,	best estimator xgboost's best error=7.6577
[flaml.automl: 09-16 06:49:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:49:05] {3072} INFO -  at 12.9s,	estimator xgboost's best error=7.6577,	best estimator xgboost's best error=7.6577
[flaml.automl: 09-16 06:49:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:49:09] {3072} INFO -  at 17.8s,	estimator xgboost's best error=7.6577,	best estimator xgboost's best error=7.6577
[flaml.automl: 09-16 06:49:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:49:13] {3072} INFO -  at 21.0s,	estimator xgboost's best error=6.0677,	best estimator xgboost's best error=6.0677
[flaml.automl: 09-16 06:49:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:49:16] {3072} INFO -  at 24.5s,	estimator xgboost's best error=6.0677,	best estimator xgboost's best error=6.0677
[flaml.automl: 09-16 06:49:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:49:20] {3072} INFO -  at 27.9s,	estimator xgboost's best error=6.0061,	best estimator xgboost's best error=6.0061
[flaml.automl: 09-16 06:49:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:49:22] {3072} INFO -  at 30.8s,	estimator xgboost's best error=6.0061,	best estimator xgboost's best error=6.0061
[flaml.automl: 09-16 06:49:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:49:25] {3072} INFO -  at 33.5s,	estimator xgboost's best error=6.0061,	best estimator xgboost's best error=6.0061
[flaml.automl: 09-16 06:49:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:49:28] {3072} INFO -  at 36.1s,	estimator xgboost's best error=6.0061,	best estimator xgboost's best error=6.0061
[flaml.automl: 09-16 06:49:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:49:30] {3072} INFO -  at 38.6s,	estimator xgboost's best error=5.2067,	best estimator xgboost's best error=5.2067
[flaml.automl: 09-16 06:49:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:49:32] {3072} INFO -  at 40.4s,	estimator xgboost's best error=5.2067,	best estimator xgboost's best error=5.2067
[flaml.automl: 09-16 06:49:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:49:34] {3072} INFO -  at 42.4s,	estimator xgboost's best error=5.1537,	best estimator xgboost's best error=5.1537
[flaml.automl: 09-16 06:49:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 06:49:36] {3072} INFO -  at 44.4s,	estimator xgboost's best error=5.1537,	best estimator xgboost's best error=5.1537
[flaml.automl: 09-16 06:49:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 06:49:38] {3072} INFO -  at 45.9s,	estimator xgboost's best error=5.1537,	best estimator xgboost's best error=5.1537
[flaml.automl: 09-16 06:49:38] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 06:49:39] {3072} INFO -  at 47.7s,	estimator xgboost's best error=5.1537,	best estimator xgboost's best error=5.1537
[flaml.automl: 09-16 06:49:39] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 06:49:40] {3072} INFO -  at 48.9s,	estimator xgboost's best error=5.1537,	best estimator xgboost's best error=5.1537
[flaml.automl: 09-16 06:49:40] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 06:49:47] {3072} INFO -  at 55.6s,	estimator xgboost's best error=4.8215,	best estimator xgboost's best error=4.8215
[flaml.automl: 09-16 06:49:58] {3335} INFO - retrain xgboost for 10.8s
[flaml.automl: 09-16 06:49:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:49:58] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:49:58] {2637} INFO - Time taken to find the best model: 55.558573961257935
[flaml.automl: 09-16 06:49:58] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 41315}
PM2.5(0)最佳损失：-3.821512374398081
PM2.5(0)最好结果：{'pred_time': 1.9387050122639318e-05, 'wall_clock_time': 55.558573961257935, 'metric_for_logging': {'pred_time': 1.9387050122639318e-05}, 'val_loss': 4.821512374398081, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 41315}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'config/FLAML_sample_size': 41315, 'experiment_tag': 'exp', 'time_total_s': 6.6607346534729}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8039165673225475
PM2.5(0)的mse=54.89443540229607
PM2.5(0)的mae=4.881360560065507
PM2.5(0)的mar=0.40566796020213747
总共花费的时间为：67.19
承德市
1063A
1064A
1065A
[flaml.automl: 09-16 06:59:21] {2390} INFO - task = regression
[flaml.automl: 09-16 06:59:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:59:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:59:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:59:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:59:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:59:23] {3025} INFO - Estimated sufficient time budget=22291s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 06:59:23] {3072} INFO -  at 2.4s,	estimator xgboost's best error=16.6991,	best estimator xgboost's best error=16.6991
[flaml.automl: 09-16 06:59:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:59:27] {3072} INFO -  at 6.2s,	estimator xgboost's best error=8.4396,	best estimator xgboost's best error=8.4396
[flaml.automl: 09-16 06:59:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:59:29] {3072} INFO -  at 8.3s,	estimator xgboost's best error=8.4396,	best estimator xgboost's best error=8.4396
[flaml.automl: 09-16 06:59:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:59:48] {3072} INFO -  at 27.1s,	estimator xgboost's best error=8.4396,	best estimator xgboost's best error=8.4396
[flaml.automl: 09-16 06:59:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:59:50] {3072} INFO -  at 29.2s,	estimator xgboost's best error=6.0693,	best estimator xgboost's best error=6.0693
[flaml.automl: 09-16 06:59:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:59:53] {3072} INFO -  at 32.0s,	estimator xgboost's best error=5.7748,	best estimator xgboost's best error=5.7748
[flaml.automl: 09-16 06:59:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:59:57] {3072} INFO -  at 35.8s,	estimator xgboost's best error=5.2469,	best estimator xgboost's best error=5.2469
[flaml.automl: 09-16 06:59:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:00:04] {3072} INFO -  at 43.2s,	estimator xgboost's best error=5.2469,	best estimator xgboost's best error=5.2469
[flaml.automl: 09-16 07:00:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:00:08] {3072} INFO -  at 47.6s,	estimator xgboost's best error=5.2469,	best estimator xgboost's best error=5.2469
[flaml.automl: 09-16 07:00:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:00:17] {3072} INFO -  at 55.8s,	estimator xgboost's best error=5.0241,	best estimator xgboost's best error=5.0241
[flaml.automl: 09-16 07:00:25] {3335} INFO - retrain xgboost for 8.0s
[flaml.automl: 09-16 07:00:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 07:00:25] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:00:25] {2637} INFO - Time taken to find the best model: 55.83789777755737
[flaml.automl: 09-16 07:00:25] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-4.024080573122433
PM2.5(0)最好结果：{'pred_time': 3.4041939306942456e-05, 'wall_clock_time': 55.83789777755737, 'metric_for_logging': {'pred_time': 3.4041939306942456e-05}, 'val_loss': 5.024080573122433, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 8.232051849365234}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7844022813782581
PM2.5(0)的mse=53.104858826882214
PM2.5(0)的mae=5.157271781833593
PM2.5(0)的mar=0.4238784431050036
总共花费的时间为：64.68
廊坊市
1070A
2919A
[flaml.automl: 09-16 07:07:54] {2390} INFO - task = regression
[flaml.automl: 09-16 07:07:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:07:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:07:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:07:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:07:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:07:56] {3025} INFO - Estimated sufficient time budget=12144s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 07:07:56] {3072} INFO -  at 1.3s,	estimator xgboost's best error=21.1705,	best estimator xgboost's best error=21.1705
[flaml.automl: 09-16 07:07:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:07:58] {3072} INFO -  at 3.4s,	estimator xgboost's best error=10.6214,	best estimator xgboost's best error=10.6214
[flaml.automl: 09-16 07:07:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:07:59] {3072} INFO -  at 4.6s,	estimator xgboost's best error=10.6214,	best estimator xgboost's best error=10.6214
[flaml.automl: 09-16 07:07:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:08:17] {3072} INFO -  at 22.5s,	estimator xgboost's best error=10.6214,	best estimator xgboost's best error=10.6214
[flaml.automl: 09-16 07:08:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:08:20] {3072} INFO -  at 25.6s,	estimator xgboost's best error=8.4636,	best estimator xgboost's best error=8.4636
[flaml.automl: 09-16 07:08:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:08:24] {3072} INFO -  at 29.9s,	estimator xgboost's best error=7.7516,	best estimator xgboost's best error=7.7516
[flaml.automl: 09-16 07:08:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:08:29] {3072} INFO -  at 34.3s,	estimator xgboost's best error=6.5435,	best estimator xgboost's best error=6.5435
[flaml.automl: 09-16 07:08:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:08:35] {3072} INFO -  at 40.7s,	estimator xgboost's best error=6.5435,	best estimator xgboost's best error=6.5435
[flaml.automl: 09-16 07:08:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:08:40] {3072} INFO -  at 45.1s,	estimator xgboost's best error=6.5435,	best estimator xgboost's best error=6.5435
[flaml.automl: 09-16 07:08:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:08:48] {3072} INFO -  at 53.3s,	estimator xgboost's best error=6.0923,	best estimator xgboost's best error=6.0923
[flaml.automl: 09-16 07:08:56] {3335} INFO - retrain xgboost for 8.0s
[flaml.automl: 09-16 07:08:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 07:08:56] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:08:56] {2637} INFO - Time taken to find the best model: 53.264564514160156
[flaml.automl: 09-16 07:08:56] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-5.092286603977258
PM2.5(0)最好结果：{'pred_time': 5.265487331387178e-05, 'wall_clock_time': 53.264564514160156, 'metric_for_logging': {'pred_time': 5.265487331387178e-05}, 'val_loss': 6.092286603977258, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 8.143539428710938}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.867379096798084
PM2.5(0)的mse=73.86366968879146
PM2.5(0)的mae=6.311345253762024
PM2.5(0)的mar=0.4001317486090928
总共花费的时间为：61.72
沧州市
1071A
1073A
3324A
[flaml.automl: 09-16 07:18:03] {2390} INFO - task = regression
[flaml.automl: 09-16 07:18:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:18:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:18:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:18:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:18:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:18:04] {3025} INFO - Estimated sufficient time budget=12347s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 07:18:04] {3072} INFO -  at 1.4s,	estimator xgboost's best error=23.7395,	best estimator xgboost's best error=23.7395
[flaml.automl: 09-16 07:18:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:18:06] {3072} INFO -  at 3.5s,	estimator xgboost's best error=11.6424,	best estimator xgboost's best error=11.6424
[flaml.automl: 09-16 07:18:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:18:07] {3072} INFO -  at 4.7s,	estimator xgboost's best error=11.6424,	best estimator xgboost's best error=11.6424
[flaml.automl: 09-16 07:18:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:18:17] {3072} INFO -  at 14.6s,	estimator xgboost's best error=11.6424,	best estimator xgboost's best error=11.6424
[flaml.automl: 09-16 07:18:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:18:18] {3072} INFO -  at 15.8s,	estimator xgboost's best error=8.1133,	best estimator xgboost's best error=8.1133
[flaml.automl: 09-16 07:18:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:18:20] {3072} INFO -  at 17.4s,	estimator xgboost's best error=7.5561,	best estimator xgboost's best error=7.5561
[flaml.automl: 09-16 07:18:20] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:18:22] {3072} INFO -  at 19.0s,	estimator xgboost's best error=6.5941,	best estimator xgboost's best error=6.5941
[flaml.automl: 09-16 07:18:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:18:24] {3072} INFO -  at 21.7s,	estimator xgboost's best error=6.5941,	best estimator xgboost's best error=6.5941
[flaml.automl: 09-16 07:18:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:18:26] {3072} INFO -  at 23.4s,	estimator xgboost's best error=6.5941,	best estimator xgboost's best error=6.5941
[flaml.automl: 09-16 07:18:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:18:29] {3072} INFO -  at 26.4s,	estimator xgboost's best error=6.1806,	best estimator xgboost's best error=6.1806
[flaml.automl: 09-16 07:18:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:18:31] {3072} INFO -  at 28.0s,	estimator xgboost's best error=6.1806,	best estimator xgboost's best error=6.1806
[flaml.automl: 09-16 07:18:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:18:32] {3072} INFO -  at 29.2s,	estimator xgboost's best error=6.1806,	best estimator xgboost's best error=6.1806
[flaml.automl: 09-16 07:18:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:18:45] {3072} INFO -  at 42.9s,	estimator xgboost's best error=5.5737,	best estimator xgboost's best error=5.5737
[flaml.automl: 09-16 07:18:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:19:03] {3072} INFO -  at 60.2s,	estimator xgboost's best error=5.3171,	best estimator xgboost's best error=5.3171
[flaml.automl: 09-16 07:20:02] {3335} INFO - retrain xgboost for 59.5s
[flaml.automl: 09-16 07:20:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=34, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:20:02] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:20:02] {2637} INFO - Time taken to find the best model: 60.20794224739075
[flaml.automl: 09-16 07:20:02] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-4.317079533599244
PM2.5(0)最好结果：{'pred_time': 2.4968524936082292e-05, 'wall_clock_time': 60.20794224739075, 'metric_for_logging': {'pred_time': 2.4968524936082292e-05}, 'val_loss': 5.317079533599244, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 17.348176956176758}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=34, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9089659195909614
PM2.5(0)的mse=53.411218698233874
PM2.5(0)的mae=5.225936543841855
PM2.5(0)的mar=0.25056032171867837
总共花费的时间为：120.27
衡水市
1074A
1075A
3188A
3459A
[flaml.automl: 09-16 07:32:52] {2390} INFO - task = regression
[flaml.automl: 09-16 07:32:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:32:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:32:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:32:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:32:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:32:53] {3025} INFO - Estimated sufficient time budget=77505s. Estimated necessary time budget=78s.
[flaml.automl: 09-16 07:32:53] {3072} INFO -  at 2.1s,	estimator xgboost's best error=24.6576,	best estimator xgboost's best error=24.6576
[flaml.automl: 09-16 07:32:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:32:57] {3072} INFO -  at 5.5s,	estimator xgboost's best error=11.9547,	best estimator xgboost's best error=11.9547
[flaml.automl: 09-16 07:32:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:32:59] {3072} INFO -  at 7.6s,	estimator xgboost's best error=11.9547,	best estimator xgboost's best error=11.9547
[flaml.automl: 09-16 07:32:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:33:05] {3072} INFO -  at 13.4s,	estimator xgboost's best error=11.9547,	best estimator xgboost's best error=11.9547
[flaml.automl: 09-16 07:33:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:33:06] {3072} INFO -  at 15.2s,	estimator xgboost's best error=8.0660,	best estimator xgboost's best error=8.0660
[flaml.automl: 09-16 07:33:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:33:09] {3072} INFO -  at 17.5s,	estimator xgboost's best error=8.0660,	best estimator xgboost's best error=8.0660
[flaml.automl: 09-16 07:33:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:33:12] {3072} INFO -  at 20.5s,	estimator xgboost's best error=6.1758,	best estimator xgboost's best error=6.1758
[flaml.automl: 09-16 07:33:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:33:16] {3072} INFO -  at 24.2s,	estimator xgboost's best error=6.1758,	best estimator xgboost's best error=6.1758
[flaml.automl: 09-16 07:33:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:33:18] {3072} INFO -  at 26.9s,	estimator xgboost's best error=6.1758,	best estimator xgboost's best error=6.1758
[flaml.automl: 09-16 07:33:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:33:22] {3072} INFO -  at 30.5s,	estimator xgboost's best error=6.1758,	best estimator xgboost's best error=6.1758
[flaml.automl: 09-16 07:33:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:33:24] {3072} INFO -  at 33.0s,	estimator xgboost's best error=6.1758,	best estimator xgboost's best error=6.1758
[flaml.automl: 09-16 07:33:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:33:27] {3072} INFO -  at 35.8s,	estimator xgboost's best error=6.1758,	best estimator xgboost's best error=6.1758
[flaml.automl: 09-16 07:33:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:33:29] {3072} INFO -  at 37.8s,	estimator xgboost's best error=6.1758,	best estimator xgboost's best error=6.1758
[flaml.automl: 09-16 07:33:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:33:41] {3072} INFO -  at 49.4s,	estimator xgboost's best error=5.3371,	best estimator xgboost's best error=5.3371
[flaml.automl: 09-16 07:34:00] {3335} INFO - retrain xgboost for 19.3s
[flaml.automl: 09-16 07:34:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:34:00] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:34:00] {2637} INFO - Time taken to find the best model: 49.398903131484985
[flaml.automl: 09-16 07:34:00] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 42597}
PM2.5(0)最佳损失：-4.337113880832889
PM2.5(0)最好结果：{'pred_time': 1.0829360665376548e-05, 'wall_clock_time': 49.398903131484985, 'metric_for_logging': {'pred_time': 1.0829360665376548e-05}, 'val_loss': 5.337113880832889, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 42597}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 42597, 'experiment_tag': 'exp', 'time_total_s': 11.57077145576477}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9246958838683796
PM2.5(0)的mse=57.497760065851686
PM2.5(0)的mae=5.278020811216913
PM2.5(0)的mar=0.22211180483548407
总共花费的时间为：69.41
邢台市
1078A
1079A
1080A
[flaml.automl: 09-16 07:44:00] {2390} INFO - task = regression
[flaml.automl: 09-16 07:44:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:44:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:44:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:44:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:44:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:44:01] {3025} INFO - Estimated sufficient time budget=12287s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 07:44:01] {3072} INFO -  at 1.3s,	estimator xgboost's best error=27.5883,	best estimator xgboost's best error=27.5883
[flaml.automl: 09-16 07:44:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:44:03] {3072} INFO -  at 3.5s,	estimator xgboost's best error=13.0907,	best estimator xgboost's best error=13.0907
[flaml.automl: 09-16 07:44:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:44:04] {3072} INFO -  at 4.7s,	estimator xgboost's best error=13.0907,	best estimator xgboost's best error=13.0907
[flaml.automl: 09-16 07:44:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:44:14] {3072} INFO -  at 14.7s,	estimator xgboost's best error=13.0907,	best estimator xgboost's best error=13.0907
[flaml.automl: 09-16 07:44:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:44:15] {3072} INFO -  at 15.8s,	estimator xgboost's best error=9.9779,	best estimator xgboost's best error=9.9779
[flaml.automl: 09-16 07:44:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:44:17] {3072} INFO -  at 17.4s,	estimator xgboost's best error=9.9779,	best estimator xgboost's best error=9.9779
[flaml.automl: 09-16 07:44:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:44:19] {3072} INFO -  at 19.1s,	estimator xgboost's best error=6.3190,	best estimator xgboost's best error=6.3190
[flaml.automl: 09-16 07:44:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:44:21] {3072} INFO -  at 21.8s,	estimator xgboost's best error=6.3190,	best estimator xgboost's best error=6.3190
[flaml.automl: 09-16 07:44:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:44:23] {3072} INFO -  at 23.5s,	estimator xgboost's best error=6.3190,	best estimator xgboost's best error=6.3190
[flaml.automl: 09-16 07:44:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:44:26] {3072} INFO -  at 26.5s,	estimator xgboost's best error=6.3190,	best estimator xgboost's best error=6.3190
[flaml.automl: 09-16 07:44:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:44:28] {3072} INFO -  at 28.5s,	estimator xgboost's best error=6.3190,	best estimator xgboost's best error=6.3190
[flaml.automl: 09-16 07:44:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:44:30] {3072} INFO -  at 30.6s,	estimator xgboost's best error=6.3190,	best estimator xgboost's best error=6.3190
[flaml.automl: 09-16 07:44:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:44:49] {3072} INFO -  at 49.9s,	estimator xgboost's best error=5.3186,	best estimator xgboost's best error=5.3186
[flaml.automl: 09-16 07:45:12] {3335} INFO - retrain xgboost for 22.4s
[flaml.automl: 09-16 07:45:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:45:12] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:45:12] {2637} INFO - Time taken to find the best model: 49.91963791847229
[flaml.automl: 09-16 07:45:12] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
PM2.5(0)最佳损失：-4.318610807711428
PM2.5(0)最好结果：{'pred_time': 3.235605344817969e-05, 'wall_clock_time': 49.91963791847229, 'metric_for_logging': {'pred_time': 3.235605344817969e-05}, 'val_loss': 5.318610807711428, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 19.28911066055298}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9389151291613289
PM2.5(0)的mse=59.98395117562359
PM2.5(0)的mae=5.300574841569923
PM2.5(0)的mar=0.2000718574433934
总共花费的时间为：73.02
太原市
1081A
1084A
1085A
1086A
1087A
3185A
[flaml.automl: 09-16 08:03:20] {2390} INFO - task = regression
[flaml.automl: 09-16 08:03:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:03:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:03:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:03:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:03:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:03:22] {3025} INFO - Estimated sufficient time budget=77778s. Estimated necessary time budget=78s.
[flaml.automl: 09-16 08:03:22] {3072} INFO -  at 1.5s,	estimator xgboost's best error=28.6965,	best estimator xgboost's best error=28.6965
[flaml.automl: 09-16 08:03:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:03:24] {3072} INFO -  at 3.6s,	estimator xgboost's best error=13.8776,	best estimator xgboost's best error=13.8776
[flaml.automl: 09-16 08:03:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:03:25] {3072} INFO -  at 4.7s,	estimator xgboost's best error=13.8776,	best estimator xgboost's best error=13.8776
[flaml.automl: 09-16 08:03:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:03:29] {3072} INFO -  at 8.5s,	estimator xgboost's best error=13.8776,	best estimator xgboost's best error=13.8776
[flaml.automl: 09-16 08:03:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:03:30] {3072} INFO -  at 9.6s,	estimator xgboost's best error=9.5274,	best estimator xgboost's best error=9.5274
[flaml.automl: 09-16 08:03:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:03:31] {3072} INFO -  at 11.2s,	estimator xgboost's best error=9.5274,	best estimator xgboost's best error=9.5274
[flaml.automl: 09-16 08:03:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:03:33] {3072} INFO -  at 12.9s,	estimator xgboost's best error=7.0416,	best estimator xgboost's best error=7.0416
[flaml.automl: 09-16 08:03:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:03:36] {3072} INFO -  at 15.5s,	estimator xgboost's best error=7.0416,	best estimator xgboost's best error=7.0416
[flaml.automl: 09-16 08:03:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:03:37] {3072} INFO -  at 17.2s,	estimator xgboost's best error=7.0416,	best estimator xgboost's best error=7.0416
[flaml.automl: 09-16 08:03:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:03:40] {3072} INFO -  at 20.2s,	estimator xgboost's best error=7.0416,	best estimator xgboost's best error=7.0416
[flaml.automl: 09-16 08:03:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:03:42] {3072} INFO -  at 21.6s,	estimator xgboost's best error=7.0416,	best estimator xgboost's best error=7.0416
[flaml.automl: 09-16 08:03:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:03:44] {3072} INFO -  at 23.3s,	estimator xgboost's best error=7.0411,	best estimator xgboost's best error=7.0411
[flaml.automl: 09-16 08:03:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:03:45] {3072} INFO -  at 24.5s,	estimator xgboost's best error=7.0411,	best estimator xgboost's best error=7.0411
[flaml.automl: 09-16 08:03:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:03:52] {3072} INFO -  at 31.6s,	estimator xgboost's best error=5.9437,	best estimator xgboost's best error=5.9437
[flaml.automl: 09-16 08:03:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:04:05] {3072} INFO -  at 44.4s,	estimator xgboost's best error=5.8244,	best estimator xgboost's best error=5.8244
[flaml.automl: 09-16 08:04:05] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 08:04:17] {3072} INFO -  at 56.9s,	estimator xgboost's best error=5.8244,	best estimator xgboost's best error=5.8244
[flaml.automl: 09-16 08:04:41] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-16 08:04:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:04:41] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:04:41] {2637} INFO - Time taken to find the best model: 44.38247108459473
[flaml.automl: 09-16 08:04:41] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 65658}
PM2.5(0)最佳损失：-4.824384338859665
PM2.5(0)最好结果：{'pred_time': 5.667669731273986e-06, 'wall_clock_time': 44.38247108459473, 'metric_for_logging': {'pred_time': 5.667669731273986e-06}, 'val_loss': 5.824384338859665, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 65658}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 65658, 'experiment_tag': 'exp', 'time_total_s': 12.811124801635742}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8968911202786525
PM2.5(0)的mse=94.70191274446927
PM2.5(0)的mae=6.087736741136619
PM2.5(0)的mar=0.18196352197535365
总共花费的时间为：81.94
呼和浩特市
1095A
1097A
3698A
3699A
[flaml.automl: 09-16 08:17:28] {2390} INFO - task = regression
[flaml.automl: 09-16 08:17:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:17:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:17:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:17:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:17:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:17:29] {3025} INFO - Estimated sufficient time budget=54155s. Estimated necessary time budget=54s.
[flaml.automl: 09-16 08:17:29] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.0446,	best estimator xgboost's best error=17.0446
[flaml.automl: 09-16 08:17:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:17:31] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.7876,	best estimator xgboost's best error=8.7876
[flaml.automl: 09-16 08:17:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:17:32] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.7876,	best estimator xgboost's best error=8.7876
[flaml.automl: 09-16 08:17:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:17:38] {3072} INFO -  at 10.5s,	estimator xgboost's best error=8.7876,	best estimator xgboost's best error=8.7876
[flaml.automl: 09-16 08:17:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:17:39] {3072} INFO -  at 11.7s,	estimator xgboost's best error=7.4563,	best estimator xgboost's best error=7.4563
[flaml.automl: 09-16 08:17:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:17:41] {3072} INFO -  at 13.3s,	estimator xgboost's best error=6.5081,	best estimator xgboost's best error=6.5081
[flaml.automl: 09-16 08:17:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:17:42] {3072} INFO -  at 14.9s,	estimator xgboost's best error=5.9762,	best estimator xgboost's best error=5.9762
[flaml.automl: 09-16 08:17:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:17:45] {3072} INFO -  at 17.6s,	estimator xgboost's best error=5.9762,	best estimator xgboost's best error=5.9762
[flaml.automl: 09-16 08:17:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:17:47] {3072} INFO -  at 19.2s,	estimator xgboost's best error=5.9762,	best estimator xgboost's best error=5.9762
[flaml.automl: 09-16 08:17:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:17:50] {3072} INFO -  at 22.2s,	estimator xgboost's best error=5.3133,	best estimator xgboost's best error=5.3133
[flaml.automl: 09-16 08:17:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:17:51] {3072} INFO -  at 23.9s,	estimator xgboost's best error=5.3133,	best estimator xgboost's best error=5.3133
[flaml.automl: 09-16 08:17:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:17:52] {3072} INFO -  at 25.0s,	estimator xgboost's best error=5.3133,	best estimator xgboost's best error=5.3133
[flaml.automl: 09-16 08:17:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:17:56] {3072} INFO -  at 28.8s,	estimator xgboost's best error=5.1436,	best estimator xgboost's best error=5.1436
[flaml.automl: 09-16 08:17:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:17:59] {3072} INFO -  at 31.6s,	estimator xgboost's best error=5.1436,	best estimator xgboost's best error=5.1436
[flaml.automl: 09-16 08:17:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:18:02] {3072} INFO -  at 34.2s,	estimator xgboost's best error=5.1436,	best estimator xgboost's best error=5.1436
[flaml.automl: 09-16 08:18:02] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 08:18:03] {3072} INFO -  at 36.0s,	estimator xgboost's best error=5.1436,	best estimator xgboost's best error=5.1436
[flaml.automl: 09-16 08:18:03] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 08:18:06] {3072} INFO -  at 38.2s,	estimator xgboost's best error=5.0837,	best estimator xgboost's best error=5.0837
[flaml.automl: 09-16 08:18:06] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 08:18:08] {3072} INFO -  at 40.6s,	estimator xgboost's best error=5.0837,	best estimator xgboost's best error=5.0837
[flaml.automl: 09-16 08:18:08] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 08:18:09] {3072} INFO -  at 41.9s,	estimator xgboost's best error=5.0837,	best estimator xgboost's best error=5.0837
[flaml.automl: 09-16 08:18:09] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 08:18:11] {3072} INFO -  at 43.8s,	estimator xgboost's best error=5.0837,	best estimator xgboost's best error=5.0837
[flaml.automl: 09-16 08:18:11] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 08:18:12] {3072} INFO -  at 45.0s,	estimator xgboost's best error=5.0837,	best estimator xgboost's best error=5.0837
[flaml.automl: 09-16 08:18:12] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 08:18:22] {3072} INFO -  at 54.9s,	estimator xgboost's best error=4.9309,	best estimator xgboost's best error=4.9309
[flaml.automl: 09-16 08:18:37] {3335} INFO - retrain xgboost for 14.6s
[flaml.automl: 09-16 08:18:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9508280259547836, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8344084316033451,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=0.0066459383199444525, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0031424921315017056, reg_lambda=0.10503395230912585,
             scale_pos_weight=1, subsample=0.8568446847279476,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 08:18:37] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:18:37] {2637} INFO - Time taken to find the best model: 54.9215784072876
[flaml.automl: 09-16 08:18:37] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 9, 'min_child_weight': 0.0066459383199444525, 'learning_rate': 0.8344084316033451, 'subsample': 0.8568446847279476, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9508280259547836, 'reg_alpha': 0.0031424921315017056, 'reg_lambda': 0.10503395230912585, 'FLAML_sample_size': 43491}
PM2.5(0)最佳损失：-3.9308848393190337
PM2.5(0)最好结果：{'pred_time': 1.6562518880107302e-05, 'wall_clock_time': 54.9215784072876, 'metric_for_logging': {'pred_time': 1.6562518880107302e-05}, 'val_loss': 4.930884839319034, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 9, 'min_child_weight': 0.0066459383199444525, 'learning_rate': 0.8344084316033451, 'subsample': 0.8568446847279476, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9508280259547836, 'reg_alpha': 0.0031424921315017056, 'reg_lambda': 0.10503395230912585, 'FLAML_sample_size': 43491}, 'config/n_estimators': 15, 'config/max_leaves': 9, 'config/min_child_weight': 0.0066459383199444525, 'config/learning_rate': 0.8344084316033451, 'config/subsample': 0.8568446847279476, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9508280259547836, 'config/reg_alpha': 0.0031424921315017056, 'config/reg_lambda': 0.10503395230912585, 'config/FLAML_sample_size': 43491, 'experiment_tag': 'exp', 'time_total_s': 9.895739555358887}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9508280259547836, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8344084316033451,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=0.0066459383199444525, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0031424921315017056, reg_lambda=0.10503395230912585,
             scale_pos_weight=1, subsample=0.8568446847279476,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8738705085254366
PM2.5(0)的mse=65.78664192583793
PM2.5(0)的mae=4.944365320192369
PM2.5(0)的mar=0.34096371882328913
总共花费的时间为：70.17
沈阳市
1098A
1099A
1100A
1104A
1105A
1106A
2900A
[flaml.automl: 09-16 08:39:57] {2390} INFO - task = regression
[flaml.automl: 09-16 08:39:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:39:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:39:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:39:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:39:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:39:58] {3025} INFO - Estimated sufficient time budget=88515s. Estimated necessary time budget=89s.
[flaml.automl: 09-16 08:39:58] {3072} INFO -  at 1.5s,	estimator xgboost's best error=20.5879,	best estimator xgboost's best error=20.5879
[flaml.automl: 09-16 08:39:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:40:00] {3072} INFO -  at 3.6s,	estimator xgboost's best error=10.0897,	best estimator xgboost's best error=10.0897
[flaml.automl: 09-16 08:40:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:40:02] {3072} INFO -  at 4.8s,	estimator xgboost's best error=10.0897,	best estimator xgboost's best error=10.0897
[flaml.automl: 09-16 08:40:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:40:05] {3072} INFO -  at 8.0s,	estimator xgboost's best error=10.0897,	best estimator xgboost's best error=10.0897
[flaml.automl: 09-16 08:40:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:40:06] {3072} INFO -  at 9.1s,	estimator xgboost's best error=7.4453,	best estimator xgboost's best error=7.4453
[flaml.automl: 09-16 08:40:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:40:07] {3072} INFO -  at 10.7s,	estimator xgboost's best error=7.4453,	best estimator xgboost's best error=7.4453
[flaml.automl: 09-16 08:40:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:40:09] {3072} INFO -  at 12.4s,	estimator xgboost's best error=5.6764,	best estimator xgboost's best error=5.6764
[flaml.automl: 09-16 08:40:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:40:12] {3072} INFO -  at 15.1s,	estimator xgboost's best error=5.6764,	best estimator xgboost's best error=5.6764
[flaml.automl: 09-16 08:40:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:40:13] {3072} INFO -  at 16.7s,	estimator xgboost's best error=5.6764,	best estimator xgboost's best error=5.6764
[flaml.automl: 09-16 08:40:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:40:16] {3072} INFO -  at 19.3s,	estimator xgboost's best error=5.6764,	best estimator xgboost's best error=5.6764
[flaml.automl: 09-16 08:40:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:40:18] {3072} INFO -  at 20.8s,	estimator xgboost's best error=5.6764,	best estimator xgboost's best error=5.6764
[flaml.automl: 09-16 08:40:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:40:19] {3072} INFO -  at 22.5s,	estimator xgboost's best error=5.6759,	best estimator xgboost's best error=5.6759
[flaml.automl: 09-16 08:40:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:40:20] {3072} INFO -  at 23.7s,	estimator xgboost's best error=5.6759,	best estimator xgboost's best error=5.6759
[flaml.automl: 09-16 08:40:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:40:28] {3072} INFO -  at 30.8s,	estimator xgboost's best error=5.0678,	best estimator xgboost's best error=5.0678
[flaml.automl: 09-16 08:40:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:40:41] {3072} INFO -  at 43.7s,	estimator xgboost's best error=4.9202,	best estimator xgboost's best error=4.9202
[flaml.automl: 09-16 08:40:41] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 08:40:48] {3072} INFO -  at 50.8s,	estimator xgboost's best error=4.9202,	best estimator xgboost's best error=4.9202
[flaml.automl: 09-16 08:41:00] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-16 08:41:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:41:00] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:41:00] {2637} INFO - Time taken to find the best model: 43.74926543235779
[flaml.automl: 09-16 08:41:00] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 75517}
PM2.5(0)最佳损失：-3.9202362426826687
PM2.5(0)最好结果：{'pred_time': 4.916520130647094e-06, 'wall_clock_time': 43.74926543235779, 'metric_for_logging': {'pred_time': 4.916520130647094e-06}, 'val_loss': 4.920236242682669, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 75517}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 75517, 'experiment_tag': 'exp', 'time_total_s': 12.927199602127075}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8938293320593191
PM2.5(0)的mse=62.96356837928703
PM2.5(0)的mae=4.9372576779431405
PM2.5(0)的mar=0.19461337334665307
总共花费的时间为：64.84
大连市
1110A
1117A
[flaml.automl: 09-16 08:47:23] {2390} INFO - task = regression
[flaml.automl: 09-16 08:47:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:47:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:47:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:47:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:47:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:47:24] {3025} INFO - Estimated sufficient time budget=11923s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 08:47:24] {3072} INFO -  at 1.3s,	estimator xgboost's best error=16.2854,	best estimator xgboost's best error=16.2854
[flaml.automl: 09-16 08:47:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:47:26] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.0726,	best estimator xgboost's best error=8.0726
[flaml.automl: 09-16 08:47:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:47:27] {3072} INFO -  at 4.5s,	estimator xgboost's best error=8.0726,	best estimator xgboost's best error=8.0726
[flaml.automl: 09-16 08:47:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:47:37] {3072} INFO -  at 13.9s,	estimator xgboost's best error=8.0726,	best estimator xgboost's best error=8.0726
[flaml.automl: 09-16 08:47:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:47:38] {3072} INFO -  at 15.0s,	estimator xgboost's best error=5.7258,	best estimator xgboost's best error=5.7258
[flaml.automl: 09-16 08:47:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:47:39] {3072} INFO -  at 16.6s,	estimator xgboost's best error=5.5687,	best estimator xgboost's best error=5.5687
[flaml.automl: 09-16 08:47:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:47:41] {3072} INFO -  at 18.2s,	estimator xgboost's best error=4.7106,	best estimator xgboost's best error=4.7106
[flaml.automl: 09-16 08:47:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:47:44] {3072} INFO -  at 20.9s,	estimator xgboost's best error=4.7106,	best estimator xgboost's best error=4.7106
[flaml.automl: 09-16 08:47:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:47:45] {3072} INFO -  at 22.5s,	estimator xgboost's best error=4.7106,	best estimator xgboost's best error=4.7106
[flaml.automl: 09-16 08:47:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:47:48] {3072} INFO -  at 25.5s,	estimator xgboost's best error=4.7106,	best estimator xgboost's best error=4.7106
[flaml.automl: 09-16 08:47:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:47:50] {3072} INFO -  at 27.2s,	estimator xgboost's best error=4.6069,	best estimator xgboost's best error=4.6069
[flaml.automl: 09-16 08:47:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:47:51] {3072} INFO -  at 28.4s,	estimator xgboost's best error=4.6069,	best estimator xgboost's best error=4.6069
[flaml.automl: 09-16 08:47:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:47:58] {3072} INFO -  at 34.8s,	estimator xgboost's best error=3.8932,	best estimator xgboost's best error=3.8932
[flaml.automl: 09-16 08:47:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:48:09] {3072} INFO -  at 46.0s,	estimator xgboost's best error=3.8932,	best estimator xgboost's best error=3.8932
[flaml.automl: 09-16 08:48:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:48:12] {3072} INFO -  at 49.4s,	estimator xgboost's best error=3.8932,	best estimator xgboost's best error=3.8932
[flaml.automl: 09-16 08:48:12] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 08:48:22] {3072} INFO -  at 59.5s,	estimator xgboost's best error=3.8932,	best estimator xgboost's best error=3.8932
[flaml.automl: 09-16 08:48:29] {3335} INFO - retrain xgboost for 6.5s
[flaml.automl: 09-16 08:48:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:48:29] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:48:29] {2637} INFO - Time taken to find the best model: 34.840638160705566
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}
PM2.5(0)最佳损失：-2.8931853347722245
PM2.5(0)最好结果：{'pred_time': 1.6533729737025956e-05, 'wall_clock_time': 34.840638160705566, 'metric_for_logging': {'pred_time': 1.6533729737025956e-05}, 'val_loss': 3.8931853347722245, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'experiment_tag': 'exp', 'time_total_s': 6.476963758468628}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9021914246289984
PM2.5(0)的mse=37.74130382854229
PM2.5(0)的mae=3.6684612603977906
PM2.5(0)的mar=0.20097385480143765
总共花费的时间为：66.36
长春市
1119A
1120A
1121A
1122A
1124A
1125A
1126A
1128A
[flaml.automl: 09-16 09:14:19] {2390} INFO - task = regression
[flaml.automl: 09-16 09:14:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 09:14:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 09:14:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 09:14:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 09:14:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 09:14:21] {3025} INFO - Estimated sufficient time budget=102006s. Estimated necessary time budget=102s.
[flaml.automl: 09-16 09:14:21] {3072} INFO -  at 1.6s,	estimator xgboost's best error=18.0864,	best estimator xgboost's best error=18.0864
[flaml.automl: 09-16 09:14:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 09:14:23] {3072} INFO -  at 3.7s,	estimator xgboost's best error=9.0219,	best estimator xgboost's best error=9.0219
[flaml.automl: 09-16 09:14:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 09:14:24] {3072} INFO -  at 5.0s,	estimator xgboost's best error=9.0219,	best estimator xgboost's best error=9.0219
[flaml.automl: 09-16 09:14:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 09:14:27] {3072} INFO -  at 7.7s,	estimator xgboost's best error=9.0219,	best estimator xgboost's best error=9.0219
[flaml.automl: 09-16 09:14:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 09:14:28] {3072} INFO -  at 8.9s,	estimator xgboost's best error=7.6848,	best estimator xgboost's best error=7.6848
[flaml.automl: 09-16 09:14:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 09:14:29] {3072} INFO -  at 10.5s,	estimator xgboost's best error=6.4252,	best estimator xgboost's best error=6.4252
[flaml.automl: 09-16 09:14:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 09:14:31] {3072} INFO -  at 12.1s,	estimator xgboost's best error=6.1253,	best estimator xgboost's best error=6.1253
[flaml.automl: 09-16 09:14:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 09:14:34] {3072} INFO -  at 14.8s,	estimator xgboost's best error=6.1253,	best estimator xgboost's best error=6.1253
[flaml.automl: 09-16 09:14:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 09:14:35] {3072} INFO -  at 16.4s,	estimator xgboost's best error=6.1253,	best estimator xgboost's best error=6.1253
[flaml.automl: 09-16 09:14:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 09:14:38] {3072} INFO -  at 18.6s,	estimator xgboost's best error=5.9117,	best estimator xgboost's best error=5.9117
[flaml.automl: 09-16 09:14:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 09:14:39] {3072} INFO -  at 20.2s,	estimator xgboost's best error=5.9117,	best estimator xgboost's best error=5.9117
[flaml.automl: 09-16 09:14:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 09:14:40] {3072} INFO -  at 21.4s,	estimator xgboost's best error=5.9117,	best estimator xgboost's best error=5.9117
[flaml.automl: 09-16 09:14:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 09:14:43] {3072} INFO -  at 23.7s,	estimator xgboost's best error=5.1549,	best estimator xgboost's best error=5.1549
[flaml.automl: 09-16 09:14:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 09:14:44] {3072} INFO -  at 25.3s,	estimator xgboost's best error=5.1549,	best estimator xgboost's best error=5.1549
[flaml.automl: 09-16 09:14:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 09:14:47] {3072} INFO -  at 27.9s,	estimator xgboost's best error=5.0643,	best estimator xgboost's best error=5.0643
[flaml.automl: 09-16 09:14:47] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 09:14:51] {3072} INFO -  at 31.9s,	estimator xgboost's best error=5.0643,	best estimator xgboost's best error=5.0643
[flaml.automl: 09-16 09:14:51] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 09:14:52] {3072} INFO -  at 33.5s,	estimator xgboost's best error=5.0643,	best estimator xgboost's best error=5.0643
[flaml.automl: 09-16 09:14:52] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 09:14:54] {3072} INFO -  at 34.8s,	estimator xgboost's best error=5.0643,	best estimator xgboost's best error=5.0643
[flaml.automl: 09-16 09:14:54] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 09:14:59] {3072} INFO -  at 40.0s,	estimator xgboost's best error=5.0643,	best estimator xgboost's best error=5.0643
[flaml.automl: 09-16 09:14:59] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 09:15:12] {3072} INFO -  at 53.4s,	estimator xgboost's best error=4.7973,	best estimator xgboost's best error=4.7973
[flaml.automl: 09-16 09:15:26] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-16 09:15:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7063431279374065, colsample_bynode=1,
             colsample_bytree=0.7891928823597631, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=21, min_child_weight=0.04606527892183358,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.011846412336189025, reg_lambda=1.0350015394258016,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 09:15:26] {2636} INFO - fit succeeded
[flaml.automl: 09-16 09:15:26] {2637} INFO - Time taken to find the best model: 53.35810160636902
[flaml.automl: 09-16 09:15:26] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 21, 'min_child_weight': 0.04606527892183358, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7063431279374065, 'colsample_bytree': 0.7891928823597631, 'reg_alpha': 0.011846412336189025, 'reg_lambda': 1.0350015394258016, 'FLAML_sample_size': 84833}
PM2.5(0)最佳损失：-3.7973357133366568
PM2.5(0)最好结果：{'pred_time': 8.64513962986665e-06, 'wall_clock_time': 53.35810160636902, 'metric_for_logging': {'pred_time': 8.64513962986665e-06}, 'val_loss': 4.797335713336657, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 21, 'min_child_weight': 0.04606527892183358, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7063431279374065, 'colsample_bytree': 0.7891928823597631, 'reg_alpha': 0.011846412336189025, 'reg_lambda': 1.0350015394258016, 'FLAML_sample_size': 84833}, 'config/n_estimators': 6, 'config/max_leaves': 21, 'config/min_child_weight': 0.04606527892183358, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7063431279374065, 'config/colsample_bytree': 0.7891928823597631, 'config/reg_alpha': 0.011846412336189025, 'config/reg_lambda': 1.0350015394258016, 'config/FLAML_sample_size': 84833, 'experiment_tag': 'exp', 'time_total_s': 13.31899905204773}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7063431279374065, colsample_bynode=1,
             colsample_bytree=0.7891928823597631, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=21, min_child_weight=0.04606527892183358,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.011846412336189025, reg_lambda=1.0350015394258016,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8725622151627396
PM2.5(0)的mse=66.95091151190954
PM2.5(0)的mae=4.8691554336869265
PM2.5(0)的mar=0.21908079137303393
总共花费的时间为：68.34
哈尔滨市
1129A
1130A
1139A
1140A
[flaml.automl: 09-16 09:29:50] {2390} INFO - task = regression
[flaml.automl: 09-16 09:29:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 09:29:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 09:29:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 09:29:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 09:29:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 09:29:51] {3025} INFO - Estimated sufficient time budget=49307s. Estimated necessary time budget=49s.
[flaml.automl: 09-16 09:29:51] {3072} INFO -  at 1.4s,	estimator xgboost's best error=20.2509,	best estimator xgboost's best error=20.2509
[flaml.automl: 09-16 09:29:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 09:29:53] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.0514,	best estimator xgboost's best error=10.0514
[flaml.automl: 09-16 09:29:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 09:29:55] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.0514,	best estimator xgboost's best error=10.0514
[flaml.automl: 09-16 09:29:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 09:30:01] {3072} INFO -  at 11.2s,	estimator xgboost's best error=10.0514,	best estimator xgboost's best error=10.0514
[flaml.automl: 09-16 09:30:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 09:30:03] {3072} INFO -  at 13.3s,	estimator xgboost's best error=9.1616,	best estimator xgboost's best error=9.1616
[flaml.automl: 09-16 09:30:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 09:30:07] {3072} INFO -  at 16.8s,	estimator xgboost's best error=7.9874,	best estimator xgboost's best error=7.9874
[flaml.automl: 09-16 09:30:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 09:30:11] {3072} INFO -  at 21.3s,	estimator xgboost's best error=7.4879,	best estimator xgboost's best error=7.4879
[flaml.automl: 09-16 09:30:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 09:30:15] {3072} INFO -  at 25.3s,	estimator xgboost's best error=7.4879,	best estimator xgboost's best error=7.4879
[flaml.automl: 09-16 09:30:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 09:30:19] {3072} INFO -  at 29.3s,	estimator xgboost's best error=7.4879,	best estimator xgboost's best error=7.4879
[flaml.automl: 09-16 09:30:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 09:30:22] {3072} INFO -  at 31.7s,	estimator xgboost's best error=7.4879,	best estimator xgboost's best error=7.4879
[flaml.automl: 09-16 09:30:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 09:30:24] {3072} INFO -  at 34.2s,	estimator xgboost's best error=7.4879,	best estimator xgboost's best error=7.4879
[flaml.automl: 09-16 09:30:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 09:30:30] {3072} INFO -  at 39.7s,	estimator xgboost's best error=7.4879,	best estimator xgboost's best error=7.4879
[flaml.automl: 09-16 09:30:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 09:30:34] {3072} INFO -  at 44.2s,	estimator xgboost's best error=7.4879,	best estimator xgboost's best error=7.4879
[flaml.automl: 09-16 09:30:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 09:30:48] {3072} INFO -  at 57.9s,	estimator xgboost's best error=5.7152,	best estimator xgboost's best error=5.7152
[flaml.automl: 09-16 09:31:11] {3335} INFO - retrain xgboost for 23.4s
[flaml.automl: 09-16 09:31:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 09:31:11] {2636} INFO - fit succeeded
[flaml.automl: 09-16 09:31:11] {2637} INFO - Time taken to find the best model: 57.88291072845459
[flaml.automl: 09-16 09:31:11] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 41267}
PM2.5(0)最佳损失：-4.7152264164984095
PM2.5(0)最好结果：{'pred_time': 3.910007510079185e-05, 'wall_clock_time': 57.88291072845459, 'metric_for_logging': {'pred_time': 3.910007510079185e-05}, 'val_loss': 5.7152264164984095, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 41267}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 41267, 'experiment_tag': 'exp', 'time_total_s': 13.678207159042358}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8720617730416348
PM2.5(0)的mse=122.622198882795
PM2.5(0)的mae=5.676738843131697
PM2.5(0)的mar=0.22220707846746404
总共花费的时间为：82.15
上海市
1143A
1144A
1145A
1148A
1150A
3265A
3266A
3269A
3270A
3271A
3272A
3273A
3274A
3544A
[flaml.automl: 09-16 10:12:03] {2390} INFO - task = regression
[flaml.automl: 09-16 10:12:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 10:12:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 10:12:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 10:12:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 10:12:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 10:12:05] {3025} INFO - Estimated sufficient time budget=180249s. Estimated necessary time budget=180s.
[flaml.automl: 09-16 10:12:05] {3072} INFO -  at 2.0s,	estimator xgboost's best error=16.1322,	best estimator xgboost's best error=16.1322
[flaml.automl: 09-16 10:12:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 10:12:07] {3072} INFO -  at 3.9s,	estimator xgboost's best error=8.7511,	best estimator xgboost's best error=8.7511
[flaml.automl: 09-16 10:12:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 10:12:08] {3072} INFO -  at 5.1s,	estimator xgboost's best error=8.7511,	best estimator xgboost's best error=8.7511
[flaml.automl: 09-16 10:12:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 10:12:09] {3072} INFO -  at 6.8s,	estimator xgboost's best error=8.7511,	best estimator xgboost's best error=8.7511
[flaml.automl: 09-16 10:12:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 10:12:11] {3072} INFO -  at 7.9s,	estimator xgboost's best error=5.7205,	best estimator xgboost's best error=5.7205
[flaml.automl: 09-16 10:12:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 10:12:12] {3072} INFO -  at 9.5s,	estimator xgboost's best error=5.7205,	best estimator xgboost's best error=5.7205
[flaml.automl: 09-16 10:12:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 10:12:14] {3072} INFO -  at 11.0s,	estimator xgboost's best error=4.3147,	best estimator xgboost's best error=4.3147
[flaml.automl: 09-16 10:12:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 10:12:15] {3072} INFO -  at 12.4s,	estimator xgboost's best error=4.3147,	best estimator xgboost's best error=4.3147
[flaml.automl: 09-16 10:12:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 10:12:16] {3072} INFO -  at 13.8s,	estimator xgboost's best error=4.3147,	best estimator xgboost's best error=4.3147
[flaml.automl: 09-16 10:12:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 10:12:18] {3072} INFO -  at 15.2s,	estimator xgboost's best error=4.3147,	best estimator xgboost's best error=4.3147
[flaml.automl: 09-16 10:12:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 10:12:19] {3072} INFO -  at 16.4s,	estimator xgboost's best error=4.2217,	best estimator xgboost's best error=4.2217
[flaml.automl: 09-16 10:12:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 10:12:20] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.2217,	best estimator xgboost's best error=4.2217
[flaml.automl: 09-16 10:12:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 10:12:21] {3072} INFO -  at 18.7s,	estimator xgboost's best error=3.5655,	best estimator xgboost's best error=3.5655
[flaml.automl: 09-16 10:12:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 10:12:22] {3072} INFO -  at 19.7s,	estimator xgboost's best error=3.5655,	best estimator xgboost's best error=3.5655
[flaml.automl: 09-16 10:12:22] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 10:12:23] {3072} INFO -  at 20.6s,	estimator xgboost's best error=3.5655,	best estimator xgboost's best error=3.5655
[flaml.automl: 09-16 10:12:23] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 10:12:24] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.5655,	best estimator xgboost's best error=3.5655
[flaml.automl: 09-16 10:12:24] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 10:12:25] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.5655,	best estimator xgboost's best error=3.5655
[flaml.automl: 09-16 10:12:25] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 10:12:31] {3072} INFO -  at 28.8s,	estimator xgboost's best error=3.3505,	best estimator xgboost's best error=3.3505
[flaml.automl: 09-16 10:12:31] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 10:12:34] {3072} INFO -  at 31.3s,	estimator xgboost's best error=3.3505,	best estimator xgboost's best error=3.3505
[flaml.automl: 09-16 10:12:34] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 10:12:49] {3072} INFO -  at 46.3s,	estimator xgboost's best error=3.3505,	best estimator xgboost's best error=3.3505
[flaml.automl: 09-16 10:12:49] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 10:12:51] {3072} INFO -  at 48.6s,	estimator xgboost's best error=3.3505,	best estimator xgboost's best error=3.3505
[flaml.automl: 09-16 10:12:51] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 10:13:02] {3072} INFO -  at 59.1s,	estimator xgboost's best error=3.2980,	best estimator xgboost's best error=3.2980
[flaml.automl: 09-16 10:13:29] {3335} INFO - retrain xgboost for 27.5s
[flaml.automl: 09-16 10:13:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8710285348552388, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=4.539940750562486,
             missing=nan, monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015201510905823695, reg_lambda=0.015307691101197542,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 10:13:29] {2636} INFO - fit succeeded
[flaml.automl: 09-16 10:13:29] {2637} INFO - Time taken to find the best model: 59.101412534713745
[flaml.automl: 09-16 10:13:29] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 18, 'max_leaves': 18, 'min_child_weight': 4.539940750562486, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8710285348552388, 'reg_alpha': 0.0015201510905823695, 'reg_lambda': 0.015307691101197542, 'FLAML_sample_size': 148579}
PM2.5(0)最佳损失：-2.298020319369944
PM2.5(0)最好结果：{'pred_time': 2.5164865120253417e-06, 'wall_clock_time': 59.101412534713745, 'metric_for_logging': {'pred_time': 2.5164865120253417e-06}, 'val_loss': 3.298020319369944, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 18, 'min_child_weight': 4.539940750562486, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8710285348552388, 'reg_alpha': 0.0015201510905823695, 'reg_lambda': 0.015307691101197542, 'FLAML_sample_size': 148579}, 'config/n_estimators': 18, 'config/max_leaves': 18, 'config/min_child_weight': 4.539940750562486, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8710285348552388, 'config/reg_alpha': 0.0015201510905823695, 'config/reg_lambda': 0.015307691101197542, 'config/FLAML_sample_size': 148579, 'experiment_tag': 'exp', 'time_total_s': 10.458821535110474}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8710285348552388, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=4.539940750562486,
             missing=nan, monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015201510905823695, reg_lambda=0.015307691101197542,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.919566609637799
PM2.5(0)的mse=21.869131501454813
PM2.5(0)的mae=3.2259936292411666
PM2.5(0)的mar=0.18770506463853615
总共花费的时间为：88.84
南京市
1151A
1152A
1153A
1154A
3423A
3424A
3427A
[flaml.automl: 09-16 10:34:50] {2390} INFO - task = regression
[flaml.automl: 09-16 10:34:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 10:34:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 10:34:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 10:34:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 10:34:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 10:34:52] {3025} INFO - Estimated sufficient time budget=89397s. Estimated necessary time budget=89s.
[flaml.automl: 09-16 10:34:52] {3072} INFO -  at 1.5s,	estimator xgboost's best error=18.1503,	best estimator xgboost's best error=18.1503
[flaml.automl: 09-16 10:34:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 10:34:54] {3072} INFO -  at 3.6s,	estimator xgboost's best error=8.8238,	best estimator xgboost's best error=8.8238
[flaml.automl: 09-16 10:34:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 10:34:55] {3072} INFO -  at 4.8s,	estimator xgboost's best error=8.8238,	best estimator xgboost's best error=8.8238
[flaml.automl: 09-16 10:34:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 10:34:58] {3072} INFO -  at 8.1s,	estimator xgboost's best error=8.8238,	best estimator xgboost's best error=8.8238
[flaml.automl: 09-16 10:34:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 10:34:59] {3072} INFO -  at 9.2s,	estimator xgboost's best error=6.3927,	best estimator xgboost's best error=6.3927
[flaml.automl: 09-16 10:34:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 10:35:01] {3072} INFO -  at 10.8s,	estimator xgboost's best error=6.3927,	best estimator xgboost's best error=6.3927
[flaml.automl: 09-16 10:35:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 10:35:03] {3072} INFO -  at 12.5s,	estimator xgboost's best error=4.5730,	best estimator xgboost's best error=4.5730
[flaml.automl: 09-16 10:35:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 10:35:05] {3072} INFO -  at 15.2s,	estimator xgboost's best error=4.5730,	best estimator xgboost's best error=4.5730
[flaml.automl: 09-16 10:35:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 10:35:07] {3072} INFO -  at 16.9s,	estimator xgboost's best error=4.5730,	best estimator xgboost's best error=4.5730
[flaml.automl: 09-16 10:35:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 10:35:10] {3072} INFO -  at 19.5s,	estimator xgboost's best error=4.5730,	best estimator xgboost's best error=4.5730
[flaml.automl: 09-16 10:35:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 10:35:11] {3072} INFO -  at 20.9s,	estimator xgboost's best error=4.5720,	best estimator xgboost's best error=4.5720
[flaml.automl: 09-16 10:35:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 10:35:12] {3072} INFO -  at 22.1s,	estimator xgboost's best error=4.5720,	best estimator xgboost's best error=4.5720
[flaml.automl: 09-16 10:35:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 10:35:14] {3072} INFO -  at 24.4s,	estimator xgboost's best error=4.1050,	best estimator xgboost's best error=4.1050
[flaml.automl: 09-16 10:35:14] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 10:35:17] {3072} INFO -  at 26.8s,	estimator xgboost's best error=4.0996,	best estimator xgboost's best error=4.0996
[flaml.automl: 09-16 10:35:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 10:35:18] {3072} INFO -  at 28.1s,	estimator xgboost's best error=4.0996,	best estimator xgboost's best error=4.0996
[flaml.automl: 09-16 10:35:18] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 10:35:20] {3072} INFO -  at 29.7s,	estimator xgboost's best error=4.0996,	best estimator xgboost's best error=4.0996
[flaml.automl: 09-16 10:35:20] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 10:35:21] {3072} INFO -  at 31.3s,	estimator xgboost's best error=4.0996,	best estimator xgboost's best error=4.0996
[flaml.automl: 09-16 10:35:21] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 10:35:23] {3072} INFO -  at 32.9s,	estimator xgboost's best error=4.0996,	best estimator xgboost's best error=4.0996
[flaml.automl: 09-16 10:35:23] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 10:35:43] {3072} INFO -  at 53.1s,	estimator xgboost's best error=3.8289,	best estimator xgboost's best error=3.8289
[flaml.automl: 09-16 10:36:03] {3335} INFO - retrain xgboost for 20.2s
[flaml.automl: 09-16 10:36:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 10:36:03] {2636} INFO - fit succeeded
[flaml.automl: 09-16 10:36:03] {2637} INFO - Time taken to find the best model: 53.11165761947632
[flaml.automl: 09-16 10:36:03] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 74508}
PM2.5(0)最佳损失：-2.8288829262122137
PM2.5(0)最好结果：{'pred_time': 7.334503089046835e-06, 'wall_clock_time': 53.11165761947632, 'metric_for_logging': {'pred_time': 7.334503089046835e-06}, 'val_loss': 3.8288829262122137, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 74508}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 74508, 'experiment_tag': 'exp', 'time_total_s': 20.16763973236084}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9087808836150217
PM2.5(0)的mse=26.929295160223198
PM2.5(0)的mae=3.764481803388333
PM2.5(0)的mar=0.21894875200689085
总共花费的时间为：74.58
苏州市
1160A
1164A
1165A
1166A
1167A
3289A
3290A
3425A
3431A
[flaml.automl: 09-16 11:02:18] {2390} INFO - task = regression
[flaml.automl: 09-16 11:02:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:02:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:02:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:02:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:02:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:02:19] {3025} INFO - Estimated sufficient time budget=109636s. Estimated necessary time budget=110s.
[flaml.automl: 09-16 11:02:19] {3072} INFO -  at 1.6s,	estimator xgboost's best error=17.8107,	best estimator xgboost's best error=17.8107
[flaml.automl: 09-16 11:02:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:02:21] {3072} INFO -  at 3.7s,	estimator xgboost's best error=8.4159,	best estimator xgboost's best error=8.4159
[flaml.automl: 09-16 11:02:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:02:22] {3072} INFO -  at 4.9s,	estimator xgboost's best error=8.4159,	best estimator xgboost's best error=8.4159
[flaml.automl: 09-16 11:02:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:02:25] {3072} INFO -  at 7.6s,	estimator xgboost's best error=8.4159,	best estimator xgboost's best error=8.4159
[flaml.automl: 09-16 11:02:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:02:26] {3072} INFO -  at 8.8s,	estimator xgboost's best error=5.2815,	best estimator xgboost's best error=5.2815
[flaml.automl: 09-16 11:02:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:02:28] {3072} INFO -  at 10.3s,	estimator xgboost's best error=5.0631,	best estimator xgboost's best error=5.0631
[flaml.automl: 09-16 11:02:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:02:30] {3072} INFO -  at 11.9s,	estimator xgboost's best error=4.1663,	best estimator xgboost's best error=4.1663
[flaml.automl: 09-16 11:02:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:02:32] {3072} INFO -  at 14.2s,	estimator xgboost's best error=4.1663,	best estimator xgboost's best error=4.1663
[flaml.automl: 09-16 11:02:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:02:33] {3072} INFO -  at 15.8s,	estimator xgboost's best error=4.1663,	best estimator xgboost's best error=4.1663
[flaml.automl: 09-16 11:02:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:02:36] {3072} INFO -  at 18.0s,	estimator xgboost's best error=4.1663,	best estimator xgboost's best error=4.1663
[flaml.automl: 09-16 11:02:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:02:37] {3072} INFO -  at 19.5s,	estimator xgboost's best error=4.0636,	best estimator xgboost's best error=4.0636
[flaml.automl: 09-16 11:02:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:02:38] {3072} INFO -  at 20.6s,	estimator xgboost's best error=4.0636,	best estimator xgboost's best error=4.0636
[flaml.automl: 09-16 11:02:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:02:40] {3072} INFO -  at 22.4s,	estimator xgboost's best error=3.4933,	best estimator xgboost's best error=3.4933
[flaml.automl: 09-16 11:02:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:02:42] {3072} INFO -  at 24.2s,	estimator xgboost's best error=3.4933,	best estimator xgboost's best error=3.4933
[flaml.automl: 09-16 11:02:42] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:02:43] {3072} INFO -  at 25.7s,	estimator xgboost's best error=3.4933,	best estimator xgboost's best error=3.4933
[flaml.automl: 09-16 11:02:43] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 11:02:44] {3072} INFO -  at 26.8s,	estimator xgboost's best error=3.4933,	best estimator xgboost's best error=3.4933
[flaml.automl: 09-16 11:02:44] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 11:02:46] {3072} INFO -  at 28.5s,	estimator xgboost's best error=3.4933,	best estimator xgboost's best error=3.4933
[flaml.automl: 09-16 11:02:46] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 11:02:52] {3072} INFO -  at 34.6s,	estimator xgboost's best error=3.2681,	best estimator xgboost's best error=3.2681
[flaml.automl: 09-16 11:02:52] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 11:02:55] {3072} INFO -  at 37.0s,	estimator xgboost's best error=3.2681,	best estimator xgboost's best error=3.2681
[flaml.automl: 09-16 11:02:55] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 11:03:10] {3072} INFO -  at 51.9s,	estimator xgboost's best error=3.2681,	best estimator xgboost's best error=3.2681
[flaml.automl: 09-16 11:03:10] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 11:03:13] {3072} INFO -  at 55.6s,	estimator xgboost's best error=3.2681,	best estimator xgboost's best error=3.2681
[flaml.automl: 09-16 11:03:24] {3335} INFO - retrain xgboost for 11.0s
[flaml.automl: 09-16 11:03:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:03:24] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:03:24] {2637} INFO - Time taken to find the best model: 34.55033087730408
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024, 'FLAML_sample_size': 90978}
PM2.5(0)最佳损失：-2.2681350534423803
PM2.5(0)最好结果：{'pred_time': 4.7639376426830366e-06, 'wall_clock_time': 34.55033087730408, 'metric_for_logging': {'pred_time': 4.7639376426830366e-06}, 'val_loss': 3.2681350534423803, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024, 'FLAML_sample_size': 90978}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'config/FLAML_sample_size': 90978, 'experiment_tag': 'exp', 'time_total_s': 6.062302589416504}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9213120974967142
PM2.5(0)的mse=24.412292837422598
PM2.5(0)的mae=3.2542707258915033
PM2.5(0)的mar=0.14833681914493563
总共花费的时间为：68.22
南通市
1168A
1169A
1171A
1172A
3291A
3432A
[flaml.automl: 09-16 11:21:12] {2390} INFO - task = regression
[flaml.automl: 09-16 11:21:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:21:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:21:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:21:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:21:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:21:13] {3025} INFO - Estimated sufficient time budget=76834s. Estimated necessary time budget=77s.
[flaml.automl: 09-16 11:21:13] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.4581,	best estimator xgboost's best error=17.4581
[flaml.automl: 09-16 11:21:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:21:15] {3072} INFO -  at 3.6s,	estimator xgboost's best error=8.6448,	best estimator xgboost's best error=8.6448
[flaml.automl: 09-16 11:21:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:21:16] {3072} INFO -  at 4.8s,	estimator xgboost's best error=8.6448,	best estimator xgboost's best error=8.6448
[flaml.automl: 09-16 11:21:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:21:20] {3072} INFO -  at 8.5s,	estimator xgboost's best error=8.6448,	best estimator xgboost's best error=8.6448
[flaml.automl: 09-16 11:21:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:21:21] {3072} INFO -  at 9.7s,	estimator xgboost's best error=6.0097,	best estimator xgboost's best error=6.0097
[flaml.automl: 09-16 11:21:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:21:23] {3072} INFO -  at 11.3s,	estimator xgboost's best error=6.0097,	best estimator xgboost's best error=6.0097
[flaml.automl: 09-16 11:21:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:21:24] {3072} INFO -  at 13.0s,	estimator xgboost's best error=4.8845,	best estimator xgboost's best error=4.8845
[flaml.automl: 09-16 11:21:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:21:27] {3072} INFO -  at 15.6s,	estimator xgboost's best error=4.8845,	best estimator xgboost's best error=4.8845
[flaml.automl: 09-16 11:21:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:21:29] {3072} INFO -  at 17.3s,	estimator xgboost's best error=4.8845,	best estimator xgboost's best error=4.8845
[flaml.automl: 09-16 11:21:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:21:32] {3072} INFO -  at 20.3s,	estimator xgboost's best error=4.8845,	best estimator xgboost's best error=4.8845
[flaml.automl: 09-16 11:21:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:21:33] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.8845,	best estimator xgboost's best error=4.8845
[flaml.automl: 09-16 11:21:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:21:35] {3072} INFO -  at 23.4s,	estimator xgboost's best error=4.8845,	best estimator xgboost's best error=4.8845
[flaml.automl: 09-16 11:21:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:21:36] {3072} INFO -  at 24.6s,	estimator xgboost's best error=4.8845,	best estimator xgboost's best error=4.8845
[flaml.automl: 09-16 11:21:36] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:21:43] {3072} INFO -  at 31.7s,	estimator xgboost's best error=4.3545,	best estimator xgboost's best error=4.3545
[flaml.automl: 09-16 11:21:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:21:56] {3072} INFO -  at 44.5s,	estimator xgboost's best error=4.3197,	best estimator xgboost's best error=4.3197
[flaml.automl: 09-16 11:21:56] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 11:22:03] {3072} INFO -  at 51.6s,	estimator xgboost's best error=4.3197,	best estimator xgboost's best error=4.3197
[flaml.automl: 09-16 11:22:16] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-16 11:22:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:22:16] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:22:16] {2637} INFO - Time taken to find the best model: 44.548465967178345
[flaml.automl: 09-16 11:22:16] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64655}
PM2.5(0)最佳损失：-3.319683337961645
PM2.5(0)最好结果：{'pred_time': 5.678603537629601e-06, 'wall_clock_time': 44.548465967178345, 'metric_for_logging': {'pred_time': 5.678603537629601e-06}, 'val_loss': 4.319683337961645, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64655}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 64655, 'experiment_tag': 'exp', 'time_total_s': 12.85731291770935}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8924839797969871
PM2.5(0)的mse=37.77980030904303
PM2.5(0)的mae=4.2930296156679075
PM2.5(0)的mar=0.24286730205305043
总共花费的时间为：65.40
连云港市
1173A
3000A
3008A
3009A
3434A
3664A
[flaml.automl: 09-16 11:40:03] {2390} INFO - task = regression
[flaml.automl: 09-16 11:40:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:40:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:40:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:40:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:40:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:40:04] {3025} INFO - Estimated sufficient time budget=77989s. Estimated necessary time budget=78s.
[flaml.automl: 09-16 11:40:04] {3072} INFO -  at 1.5s,	estimator xgboost's best error=18.4496,	best estimator xgboost's best error=18.4496
[flaml.automl: 09-16 11:40:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:40:06] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.1099,	best estimator xgboost's best error=9.1099
[flaml.automl: 09-16 11:40:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:40:07] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.1099,	best estimator xgboost's best error=9.1099
[flaml.automl: 09-16 11:40:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:40:11] {3072} INFO -  at 8.6s,	estimator xgboost's best error=9.1099,	best estimator xgboost's best error=9.1099
[flaml.automl: 09-16 11:40:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:40:12] {3072} INFO -  at 9.8s,	estimator xgboost's best error=6.4612,	best estimator xgboost's best error=6.4612
[flaml.automl: 09-16 11:40:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:40:14] {3072} INFO -  at 11.3s,	estimator xgboost's best error=6.4612,	best estimator xgboost's best error=6.4612
[flaml.automl: 09-16 11:40:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:40:16] {3072} INFO -  at 13.0s,	estimator xgboost's best error=4.9608,	best estimator xgboost's best error=4.9608
[flaml.automl: 09-16 11:40:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:40:18] {3072} INFO -  at 15.7s,	estimator xgboost's best error=4.9608,	best estimator xgboost's best error=4.9608
[flaml.automl: 09-16 11:40:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:40:20] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.9608,	best estimator xgboost's best error=4.9608
[flaml.automl: 09-16 11:40:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:40:23] {3072} INFO -  at 20.4s,	estimator xgboost's best error=4.9608,	best estimator xgboost's best error=4.9608
[flaml.automl: 09-16 11:40:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:40:24] {3072} INFO -  at 21.8s,	estimator xgboost's best error=4.9608,	best estimator xgboost's best error=4.9608
[flaml.automl: 09-16 11:40:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:40:26] {3072} INFO -  at 23.6s,	estimator xgboost's best error=4.9122,	best estimator xgboost's best error=4.9122
[flaml.automl: 09-16 11:40:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:40:27] {3072} INFO -  at 24.7s,	estimator xgboost's best error=4.9122,	best estimator xgboost's best error=4.9122
[flaml.automl: 09-16 11:40:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:40:34] {3072} INFO -  at 31.8s,	estimator xgboost's best error=4.3675,	best estimator xgboost's best error=4.3675
[flaml.automl: 09-16 11:40:34] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:40:47] {3072} INFO -  at 44.7s,	estimator xgboost's best error=4.2664,	best estimator xgboost's best error=4.2664
[flaml.automl: 09-16 11:40:47] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 11:40:54] {3072} INFO -  at 51.8s,	estimator xgboost's best error=4.2664,	best estimator xgboost's best error=4.2664
[flaml.automl: 09-16 11:41:07] {3335} INFO - retrain xgboost for 12.9s
[flaml.automl: 09-16 11:41:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:41:07] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:41:07] {2637} INFO - Time taken to find the best model: 44.67332100868225
[flaml.automl: 09-16 11:41:07] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64870}
PM2.5(0)最佳损失：-3.2664436799506102
PM2.5(0)最好结果：{'pred_time': 5.6560904812998035e-06, 'wall_clock_time': 44.67332100868225, 'metric_for_logging': {'pred_time': 5.6560904812998035e-06}, 'val_loss': 4.26644367995061, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64870}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 64870, 'experiment_tag': 'exp', 'time_total_s': 12.858762741088867}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9064358901056836
PM2.5(0)的mse=43.9997301736964
PM2.5(0)的mae=4.314488382420002
PM2.5(0)的mar=0.25630398571027896
总共花费的时间为：65.57
徐州市
1177A
3006A
3288A
[flaml.automl: 09-16 11:50:40] {2390} INFO - task = regression
[flaml.automl: 09-16 11:50:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:50:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:50:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:50:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:50:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:50:41] {3025} INFO - Estimated sufficient time budget=12019s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 11:50:41] {3072} INFO -  at 1.4s,	estimator xgboost's best error=24.9312,	best estimator xgboost's best error=24.9312
[flaml.automl: 09-16 11:50:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:50:43] {3072} INFO -  at 3.5s,	estimator xgboost's best error=11.7875,	best estimator xgboost's best error=11.7875
[flaml.automl: 09-16 11:50:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:50:45] {3072} INFO -  at 4.6s,	estimator xgboost's best error=11.7875,	best estimator xgboost's best error=11.7875
[flaml.automl: 09-16 11:50:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:50:54] {3072} INFO -  at 14.5s,	estimator xgboost's best error=11.7875,	best estimator xgboost's best error=11.7875
[flaml.automl: 09-16 11:50:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:50:56] {3072} INFO -  at 15.6s,	estimator xgboost's best error=8.3517,	best estimator xgboost's best error=8.3517
[flaml.automl: 09-16 11:50:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:50:57] {3072} INFO -  at 17.2s,	estimator xgboost's best error=8.3517,	best estimator xgboost's best error=8.3517
[flaml.automl: 09-16 11:50:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:50:59] {3072} INFO -  at 18.9s,	estimator xgboost's best error=5.8269,	best estimator xgboost's best error=5.8269
[flaml.automl: 09-16 11:50:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:51:02] {3072} INFO -  at 21.5s,	estimator xgboost's best error=5.8269,	best estimator xgboost's best error=5.8269
[flaml.automl: 09-16 11:51:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:51:03] {3072} INFO -  at 23.2s,	estimator xgboost's best error=5.8269,	best estimator xgboost's best error=5.8269
[flaml.automl: 09-16 11:51:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:51:06] {3072} INFO -  at 26.2s,	estimator xgboost's best error=5.8269,	best estimator xgboost's best error=5.8269
[flaml.automl: 09-16 11:51:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:51:08] {3072} INFO -  at 27.7s,	estimator xgboost's best error=5.8269,	best estimator xgboost's best error=5.8269
[flaml.automl: 09-16 11:51:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:51:09] {3072} INFO -  at 28.8s,	estimator xgboost's best error=5.8269,	best estimator xgboost's best error=5.8269
[flaml.automl: 09-16 11:51:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:51:16] {3072} INFO -  at 35.9s,	estimator xgboost's best error=5.0694,	best estimator xgboost's best error=5.0694
[flaml.automl: 09-16 11:51:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:51:29] {3072} INFO -  at 48.7s,	estimator xgboost's best error=4.8892,	best estimator xgboost's best error=4.8892
[flaml.automl: 09-16 11:51:41] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-16 11:51:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:51:41] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:51:41] {2637} INFO - Time taken to find the best model: 48.67445755004883
[flaml.automl: 09-16 11:51:41] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-3.88924582874167
PM2.5(0)最好结果：{'pred_time': 1.089648238306828e-05, 'wall_clock_time': 48.67445755004883, 'metric_for_logging': {'pred_time': 1.089648238306828e-05}, 'val_loss': 4.88924582874167, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 12.797937154769897}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9249900027555117
PM2.5(0)的mse=45.21229100459999
PM2.5(0)的mae=4.980109233397086
PM2.5(0)的mar=0.19315868698290467
总共花费的时间为：61.90
扬州市
1186A
3164A
3195A
3294A
[flaml.automl: 09-16 12:04:15] {2390} INFO - task = regression
[flaml.automl: 09-16 12:04:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:04:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:04:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:04:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:04:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:04:16] {3025} INFO - Estimated sufficient time budget=51124s. Estimated necessary time budget=51s.
[flaml.automl: 09-16 12:04:16] {3072} INFO -  at 1.4s,	estimator xgboost's best error=20.7643,	best estimator xgboost's best error=20.7643
[flaml.automl: 09-16 12:04:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:04:18] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.8032,	best estimator xgboost's best error=9.8032
[flaml.automl: 09-16 12:04:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:04:20] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.8032,	best estimator xgboost's best error=9.8032
[flaml.automl: 09-16 12:04:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:04:26] {3072} INFO -  at 11.0s,	estimator xgboost's best error=9.8032,	best estimator xgboost's best error=9.8032
[flaml.automl: 09-16 12:04:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:04:27] {3072} INFO -  at 12.1s,	estimator xgboost's best error=6.8798,	best estimator xgboost's best error=6.8798
[flaml.automl: 09-16 12:04:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:04:29] {3072} INFO -  at 13.7s,	estimator xgboost's best error=6.8798,	best estimator xgboost's best error=6.8798
[flaml.automl: 09-16 12:04:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:04:30] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.4751,	best estimator xgboost's best error=4.4751
[flaml.automl: 09-16 12:04:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:04:33] {3072} INFO -  at 18.1s,	estimator xgboost's best error=4.4751,	best estimator xgboost's best error=4.4751
[flaml.automl: 09-16 12:04:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:04:35] {3072} INFO -  at 19.7s,	estimator xgboost's best error=4.4751,	best estimator xgboost's best error=4.4751
[flaml.automl: 09-16 12:04:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:04:38] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.4751,	best estimator xgboost's best error=4.4751
[flaml.automl: 09-16 12:04:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:04:39] {3072} INFO -  at 24.1s,	estimator xgboost's best error=4.3987,	best estimator xgboost's best error=4.3987
[flaml.automl: 09-16 12:04:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:04:40] {3072} INFO -  at 25.3s,	estimator xgboost's best error=4.3987,	best estimator xgboost's best error=4.3987
[flaml.automl: 09-16 12:04:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:04:44] {3072} INFO -  at 29.1s,	estimator xgboost's best error=3.7288,	best estimator xgboost's best error=3.7288
[flaml.automl: 09-16 12:04:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:04:48] {3072} INFO -  at 32.5s,	estimator xgboost's best error=3.7256,	best estimator xgboost's best error=3.7256
[flaml.automl: 09-16 12:04:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 12:04:50] {3072} INFO -  at 35.3s,	estimator xgboost's best error=3.7256,	best estimator xgboost's best error=3.7256
[flaml.automl: 09-16 12:04:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 12:04:53] {3072} INFO -  at 37.5s,	estimator xgboost's best error=3.7256,	best estimator xgboost's best error=3.7256
[flaml.automl: 09-16 12:04:53] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 12:04:55] {3072} INFO -  at 39.8s,	estimator xgboost's best error=3.7256,	best estimator xgboost's best error=3.7256
[flaml.automl: 09-16 12:04:55] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 12:04:57] {3072} INFO -  at 41.9s,	estimator xgboost's best error=3.7256,	best estimator xgboost's best error=3.7256
[flaml.automl: 09-16 12:04:57] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 12:05:08] {3072} INFO -  at 52.7s,	estimator xgboost's best error=3.5501,	best estimator xgboost's best error=3.5501
[flaml.automl: 09-16 12:05:18] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-16 12:05:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 12:05:18] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:05:18] {2637} INFO - Time taken to find the best model: 52.72042536735535
[flaml.automl: 09-16 12:05:18] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 43367}
PM2.5(0)最佳损失：-2.550111522044253
PM2.5(0)最好结果：{'pred_time': 8.962935593940085e-06, 'wall_clock_time': 52.72042536735535, 'metric_for_logging': {'pred_time': 8.962935593940085e-06}, 'val_loss': 3.550111522044253, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 43367}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 43367, 'experiment_tag': 'exp', 'time_total_s': 10.790647029876709}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9335609718761633
PM2.5(0)的mse=26.37076982798957
PM2.5(0)的mae=3.478775055452118
PM2.5(0)的mar=0.145393793429011
总共花费的时间为：64.08
无锡市
1189A
1190A
1191A
1192A
1193A
1194A
1195A
3428A
[flaml.automl: 09-16 12:29:07] {2390} INFO - task = regression
[flaml.automl: 09-16 12:29:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:29:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:29:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:29:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:29:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:29:08] {3025} INFO - Estimated sufficient time budget=102535s. Estimated necessary time budget=103s.
[flaml.automl: 09-16 12:29:08] {3072} INFO -  at 1.5s,	estimator xgboost's best error=18.1917,	best estimator xgboost's best error=18.1917
[flaml.automl: 09-16 12:29:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:29:10] {3072} INFO -  at 3.7s,	estimator xgboost's best error=8.8675,	best estimator xgboost's best error=8.8675
[flaml.automl: 09-16 12:29:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:29:11] {3072} INFO -  at 4.9s,	estimator xgboost's best error=8.8675,	best estimator xgboost's best error=8.8675
[flaml.automl: 09-16 12:29:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:29:14] {3072} INFO -  at 7.7s,	estimator xgboost's best error=8.8675,	best estimator xgboost's best error=8.8675
[flaml.automl: 09-16 12:29:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:29:15] {3072} INFO -  at 8.8s,	estimator xgboost's best error=5.8451,	best estimator xgboost's best error=5.8451
[flaml.automl: 09-16 12:29:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:29:17] {3072} INFO -  at 10.4s,	estimator xgboost's best error=5.6312,	best estimator xgboost's best error=5.6312
[flaml.automl: 09-16 12:29:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:29:19] {3072} INFO -  at 12.0s,	estimator xgboost's best error=4.9291,	best estimator xgboost's best error=4.9291
[flaml.automl: 09-16 12:29:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:29:21] {3072} INFO -  at 14.7s,	estimator xgboost's best error=4.9291,	best estimator xgboost's best error=4.9291
[flaml.automl: 09-16 12:29:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:29:23] {3072} INFO -  at 16.3s,	estimator xgboost's best error=4.9291,	best estimator xgboost's best error=4.9291
[flaml.automl: 09-16 12:29:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:29:25] {3072} INFO -  at 18.5s,	estimator xgboost's best error=4.9291,	best estimator xgboost's best error=4.9291
[flaml.automl: 09-16 12:29:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:29:26] {3072} INFO -  at 20.0s,	estimator xgboost's best error=4.8397,	best estimator xgboost's best error=4.8397
[flaml.automl: 09-16 12:29:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:29:28] {3072} INFO -  at 21.1s,	estimator xgboost's best error=4.8397,	best estimator xgboost's best error=4.8397
[flaml.automl: 09-16 12:29:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:29:29] {3072} INFO -  at 22.9s,	estimator xgboost's best error=4.5010,	best estimator xgboost's best error=4.5010
[flaml.automl: 09-16 12:29:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:29:31] {3072} INFO -  at 24.7s,	estimator xgboost's best error=4.5010,	best estimator xgboost's best error=4.5010
[flaml.automl: 09-16 12:29:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 12:29:33] {3072} INFO -  at 26.2s,	estimator xgboost's best error=4.5010,	best estimator xgboost's best error=4.5010
[flaml.automl: 09-16 12:29:33] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 12:29:34] {3072} INFO -  at 27.3s,	estimator xgboost's best error=4.5010,	best estimator xgboost's best error=4.5010
[flaml.automl: 09-16 12:29:34] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 12:29:35] {3072} INFO -  at 29.0s,	estimator xgboost's best error=4.5010,	best estimator xgboost's best error=4.5010
[flaml.automl: 09-16 12:29:35] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 12:29:42] {3072} INFO -  at 35.0s,	estimator xgboost's best error=4.3390,	best estimator xgboost's best error=4.3390
[flaml.automl: 09-16 12:29:42] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 12:29:44] {3072} INFO -  at 37.4s,	estimator xgboost's best error=4.3390,	best estimator xgboost's best error=4.3390
[flaml.automl: 09-16 12:29:44] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 12:29:59] {3072} INFO -  at 52.3s,	estimator xgboost's best error=4.2839,	best estimator xgboost's best error=4.2839
[flaml.automl: 09-16 12:30:16] {3335} INFO - retrain xgboost for 16.9s
[flaml.automl: 09-16 12:30:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8193436379278645, colsample_bynode=1,
             colsample_bytree=0.7834219076904271, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=20, min_child_weight=3.9280413874613482,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.38816518346729456, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 12:30:16] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:30:16] {2637} INFO - Time taken to find the best model: 52.31159281730652
[flaml.automl: 09-16 12:30:16] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 20, 'min_child_weight': 3.9280413874613482, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.8193436379278645, 'colsample_bytree': 0.7834219076904271, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.38816518346729456, 'FLAML_sample_size': 85766}
PM2.5(0)最佳损失：-3.2838812341469907
PM2.5(0)最好结果：{'pred_time': 4.415922375065582e-06, 'wall_clock_time': 52.31159281730652, 'metric_for_logging': {'pred_time': 4.415922375065582e-06}, 'val_loss': 4.283881234146991, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 20, 'min_child_weight': 3.9280413874613482, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.8193436379278645, 'colsample_bytree': 0.7834219076904271, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.38816518346729456, 'FLAML_sample_size': 85766}, 'config/n_estimators': 13, 'config/max_leaves': 20, 'config/min_child_weight': 3.9280413874613482, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8193436379278645, 'config/colsample_bytree': 0.7834219076904271, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.38816518346729456, 'config/FLAML_sample_size': 85766, 'experiment_tag': 'exp', 'time_total_s': 14.886458158493042}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8193436379278645, colsample_bynode=1,
             colsample_bytree=0.7834219076904271, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=20, min_child_weight=3.9280413874613482,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.38816518346729456, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8690154695847234
PM2.5(0)的mse=35.25805769774589
PM2.5(0)的mae=4.383170400488556
PM2.5(0)的mar=0.2241638361009163
总共花费的时间为：70.48
常州市
1196A
3003A
3010A
3429A
3430A
[flaml.automl: 09-16 12:46:12] {2390} INFO - task = regression
[flaml.automl: 09-16 12:46:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:46:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:46:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:46:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:46:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:46:13] {3025} INFO - Estimated sufficient time budget=64834s. Estimated necessary time budget=65s.
[flaml.automl: 09-16 12:46:13] {3072} INFO -  at 1.4s,	estimator xgboost's best error=21.2051,	best estimator xgboost's best error=21.2051
[flaml.automl: 09-16 12:46:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:46:15] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.1766,	best estimator xgboost's best error=10.1766
[flaml.automl: 09-16 12:46:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:46:16] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.1766,	best estimator xgboost's best error=10.1766
[flaml.automl: 09-16 12:46:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:46:21] {3072} INFO -  at 9.5s,	estimator xgboost's best error=10.1766,	best estimator xgboost's best error=10.1766
[flaml.automl: 09-16 12:46:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:46:22] {3072} INFO -  at 10.7s,	estimator xgboost's best error=6.5843,	best estimator xgboost's best error=6.5843
[flaml.automl: 09-16 12:46:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:46:24] {3072} INFO -  at 12.3s,	estimator xgboost's best error=6.5843,	best estimator xgboost's best error=6.5843
[flaml.automl: 09-16 12:46:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:46:26] {3072} INFO -  at 14.0s,	estimator xgboost's best error=5.0561,	best estimator xgboost's best error=5.0561
[flaml.automl: 09-16 12:46:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:46:28] {3072} INFO -  at 16.6s,	estimator xgboost's best error=5.0561,	best estimator xgboost's best error=5.0561
[flaml.automl: 09-16 12:46:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:46:30] {3072} INFO -  at 18.3s,	estimator xgboost's best error=5.0561,	best estimator xgboost's best error=5.0561
[flaml.automl: 09-16 12:46:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:46:33] {3072} INFO -  at 21.3s,	estimator xgboost's best error=5.0561,	best estimator xgboost's best error=5.0561
[flaml.automl: 09-16 12:46:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:46:34] {3072} INFO -  at 22.7s,	estimator xgboost's best error=5.0084,	best estimator xgboost's best error=5.0084
[flaml.automl: 09-16 12:46:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:46:35] {3072} INFO -  at 23.9s,	estimator xgboost's best error=5.0084,	best estimator xgboost's best error=5.0084
[flaml.automl: 09-16 12:46:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:46:38] {3072} INFO -  at 26.7s,	estimator xgboost's best error=4.6192,	best estimator xgboost's best error=4.6192
[flaml.automl: 09-16 12:46:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:46:41] {3072} INFO -  at 29.3s,	estimator xgboost's best error=4.3999,	best estimator xgboost's best error=4.3999
[flaml.automl: 09-16 12:46:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 12:46:43] {3072} INFO -  at 31.6s,	estimator xgboost's best error=4.3999,	best estimator xgboost's best error=4.3999
[flaml.automl: 09-16 12:46:43] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 12:46:45] {3072} INFO -  at 33.8s,	estimator xgboost's best error=4.3999,	best estimator xgboost's best error=4.3999
[flaml.automl: 09-16 12:46:45] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 12:46:47] {3072} INFO -  at 35.9s,	estimator xgboost's best error=4.3999,	best estimator xgboost's best error=4.3999
[flaml.automl: 09-16 12:46:47] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 12:46:50] {3072} INFO -  at 38.0s,	estimator xgboost's best error=4.3999,	best estimator xgboost's best error=4.3999
[flaml.automl: 09-16 12:46:50] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 12:47:00] {3072} INFO -  at 48.8s,	estimator xgboost's best error=4.1912,	best estimator xgboost's best error=4.1912
[flaml.automl: 09-16 12:47:00] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 12:47:04] {3072} INFO -  at 52.9s,	estimator xgboost's best error=4.1912,	best estimator xgboost's best error=4.1912
[flaml.automl: 09-16 12:47:15] {3335} INFO - retrain xgboost for 10.8s
[flaml.automl: 09-16 12:47:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 12:47:15] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:47:15] {2637} INFO - Time taken to find the best model: 48.788108348846436
[flaml.automl: 09-16 12:47:15] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 54555}
PM2.5(0)最佳损失：-3.1911672999936727
PM2.5(0)最好结果：{'pred_time': 6.74765482077855e-06, 'wall_clock_time': 48.788108348846436, 'metric_for_logging': {'pred_time': 6.74765482077855e-06}, 'val_loss': 4.191167299993673, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 54555}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 54555, 'experiment_tag': 'exp', 'time_total_s': 10.774931192398071}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9202235207189835
PM2.5(0)的mse=35.756187781986775
PM2.5(0)的mae=4.106311654605662
PM2.5(0)的mar=0.17463723457171126
总共花费的时间为：64.51
镇江市
3287A
[flaml.automl: 09-16 12:50:20] {2390} INFO - task = regression
[flaml.automl: 09-16 12:50:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:50:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:50:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:50:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:50:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:50:21] {3025} INFO - Estimated sufficient time budget=12054s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 12:50:21] {3072} INFO -  at 1.3s,	estimator xgboost's best error=22.0684,	best estimator xgboost's best error=22.0684
[flaml.automl: 09-16 12:50:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:50:23] {3072} INFO -  at 3.1s,	estimator xgboost's best error=11.8108,	best estimator xgboost's best error=11.8108
[flaml.automl: 09-16 12:50:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:50:24] {3072} INFO -  at 4.3s,	estimator xgboost's best error=11.8108,	best estimator xgboost's best error=11.8108
[flaml.automl: 09-16 12:50:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:50:34] {3072} INFO -  at 14.5s,	estimator xgboost's best error=11.8108,	best estimator xgboost's best error=11.8108
[flaml.automl: 09-16 12:50:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:50:36] {3072} INFO -  at 16.6s,	estimator xgboost's best error=6.0547,	best estimator xgboost's best error=6.0547
[flaml.automl: 09-16 12:50:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:50:39] {3072} INFO -  at 19.5s,	estimator xgboost's best error=4.8093,	best estimator xgboost's best error=4.8093
[flaml.automl: 09-16 12:50:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:50:42] {3072} INFO -  at 22.4s,	estimator xgboost's best error=4.8093,	best estimator xgboost's best error=4.8093
[flaml.automl: 09-16 12:50:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:50:46] {3072} INFO -  at 26.4s,	estimator xgboost's best error=4.8093,	best estimator xgboost's best error=4.8093
[flaml.automl: 09-16 12:50:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:50:48] {3072} INFO -  at 28.5s,	estimator xgboost's best error=4.4559,	best estimator xgboost's best error=4.4559
[flaml.automl: 09-16 12:50:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:50:51] {3072} INFO -  at 31.9s,	estimator xgboost's best error=4.4445,	best estimator xgboost's best error=4.4445
[flaml.automl: 09-16 12:50:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:50:54] {3072} INFO -  at 34.0s,	estimator xgboost's best error=4.4445,	best estimator xgboost's best error=4.4445
[flaml.automl: 09-16 12:50:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:50:56] {3072} INFO -  at 36.3s,	estimator xgboost's best error=4.4445,	best estimator xgboost's best error=4.4445
[flaml.automl: 09-16 12:50:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:51:07] {3072} INFO -  at 47.7s,	estimator xgboost's best error=3.9548,	best estimator xgboost's best error=3.9548
[flaml.automl: 09-16 12:51:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:51:19] {3072} INFO -  at 59.5s,	estimator xgboost's best error=3.8672,	best estimator xgboost's best error=3.8672
[flaml.automl: 09-16 12:51:35] {3335} INFO - retrain xgboost for 15.9s
[flaml.automl: 09-16 12:51:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8905849698315815, colsample_bynode=1,
             colsample_bytree=0.6049377986101554, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.003558383396019164, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0068159919467112124, reg_lambda=2.986135233507987,
             scale_pos_weight=1, subsample=0.6504687750256385,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 12:51:35] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:51:35] {2637} INFO - Time taken to find the best model: 59.51302742958069
[flaml.automl: 09-16 12:51:35] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 17, 'max_leaves': 11, 'min_child_weight': 0.003558383396019164, 'learning_rate': 0.5408133424638543, 'subsample': 0.6504687750256385, 'colsample_bylevel': 0.8905849698315815, 'colsample_bytree': 0.6049377986101554, 'reg_alpha': 0.0068159919467112124, 'reg_lambda': 2.986135233507987}
PM2.5(0)最佳损失：-2.867208771522976
PM2.5(0)最好结果：{'pred_time': 6.0747965818123793e-05, 'wall_clock_time': 59.51302742958069, 'metric_for_logging': {'pred_time': 6.0747965818123793e-05}, 'val_loss': 3.867208771522976, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 11, 'min_child_weight': 0.003558383396019164, 'learning_rate': 0.5408133424638543, 'subsample': 0.6504687750256385, 'colsample_bylevel': 0.8905849698315815, 'colsample_bytree': 0.6049377986101554, 'reg_alpha': 0.0068159919467112124, 'reg_lambda': 2.986135233507987}, 'config/n_estimators': 17, 'config/max_leaves': 11, 'config/min_child_weight': 0.003558383396019164, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.6504687750256385, 'config/colsample_bylevel': 0.8905849698315815, 'config/colsample_bytree': 0.6049377986101554, 'config/reg_alpha': 0.0068159919467112124, 'config/reg_lambda': 2.986135233507987, 'experiment_tag': 'exp', 'time_total_s': 11.764529705047607}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8905849698315815, colsample_bynode=1,
             colsample_bytree=0.6049377986101554, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.003558383396019164, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0068159919467112124, reg_lambda=2.986135233507987,
             scale_pos_weight=1, subsample=0.6504687750256385,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9351055858962071
PM2.5(0)的mse=28.498817600890174
PM2.5(0)的mae=3.6705867493756097
PM2.5(0)的mar=0.13164015498829573
总共花费的时间为：75.66
泰州市
1206A
1207A
3295A
3435A
[flaml.automl: 09-16 13:04:47] {2390} INFO - task = regression
[flaml.automl: 09-16 13:04:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:04:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:04:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:04:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:04:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:04:48] {3025} INFO - Estimated sufficient time budget=51262s. Estimated necessary time budget=51s.
[flaml.automl: 09-16 13:04:48] {3072} INFO -  at 1.4s,	estimator xgboost's best error=21.1048,	best estimator xgboost's best error=21.1048
[flaml.automl: 09-16 13:04:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:04:50] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.9392,	best estimator xgboost's best error=9.9392
[flaml.automl: 09-16 13:04:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:04:52] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.9392,	best estimator xgboost's best error=9.9392
[flaml.automl: 09-16 13:04:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:04:58] {3072} INFO -  at 10.9s,	estimator xgboost's best error=9.9392,	best estimator xgboost's best error=9.9392
[flaml.automl: 09-16 13:04:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:04:59] {3072} INFO -  at 12.1s,	estimator xgboost's best error=6.4434,	best estimator xgboost's best error=6.4434
[flaml.automl: 09-16 13:04:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:05:01] {3072} INFO -  at 13.6s,	estimator xgboost's best error=6.4047,	best estimator xgboost's best error=6.4047
[flaml.automl: 09-16 13:05:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:05:02] {3072} INFO -  at 15.3s,	estimator xgboost's best error=5.3170,	best estimator xgboost's best error=5.3170
[flaml.automl: 09-16 13:05:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:05:05] {3072} INFO -  at 17.9s,	estimator xgboost's best error=5.3170,	best estimator xgboost's best error=5.3170
[flaml.automl: 09-16 13:05:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:05:07] {3072} INFO -  at 19.6s,	estimator xgboost's best error=5.3170,	best estimator xgboost's best error=5.3170
[flaml.automl: 09-16 13:05:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:05:10] {3072} INFO -  at 22.6s,	estimator xgboost's best error=5.1821,	best estimator xgboost's best error=5.1821
[flaml.automl: 09-16 13:05:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:05:11] {3072} INFO -  at 24.2s,	estimator xgboost's best error=5.1821,	best estimator xgboost's best error=5.1821
[flaml.automl: 09-16 13:05:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:05:12] {3072} INFO -  at 25.3s,	estimator xgboost's best error=5.1821,	best estimator xgboost's best error=5.1821
[flaml.automl: 09-16 13:05:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:05:16] {3072} INFO -  at 29.1s,	estimator xgboost's best error=4.9133,	best estimator xgboost's best error=4.9133
[flaml.automl: 09-16 13:05:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:05:19] {3072} INFO -  at 32.0s,	estimator xgboost's best error=4.9133,	best estimator xgboost's best error=4.9133
[flaml.automl: 09-16 13:05:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:05:22] {3072} INFO -  at 34.6s,	estimator xgboost's best error=4.8760,	best estimator xgboost's best error=4.8760
[flaml.automl: 09-16 13:05:22] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 13:05:24] {3072} INFO -  at 36.8s,	estimator xgboost's best error=4.8760,	best estimator xgboost's best error=4.8760
[flaml.automl: 09-16 13:05:24] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 13:05:26] {3072} INFO -  at 39.0s,	estimator xgboost's best error=4.7156,	best estimator xgboost's best error=4.7156
[flaml.automl: 09-16 13:05:26] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 13:05:28] {3072} INFO -  at 40.6s,	estimator xgboost's best error=4.7156,	best estimator xgboost's best error=4.7156
[flaml.automl: 09-16 13:05:28] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 13:05:29] {3072} INFO -  at 42.3s,	estimator xgboost's best error=4.7156,	best estimator xgboost's best error=4.7156
[flaml.automl: 09-16 13:05:29] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 13:05:31] {3072} INFO -  at 43.9s,	estimator xgboost's best error=4.7156,	best estimator xgboost's best error=4.7156
[flaml.automl: 09-16 13:05:31] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 13:05:32] {3072} INFO -  at 45.4s,	estimator xgboost's best error=4.6921,	best estimator xgboost's best error=4.6921
[flaml.automl: 09-16 13:05:32] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 13:05:35] {3072} INFO -  at 48.1s,	estimator xgboost's best error=4.6921,	best estimator xgboost's best error=4.6921
[flaml.automl: 09-16 13:05:35] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 13:05:36] {3072} INFO -  at 49.0s,	estimator xgboost's best error=4.6921,	best estimator xgboost's best error=4.6921
[flaml.automl: 09-16 13:05:36] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-16 13:05:39] {3072} INFO -  at 52.3s,	estimator xgboost's best error=4.6747,	best estimator xgboost's best error=4.6747
[flaml.automl: 09-16 13:05:39] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-16 13:05:41] {3072} INFO -  at 53.8s,	estimator xgboost's best error=4.6747,	best estimator xgboost's best error=4.6747
[flaml.automl: 09-16 13:05:41] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-16 13:05:42] {3072} INFO -  at 54.9s,	estimator xgboost's best error=4.6747,	best estimator xgboost's best error=4.6747
[flaml.automl: 09-16 13:05:42] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-16 13:05:53] {3072} INFO -  at 65.7s,	estimator xgboost's best error=4.6747,	best estimator xgboost's best error=4.6747
[flaml.automl: 09-16 13:06:45] {3335} INFO - retrain xgboost for 52.7s
[flaml.automl: 09-16 13:06:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 13:06:45] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:06:45] {2637} INFO - Time taken to find the best model: 52.29463601112366
[flaml.automl: 09-16 13:06:45] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}
PM2.5(0)最佳损失：-3.6746916196067927
PM2.5(0)最好结果：{'pred_time': 8.88558006932086e-06, 'wall_clock_time': 52.29463601112366, 'metric_for_logging': {'pred_time': 8.88558006932086e-06}, 'val_loss': 4.674691619606793, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}, 'config/n_estimators': 15, 'config/max_leaves': 57, 'config/min_child_weight': 0.0011006053677836264, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532848210950454, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.0763912614998318, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.326716661453247}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9035163437096828
PM2.5(0)的mse=39.648643943265725
PM2.5(0)的mae=4.449469348986263
PM2.5(0)的mar=0.2020844011258909
总共花费的时间为：119.17
淮安市
1210A
1211A
1213A
1214A
3426A
[flaml.automl: 09-16 13:22:40] {2390} INFO - task = regression
[flaml.automl: 09-16 13:22:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:22:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:22:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:22:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:22:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:22:42] {3025} INFO - Estimated sufficient time budget=63956s. Estimated necessary time budget=64s.
[flaml.automl: 09-16 13:22:42] {3072} INFO -  at 1.4s,	estimator xgboost's best error=21.4863,	best estimator xgboost's best error=21.4863
[flaml.automl: 09-16 13:22:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:22:44] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.0097,	best estimator xgboost's best error=10.0097
[flaml.automl: 09-16 13:22:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:22:45] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.0097,	best estimator xgboost's best error=10.0097
[flaml.automl: 09-16 13:22:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:22:50] {3072} INFO -  at 9.5s,	estimator xgboost's best error=10.0097,	best estimator xgboost's best error=10.0097
[flaml.automl: 09-16 13:22:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:22:51] {3072} INFO -  at 10.7s,	estimator xgboost's best error=6.1965,	best estimator xgboost's best error=6.1965
[flaml.automl: 09-16 13:22:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:22:52] {3072} INFO -  at 12.2s,	estimator xgboost's best error=6.1965,	best estimator xgboost's best error=6.1965
[flaml.automl: 09-16 13:22:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:22:54] {3072} INFO -  at 13.9s,	estimator xgboost's best error=4.5707,	best estimator xgboost's best error=4.5707
[flaml.automl: 09-16 13:22:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:22:57] {3072} INFO -  at 16.6s,	estimator xgboost's best error=4.5707,	best estimator xgboost's best error=4.5707
[flaml.automl: 09-16 13:22:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:22:58] {3072} INFO -  at 18.2s,	estimator xgboost's best error=4.5707,	best estimator xgboost's best error=4.5707
[flaml.automl: 09-16 13:22:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:23:01] {3072} INFO -  at 21.2s,	estimator xgboost's best error=4.5707,	best estimator xgboost's best error=4.5707
[flaml.automl: 09-16 13:23:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:23:03] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.5707,	best estimator xgboost's best error=4.5707
[flaml.automl: 09-16 13:23:03] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:23:05] {3072} INFO -  at 24.4s,	estimator xgboost's best error=4.5618,	best estimator xgboost's best error=4.5618
[flaml.automl: 09-16 13:23:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:23:06] {3072} INFO -  at 25.6s,	estimator xgboost's best error=4.5618,	best estimator xgboost's best error=4.5618
[flaml.automl: 09-16 13:23:06] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:23:13] {3072} INFO -  at 32.5s,	estimator xgboost's best error=3.6975,	best estimator xgboost's best error=3.6975
[flaml.automl: 09-16 13:23:13] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:23:25] {3072} INFO -  at 45.2s,	estimator xgboost's best error=3.6142,	best estimator xgboost's best error=3.6142
[flaml.automl: 09-16 13:23:25] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 13:23:32] {3072} INFO -  at 52.1s,	estimator xgboost's best error=3.6142,	best estimator xgboost's best error=3.6142
[flaml.automl: 09-16 13:23:45] {3335} INFO - retrain xgboost for 12.6s
[flaml.automl: 09-16 13:23:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:23:45] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:23:45] {2637} INFO - Time taken to find the best model: 45.15985560417175
[flaml.automl: 09-16 13:23:45] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 53370}
PM2.5(0)最佳损失：-2.6141788866242153
PM2.5(0)最好结果：{'pred_time': 7.449344486435315e-06, 'wall_clock_time': 45.15985560417175, 'metric_for_logging': {'pred_time': 7.449344486435315e-06}, 'val_loss': 3.6141788866242153, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 53370}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 53370, 'experiment_tag': 'exp', 'time_total_s': 12.622220039367676}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9411074408826214
PM2.5(0)的mse=34.53359248239058
PM2.5(0)的mae=3.7518123103562906
PM2.5(0)的mar=0.13843518107668393
总共花费的时间为：65.61
盐城市
1215A
1216A
3293A
3436A
[flaml.automl: 09-16 13:36:04] {2390} INFO - task = regression
[flaml.automl: 09-16 13:36:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:36:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:36:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:36:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:36:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:36:05] {3025} INFO - Estimated sufficient time budget=51611s. Estimated necessary time budget=52s.
[flaml.automl: 09-16 13:36:05] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.4506,	best estimator xgboost's best error=17.4506
[flaml.automl: 09-16 13:36:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:36:07] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.4095,	best estimator xgboost's best error=8.4095
[flaml.automl: 09-16 13:36:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:36:09] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.4095,	best estimator xgboost's best error=8.4095
[flaml.automl: 09-16 13:36:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:36:15] {3072} INFO -  at 11.0s,	estimator xgboost's best error=8.4095,	best estimator xgboost's best error=8.4095
[flaml.automl: 09-16 13:36:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:36:16] {3072} INFO -  at 12.1s,	estimator xgboost's best error=5.5084,	best estimator xgboost's best error=5.5084
[flaml.automl: 09-16 13:36:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:36:18] {3072} INFO -  at 13.7s,	estimator xgboost's best error=5.2215,	best estimator xgboost's best error=5.2215
[flaml.automl: 09-16 13:36:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:36:19] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.2401,	best estimator xgboost's best error=4.2401
[flaml.automl: 09-16 13:36:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:36:22] {3072} INFO -  at 18.0s,	estimator xgboost's best error=4.2401,	best estimator xgboost's best error=4.2401
[flaml.automl: 09-16 13:36:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:36:24] {3072} INFO -  at 19.7s,	estimator xgboost's best error=4.1664,	best estimator xgboost's best error=4.1664
[flaml.automl: 09-16 13:36:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:36:27] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.0979,	best estimator xgboost's best error=4.0979
[flaml.automl: 09-16 13:36:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:36:28] {3072} INFO -  at 23.8s,	estimator xgboost's best error=4.0979,	best estimator xgboost's best error=4.0979
[flaml.automl: 09-16 13:36:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:36:31] {3072} INFO -  at 27.6s,	estimator xgboost's best error=3.7319,	best estimator xgboost's best error=3.7319
[flaml.automl: 09-16 13:36:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:36:35] {3072} INFO -  at 31.1s,	estimator xgboost's best error=3.5786,	best estimator xgboost's best error=3.5786
[flaml.automl: 09-16 13:36:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:36:38] {3072} INFO -  at 33.9s,	estimator xgboost's best error=3.5786,	best estimator xgboost's best error=3.5786
[flaml.automl: 09-16 13:36:38] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:36:40] {3072} INFO -  at 36.4s,	estimator xgboost's best error=3.5786,	best estimator xgboost's best error=3.5786
[flaml.automl: 09-16 13:36:40] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 13:36:43] {3072} INFO -  at 39.0s,	estimator xgboost's best error=3.3904,	best estimator xgboost's best error=3.3904
[flaml.automl: 09-16 13:36:43] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 13:36:45] {3072} INFO -  at 41.3s,	estimator xgboost's best error=3.3904,	best estimator xgboost's best error=3.3904
[flaml.automl: 09-16 13:36:45] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 13:36:47] {3072} INFO -  at 43.3s,	estimator xgboost's best error=3.3904,	best estimator xgboost's best error=3.3904
[flaml.automl: 09-16 13:36:47] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 13:36:49] {3072} INFO -  at 45.2s,	estimator xgboost's best error=3.3904,	best estimator xgboost's best error=3.3904
[flaml.automl: 09-16 13:36:49] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 13:36:50] {3072} INFO -  at 46.1s,	estimator xgboost's best error=3.3904,	best estimator xgboost's best error=3.3904
[flaml.automl: 09-16 13:36:50] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 13:37:04] {3072} INFO -  at 60.3s,	estimator xgboost's best error=3.2001,	best estimator xgboost's best error=3.2001
[flaml.automl: 09-16 13:37:26] {3335} INFO - retrain xgboost for 21.6s
[flaml.automl: 09-16 13:37:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.902249106663565, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5508049883873372,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.0017454776322958724, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003919588748975853, reg_lambda=0.27794049904081597,
             scale_pos_weight=1, subsample=0.6989222769179986,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 13:37:26] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:37:26] {2637} INFO - Time taken to find the best model: 60.325074434280396
[flaml.automl: 09-16 13:37:26] {2648} WARNING - Time taken to find the best model is 101% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.0017454776322958724, 'learning_rate': 0.5508049883873372, 'subsample': 0.6989222769179986, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.902249106663565, 'reg_alpha': 0.003919588748975853, 'reg_lambda': 0.27794049904081597, 'FLAML_sample_size': 42979}
PM2.5(0)最佳损失：-2.2000806654617815
PM2.5(0)最好结果：{'pred_time': 1.7459069065113163e-05, 'wall_clock_time': 60.325074434280396, 'metric_for_logging': {'pred_time': 1.7459069065113163e-05}, 'val_loss': 3.2000806654617815, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.0017454776322958724, 'learning_rate': 0.5508049883873372, 'subsample': 0.6989222769179986, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.902249106663565, 'reg_alpha': 0.003919588748975853, 'reg_lambda': 0.27794049904081597, 'FLAML_sample_size': 42979}, 'config/n_estimators': 35, 'config/max_leaves': 7, 'config/min_child_weight': 0.0017454776322958724, 'config/learning_rate': 0.5508049883873372, 'config/subsample': 0.6989222769179986, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.902249106663565, 'config/reg_alpha': 0.003919588748975853, 'config/reg_lambda': 0.27794049904081597, 'config/FLAML_sample_size': 42979, 'experiment_tag': 'exp', 'time_total_s': 14.193127870559692}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.902249106663565, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5508049883873372,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.0017454776322958724, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003919588748975853, reg_lambda=0.27794049904081597,
             scale_pos_weight=1, subsample=0.6989222769179986,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9309215236396952
PM2.5(0)的mse=28.02847813733443
PM2.5(0)的mae=3.333507185896979
PM2.5(0)的mar=0.1733462733476463
总共花费的时间为：82.73
宿迁市
3191A
[flaml.automl: 09-16 13:40:37] {2390} INFO - task = regression
[flaml.automl: 09-16 13:40:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:40:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:40:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:40:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:40:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:40:41] {3025} INFO - Estimated sufficient time budget=32785s. Estimated necessary time budget=33s.
[flaml.automl: 09-16 13:40:41] {3072} INFO -  at 3.3s,	estimator xgboost's best error=24.8582,	best estimator xgboost's best error=24.8582
[flaml.automl: 09-16 13:40:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:40:46] {3072} INFO -  at 8.3s,	estimator xgboost's best error=13.7443,	best estimator xgboost's best error=13.7443
[flaml.automl: 09-16 13:40:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:40:49] {3072} INFO -  at 11.5s,	estimator xgboost's best error=13.7443,	best estimator xgboost's best error=13.7443
[flaml.automl: 09-16 13:40:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:41:06] {3072} INFO -  at 28.4s,	estimator xgboost's best error=13.7443,	best estimator xgboost's best error=13.7443
[flaml.automl: 09-16 13:41:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:41:08] {3072} INFO -  at 30.5s,	estimator xgboost's best error=7.7946,	best estimator xgboost's best error=7.7946
[flaml.automl: 09-16 13:41:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:41:11] {3072} INFO -  at 33.8s,	estimator xgboost's best error=6.1195,	best estimator xgboost's best error=6.1195
[flaml.automl: 09-16 13:41:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:41:16] {3072} INFO -  at 38.3s,	estimator xgboost's best error=6.1195,	best estimator xgboost's best error=6.1195
[flaml.automl: 09-16 13:41:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:41:22] {3072} INFO -  at 44.3s,	estimator xgboost's best error=6.1195,	best estimator xgboost's best error=6.1195
[flaml.automl: 09-16 13:41:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:41:25] {3072} INFO -  at 47.4s,	estimator xgboost's best error=5.8660,	best estimator xgboost's best error=5.8660
[flaml.automl: 09-16 13:41:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:41:30] {3072} INFO -  at 52.4s,	estimator xgboost's best error=5.8660,	best estimator xgboost's best error=5.8660
[flaml.automl: 09-16 13:41:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:41:33] {3072} INFO -  at 55.4s,	estimator xgboost's best error=5.8660,	best estimator xgboost's best error=5.8660
[flaml.automl: 09-16 13:41:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:41:36] {3072} INFO -  at 58.5s,	estimator xgboost's best error=5.8660,	best estimator xgboost's best error=5.8660
[flaml.automl: 09-16 13:41:39] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-16 13:41:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6653719127866307, colsample_bynode=1,
             colsample_bytree=0.5872814598333542, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=4, min_child_weight=0.6008315285021533,
             missing=nan, monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.008482962580746356, reg_lambda=83.39200099767129,
             scale_pos_weight=1, subsample=0.8098031956057953,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 13:41:39] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:41:39] {2637} INFO - Time taken to find the best model: 47.43873643875122
[flaml.automl: 09-16 13:41:39] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 0.6008315285021533, 'learning_rate': 1.0, 'subsample': 0.8098031956057953, 'colsample_bylevel': 0.6653719127866307, 'colsample_bytree': 0.5872814598333542, 'reg_alpha': 0.008482962580746356, 'reg_lambda': 83.39200099767129}
PM2.5(0)最佳损失：-4.866012291343643
PM2.5(0)最好结果：{'pred_time': 9.400080068418458e-05, 'wall_clock_time': 47.43873643875122, 'metric_for_logging': {'pred_time': 9.400080068418458e-05}, 'val_loss': 5.866012291343643, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 0.6008315285021533, 'learning_rate': 1.0, 'subsample': 0.8098031956057953, 'colsample_bylevel': 0.6653719127866307, 'colsample_bytree': 0.5872814598333542, 'reg_alpha': 0.008482962580746356, 'reg_lambda': 83.39200099767129}, 'config/n_estimators': 4, 'config/max_leaves': 4, 'config/min_child_weight': 0.6008315285021533, 'config/learning_rate': 1.0, 'config/subsample': 0.8098031956057953, 'config/colsample_bylevel': 0.6653719127866307, 'config/colsample_bytree': 0.5872814598333542, 'config/reg_alpha': 0.008482962580746356, 'config/reg_lambda': 83.39200099767129, 'experiment_tag': 'exp', 'time_total_s': 3.1189777851104736}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6653719127866307, colsample_bynode=1,
             colsample_bytree=0.5872814598333542, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=4, min_child_weight=0.6008315285021533,
             missing=nan, monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.008482962580746356, reg_lambda=83.39200099767129,
             scale_pos_weight=1, subsample=0.8098031956057953,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8778901855407767
PM2.5(0)的mse=74.08894406866798
PM2.5(0)的mae=6.395994765285388
PM2.5(0)的mar=0.3217204806680908
总共花费的时间为：61.79
杭州市
1227A
1231A
1232A
3557A
3558A
3656A
[flaml.automl: 09-16 14:00:52] {2390} INFO - task = regression
[flaml.automl: 09-16 14:00:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:00:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:00:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:00:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:00:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:00:53] {3025} INFO - Estimated sufficient time budget=80994s. Estimated necessary time budget=81s.
[flaml.automl: 09-16 14:00:53] {3072} INFO -  at 1.5s,	estimator xgboost's best error=18.1074,	best estimator xgboost's best error=18.1074
[flaml.automl: 09-16 14:00:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:00:55] {3072} INFO -  at 3.6s,	estimator xgboost's best error=8.5634,	best estimator xgboost's best error=8.5634
[flaml.automl: 09-16 14:00:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:00:56] {3072} INFO -  at 4.8s,	estimator xgboost's best error=8.5634,	best estimator xgboost's best error=8.5634
[flaml.automl: 09-16 14:00:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:01:00] {3072} INFO -  at 8.6s,	estimator xgboost's best error=8.5634,	best estimator xgboost's best error=8.5634
[flaml.automl: 09-16 14:01:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:01:01] {3072} INFO -  at 9.7s,	estimator xgboost's best error=5.5365,	best estimator xgboost's best error=5.5365
[flaml.automl: 09-16 14:01:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:01:03] {3072} INFO -  at 11.3s,	estimator xgboost's best error=5.3156,	best estimator xgboost's best error=5.3156
[flaml.automl: 09-16 14:01:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:01:04] {3072} INFO -  at 12.9s,	estimator xgboost's best error=4.3123,	best estimator xgboost's best error=4.3123
[flaml.automl: 09-16 14:01:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:01:07] {3072} INFO -  at 15.6s,	estimator xgboost's best error=4.3123,	best estimator xgboost's best error=4.3123
[flaml.automl: 09-16 14:01:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:01:09] {3072} INFO -  at 17.2s,	estimator xgboost's best error=4.3123,	best estimator xgboost's best error=4.3123
[flaml.automl: 09-16 14:01:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:01:12] {3072} INFO -  at 20.3s,	estimator xgboost's best error=4.2758,	best estimator xgboost's best error=4.2758
[flaml.automl: 09-16 14:01:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:01:13] {3072} INFO -  at 21.9s,	estimator xgboost's best error=4.2758,	best estimator xgboost's best error=4.2758
[flaml.automl: 09-16 14:01:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:01:14] {3072} INFO -  at 23.0s,	estimator xgboost's best error=4.2758,	best estimator xgboost's best error=4.2758
[flaml.automl: 09-16 14:01:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:01:16] {3072} INFO -  at 25.0s,	estimator xgboost's best error=4.2758,	best estimator xgboost's best error=4.2758
[flaml.automl: 09-16 14:01:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:01:19] {3072} INFO -  at 27.3s,	estimator xgboost's best error=4.2758,	best estimator xgboost's best error=4.2758
[flaml.automl: 09-16 14:01:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 14:01:22] {3072} INFO -  at 30.3s,	estimator xgboost's best error=3.9295,	best estimator xgboost's best error=3.9295
[flaml.automl: 09-16 14:01:22] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 14:01:27] {3072} INFO -  at 35.3s,	estimator xgboost's best error=3.9295,	best estimator xgboost's best error=3.9295
[flaml.automl: 09-16 14:01:27] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 14:01:29] {3072} INFO -  at 37.2s,	estimator xgboost's best error=3.8895,	best estimator xgboost's best error=3.8895
[flaml.automl: 09-16 14:01:29] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 14:01:30] {3072} INFO -  at 38.9s,	estimator xgboost's best error=3.8895,	best estimator xgboost's best error=3.8895
[flaml.automl: 09-16 14:01:30] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 14:01:35] {3072} INFO -  at 43.2s,	estimator xgboost's best error=3.7186,	best estimator xgboost's best error=3.7186
[flaml.automl: 09-16 14:01:35] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 14:01:36] {3072} INFO -  at 44.9s,	estimator xgboost's best error=3.7186,	best estimator xgboost's best error=3.7186
[flaml.automl: 09-16 14:01:36] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 14:01:50] {3072} INFO -  at 58.2s,	estimator xgboost's best error=3.4060,	best estimator xgboost's best error=3.4060
[flaml.automl: 09-16 14:02:03] {3335} INFO - retrain xgboost for 13.3s
[flaml.automl: 09-16 14:02:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:02:03] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:02:03] {2637} INFO - Time taken to find the best model: 58.20335531234741
[flaml.automl: 09-16 14:02:03] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 66753}
PM2.5(0)最佳损失：-2.4059742216064013
PM2.5(0)最好结果：{'pred_time': 5.4504261340822415e-06, 'wall_clock_time': 58.20335531234741, 'metric_for_logging': {'pred_time': 5.4504261340822415e-06}, 'val_loss': 3.4059742216064013, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 66753}, 'config/n_estimators': 13, 'config/max_leaves': 18, 'config/min_child_weight': 0.002212987346630184, 'config/learning_rate': 1.0, 'config/subsample': 0.9736629142498529, 'config/colsample_bylevel': 0.8717925041332704, 'config/colsample_bytree': 0.9857450191529009, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.04619111740498511, 'config/FLAML_sample_size': 66753, 'experiment_tag': 'exp', 'time_total_s': 13.280301332473755}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9110347013492064
PM2.5(0)的mse=26.994359150018873
PM2.5(0)的mae=3.385443527449453
PM2.5(0)的mar=0.15447516957450813
总共花费的时间为：72.49
宁波市
1235A
1236A
1239A
1240A
2871A
3710A
[flaml.automl: 09-16 14:20:31] {2390} INFO - task = regression
[flaml.automl: 09-16 14:20:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:20:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:20:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:20:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:20:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:20:32] {3025} INFO - Estimated sufficient time budget=78507s. Estimated necessary time budget=79s.
[flaml.automl: 09-16 14:20:32] {3072} INFO -  at 1.5s,	estimator xgboost's best error=13.2635,	best estimator xgboost's best error=13.2635
[flaml.automl: 09-16 14:20:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:20:34] {3072} INFO -  at 3.6s,	estimator xgboost's best error=6.4591,	best estimator xgboost's best error=6.4591
[flaml.automl: 09-16 14:20:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:20:35] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.4591,	best estimator xgboost's best error=6.4591
[flaml.automl: 09-16 14:20:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:20:39] {3072} INFO -  at 8.5s,	estimator xgboost's best error=6.4591,	best estimator xgboost's best error=6.4591
[flaml.automl: 09-16 14:20:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:20:40] {3072} INFO -  at 9.7s,	estimator xgboost's best error=4.3094,	best estimator xgboost's best error=4.3094
[flaml.automl: 09-16 14:20:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:20:42] {3072} INFO -  at 11.2s,	estimator xgboost's best error=4.3094,	best estimator xgboost's best error=4.3094
[flaml.automl: 09-16 14:20:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:20:43] {3072} INFO -  at 12.9s,	estimator xgboost's best error=3.2663,	best estimator xgboost's best error=3.2663
[flaml.automl: 09-16 14:20:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:20:46] {3072} INFO -  at 15.6s,	estimator xgboost's best error=3.2663,	best estimator xgboost's best error=3.2663
[flaml.automl: 09-16 14:20:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:20:48] {3072} INFO -  at 17.3s,	estimator xgboost's best error=3.2663,	best estimator xgboost's best error=3.2663
[flaml.automl: 09-16 14:20:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:20:51] {3072} INFO -  at 20.3s,	estimator xgboost's best error=3.2663,	best estimator xgboost's best error=3.2663
[flaml.automl: 09-16 14:20:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:20:52] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.2663,	best estimator xgboost's best error=3.2663
[flaml.automl: 09-16 14:20:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:20:54] {3072} INFO -  at 23.4s,	estimator xgboost's best error=3.2661,	best estimator xgboost's best error=3.2661
[flaml.automl: 09-16 14:20:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:20:55] {3072} INFO -  at 24.6s,	estimator xgboost's best error=3.2661,	best estimator xgboost's best error=3.2661
[flaml.automl: 09-16 14:20:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:21:02] {3072} INFO -  at 31.7s,	estimator xgboost's best error=2.8365,	best estimator xgboost's best error=2.8365
[flaml.automl: 09-16 14:21:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 14:21:15] {3072} INFO -  at 44.5s,	estimator xgboost's best error=2.7711,	best estimator xgboost's best error=2.7711
[flaml.automl: 09-16 14:21:15] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 14:21:22] {3072} INFO -  at 51.6s,	estimator xgboost's best error=2.7711,	best estimator xgboost's best error=2.7711
[flaml.automl: 09-16 14:21:35] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-16 14:21:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:21:35] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:21:35] {2637} INFO - Time taken to find the best model: 44.50964593887329
[flaml.automl: 09-16 14:21:35] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 66196}
PM2.5(0)最佳损失：-1.7711375769128481
PM2.5(0)最好结果：{'pred_time': 5.57402283552874e-06, 'wall_clock_time': 44.50964593887329, 'metric_for_logging': {'pred_time': 5.57402283552874e-06}, 'val_loss': 2.771137576912848, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 66196}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 66196, 'experiment_tag': 'exp', 'time_total_s': 12.857409954071045}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8932245771458034
PM2.5(0)的mse=20.275009648741804
PM2.5(0)的mae=2.8537967131535456
PM2.5(0)的mar=0.1706110719019184
总共花费的时间为：65.51
温州市
1242A
1243A
1244A
[flaml.automl: 09-16 14:30:54] {2390} INFO - task = regression
[flaml.automl: 09-16 14:30:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:30:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:30:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:30:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:30:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:30:56] {3025} INFO - Estimated sufficient time budget=12146s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 14:30:56] {3072} INFO -  at 1.3s,	estimator xgboost's best error=15.3100,	best estimator xgboost's best error=15.3100
[flaml.automl: 09-16 14:30:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:30:58] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.2566,	best estimator xgboost's best error=7.2566
[flaml.automl: 09-16 14:30:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:30:59] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.2566,	best estimator xgboost's best error=7.2566
[flaml.automl: 09-16 14:30:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:31:09] {3072} INFO -  at 14.7s,	estimator xgboost's best error=7.2566,	best estimator xgboost's best error=7.2566
[flaml.automl: 09-16 14:31:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:31:10] {3072} INFO -  at 15.8s,	estimator xgboost's best error=5.4105,	best estimator xgboost's best error=5.4105
[flaml.automl: 09-16 14:31:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:31:12] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.7145,	best estimator xgboost's best error=4.7145
[flaml.automl: 09-16 14:31:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:31:13] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.1656,	best estimator xgboost's best error=4.1656
[flaml.automl: 09-16 14:31:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:31:16] {3072} INFO -  at 21.8s,	estimator xgboost's best error=4.1656,	best estimator xgboost's best error=4.1656
[flaml.automl: 09-16 14:31:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:31:18] {3072} INFO -  at 23.4s,	estimator xgboost's best error=4.1656,	best estimator xgboost's best error=4.1656
[flaml.automl: 09-16 14:31:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:31:21] {3072} INFO -  at 26.4s,	estimator xgboost's best error=3.9367,	best estimator xgboost's best error=3.9367
[flaml.automl: 09-16 14:31:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:31:22] {3072} INFO -  at 28.1s,	estimator xgboost's best error=3.9367,	best estimator xgboost's best error=3.9367
[flaml.automl: 09-16 14:31:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:31:23] {3072} INFO -  at 29.2s,	estimator xgboost's best error=3.9367,	best estimator xgboost's best error=3.9367
[flaml.automl: 09-16 14:31:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:31:37] {3072} INFO -  at 42.9s,	estimator xgboost's best error=3.5664,	best estimator xgboost's best error=3.5664
[flaml.automl: 09-16 14:31:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:31:54] {3072} INFO -  at 59.4s,	estimator xgboost's best error=3.3763,	best estimator xgboost's best error=3.3763
[flaml.automl: 09-16 14:32:23] {3335} INFO - retrain xgboost for 29.2s
[flaml.automl: 09-16 14:32:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:32:23] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:32:23] {2637} INFO - Time taken to find the best model: 59.4445116519928
[flaml.automl: 09-16 14:32:23] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-2.376338409131457
PM2.5(0)最好结果：{'pred_time': 1.0843485079962632e-05, 'wall_clock_time': 59.4445116519928, 'metric_for_logging': {'pred_time': 1.0843485079962632e-05}, 'val_loss': 3.376338409131457, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.511242628097534}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8718822815718492
PM2.5(0)的mse=26.33004422986191
PM2.5(0)的mae=3.4522619495379705
PM2.5(0)的mar=0.17609927849863857
总共花费的时间为：89.35
绍兴市
2921A
3408A
3409A
3410A
3411A
3560A
3658A
[flaml.automl: 09-16 14:53:46] {2390} INFO - task = regression
[flaml.automl: 09-16 14:53:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:53:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:53:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:53:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:53:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:53:47] {3025} INFO - Estimated sufficient time budget=97651s. Estimated necessary time budget=98s.
[flaml.automl: 09-16 14:53:47] {3072} INFO -  at 1.6s,	estimator xgboost's best error=17.0482,	best estimator xgboost's best error=17.0482
[flaml.automl: 09-16 14:53:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:53:49] {3072} INFO -  at 3.7s,	estimator xgboost's best error=8.1148,	best estimator xgboost's best error=8.1148
[flaml.automl: 09-16 14:53:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:53:51] {3072} INFO -  at 4.9s,	estimator xgboost's best error=8.1148,	best estimator xgboost's best error=8.1148
[flaml.automl: 09-16 14:53:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:53:54] {3072} INFO -  at 8.2s,	estimator xgboost's best error=8.1148,	best estimator xgboost's best error=8.1148
[flaml.automl: 09-16 14:53:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:53:55] {3072} INFO -  at 9.3s,	estimator xgboost's best error=5.1158,	best estimator xgboost's best error=5.1158
[flaml.automl: 09-16 14:53:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:53:57] {3072} INFO -  at 10.9s,	estimator xgboost's best error=4.9333,	best estimator xgboost's best error=4.9333
[flaml.automl: 09-16 14:53:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:53:58] {3072} INFO -  at 12.5s,	estimator xgboost's best error=4.0680,	best estimator xgboost's best error=4.0680
[flaml.automl: 09-16 14:53:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:54:01] {3072} INFO -  at 15.2s,	estimator xgboost's best error=4.0680,	best estimator xgboost's best error=4.0680
[flaml.automl: 09-16 14:54:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:54:03] {3072} INFO -  at 16.8s,	estimator xgboost's best error=4.0680,	best estimator xgboost's best error=4.0680
[flaml.automl: 09-16 14:54:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:54:05] {3072} INFO -  at 19.4s,	estimator xgboost's best error=4.0680,	best estimator xgboost's best error=4.0680
[flaml.automl: 09-16 14:54:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:54:07] {3072} INFO -  at 20.9s,	estimator xgboost's best error=3.9501,	best estimator xgboost's best error=3.9501
[flaml.automl: 09-16 14:54:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:54:08] {3072} INFO -  at 22.0s,	estimator xgboost's best error=3.9501,	best estimator xgboost's best error=3.9501
[flaml.automl: 09-16 14:54:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:54:10] {3072} INFO -  at 24.3s,	estimator xgboost's best error=3.4597,	best estimator xgboost's best error=3.4597
[flaml.automl: 09-16 14:54:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:54:12] {3072} INFO -  at 26.5s,	estimator xgboost's best error=3.4354,	best estimator xgboost's best error=3.4354
[flaml.automl: 09-16 14:54:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 14:54:14] {3072} INFO -  at 28.3s,	estimator xgboost's best error=3.4354,	best estimator xgboost's best error=3.4354
[flaml.automl: 09-16 14:54:14] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 14:54:15] {3072} INFO -  at 29.8s,	estimator xgboost's best error=3.4354,	best estimator xgboost's best error=3.4354
[flaml.automl: 09-16 14:54:15] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 14:54:17] {3072} INFO -  at 31.6s,	estimator xgboost's best error=3.4354,	best estimator xgboost's best error=3.4354
[flaml.automl: 09-16 14:54:17] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 14:54:19] {3072} INFO -  at 33.2s,	estimator xgboost's best error=3.4354,	best estimator xgboost's best error=3.4354
[flaml.automl: 09-16 14:54:19] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 14:54:30] {3072} INFO -  at 44.0s,	estimator xgboost's best error=3.2358,	best estimator xgboost's best error=3.2358
[flaml.automl: 09-16 14:54:30] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 14:54:34] {3072} INFO -  at 48.1s,	estimator xgboost's best error=3.2358,	best estimator xgboost's best error=3.2358
[flaml.automl: 09-16 14:54:34] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 14:54:45] {3072} INFO -  at 59.4s,	estimator xgboost's best error=3.2358,	best estimator xgboost's best error=3.2358
[flaml.automl: 09-16 14:54:56] {3335} INFO - retrain xgboost for 10.8s
[flaml.automl: 09-16 14:54:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:54:56] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:54:56] {2637} INFO - Time taken to find the best model: 43.97799873352051
[flaml.automl: 09-16 14:54:56] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463, 'FLAML_sample_size': 77233}
PM2.5(0)最佳损失：-2.2358008279747077
PM2.5(0)最好结果：{'pred_time': 5.043933675495291e-06, 'wall_clock_time': 43.97799873352051, 'metric_for_logging': {'pred_time': 5.043933675495291e-06}, 'val_loss': 3.2358008279747077, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463, 'FLAML_sample_size': 77233}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 6.532042959250319, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8835265175628242, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.10065328594609463, 'config/FLAML_sample_size': 77233, 'experiment_tag': 'exp', 'time_total_s': 10.795103311538696}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8927962237862767
PM2.5(0)的mse=27.649092285246017
PM2.5(0)的mae=3.2804394925522065
PM2.5(0)的mar=0.15551351152190265
总共花费的时间为：71.45
湖州市
1250A
3562A
[flaml.automl: 09-16 15:01:27] {2390} INFO - task = regression
[flaml.automl: 09-16 15:01:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:01:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:01:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:01:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:01:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:01:29] {3025} INFO - Estimated sufficient time budget=11980s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:01:29] {3072} INFO -  at 1.3s,	estimator xgboost's best error=17.0806,	best estimator xgboost's best error=17.0806
[flaml.automl: 09-16 15:01:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:01:31] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.0360,	best estimator xgboost's best error=8.0360
[flaml.automl: 09-16 15:01:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:01:32] {3072} INFO -  at 4.6s,	estimator xgboost's best error=8.0360,	best estimator xgboost's best error=8.0360
[flaml.automl: 09-16 15:01:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:01:41] {3072} INFO -  at 14.1s,	estimator xgboost's best error=8.0360,	best estimator xgboost's best error=8.0360
[flaml.automl: 09-16 15:01:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:01:42] {3072} INFO -  at 15.2s,	estimator xgboost's best error=5.8159,	best estimator xgboost's best error=5.8159
[flaml.automl: 09-16 15:01:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:01:44] {3072} INFO -  at 16.8s,	estimator xgboost's best error=5.2159,	best estimator xgboost's best error=5.2159
[flaml.automl: 09-16 15:01:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:01:46] {3072} INFO -  at 18.4s,	estimator xgboost's best error=4.3141,	best estimator xgboost's best error=4.3141
[flaml.automl: 09-16 15:01:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:01:48] {3072} INFO -  at 21.1s,	estimator xgboost's best error=4.3141,	best estimator xgboost's best error=4.3141
[flaml.automl: 09-16 15:01:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:01:50] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.1527,	best estimator xgboost's best error=4.1527
[flaml.automl: 09-16 15:01:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:01:53] {3072} INFO -  at 25.8s,	estimator xgboost's best error=3.6981,	best estimator xgboost's best error=3.6981
[flaml.automl: 09-16 15:01:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:01:54] {3072} INFO -  at 26.9s,	estimator xgboost's best error=3.6981,	best estimator xgboost's best error=3.6981
[flaml.automl: 09-16 15:01:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:02:06] {3072} INFO -  at 38.9s,	estimator xgboost's best error=3.4690,	best estimator xgboost's best error=3.4690
[flaml.automl: 09-16 15:02:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:02:27] {3072} INFO -  at 59.4s,	estimator xgboost's best error=3.4319,	best estimator xgboost's best error=3.4319
[flaml.automl: 09-16 15:02:48] {3335} INFO - retrain xgboost for 21.9s
[flaml.automl: 09-16 15:02:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 15:02:48] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:02:48] {2637} INFO - Time taken to find the best model: 59.40259909629822
[flaml.automl: 09-16 15:02:48] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}
PM2.5(0)最佳损失：-2.4318543688810745
PM2.5(0)最好结果：{'pred_time': 1.6652764687110747e-05, 'wall_clock_time': 59.40259909629822, 'metric_for_logging': {'pred_time': 1.6652764687110747e-05}, 'val_loss': 3.4318543688810745, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.006967902231194494, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7520708370056695, 'config/colsample_bylevel': 0.948506134666318, 'config/colsample_bytree': 0.8223167301060249, 'config/reg_alpha': 0.0038027739156058313, 'config/reg_lambda': 0.5403518701157697, 'experiment_tag': 'exp', 'time_total_s': 20.53803849220276}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8898790578962787
PM2.5(0)的mse=28.840973742319314
PM2.5(0)的mae=3.510696504284458
PM2.5(0)的mar=0.16591258544480808
总共花费的时间为：81.70
嘉兴市
1253A
3407A
[flaml.automl: 09-16 15:10:00] {2390} INFO - task = regression
[flaml.automl: 09-16 15:10:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:10:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:10:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:10:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:10:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:10:02] {3025} INFO - Estimated sufficient time budget=12063s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:10:02] {3072} INFO -  at 1.3s,	estimator xgboost's best error=16.1008,	best estimator xgboost's best error=16.1008
[flaml.automl: 09-16 15:10:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:10:04] {3072} INFO -  at 3.4s,	estimator xgboost's best error=7.5662,	best estimator xgboost's best error=7.5662
[flaml.automl: 09-16 15:10:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:10:05] {3072} INFO -  at 4.6s,	estimator xgboost's best error=7.5662,	best estimator xgboost's best error=7.5662
[flaml.automl: 09-16 15:10:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:10:14] {3072} INFO -  at 14.1s,	estimator xgboost's best error=7.5662,	best estimator xgboost's best error=7.5662
[flaml.automl: 09-16 15:10:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:10:16] {3072} INFO -  at 15.2s,	estimator xgboost's best error=5.3376,	best estimator xgboost's best error=5.3376
[flaml.automl: 09-16 15:10:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:10:17] {3072} INFO -  at 16.8s,	estimator xgboost's best error=5.3376,	best estimator xgboost's best error=5.3376
[flaml.automl: 09-16 15:10:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:10:19] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.6278,	best estimator xgboost's best error=3.6278
[flaml.automl: 09-16 15:10:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:10:22] {3072} INFO -  at 21.2s,	estimator xgboost's best error=3.6278,	best estimator xgboost's best error=3.6278
[flaml.automl: 09-16 15:10:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:10:23] {3072} INFO -  at 22.9s,	estimator xgboost's best error=3.6278,	best estimator xgboost's best error=3.6278
[flaml.automl: 09-16 15:10:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:10:26] {3072} INFO -  at 25.9s,	estimator xgboost's best error=3.6278,	best estimator xgboost's best error=3.6278
[flaml.automl: 09-16 15:10:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:10:28] {3072} INFO -  at 27.6s,	estimator xgboost's best error=3.4944,	best estimator xgboost's best error=3.4944
[flaml.automl: 09-16 15:10:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:10:29] {3072} INFO -  at 28.8s,	estimator xgboost's best error=3.4944,	best estimator xgboost's best error=3.4944
[flaml.automl: 09-16 15:10:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:10:36] {3072} INFO -  at 35.3s,	estimator xgboost's best error=3.1281,	best estimator xgboost's best error=3.1281
[flaml.automl: 09-16 15:10:36] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:10:47] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.0691,	best estimator xgboost's best error=3.0691
[flaml.automl: 09-16 15:10:47] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:10:53] {3072} INFO -  at 52.9s,	estimator xgboost's best error=3.0691,	best estimator xgboost's best error=3.0691
[flaml.automl: 09-16 15:11:04] {3335} INFO - retrain xgboost for 11.0s
[flaml.automl: 09-16 15:11:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:11:04] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:11:04] {2637} INFO - Time taken to find the best model: 46.38611078262329
[flaml.automl: 09-16 15:11:04] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-2.0690818524370465
PM2.5(0)最好结果：{'pred_time': 1.6810029978028347e-05, 'wall_clock_time': 46.38611078262329, 'metric_for_logging': {'pred_time': 1.6810029978028347e-05}, 'val_loss': 3.0690818524370465, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 11.101943492889404}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9134164900753079
PM2.5(0)的mse=22.840533303823868
PM2.5(0)的mae=3.12514001861496
PM2.5(0)的mar=0.1497633910817064
总共花费的时间为：64.31
台州市
1256A
1257A
3564A
[flaml.automl: 09-16 15:20:56] {2390} INFO - task = regression
[flaml.automl: 09-16 15:20:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:20:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:20:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:20:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:20:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:20:58] {3025} INFO - Estimated sufficient time budget=18335s. Estimated necessary time budget=18s.
[flaml.automl: 09-16 15:20:58] {3072} INFO -  at 2.0s,	estimator xgboost's best error=14.0424,	best estimator xgboost's best error=14.0424
[flaml.automl: 09-16 15:20:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:21:02] {3072} INFO -  at 6.0s,	estimator xgboost's best error=6.5666,	best estimator xgboost's best error=6.5666
[flaml.automl: 09-16 15:21:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:21:04] {3072} INFO -  at 8.2s,	estimator xgboost's best error=6.5666,	best estimator xgboost's best error=6.5666
[flaml.automl: 09-16 15:21:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:21:17] {3072} INFO -  at 20.9s,	estimator xgboost's best error=6.5666,	best estimator xgboost's best error=6.5666
[flaml.automl: 09-16 15:21:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:21:18] {3072} INFO -  at 22.0s,	estimator xgboost's best error=4.7747,	best estimator xgboost's best error=4.7747
[flaml.automl: 09-16 15:21:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:21:20] {3072} INFO -  at 23.6s,	estimator xgboost's best error=4.7747,	best estimator xgboost's best error=4.7747
[flaml.automl: 09-16 15:21:20] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:21:21] {3072} INFO -  at 25.3s,	estimator xgboost's best error=3.2071,	best estimator xgboost's best error=3.2071
[flaml.automl: 09-16 15:21:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:21:24] {3072} INFO -  at 28.0s,	estimator xgboost's best error=3.2071,	best estimator xgboost's best error=3.2071
[flaml.automl: 09-16 15:21:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:21:26] {3072} INFO -  at 29.6s,	estimator xgboost's best error=3.2071,	best estimator xgboost's best error=3.2071
[flaml.automl: 09-16 15:21:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:21:29] {3072} INFO -  at 32.6s,	estimator xgboost's best error=3.2071,	best estimator xgboost's best error=3.2071
[flaml.automl: 09-16 15:21:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:21:30] {3072} INFO -  at 34.1s,	estimator xgboost's best error=3.1805,	best estimator xgboost's best error=3.1805
[flaml.automl: 09-16 15:21:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:21:31] {3072} INFO -  at 35.3s,	estimator xgboost's best error=3.1805,	best estimator xgboost's best error=3.1805
[flaml.automl: 09-16 15:21:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:21:37] {3072} INFO -  at 41.2s,	estimator xgboost's best error=2.8095,	best estimator xgboost's best error=2.8095
[flaml.automl: 09-16 15:21:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:21:48] {3072} INFO -  at 51.9s,	estimator xgboost's best error=2.7487,	best estimator xgboost's best error=2.7487
[flaml.automl: 09-16 15:21:59] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-16 15:21:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:21:59] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:21:59] {2637} INFO - Time taken to find the best model: 51.915356397628784
[flaml.automl: 09-16 15:21:59] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-1.7487322311728621
PM2.5(0)最好结果：{'pred_time': 1.2136285158789695e-05, 'wall_clock_time': 51.915356397628784, 'metric_for_logging': {'pred_time': 1.2136285158789695e-05}, 'val_loss': 2.748732231172862, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 10.702744007110596}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9040312705776826
PM2.5(0)的mse=17.423080135381166
PM2.5(0)的mae=2.6658760077351062
PM2.5(0)的mar=0.14789734297322657
总共花费的时间为：63.29
舟山市
1258A
1259A
[flaml.automl: 09-16 15:28:08] {2390} INFO - task = regression
[flaml.automl: 09-16 15:28:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:28:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:28:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:28:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:28:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:28:10] {3025} INFO - Estimated sufficient time budget=22649s. Estimated necessary time budget=23s.
[flaml.automl: 09-16 15:28:10] {3072} INFO -  at 2.4s,	estimator xgboost's best error=8.9602,	best estimator xgboost's best error=8.9602
[flaml.automl: 09-16 15:28:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:28:13] {3072} INFO -  at 6.0s,	estimator xgboost's best error=4.2272,	best estimator xgboost's best error=4.2272
[flaml.automl: 09-16 15:28:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:28:16] {3072} INFO -  at 8.1s,	estimator xgboost's best error=4.2272,	best estimator xgboost's best error=4.2272
[flaml.automl: 09-16 15:28:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:28:31] {3072} INFO -  at 23.7s,	estimator xgboost's best error=4.2272,	best estimator xgboost's best error=4.2272
[flaml.automl: 09-16 15:28:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:28:33] {3072} INFO -  at 25.6s,	estimator xgboost's best error=3.0849,	best estimator xgboost's best error=3.0849
[flaml.automl: 09-16 15:28:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:28:35] {3072} INFO -  at 27.9s,	estimator xgboost's best error=3.0849,	best estimator xgboost's best error=3.0849
[flaml.automl: 09-16 15:28:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:28:37] {3072} INFO -  at 29.6s,	estimator xgboost's best error=1.9918,	best estimator xgboost's best error=1.9918
[flaml.automl: 09-16 15:28:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:28:40] {3072} INFO -  at 32.3s,	estimator xgboost's best error=1.9918,	best estimator xgboost's best error=1.9918
[flaml.automl: 09-16 15:28:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:28:41] {3072} INFO -  at 33.9s,	estimator xgboost's best error=1.9918,	best estimator xgboost's best error=1.9918
[flaml.automl: 09-16 15:28:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:28:44] {3072} INFO -  at 36.9s,	estimator xgboost's best error=1.9918,	best estimator xgboost's best error=1.9918
[flaml.automl: 09-16 15:28:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:28:46] {3072} INFO -  at 38.6s,	estimator xgboost's best error=1.9294,	best estimator xgboost's best error=1.9294
[flaml.automl: 09-16 15:28:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:28:47] {3072} INFO -  at 39.7s,	estimator xgboost's best error=1.9294,	best estimator xgboost's best error=1.9294
[flaml.automl: 09-16 15:28:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:28:54] {3072} INFO -  at 46.1s,	estimator xgboost's best error=1.6981,	best estimator xgboost's best error=1.6981
[flaml.automl: 09-16 15:28:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:29:07] {3072} INFO -  at 59.8s,	estimator xgboost's best error=1.6813,	best estimator xgboost's best error=1.6813
[flaml.automl: 09-16 15:29:26] {3335} INFO - retrain xgboost for 18.6s
[flaml.automl: 09-16 15:29:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:29:26] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:29:26] {2637} INFO - Time taken to find the best model: 59.78403186798096
[flaml.automl: 09-16 15:29:26] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-0.681284538086723
PM2.5(0)最好结果：{'pred_time': 2.6321489047380833e-05, 'wall_clock_time': 59.78403186798096, 'metric_for_logging': {'pred_time': 2.6321489047380833e-05}, 'val_loss': 1.681284538086723, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 13.633780717849731}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9140558033280102
PM2.5(0)的mse=6.611230001340182
PM2.5(0)的mae=1.6508108861744404
PM2.5(0)的mar=0.14134658456308202
总共花费的时间为：78.94
金华市
金华市没有数据
衢州市
1264A
1265A
[flaml.automl: 09-16 15:35:35] {2390} INFO - task = regression
[flaml.automl: 09-16 15:35:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:35:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:35:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:35:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:35:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:35:37] {3025} INFO - Estimated sufficient time budget=20246s. Estimated necessary time budget=20s.
[flaml.automl: 09-16 15:35:37] {3072} INFO -  at 2.2s,	estimator xgboost's best error=15.2785,	best estimator xgboost's best error=15.2785
[flaml.automl: 09-16 15:35:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:35:41] {3072} INFO -  at 5.6s,	estimator xgboost's best error=7.2873,	best estimator xgboost's best error=7.2873
[flaml.automl: 09-16 15:35:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:35:43] {3072} INFO -  at 7.6s,	estimator xgboost's best error=7.2873,	best estimator xgboost's best error=7.2873
[flaml.automl: 09-16 15:35:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:35:59] {3072} INFO -  at 23.8s,	estimator xgboost's best error=7.2873,	best estimator xgboost's best error=7.2873
[flaml.automl: 09-16 15:35:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:36:01] {3072} INFO -  at 25.8s,	estimator xgboost's best error=5.0345,	best estimator xgboost's best error=5.0345
[flaml.automl: 09-16 15:36:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:36:03] {3072} INFO -  at 27.4s,	estimator xgboost's best error=5.0345,	best estimator xgboost's best error=5.0345
[flaml.automl: 09-16 15:36:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:36:04] {3072} INFO -  at 29.1s,	estimator xgboost's best error=3.5667,	best estimator xgboost's best error=3.5667
[flaml.automl: 09-16 15:36:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:36:07] {3072} INFO -  at 31.8s,	estimator xgboost's best error=3.5667,	best estimator xgboost's best error=3.5667
[flaml.automl: 09-16 15:36:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:36:09] {3072} INFO -  at 33.4s,	estimator xgboost's best error=3.5667,	best estimator xgboost's best error=3.5667
[flaml.automl: 09-16 15:36:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:36:12] {3072} INFO -  at 36.5s,	estimator xgboost's best error=3.5667,	best estimator xgboost's best error=3.5667
[flaml.automl: 09-16 15:36:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:36:13] {3072} INFO -  at 38.1s,	estimator xgboost's best error=3.3849,	best estimator xgboost's best error=3.3849
[flaml.automl: 09-16 15:36:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:36:15] {3072} INFO -  at 39.3s,	estimator xgboost's best error=3.3849,	best estimator xgboost's best error=3.3849
[flaml.automl: 09-16 15:36:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:36:21] {3072} INFO -  at 45.8s,	estimator xgboost's best error=3.0454,	best estimator xgboost's best error=3.0454
[flaml.automl: 09-16 15:36:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:36:32] {3072} INFO -  at 56.9s,	estimator xgboost's best error=2.9700,	best estimator xgboost's best error=2.9700
[flaml.automl: 09-16 15:36:43] {3335} INFO - retrain xgboost for 11.1s
[flaml.automl: 09-16 15:36:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:36:43] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:36:43] {2637} INFO - Time taken to find the best model: 56.88715982437134
[flaml.automl: 09-16 15:36:43] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-1.9699759862047608
PM2.5(0)最好结果：{'pred_time': 1.578039603887517e-05, 'wall_clock_time': 56.88715982437134, 'metric_for_logging': {'pred_time': 1.578039603887517e-05}, 'val_loss': 2.9699759862047608, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 11.132551431655884}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8960035446841214
PM2.5(0)的mse=18.266387427529754
PM2.5(0)的mae=2.9197979933577156
PM2.5(0)的mar=0.16129042222956755
总共花费的时间为：68.47
丽水市
1267A
[flaml.automl: 09-16 15:40:34] {2390} INFO - task = regression
[flaml.automl: 09-16 15:40:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:40:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:40:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:40:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:40:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:40:37] {3025} INFO - Estimated sufficient time budget=32757s. Estimated necessary time budget=33s.
[flaml.automl: 09-16 15:40:37] {3072} INFO -  at 3.3s,	estimator xgboost's best error=12.7486,	best estimator xgboost's best error=12.7486
[flaml.automl: 09-16 15:40:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:40:42] {3072} INFO -  at 8.3s,	estimator xgboost's best error=6.9556,	best estimator xgboost's best error=6.9556
[flaml.automl: 09-16 15:40:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:40:45] {3072} INFO -  at 11.4s,	estimator xgboost's best error=6.9556,	best estimator xgboost's best error=6.9556
[flaml.automl: 09-16 15:40:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:41:04] {3072} INFO -  at 30.5s,	estimator xgboost's best error=6.9556,	best estimator xgboost's best error=6.9556
[flaml.automl: 09-16 15:41:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:41:07] {3072} INFO -  at 33.5s,	estimator xgboost's best error=3.8421,	best estimator xgboost's best error=3.8421
[flaml.automl: 09-16 15:41:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:41:11] {3072} INFO -  at 37.7s,	estimator xgboost's best error=3.4355,	best estimator xgboost's best error=3.4355
[flaml.automl: 09-16 15:41:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:41:16] {3072} INFO -  at 42.1s,	estimator xgboost's best error=3.4173,	best estimator xgboost's best error=3.4173
[flaml.automl: 09-16 15:41:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:41:20] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.4173,	best estimator xgboost's best error=3.4173
[flaml.automl: 09-16 15:41:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:41:23] {3072} INFO -  at 49.4s,	estimator xgboost's best error=3.4006,	best estimator xgboost's best error=3.4006
[flaml.automl: 09-16 15:41:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:41:28] {3072} INFO -  at 54.1s,	estimator xgboost's best error=3.1323,	best estimator xgboost's best error=3.1323
[flaml.automl: 09-16 15:41:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:41:29] {3072} INFO -  at 55.2s,	estimator xgboost's best error=3.1323,	best estimator xgboost's best error=3.1323
[flaml.automl: 09-16 15:41:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:41:33] {3072} INFO -  at 59.3s,	estimator xgboost's best error=2.8451,	best estimator xgboost's best error=2.8451
[flaml.automl: 09-16 15:41:42] {3335} INFO - retrain xgboost for 9.5s
[flaml.automl: 09-16 15:41:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:41:42] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:41:42] {2637} INFO - Time taken to find the best model: 59.258458852767944
[flaml.automl: 09-16 15:41:42] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
PM2.5(0)最佳损失：-1.8451464207835215
PM2.5(0)最好结果：{'pred_time': 3.290374249343556e-05, 'wall_clock_time': 59.258458852767944, 'metric_for_logging': {'pred_time': 3.290374249343556e-05}, 'val_loss': 2.8451464207835215, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 12, 'config/max_leaves': 14, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 4.053121089935303}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8927735985608639
PM2.5(0)的mse=17.620761224310474
PM2.5(0)的mae=2.912677696874731
PM2.5(0)的mar=0.21772112666904353
总共花费的时间为：68.96
合肥市
1273A
1274A
1275A
1277A
1278A
1279A
3464A
[flaml.automl: 09-16 16:03:02] {2390} INFO - task = regression
[flaml.automl: 09-16 16:03:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:03:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:03:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:03:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:03:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:03:04] {3025} INFO - Estimated sufficient time budget=87500s. Estimated necessary time budget=88s.
[flaml.automl: 09-16 16:03:04] {3072} INFO -  at 1.5s,	estimator xgboost's best error=19.1062,	best estimator xgboost's best error=19.1062
[flaml.automl: 09-16 16:03:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:03:06] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.3027,	best estimator xgboost's best error=9.3027
[flaml.automl: 09-16 16:03:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:03:07] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.3027,	best estimator xgboost's best error=9.3027
[flaml.automl: 09-16 16:03:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:03:10] {3072} INFO -  at 8.1s,	estimator xgboost's best error=9.3027,	best estimator xgboost's best error=9.3027
[flaml.automl: 09-16 16:03:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:03:11] {3072} INFO -  at 9.2s,	estimator xgboost's best error=6.9100,	best estimator xgboost's best error=6.9100
[flaml.automl: 09-16 16:03:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:03:13] {3072} INFO -  at 10.8s,	estimator xgboost's best error=6.9100,	best estimator xgboost's best error=6.9100
[flaml.automl: 09-16 16:03:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:03:14] {3072} INFO -  at 12.5s,	estimator xgboost's best error=4.8998,	best estimator xgboost's best error=4.8998
[flaml.automl: 09-16 16:03:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:03:17] {3072} INFO -  at 15.2s,	estimator xgboost's best error=4.8998,	best estimator xgboost's best error=4.8998
[flaml.automl: 09-16 16:03:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:03:19] {3072} INFO -  at 16.8s,	estimator xgboost's best error=4.8998,	best estimator xgboost's best error=4.8998
[flaml.automl: 09-16 16:03:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:03:21] {3072} INFO -  at 19.4s,	estimator xgboost's best error=4.8998,	best estimator xgboost's best error=4.8998
[flaml.automl: 09-16 16:03:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:03:23] {3072} INFO -  at 20.8s,	estimator xgboost's best error=4.8998,	best estimator xgboost's best error=4.8998
[flaml.automl: 09-16 16:03:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:03:25] {3072} INFO -  at 22.6s,	estimator xgboost's best error=4.8998,	best estimator xgboost's best error=4.8998
[flaml.automl: 09-16 16:03:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:03:26] {3072} INFO -  at 23.8s,	estimator xgboost's best error=4.8998,	best estimator xgboost's best error=4.8998
[flaml.automl: 09-16 16:03:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:03:33] {3072} INFO -  at 30.9s,	estimator xgboost's best error=4.2884,	best estimator xgboost's best error=4.2884
[flaml.automl: 09-16 16:03:33] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 16:03:46] {3072} INFO -  at 43.7s,	estimator xgboost's best error=4.2597,	best estimator xgboost's best error=4.2597
[flaml.automl: 09-16 16:03:46] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 16:03:53] {3072} INFO -  at 50.8s,	estimator xgboost's best error=4.2597,	best estimator xgboost's best error=4.2597
[flaml.automl: 09-16 16:04:06] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-16 16:04:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 16:04:06] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:04:06] {2637} INFO - Time taken to find the best model: 43.712626695632935
[flaml.automl: 09-16 16:04:06] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 73035}
PM2.5(0)最佳损失：-3.2596673019828906
PM2.5(0)最好结果：{'pred_time': 5.066835451384611e-06, 'wall_clock_time': 43.712626695632935, 'metric_for_logging': {'pred_time': 5.066835451384611e-06}, 'val_loss': 4.259667301982891, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 73035}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 73035, 'experiment_tag': 'exp', 'time_total_s': 12.846868753433228}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8934551547150775
PM2.5(0)的mse=36.033329208678516
PM2.5(0)的mae=4.304348607609036
PM2.5(0)的mar=0.22218310060389454
总共花费的时间为：64.72
福州市
1280A
1285A
3048A
3526A
[flaml.automl: 09-16 16:16:20] {2390} INFO - task = regression
[flaml.automl: 09-16 16:16:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:16:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:16:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:16:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:16:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:16:21] {3025} INFO - Estimated sufficient time budget=49315s. Estimated necessary time budget=49s.
[flaml.automl: 09-16 16:16:21] {3072} INFO -  at 1.4s,	estimator xgboost's best error=11.6694,	best estimator xgboost's best error=11.6694
[flaml.automl: 09-16 16:16:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:16:23] {3072} INFO -  at 3.4s,	estimator xgboost's best error=5.5935,	best estimator xgboost's best error=5.5935
[flaml.automl: 09-16 16:16:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:16:24] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.5935,	best estimator xgboost's best error=5.5935
[flaml.automl: 09-16 16:16:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:16:31] {3072} INFO -  at 11.0s,	estimator xgboost's best error=5.5935,	best estimator xgboost's best error=5.5935
[flaml.automl: 09-16 16:16:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:16:32] {3072} INFO -  at 12.1s,	estimator xgboost's best error=4.1560,	best estimator xgboost's best error=4.1560
[flaml.automl: 09-16 16:16:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:16:33] {3072} INFO -  at 13.7s,	estimator xgboost's best error=3.7422,	best estimator xgboost's best error=3.7422
[flaml.automl: 09-16 16:16:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:16:35] {3072} INFO -  at 15.3s,	estimator xgboost's best error=3.1867,	best estimator xgboost's best error=3.1867
[flaml.automl: 09-16 16:16:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:16:38] {3072} INFO -  at 18.0s,	estimator xgboost's best error=3.1867,	best estimator xgboost's best error=3.1867
[flaml.automl: 09-16 16:16:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:16:39] {3072} INFO -  at 19.6s,	estimator xgboost's best error=3.1867,	best estimator xgboost's best error=3.1867
[flaml.automl: 09-16 16:16:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:16:42] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.0086,	best estimator xgboost's best error=3.0086
[flaml.automl: 09-16 16:16:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:16:44] {3072} INFO -  at 24.3s,	estimator xgboost's best error=3.0086,	best estimator xgboost's best error=3.0086
[flaml.automl: 09-16 16:16:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:16:45] {3072} INFO -  at 25.4s,	estimator xgboost's best error=3.0086,	best estimator xgboost's best error=3.0086
[flaml.automl: 09-16 16:16:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:16:49] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.7443,	best estimator xgboost's best error=2.7443
[flaml.automl: 09-16 16:16:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:16:52] {3072} INFO -  at 32.7s,	estimator xgboost's best error=2.6174,	best estimator xgboost's best error=2.6174
[flaml.automl: 09-16 16:16:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 16:16:55] {3072} INFO -  at 35.4s,	estimator xgboost's best error=2.6174,	best estimator xgboost's best error=2.6174
[flaml.automl: 09-16 16:16:55] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 16:16:58] {3072} INFO -  at 37.9s,	estimator xgboost's best error=2.6174,	best estimator xgboost's best error=2.6174
[flaml.automl: 09-16 16:16:58] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 16:17:00] {3072} INFO -  at 40.5s,	estimator xgboost's best error=2.5603,	best estimator xgboost's best error=2.5603
[flaml.automl: 09-16 16:17:00] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 16:17:02] {3072} INFO -  at 42.8s,	estimator xgboost's best error=2.5603,	best estimator xgboost's best error=2.5603
[flaml.automl: 09-16 16:17:02] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 16:17:04] {3072} INFO -  at 44.5s,	estimator xgboost's best error=2.5603,	best estimator xgboost's best error=2.5603
[flaml.automl: 09-16 16:17:04] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 16:17:06] {3072} INFO -  at 45.9s,	estimator xgboost's best error=2.5603,	best estimator xgboost's best error=2.5603
[flaml.automl: 09-16 16:17:06] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 16:17:07] {3072} INFO -  at 47.6s,	estimator xgboost's best error=2.5603,	best estimator xgboost's best error=2.5603
[flaml.automl: 09-16 16:17:07] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 16:17:19] {3072} INFO -  at 59.8s,	estimator xgboost's best error=2.4459,	best estimator xgboost's best error=2.4459
[flaml.automl: 09-16 16:17:45] {3335} INFO - retrain xgboost for 25.5s
[flaml.automl: 09-16 16:17:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4512592128754277,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.003827601076117227, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010065608612606477, reg_lambda=0.020722552365849516,
             scale_pos_weight=1, subsample=0.749001334635897,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 16:17:45] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:17:45] {2637} INFO - Time taken to find the best model: 59.78828406333923
[flaml.automl: 09-16 16:17:45] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.003827601076117227, 'learning_rate': 0.4512592128754277, 'subsample': 0.749001334635897, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0010065608612606477, 'reg_lambda': 0.020722552365849516, 'FLAML_sample_size': 41061}
PM2.5(0)最佳损失：-1.4458657290207668
PM2.5(0)最好结果：{'pred_time': 1.730961729606045e-05, 'wall_clock_time': 59.78828406333923, 'metric_for_logging': {'pred_time': 1.730961729606045e-05}, 'val_loss': 2.445865729020767, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.003827601076117227, 'learning_rate': 0.4512592128754277, 'subsample': 0.749001334635897, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0010065608612606477, 'reg_lambda': 0.020722552365849516, 'FLAML_sample_size': 41061}, 'config/n_estimators': 35, 'config/max_leaves': 7, 'config/min_child_weight': 0.003827601076117227, 'config/learning_rate': 0.4512592128754277, 'config/subsample': 0.749001334635897, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0010065608612606477, 'config/reg_lambda': 0.020722552365849516, 'config/FLAML_sample_size': 41061, 'experiment_tag': 'exp', 'time_total_s': 12.17480182647705}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4512592128754277,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.003827601076117227, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010065608612606477, reg_lambda=0.020722552365849516,
             scale_pos_weight=1, subsample=0.749001334635897,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8893080010826869
PM2.5(0)的mse=14.341344573391673
PM2.5(0)的mae=2.4702167024273844
PM2.5(0)的mar=0.18261368300078576
总共花费的时间为：85.98
厦门市
1286A
3527A
3528A
[flaml.automl: 09-16 16:26:52] {2390} INFO - task = regression
[flaml.automl: 09-16 16:26:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:26:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:26:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:26:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:26:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:26:53] {3025} INFO - Estimated sufficient time budget=12137s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 16:26:53] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.4808,	best estimator xgboost's best error=11.4808
[flaml.automl: 09-16 16:26:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:26:55] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.5022,	best estimator xgboost's best error=5.5022
[flaml.automl: 09-16 16:26:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:26:56] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.5022,	best estimator xgboost's best error=5.5022
[flaml.automl: 09-16 16:26:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:27:06] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.5022,	best estimator xgboost's best error=5.5022
[flaml.automl: 09-16 16:27:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:27:07] {3072} INFO -  at 15.8s,	estimator xgboost's best error=3.6737,	best estimator xgboost's best error=3.6737
[flaml.automl: 09-16 16:27:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:27:09] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.5714,	best estimator xgboost's best error=3.5714
[flaml.automl: 09-16 16:27:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:27:10] {3072} INFO -  at 19.0s,	estimator xgboost's best error=3.0078,	best estimator xgboost's best error=3.0078
[flaml.automl: 09-16 16:27:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:27:13] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.0078,	best estimator xgboost's best error=3.0078
[flaml.automl: 09-16 16:27:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:27:15] {3072} INFO -  at 23.4s,	estimator xgboost's best error=3.0078,	best estimator xgboost's best error=3.0078
[flaml.automl: 09-16 16:27:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:27:18] {3072} INFO -  at 26.4s,	estimator xgboost's best error=2.7738,	best estimator xgboost's best error=2.7738
[flaml.automl: 09-16 16:27:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:27:19] {3072} INFO -  at 28.1s,	estimator xgboost's best error=2.7738,	best estimator xgboost's best error=2.7738
[flaml.automl: 09-16 16:27:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:27:21] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.7738,	best estimator xgboost's best error=2.7738
[flaml.automl: 09-16 16:27:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:27:34] {3072} INFO -  at 42.9s,	estimator xgboost's best error=2.5564,	best estimator xgboost's best error=2.5564
[flaml.automl: 09-16 16:27:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:27:51] {3072} INFO -  at 59.4s,	estimator xgboost's best error=2.4110,	best estimator xgboost's best error=2.4110
[flaml.automl: 09-16 16:28:15] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-16 16:28:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 16:28:15] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:28:15] {2637} INFO - Time taken to find the best model: 59.36681032180786
[flaml.automl: 09-16 16:28:15] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-1.4110489321108117
PM2.5(0)最好结果：{'pred_time': 1.1161262579569957e-05, 'wall_clock_time': 59.36681032180786, 'metric_for_logging': {'pred_time': 1.1161262579569957e-05}, 'val_loss': 2.4110489321108117, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.510333776474}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.880251764078344
PM2.5(0)的mse=13.75901840887046
PM2.5(0)的mae=2.4193048645856594
PM2.5(0)的mar=0.17707016521425545
总共花费的时间为：83.83
南昌市
1295A
1296A
1298A
3690A
[flaml.automl: 09-16 16:40:48] {2390} INFO - task = regression
[flaml.automl: 09-16 16:40:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:40:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:40:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:40:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:40:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:40:49] {3025} INFO - Estimated sufficient time budget=50969s. Estimated necessary time budget=51s.
[flaml.automl: 09-16 16:40:49] {3072} INFO -  at 1.4s,	estimator xgboost's best error=16.8710,	best estimator xgboost's best error=16.8710
[flaml.automl: 09-16 16:40:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:40:51] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.8762,	best estimator xgboost's best error=7.8762
[flaml.automl: 09-16 16:40:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:40:52] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.8762,	best estimator xgboost's best error=7.8762
[flaml.automl: 09-16 16:40:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:40:59] {3072} INFO -  at 11.3s,	estimator xgboost's best error=7.8762,	best estimator xgboost's best error=7.8762
[flaml.automl: 09-16 16:40:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:41:01] {3072} INFO -  at 13.4s,	estimator xgboost's best error=5.5413,	best estimator xgboost's best error=5.5413
[flaml.automl: 09-16 16:41:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:41:04] {3072} INFO -  at 16.3s,	estimator xgboost's best error=5.5413,	best estimator xgboost's best error=5.5413
[flaml.automl: 09-16 16:41:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:41:06] {3072} INFO -  at 18.9s,	estimator xgboost's best error=3.4791,	best estimator xgboost's best error=3.4791
[flaml.automl: 09-16 16:41:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:41:11] {3072} INFO -  at 23.8s,	estimator xgboost's best error=3.4791,	best estimator xgboost's best error=3.4791
[flaml.automl: 09-16 16:41:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:41:14] {3072} INFO -  at 26.8s,	estimator xgboost's best error=3.4791,	best estimator xgboost's best error=3.4791
[flaml.automl: 09-16 16:41:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:41:17] {3072} INFO -  at 30.0s,	estimator xgboost's best error=3.4791,	best estimator xgboost's best error=3.4791
[flaml.automl: 09-16 16:41:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:41:20] {3072} INFO -  at 32.9s,	estimator xgboost's best error=3.4791,	best estimator xgboost's best error=3.4791
[flaml.automl: 09-16 16:41:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:41:23] {3072} INFO -  at 35.9s,	estimator xgboost's best error=3.4791,	best estimator xgboost's best error=3.4791
[flaml.automl: 09-16 16:41:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:41:25] {3072} INFO -  at 38.0s,	estimator xgboost's best error=3.4791,	best estimator xgboost's best error=3.4791
[flaml.automl: 09-16 16:41:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:41:37] {3072} INFO -  at 49.9s,	estimator xgboost's best error=2.9745,	best estimator xgboost's best error=2.9745
[flaml.automl: 09-16 16:41:48] {3335} INFO - retrain xgboost for 10.8s
[flaml.automl: 09-16 16:41:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 16:41:48] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:41:48] {2637} INFO - Time taken to find the best model: 49.872021436691284
[flaml.automl: 09-16 16:41:48] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41323}
PM2.5(0)最佳损失：-1.9744969356143103
PM2.5(0)最好结果：{'pred_time': 1.880095603158665e-05, 'wall_clock_time': 49.872021436691284, 'metric_for_logging': {'pred_time': 1.880095603158665e-05}, 'val_loss': 2.9744969356143103, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41323}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 41323, 'experiment_tag': 'exp', 'time_total_s': 11.821998119354248}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9454044374353348
PM2.5(0)的mse=17.890562900111682
PM2.5(0)的mae=2.899398531389442
PM2.5(0)的mar=0.1676766523529601
总共花费的时间为：61.35
济南市
1300A
1301A
1305A
1306A
1961A
3064A
3494A
3495A
3682A
[flaml.automl: 09-16 17:09:04] {2390} INFO - task = regression
[flaml.automl: 09-16 17:09:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 17:09:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 17:09:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 17:09:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 17:09:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 17:09:05] {3025} INFO - Estimated sufficient time budget=115303s. Estimated necessary time budget=115s.
[flaml.automl: 09-16 17:09:05] {3072} INFO -  at 1.6s,	estimator xgboost's best error=23.7677,	best estimator xgboost's best error=23.7677
[flaml.automl: 09-16 17:09:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 17:09:07] {3072} INFO -  at 3.7s,	estimator xgboost's best error=11.7122,	best estimator xgboost's best error=11.7122
[flaml.automl: 09-16 17:09:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 17:09:08] {3072} INFO -  at 4.9s,	estimator xgboost's best error=11.7122,	best estimator xgboost's best error=11.7122
[flaml.automl: 09-16 17:09:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 17:09:11] {3072} INFO -  at 7.6s,	estimator xgboost's best error=11.7122,	best estimator xgboost's best error=11.7122
[flaml.automl: 09-16 17:09:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 17:09:12] {3072} INFO -  at 8.8s,	estimator xgboost's best error=8.2243,	best estimator xgboost's best error=8.2243
[flaml.automl: 09-16 17:09:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 17:09:14] {3072} INFO -  at 10.3s,	estimator xgboost's best error=7.7338,	best estimator xgboost's best error=7.7338
[flaml.automl: 09-16 17:09:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 17:09:15] {3072} INFO -  at 12.0s,	estimator xgboost's best error=6.6067,	best estimator xgboost's best error=6.6067
[flaml.automl: 09-16 17:09:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 17:09:18] {3072} INFO -  at 14.2s,	estimator xgboost's best error=6.6067,	best estimator xgboost's best error=6.6067
[flaml.automl: 09-16 17:09:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 17:09:19] {3072} INFO -  at 15.9s,	estimator xgboost's best error=6.6067,	best estimator xgboost's best error=6.6067
[flaml.automl: 09-16 17:09:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 17:09:21] {3072} INFO -  at 18.1s,	estimator xgboost's best error=6.6067,	best estimator xgboost's best error=6.6067
[flaml.automl: 09-16 17:09:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 17:09:23] {3072} INFO -  at 19.5s,	estimator xgboost's best error=6.5532,	best estimator xgboost's best error=6.5532
[flaml.automl: 09-16 17:09:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 17:09:24] {3072} INFO -  at 20.7s,	estimator xgboost's best error=6.5532,	best estimator xgboost's best error=6.5532
[flaml.automl: 09-16 17:09:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 17:09:26] {3072} INFO -  at 22.4s,	estimator xgboost's best error=5.6978,	best estimator xgboost's best error=5.6978
[flaml.automl: 09-16 17:09:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:09:28] {3072} INFO -  at 24.2s,	estimator xgboost's best error=5.6978,	best estimator xgboost's best error=5.6978
[flaml.automl: 09-16 17:09:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 17:09:29] {3072} INFO -  at 25.8s,	estimator xgboost's best error=5.6978,	best estimator xgboost's best error=5.6978
[flaml.automl: 09-16 17:09:29] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 17:09:30] {3072} INFO -  at 26.9s,	estimator xgboost's best error=5.6978,	best estimator xgboost's best error=5.6978
[flaml.automl: 09-16 17:09:30] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 17:09:31] {3072} INFO -  at 28.2s,	estimator xgboost's best error=5.6978,	best estimator xgboost's best error=5.6978
[flaml.automl: 09-16 17:09:31] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 17:09:38] {3072} INFO -  at 34.2s,	estimator xgboost's best error=5.4116,	best estimator xgboost's best error=5.4116
[flaml.automl: 09-16 17:09:38] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 17:09:40] {3072} INFO -  at 36.7s,	estimator xgboost's best error=5.4116,	best estimator xgboost's best error=5.4116
[flaml.automl: 09-16 17:09:40] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 17:09:55] {3072} INFO -  at 51.6s,	estimator xgboost's best error=5.4116,	best estimator xgboost's best error=5.4116
[flaml.automl: 09-16 17:09:55] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 17:09:57] {3072} INFO -  at 54.0s,	estimator xgboost's best error=5.4116,	best estimator xgboost's best error=5.4116
[flaml.automl: 09-16 17:09:57] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 17:10:03] {3072} INFO -  at 59.3s,	estimator xgboost's best error=5.4116,	best estimator xgboost's best error=5.4116
[flaml.automl: 09-16 17:10:09] {3335} INFO - retrain xgboost for 6.0s
[flaml.automl: 09-16 17:10:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 17:10:09] {2636} INFO - fit succeeded
[flaml.automl: 09-16 17:10:09] {2637} INFO - Time taken to find the best model: 34.24355387687683
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024, 'FLAML_sample_size': 96504}
PM2.5(0)最佳损失：-4.411585548253554
PM2.5(0)最好结果：{'pred_time': 3.795131066836473e-06, 'wall_clock_time': 34.24355387687683, 'metric_for_logging': {'pred_time': 3.795131066836473e-06}, 'val_loss': 5.411585548253554, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024, 'FLAML_sample_size': 96504}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'config/FLAML_sample_size': 96504, 'experiment_tag': 'exp', 'time_total_s': 6.026145935058594}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9103902094338925
PM2.5(0)的mse=61.092256927331086
PM2.5(0)的mae=5.378544294829134
PM2.5(0)的mar=0.25628531611788874
总共花费的时间为：66.89
青岛市
1307A
1311A
3362A
3642A
3643A
[flaml.automl: 09-16 17:25:36] {2390} INFO - task = regression
[flaml.automl: 09-16 17:25:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 17:25:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 17:25:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 17:25:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 17:25:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 17:25:37] {3025} INFO - Estimated sufficient time budget=63054s. Estimated necessary time budget=63s.
[flaml.automl: 09-16 17:25:37] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.9563,	best estimator xgboost's best error=17.9563
[flaml.automl: 09-16 17:25:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 17:25:39] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.8500,	best estimator xgboost's best error=8.8500
[flaml.automl: 09-16 17:25:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 17:25:40] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.8500,	best estimator xgboost's best error=8.8500
[flaml.automl: 09-16 17:25:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 17:25:45] {3072} INFO -  at 9.5s,	estimator xgboost's best error=8.8500,	best estimator xgboost's best error=8.8500
[flaml.automl: 09-16 17:25:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 17:25:46] {3072} INFO -  at 10.7s,	estimator xgboost's best error=6.5185,	best estimator xgboost's best error=6.5185
[flaml.automl: 09-16 17:25:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 17:25:48] {3072} INFO -  at 12.3s,	estimator xgboost's best error=6.2459,	best estimator xgboost's best error=6.2459
[flaml.automl: 09-16 17:25:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 17:25:49] {3072} INFO -  at 13.9s,	estimator xgboost's best error=5.3965,	best estimator xgboost's best error=5.3965
[flaml.automl: 09-16 17:25:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 17:25:52] {3072} INFO -  at 16.6s,	estimator xgboost's best error=5.3965,	best estimator xgboost's best error=5.3965
[flaml.automl: 09-16 17:25:52] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 17:25:54] {3072} INFO -  at 18.2s,	estimator xgboost's best error=5.3965,	best estimator xgboost's best error=5.3965
[flaml.automl: 09-16 17:25:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 17:25:57] {3072} INFO -  at 21.2s,	estimator xgboost's best error=5.0167,	best estimator xgboost's best error=5.0167
[flaml.automl: 09-16 17:25:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 17:25:58] {3072} INFO -  at 22.8s,	estimator xgboost's best error=5.0167,	best estimator xgboost's best error=5.0167
[flaml.automl: 09-16 17:25:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 17:25:59] {3072} INFO -  at 24.0s,	estimator xgboost's best error=5.0167,	best estimator xgboost's best error=5.0167
[flaml.automl: 09-16 17:25:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 17:26:02] {3072} INFO -  at 26.8s,	estimator xgboost's best error=4.8865,	best estimator xgboost's best error=4.8865
[flaml.automl: 09-16 17:26:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:26:05] {3072} INFO -  at 29.7s,	estimator xgboost's best error=4.8865,	best estimator xgboost's best error=4.8865
[flaml.automl: 09-16 17:26:05] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 17:26:08] {3072} INFO -  at 32.3s,	estimator xgboost's best error=4.8657,	best estimator xgboost's best error=4.8657
[flaml.automl: 09-16 17:26:08] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 17:26:10] {3072} INFO -  at 34.5s,	estimator xgboost's best error=4.8657,	best estimator xgboost's best error=4.8657
[flaml.automl: 09-16 17:26:10] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 17:26:12] {3072} INFO -  at 36.7s,	estimator xgboost's best error=4.8502,	best estimator xgboost's best error=4.8502
[flaml.automl: 09-16 17:26:12] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 17:26:14] {3072} INFO -  at 38.4s,	estimator xgboost's best error=4.8502,	best estimator xgboost's best error=4.8502
[flaml.automl: 09-16 17:26:14] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 17:26:16] {3072} INFO -  at 40.1s,	estimator xgboost's best error=4.8502,	best estimator xgboost's best error=4.8502
[flaml.automl: 09-16 17:26:16] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 17:26:17] {3072} INFO -  at 41.6s,	estimator xgboost's best error=4.8502,	best estimator xgboost's best error=4.8502
[flaml.automl: 09-16 17:26:17] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 17:26:19] {3072} INFO -  at 43.2s,	estimator xgboost's best error=4.7454,	best estimator xgboost's best error=4.7454
[flaml.automl: 09-16 17:26:19] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 17:26:21] {3072} INFO -  at 45.8s,	estimator xgboost's best error=4.7454,	best estimator xgboost's best error=4.7454
[flaml.automl: 09-16 17:26:21] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 17:26:22] {3072} INFO -  at 46.7s,	estimator xgboost's best error=4.7454,	best estimator xgboost's best error=4.7454
[flaml.automl: 09-16 17:26:22] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-16 17:26:26] {3072} INFO -  at 50.0s,	estimator xgboost's best error=4.7121,	best estimator xgboost's best error=4.7121
[flaml.automl: 09-16 17:26:26] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-16 17:26:27] {3072} INFO -  at 51.5s,	estimator xgboost's best error=4.7121,	best estimator xgboost's best error=4.7121
[flaml.automl: 09-16 17:26:27] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-16 17:26:28] {3072} INFO -  at 52.7s,	estimator xgboost's best error=4.7121,	best estimator xgboost's best error=4.7121
[flaml.automl: 09-16 17:26:28] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-16 17:26:39] {3072} INFO -  at 63.5s,	estimator xgboost's best error=4.7121,	best estimator xgboost's best error=4.7121
[flaml.automl: 09-16 17:27:26] {3335} INFO - retrain xgboost for 47.5s
[flaml.automl: 09-16 17:27:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 17:27:26] {2636} INFO - fit succeeded
[flaml.automl: 09-16 17:27:26] {2637} INFO - Time taken to find the best model: 50.033955335617065
[flaml.automl: 09-16 17:27:26] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}
PM2.5(0)最佳损失：-3.7121261460065185
PM2.5(0)最好结果：{'pred_time': 7.011837867337481e-06, 'wall_clock_time': 50.033955335617065, 'metric_for_logging': {'pred_time': 7.011837867337481e-06}, 'val_loss': 4.7121261460065185, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}, 'config/n_estimators': 15, 'config/max_leaves': 57, 'config/min_child_weight': 0.0011006053677836264, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532848210950454, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.0763912614998318, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.3394980430603027}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8930152536744744
PM2.5(0)的mse=48.42561799295162
PM2.5(0)的mae=4.658206964866212
PM2.5(0)的mar=0.2425095980624488
总共花费的时间为：111.87
郑州市
1318A
1320A
1321A
1323A
1324A
3471A
3590A
3591A
[flaml.automl: 09-16 17:52:39] {2390} INFO - task = regression
[flaml.automl: 09-16 17:52:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 17:52:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 17:52:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 17:52:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 17:52:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 17:52:40] {3025} INFO - Estimated sufficient time budget=98080s. Estimated necessary time budget=98s.
[flaml.automl: 09-16 17:52:40] {3072} INFO -  at 1.5s,	estimator xgboost's best error=26.0797,	best estimator xgboost's best error=26.0797
[flaml.automl: 09-16 17:52:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 17:52:42] {3072} INFO -  at 3.7s,	estimator xgboost's best error=12.3265,	best estimator xgboost's best error=12.3265
[flaml.automl: 09-16 17:52:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 17:52:43] {3072} INFO -  at 4.9s,	estimator xgboost's best error=12.3265,	best estimator xgboost's best error=12.3265
[flaml.automl: 09-16 17:52:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 17:52:47] {3072} INFO -  at 8.2s,	estimator xgboost's best error=12.3265,	best estimator xgboost's best error=12.3265
[flaml.automl: 09-16 17:52:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 17:52:48] {3072} INFO -  at 9.3s,	estimator xgboost's best error=8.4340,	best estimator xgboost's best error=8.4340
[flaml.automl: 09-16 17:52:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 17:52:49] {3072} INFO -  at 10.9s,	estimator xgboost's best error=8.4340,	best estimator xgboost's best error=8.4340
[flaml.automl: 09-16 17:52:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 17:52:51] {3072} INFO -  at 12.6s,	estimator xgboost's best error=4.9859,	best estimator xgboost's best error=4.9859
[flaml.automl: 09-16 17:52:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 17:52:54] {3072} INFO -  at 15.3s,	estimator xgboost's best error=4.9859,	best estimator xgboost's best error=4.9859
[flaml.automl: 09-16 17:52:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 17:52:55] {3072} INFO -  at 16.9s,	estimator xgboost's best error=4.9859,	best estimator xgboost's best error=4.9859
[flaml.automl: 09-16 17:52:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 17:52:58] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.9859,	best estimator xgboost's best error=4.9859
[flaml.automl: 09-16 17:52:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 17:52:59] {3072} INFO -  at 20.5s,	estimator xgboost's best error=4.9859,	best estimator xgboost's best error=4.9859
[flaml.automl: 09-16 17:52:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 17:53:01] {3072} INFO -  at 22.3s,	estimator xgboost's best error=4.9300,	best estimator xgboost's best error=4.9300
[flaml.automl: 09-16 17:53:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 17:53:02] {3072} INFO -  at 23.5s,	estimator xgboost's best error=4.9300,	best estimator xgboost's best error=4.9300
[flaml.automl: 09-16 17:53:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:53:09] {3072} INFO -  at 30.6s,	estimator xgboost's best error=3.5936,	best estimator xgboost's best error=3.5936
[flaml.automl: 09-16 17:53:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 17:53:22] {3072} INFO -  at 43.4s,	estimator xgboost's best error=3.4389,	best estimator xgboost's best error=3.4389
[flaml.automl: 09-16 17:53:22] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 17:53:29] {3072} INFO -  at 50.5s,	estimator xgboost's best error=3.4389,	best estimator xgboost's best error=3.4389
[flaml.automl: 09-16 17:53:42] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-16 17:53:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 17:53:42] {2636} INFO - fit succeeded
[flaml.automl: 09-16 17:53:42] {2637} INFO - Time taken to find the best model: 43.44420909881592
[flaml.automl: 09-16 17:53:42] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 81801}
PM2.5(0)最佳损失：-2.4388926360772145
PM2.5(0)最好结果：{'pred_time': 4.539553751081428e-06, 'wall_clock_time': 43.44420909881592, 'metric_for_logging': {'pred_time': 4.539553751081428e-06}, 'val_loss': 3.4388926360772145, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 81801}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 81801, 'experiment_tag': 'exp', 'time_total_s': 12.858689546585083}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9621152787377467
PM2.5(0)的mse=32.851096586914906
PM2.5(0)的mae=3.389888422549839
PM2.5(0)的mar=0.10772966484473985
总共花费的时间为：64.66
武汉市
1325A
1326A
1327A
1328A
1329A
1331A
3153A
[flaml.automl: 09-16 18:14:30] {2390} INFO - task = regression
[flaml.automl: 09-16 18:14:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 18:14:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 18:14:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 18:14:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 18:14:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 18:14:32] {3025} INFO - Estimated sufficient time budget=177884s. Estimated necessary time budget=178s.
[flaml.automl: 09-16 18:14:32] {3072} INFO -  at 2.7s,	estimator xgboost's best error=21.6635,	best estimator xgboost's best error=21.6635
[flaml.automl: 09-16 18:14:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 18:14:36] {3072} INFO -  at 6.2s,	estimator xgboost's best error=10.2625,	best estimator xgboost's best error=10.2625
[flaml.automl: 09-16 18:14:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 18:14:38] {3072} INFO -  at 8.1s,	estimator xgboost's best error=10.2625,	best estimator xgboost's best error=10.2625
[flaml.automl: 09-16 18:14:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 18:14:41] {3072} INFO -  at 11.0s,	estimator xgboost's best error=10.2625,	best estimator xgboost's best error=10.2625
[flaml.automl: 09-16 18:14:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 18:14:42] {3072} INFO -  at 13.0s,	estimator xgboost's best error=6.8501,	best estimator xgboost's best error=6.8501
[flaml.automl: 09-16 18:14:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 18:14:45] {3072} INFO -  at 15.6s,	estimator xgboost's best error=6.8501,	best estimator xgboost's best error=6.8501
[flaml.automl: 09-16 18:14:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 18:14:47] {3072} INFO -  at 17.9s,	estimator xgboost's best error=6.7830,	best estimator xgboost's best error=6.7830
[flaml.automl: 09-16 18:14:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 18:14:50] {3072} INFO -  at 20.4s,	estimator xgboost's best error=6.7830,	best estimator xgboost's best error=6.7830
[flaml.automl: 09-16 18:14:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 18:14:52] {3072} INFO -  at 22.4s,	estimator xgboost's best error=6.7830,	best estimator xgboost's best error=6.7830
[flaml.automl: 09-16 18:14:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 18:14:54] {3072} INFO -  at 24.1s,	estimator xgboost's best error=6.7830,	best estimator xgboost's best error=6.7830
[flaml.automl: 09-16 18:14:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 18:14:56] {3072} INFO -  at 26.3s,	estimator xgboost's best error=5.2718,	best estimator xgboost's best error=5.2718
[flaml.automl: 09-16 18:14:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 18:14:58] {3072} INFO -  at 28.2s,	estimator xgboost's best error=5.2718,	best estimator xgboost's best error=5.2718
[flaml.automl: 09-16 18:14:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 18:14:59] {3072} INFO -  at 29.8s,	estimator xgboost's best error=4.4156,	best estimator xgboost's best error=4.4156
[flaml.automl: 09-16 18:14:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 18:15:01] {3072} INFO -  at 31.4s,	estimator xgboost's best error=4.4156,	best estimator xgboost's best error=4.4156
[flaml.automl: 09-16 18:15:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 18:15:02] {3072} INFO -  at 32.9s,	estimator xgboost's best error=4.4156,	best estimator xgboost's best error=4.4156
[flaml.automl: 09-16 18:15:02] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 18:15:03] {3072} INFO -  at 34.0s,	estimator xgboost's best error=4.4156,	best estimator xgboost's best error=4.4156
[flaml.automl: 09-16 18:15:03] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 18:15:05] {3072} INFO -  at 35.4s,	estimator xgboost's best error=4.4156,	best estimator xgboost's best error=4.4156
[flaml.automl: 09-16 18:15:05] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 18:15:11] {3072} INFO -  at 41.4s,	estimator xgboost's best error=3.9082,	best estimator xgboost's best error=3.9082
[flaml.automl: 09-16 18:15:11] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 18:15:13] {3072} INFO -  at 43.8s,	estimator xgboost's best error=3.9082,	best estimator xgboost's best error=3.9082
[flaml.automl: 09-16 18:15:13] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 18:15:28] {3072} INFO -  at 58.7s,	estimator xgboost's best error=3.9082,	best estimator xgboost's best error=3.9082
[flaml.automl: 09-16 18:15:34] {3335} INFO - retrain xgboost for 6.0s
[flaml.automl: 09-16 18:15:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 18:15:34] {2636} INFO - fit succeeded
[flaml.automl: 09-16 18:15:34] {2637} INFO - Time taken to find the best model: 41.41400456428528
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 76138}
PM2.5(0)最佳损失：-2.9082417143152117
PM2.5(0)最好结果：{'pred_time': 4.756450653076172e-06, 'wall_clock_time': 41.41400456428528, 'metric_for_logging': {'pred_time': 4.756450653076172e-06}, 'val_loss': 3.9082417143152117, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 76138}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'config/FLAML_sample_size': 76138, 'experiment_tag': 'exp', 'time_total_s': 6.020079135894775}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9267361679584969
PM2.5(0)的mse=34.462290240252365
PM2.5(0)的mae=3.9692856498982043
PM2.5(0)的mar=0.17753974215298335
总共花费的时间为：66.01
长沙市
1340A
1342A
1343A
1344A
[flaml.automl: 09-16 18:28:00] {2390} INFO - task = regression
[flaml.automl: 09-16 18:28:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 18:28:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 18:28:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 18:28:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 18:28:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 18:28:01] {3025} INFO - Estimated sufficient time budget=47984s. Estimated necessary time budget=48s.
[flaml.automl: 09-16 18:28:01] {3072} INFO -  at 1.4s,	estimator xgboost's best error=23.2452,	best estimator xgboost's best error=23.2452
[flaml.automl: 09-16 18:28:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 18:28:03] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.9234,	best estimator xgboost's best error=10.9234
[flaml.automl: 09-16 18:28:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 18:28:04] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.9234,	best estimator xgboost's best error=10.9234
[flaml.automl: 09-16 18:28:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 18:28:11] {3072} INFO -  at 11.0s,	estimator xgboost's best error=10.9234,	best estimator xgboost's best error=10.9234
[flaml.automl: 09-16 18:28:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 18:28:12] {3072} INFO -  at 12.1s,	estimator xgboost's best error=7.2997,	best estimator xgboost's best error=7.2997
[flaml.automl: 09-16 18:28:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 18:28:14] {3072} INFO -  at 13.7s,	estimator xgboost's best error=6.4636,	best estimator xgboost's best error=6.4636
[flaml.automl: 09-16 18:28:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 18:28:15] {3072} INFO -  at 15.3s,	estimator xgboost's best error=5.4036,	best estimator xgboost's best error=5.4036
[flaml.automl: 09-16 18:28:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 18:28:18] {3072} INFO -  at 18.0s,	estimator xgboost's best error=5.4036,	best estimator xgboost's best error=5.4036
[flaml.automl: 09-16 18:28:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 18:28:19] {3072} INFO -  at 19.6s,	estimator xgboost's best error=5.4036,	best estimator xgboost's best error=5.4036
[flaml.automl: 09-16 18:28:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 18:28:22] {3072} INFO -  at 22.6s,	estimator xgboost's best error=4.3818,	best estimator xgboost's best error=4.3818
[flaml.automl: 09-16 18:28:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 18:28:24] {3072} INFO -  at 24.2s,	estimator xgboost's best error=4.3818,	best estimator xgboost's best error=4.3818
[flaml.automl: 09-16 18:28:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 18:28:25] {3072} INFO -  at 25.4s,	estimator xgboost's best error=4.3818,	best estimator xgboost's best error=4.3818
[flaml.automl: 09-16 18:28:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 18:28:29] {3072} INFO -  at 29.1s,	estimator xgboost's best error=4.0977,	best estimator xgboost's best error=4.0977
[flaml.automl: 09-16 18:28:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 18:28:33] {3072} INFO -  at 32.7s,	estimator xgboost's best error=3.8651,	best estimator xgboost's best error=3.8651
[flaml.automl: 09-16 18:28:33] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 18:28:35] {3072} INFO -  at 35.4s,	estimator xgboost's best error=3.8651,	best estimator xgboost's best error=3.8651
[flaml.automl: 09-16 18:28:35] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 18:28:38] {3072} INFO -  at 37.9s,	estimator xgboost's best error=3.8651,	best estimator xgboost's best error=3.8651
[flaml.automl: 09-16 18:28:38] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 18:28:40] {3072} INFO -  at 40.5s,	estimator xgboost's best error=3.7292,	best estimator xgboost's best error=3.7292
[flaml.automl: 09-16 18:28:40] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 18:28:42] {3072} INFO -  at 42.6s,	estimator xgboost's best error=3.7292,	best estimator xgboost's best error=3.7292
[flaml.automl: 09-16 18:28:42] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 18:28:44] {3072} INFO -  at 44.6s,	estimator xgboost's best error=3.7292,	best estimator xgboost's best error=3.7292
[flaml.automl: 09-16 18:28:44] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 18:28:46] {3072} INFO -  at 46.2s,	estimator xgboost's best error=3.7292,	best estimator xgboost's best error=3.7292
[flaml.automl: 09-16 18:28:46] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 18:28:47] {3072} INFO -  at 47.2s,	estimator xgboost's best error=3.7292,	best estimator xgboost's best error=3.7292
[flaml.automl: 09-16 18:28:47] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 18:28:59] {3072} INFO -  at 59.6s,	estimator xgboost's best error=3.3363,	best estimator xgboost's best error=3.3363
[flaml.automl: 09-16 18:29:14] {3335} INFO - retrain xgboost for 14.4s
[flaml.automl: 09-16 18:29:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4512592128754277,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.003827601076117227, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010065608612606477, reg_lambda=0.020722552365849516,
             scale_pos_weight=1, subsample=0.749001334635897,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 18:29:14] {2636} INFO - fit succeeded
[flaml.automl: 09-16 18:29:14] {2637} INFO - Time taken to find the best model: 59.58969831466675
[flaml.automl: 09-16 18:29:14] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.003827601076117227, 'learning_rate': 0.4512592128754277, 'subsample': 0.749001334635897, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0010065608612606477, 'reg_lambda': 0.020722552365849516, 'FLAML_sample_size': 40411}
PM2.5(0)最佳损失：-2.336279114273231
PM2.5(0)最好结果：{'pred_time': 9.074186700087458e-06, 'wall_clock_time': 59.58969831466675, 'metric_for_logging': {'pred_time': 9.074186700087458e-06}, 'val_loss': 3.336279114273231, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.003827601076117227, 'learning_rate': 0.4512592128754277, 'subsample': 0.749001334635897, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0010065608612606477, 'reg_lambda': 0.020722552365849516, 'FLAML_sample_size': 40411}, 'config/n_estimators': 35, 'config/max_leaves': 7, 'config/min_child_weight': 0.003827601076117227, 'config/learning_rate': 0.4512592128754277, 'config/subsample': 0.749001334635897, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0010065608612606477, 'config/reg_lambda': 0.020722552365849516, 'config/FLAML_sample_size': 40411, 'experiment_tag': 'exp', 'time_total_s': 12.386437892913818}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4512592128754277,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.003827601076117227, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010065608612606477, reg_lambda=0.020722552365849516,
             scale_pos_weight=1, subsample=0.749001334635897,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9547540007203129
PM2.5(0)的mse=24.660087981778684
PM2.5(0)的mae=3.2030710973816072
PM2.5(0)的mar=0.12151814785758419
总共花费的时间为：74.56
广州市
1345A
1346A
1349A
1351A
1352A
1354A
1355A
2846A
3299A
3300A
3301A
3302A
3303A
3304A
3443A
3445A
3446A
[flaml.automl: 09-16 19:19:26] {2390} INFO - task = regression
[flaml.automl: 09-16 19:19:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 19:19:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 19:19:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 19:19:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 19:19:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 19:19:27] {3025} INFO - Estimated sufficient time budget=221865s. Estimated necessary time budget=222s.
[flaml.automl: 09-16 19:19:27] {3072} INFO -  at 2.0s,	estimator xgboost's best error=13.4080,	best estimator xgboost's best error=13.4080
[flaml.automl: 09-16 19:19:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 19:19:28] {3072} INFO -  at 3.3s,	estimator xgboost's best error=9.5889,	best estimator xgboost's best error=9.5889
[flaml.automl: 09-16 19:19:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 19:19:30] {3072} INFO -  at 4.5s,	estimator xgboost's best error=9.5889,	best estimator xgboost's best error=9.5889
[flaml.automl: 09-16 19:19:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 19:19:31] {3072} INFO -  at 5.8s,	estimator xgboost's best error=9.5889,	best estimator xgboost's best error=9.5889
[flaml.automl: 09-16 19:19:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 19:19:32] {3072} INFO -  at 6.9s,	estimator xgboost's best error=3.9062,	best estimator xgboost's best error=3.9062
[flaml.automl: 09-16 19:19:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 19:19:33] {3072} INFO -  at 8.1s,	estimator xgboost's best error=3.9062,	best estimator xgboost's best error=3.9062
[flaml.automl: 09-16 19:19:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 19:19:34] {3072} INFO -  at 9.3s,	estimator xgboost's best error=3.9062,	best estimator xgboost's best error=3.9062
[flaml.automl: 09-16 19:19:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 19:19:35] {3072} INFO -  at 10.3s,	estimator xgboost's best error=3.9062,	best estimator xgboost's best error=3.9062
[flaml.automl: 09-16 19:19:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 19:19:37] {3072} INFO -  at 11.5s,	estimator xgboost's best error=3.9062,	best estimator xgboost's best error=3.9062
[flaml.automl: 09-16 19:19:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 19:19:38] {3072} INFO -  at 12.6s,	estimator xgboost's best error=3.8700,	best estimator xgboost's best error=3.8700
[flaml.automl: 09-16 19:19:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 19:19:40] {3072} INFO -  at 14.5s,	estimator xgboost's best error=3.8700,	best estimator xgboost's best error=3.8700
[flaml.automl: 09-16 19:19:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 19:19:41] {3072} INFO -  at 15.7s,	estimator xgboost's best error=3.1647,	best estimator xgboost's best error=3.1647
[flaml.automl: 09-16 19:19:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 19:19:42] {3072} INFO -  at 16.8s,	estimator xgboost's best error=3.1647,	best estimator xgboost's best error=3.1647
[flaml.automl: 09-16 19:19:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 19:19:46] {3072} INFO -  at 21.3s,	estimator xgboost's best error=2.4763,	best estimator xgboost's best error=2.4763
[flaml.automl: 09-16 19:19:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 19:19:50] {3072} INFO -  at 25.1s,	estimator xgboost's best error=2.4577,	best estimator xgboost's best error=2.4577
[flaml.automl: 09-16 19:19:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 19:19:54] {3072} INFO -  at 28.5s,	estimator xgboost's best error=2.4577,	best estimator xgboost's best error=2.4577
[flaml.automl: 09-16 19:19:54] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 19:19:57] {3072} INFO -  at 31.4s,	estimator xgboost's best error=2.4577,	best estimator xgboost's best error=2.4577
[flaml.automl: 09-16 19:19:57] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 19:19:59] {3072} INFO -  at 34.3s,	estimator xgboost's best error=2.4577,	best estimator xgboost's best error=2.4577
[flaml.automl: 09-16 19:19:59] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 19:20:02] {3072} INFO -  at 36.9s,	estimator xgboost's best error=2.4577,	best estimator xgboost's best error=2.4577
[flaml.automl: 09-16 19:20:02] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 19:20:10] {3072} INFO -  at 45.1s,	estimator xgboost's best error=2.4062,	best estimator xgboost's best error=2.4062
[flaml.automl: 09-16 19:20:10] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 19:20:13] {3072} INFO -  at 48.3s,	estimator xgboost's best error=2.4062,	best estimator xgboost's best error=2.4062
[flaml.automl: 09-16 19:20:13] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 19:20:25] {3072} INFO -  at 59.8s,	estimator xgboost's best error=2.3841,	best estimator xgboost's best error=2.3841
[flaml.automl: 09-16 19:20:50] {3335} INFO - retrain xgboost for 24.9s
[flaml.automl: 09-16 19:20:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8876733795506421, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=1.335276597512497,
             missing=nan, monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.01668998862785672, scale_pos_weight=1,
             subsample=0.9838151340935325, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 19:20:50] {2636} INFO - fit succeeded
[flaml.automl: 09-16 19:20:50] {2637} INFO - Time taken to find the best model: 59.7849977016449
[flaml.automl: 09-16 19:20:50] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 14, 'min_child_weight': 1.335276597512497, 'learning_rate': 1.0, 'subsample': 0.9838151340935325, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8876733795506421, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.01668998862785672, 'FLAML_sample_size': 188560}
PM2.5(0)最佳损失：-1.3840744458974163
PM2.5(0)最好结果：{'pred_time': 2.132134839932215e-06, 'wall_clock_time': 59.7849977016449, 'metric_for_logging': {'pred_time': 2.132134839932215e-06}, 'val_loss': 2.3840744458974163, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 14, 'min_child_weight': 1.335276597512497, 'learning_rate': 1.0, 'subsample': 0.9838151340935325, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8876733795506421, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.01668998862785672, 'FLAML_sample_size': 188560}, 'config/n_estimators': 31, 'config/max_leaves': 14, 'config/min_child_weight': 1.335276597512497, 'config/learning_rate': 1.0, 'config/subsample': 0.9838151340935325, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8876733795506421, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.01668998862785672, 'config/FLAML_sample_size': 188560, 'experiment_tag': 'exp', 'time_total_s': 11.444411754608154}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8876733795506421, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=1.335276597512497,
             missing=nan, monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.01668998862785672, scale_pos_weight=1,
             subsample=0.9838151340935325, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9165297198001088
PM2.5(0)的mse=12.919036821222328
PM2.5(0)的mae=2.3355288165307817
PM2.5(0)的mar=0.14447964363510024
总共花费的时间为：87.42
深圳市
1356A
1363A
1364A
1365A
1366A
3305A
3306A
3307A
3447A
3623A
[flaml.automl: 09-16 19:50:50] {2390} INFO - task = regression
[flaml.automl: 09-16 19:50:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 19:50:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 19:50:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 19:50:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 19:50:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 19:50:51] {3025} INFO - Estimated sufficient time budget=134808s. Estimated necessary time budget=135s.
[flaml.automl: 09-16 19:50:51] {3072} INFO -  at 1.7s,	estimator xgboost's best error=9.9098,	best estimator xgboost's best error=9.9098
[flaml.automl: 09-16 19:50:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 19:50:53] {3072} INFO -  at 3.8s,	estimator xgboost's best error=4.8398,	best estimator xgboost's best error=4.8398
[flaml.automl: 09-16 19:50:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 19:50:54] {3072} INFO -  at 5.0s,	estimator xgboost's best error=4.8398,	best estimator xgboost's best error=4.8398
[flaml.automl: 09-16 19:50:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 19:50:56] {3072} INFO -  at 7.2s,	estimator xgboost's best error=4.8398,	best estimator xgboost's best error=4.8398
[flaml.automl: 09-16 19:50:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 19:50:57] {3072} INFO -  at 8.4s,	estimator xgboost's best error=3.5458,	best estimator xgboost's best error=3.5458
[flaml.automl: 09-16 19:50:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 19:50:59] {3072} INFO -  at 10.0s,	estimator xgboost's best error=3.5458,	best estimator xgboost's best error=3.5458
[flaml.automl: 09-16 19:50:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 19:51:01] {3072} INFO -  at 11.7s,	estimator xgboost's best error=2.3436,	best estimator xgboost's best error=2.3436
[flaml.automl: 09-16 19:51:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 19:51:03] {3072} INFO -  at 13.5s,	estimator xgboost's best error=2.3436,	best estimator xgboost's best error=2.3436
[flaml.automl: 09-16 19:51:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 19:51:04] {3072} INFO -  at 15.1s,	estimator xgboost's best error=2.3436,	best estimator xgboost's best error=2.3436
[flaml.automl: 09-16 19:51:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 19:51:06] {3072} INFO -  at 16.9s,	estimator xgboost's best error=2.3436,	best estimator xgboost's best error=2.3436
[flaml.automl: 09-16 19:51:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 19:51:07] {3072} INFO -  at 18.4s,	estimator xgboost's best error=2.3436,	best estimator xgboost's best error=2.3436
[flaml.automl: 09-16 19:51:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 19:51:09] {3072} INFO -  at 20.2s,	estimator xgboost's best error=2.3019,	best estimator xgboost's best error=2.3019
[flaml.automl: 09-16 19:51:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 19:51:11] {3072} INFO -  at 21.4s,	estimator xgboost's best error=2.3019,	best estimator xgboost's best error=2.3019
[flaml.automl: 09-16 19:51:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 19:51:18] {3072} INFO -  at 28.6s,	estimator xgboost's best error=1.9456,	best estimator xgboost's best error=1.9456
[flaml.automl: 09-16 19:51:18] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 19:51:31] {3072} INFO -  at 41.5s,	estimator xgboost's best error=1.9119,	best estimator xgboost's best error=1.9119
[flaml.automl: 09-16 19:51:31] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 19:51:38] {3072} INFO -  at 48.6s,	estimator xgboost's best error=1.9119,	best estimator xgboost's best error=1.9119
[flaml.automl: 09-16 19:51:50] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-16 19:51:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 19:51:50] {2636} INFO - fit succeeded
[flaml.automl: 09-16 19:51:50] {2637} INFO - Time taken to find the best model: 41.495691537857056
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 111921}
PM2.5(0)最佳损失：-0.9118675487568311
PM2.5(0)最好结果：{'pred_time': 3.2787155821301157e-06, 'wall_clock_time': 41.495691537857056, 'metric_for_logging': {'pred_time': 3.2787155821301157e-06}, 'val_loss': 1.911867548756831, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 111921}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 111921, 'experiment_tag': 'exp', 'time_total_s': 12.90248155593872}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9170770068803192
PM2.5(0)的mse=9.449242444222314
PM2.5(0)的mae=1.9094832584936965
PM2.5(0)的mar=0.17500698622285318
总共花费的时间为：63.14
珠海市
1368A
1369A
1370A
3308A
3448A
[flaml.automl: 09-16 20:07:11] {2390} INFO - task = regression
[flaml.automl: 09-16 20:07:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:07:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:07:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:07:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:07:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:07:13] {3025} INFO - Estimated sufficient time budget=66955s. Estimated necessary time budget=67s.
[flaml.automl: 09-16 20:07:13] {3072} INFO -  at 1.5s,	estimator xgboost's best error=10.6846,	best estimator xgboost's best error=10.6846
[flaml.automl: 09-16 20:07:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:07:15] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.5670,	best estimator xgboost's best error=5.5670
[flaml.automl: 09-16 20:07:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:07:16] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.5670,	best estimator xgboost's best error=5.5670
[flaml.automl: 09-16 20:07:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:07:21] {3072} INFO -  at 9.6s,	estimator xgboost's best error=5.5670,	best estimator xgboost's best error=5.5670
[flaml.automl: 09-16 20:07:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:07:22] {3072} INFO -  at 10.7s,	estimator xgboost's best error=4.5474,	best estimator xgboost's best error=4.5474
[flaml.automl: 09-16 20:07:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:07:23] {3072} INFO -  at 12.3s,	estimator xgboost's best error=4.2101,	best estimator xgboost's best error=4.2101
[flaml.automl: 09-16 20:07:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:07:25] {3072} INFO -  at 14.0s,	estimator xgboost's best error=3.9586,	best estimator xgboost's best error=3.9586
[flaml.automl: 09-16 20:07:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:07:28] {3072} INFO -  at 16.6s,	estimator xgboost's best error=3.9586,	best estimator xgboost's best error=3.9586
[flaml.automl: 09-16 20:07:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:07:29] {3072} INFO -  at 18.2s,	estimator xgboost's best error=3.9586,	best estimator xgboost's best error=3.9586
[flaml.automl: 09-16 20:07:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:07:32] {3072} INFO -  at 21.3s,	estimator xgboost's best error=3.6291,	best estimator xgboost's best error=3.6291
[flaml.automl: 09-16 20:07:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:07:34] {3072} INFO -  at 22.9s,	estimator xgboost's best error=3.6291,	best estimator xgboost's best error=3.6291
[flaml.automl: 09-16 20:07:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:07:35] {3072} INFO -  at 24.0s,	estimator xgboost's best error=3.6291,	best estimator xgboost's best error=3.6291
[flaml.automl: 09-16 20:07:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:07:38] {3072} INFO -  at 26.9s,	estimator xgboost's best error=3.6291,	best estimator xgboost's best error=3.6291
[flaml.automl: 09-16 20:07:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:07:41] {3072} INFO -  at 29.7s,	estimator xgboost's best error=3.6291,	best estimator xgboost's best error=3.6291
[flaml.automl: 09-16 20:07:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:07:44] {3072} INFO -  at 32.8s,	estimator xgboost's best error=3.6291,	best estimator xgboost's best error=3.6291
[flaml.automl: 09-16 20:07:44] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 20:07:49] {3072} INFO -  at 37.7s,	estimator xgboost's best error=3.6291,	best estimator xgboost's best error=3.6291
[flaml.automl: 09-16 20:07:49] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 20:07:51] {3072} INFO -  at 39.6s,	estimator xgboost's best error=3.6291,	best estimator xgboost's best error=3.6291
[flaml.automl: 09-16 20:07:51] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 20:07:52] {3072} INFO -  at 41.3s,	estimator xgboost's best error=3.6291,	best estimator xgboost's best error=3.6291
[flaml.automl: 09-16 20:07:52] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 20:08:00] {3072} INFO -  at 48.6s,	estimator xgboost's best error=3.4848,	best estimator xgboost's best error=3.4848
[flaml.automl: 09-16 20:08:00] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 20:08:02] {3072} INFO -  at 51.1s,	estimator xgboost's best error=3.4848,	best estimator xgboost's best error=3.4848
[flaml.automl: 09-16 20:08:02] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 20:08:10] {3072} INFO -  at 58.4s,	estimator xgboost's best error=3.4848,	best estimator xgboost's best error=3.4848
[flaml.automl: 09-16 20:08:17] {3335} INFO - retrain xgboost for 7.3s
[flaml.automl: 09-16 20:08:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5492977310397356, colsample_bynode=1,
             colsample_bytree=0.9510761842589558, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.04321599195729943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5415707634193446, scale_pos_weight=1,
             subsample=0.9351529901519405, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 20:08:17] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:08:17] {2637} INFO - Time taken to find the best model: 48.617449045181274
[flaml.automl: 09-16 20:08:17] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.04321599195729943, 'learning_rate': 1.0, 'subsample': 0.9351529901519405, 'colsample_bylevel': 0.5492977310397356, 'colsample_bytree': 0.9510761842589558, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5415707634193446, 'FLAML_sample_size': 55692}
PM2.5(0)最佳损失：-2.484760243833335
PM2.5(0)最好结果：{'pred_time': 6.508927385346228e-06, 'wall_clock_time': 48.617449045181274, 'metric_for_logging': {'pred_time': 6.508927385346228e-06}, 'val_loss': 3.484760243833335, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.04321599195729943, 'learning_rate': 1.0, 'subsample': 0.9351529901519405, 'colsample_bylevel': 0.5492977310397356, 'colsample_bytree': 0.9510761842589558, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5415707634193446, 'FLAML_sample_size': 55692}, 'config/n_estimators': 8, 'config/max_leaves': 16, 'config/min_child_weight': 0.04321599195729943, 'config/learning_rate': 1.0, 'config/subsample': 0.9351529901519405, 'config/colsample_bylevel': 0.5492977310397356, 'config/colsample_bytree': 0.9510761842589558, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.5415707634193446, 'config/FLAML_sample_size': 55692, 'experiment_tag': 'exp', 'time_total_s': 7.34019660949707}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5492977310397356, colsample_bynode=1,
             colsample_bytree=0.9510761842589558, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.04321599195729943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5415707634193446, scale_pos_weight=1,
             subsample=0.9351529901519405, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.80318688189841
PM2.5(0)的mse=23.917671932635333
PM2.5(0)的mae=3.4442257823518987
PM2.5(0)的mar=0.2901509347994253
总共花费的时间为：66.56
佛山市
1371A
1372A
1373A
1377A
1378A
3625A
[flaml.automl: 09-16 20:27:50] {2390} INFO - task = regression
[flaml.automl: 09-16 20:27:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:27:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:27:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:27:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:27:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:27:52] {3025} INFO - Estimated sufficient time budget=80336s. Estimated necessary time budget=80s.
[flaml.automl: 09-16 20:27:52] {3072} INFO -  at 1.5s,	estimator xgboost's best error=13.3246,	best estimator xgboost's best error=13.3246
[flaml.automl: 09-16 20:27:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:27:54] {3072} INFO -  at 3.6s,	estimator xgboost's best error=6.4112,	best estimator xgboost's best error=6.4112
[flaml.automl: 09-16 20:27:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:27:55] {3072} INFO -  at 4.8s,	estimator xgboost's best error=6.4112,	best estimator xgboost's best error=6.4112
[flaml.automl: 09-16 20:27:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:27:59] {3072} INFO -  at 8.5s,	estimator xgboost's best error=6.4112,	best estimator xgboost's best error=6.4112
[flaml.automl: 09-16 20:27:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:28:00] {3072} INFO -  at 9.7s,	estimator xgboost's best error=4.0993,	best estimator xgboost's best error=4.0993
[flaml.automl: 09-16 20:28:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:28:01] {3072} INFO -  at 11.3s,	estimator xgboost's best error=4.0001,	best estimator xgboost's best error=4.0001
[flaml.automl: 09-16 20:28:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:28:03] {3072} INFO -  at 12.9s,	estimator xgboost's best error=3.3580,	best estimator xgboost's best error=3.3580
[flaml.automl: 09-16 20:28:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:28:06] {3072} INFO -  at 15.6s,	estimator xgboost's best error=3.3580,	best estimator xgboost's best error=3.3580
[flaml.automl: 09-16 20:28:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:28:07] {3072} INFO -  at 17.2s,	estimator xgboost's best error=3.3580,	best estimator xgboost's best error=3.3580
[flaml.automl: 09-16 20:28:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:28:10] {3072} INFO -  at 20.2s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 20:28:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:28:12] {3072} INFO -  at 21.9s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 20:28:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:28:13] {3072} INFO -  at 23.0s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 20:28:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:28:15] {3072} INFO -  at 25.0s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 20:28:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:28:17] {3072} INFO -  at 27.2s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 20:28:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:28:20] {3072} INFO -  at 30.2s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 20:28:20] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 20:28:25] {3072} INFO -  at 35.2s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 20:28:25] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 20:28:27] {3072} INFO -  at 37.1s,	estimator xgboost's best error=3.0345,	best estimator xgboost's best error=3.0345
[flaml.automl: 09-16 20:28:27] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 20:28:29] {3072} INFO -  at 38.8s,	estimator xgboost's best error=3.0345,	best estimator xgboost's best error=3.0345
[flaml.automl: 09-16 20:28:29] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 20:28:33] {3072} INFO -  at 43.1s,	estimator xgboost's best error=2.8542,	best estimator xgboost's best error=2.8542
[flaml.automl: 09-16 20:28:33] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 20:28:35] {3072} INFO -  at 44.8s,	estimator xgboost's best error=2.8542,	best estimator xgboost's best error=2.8542
[flaml.automl: 09-16 20:28:35] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 20:28:48] {3072} INFO -  at 58.1s,	estimator xgboost's best error=2.7021,	best estimator xgboost's best error=2.7021
[flaml.automl: 09-16 20:29:01] {3335} INFO - retrain xgboost for 13.2s
[flaml.automl: 09-16 20:29:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 20:29:01] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:29:01] {2637} INFO - Time taken to find the best model: 58.09171390533447
[flaml.automl: 09-16 20:29:01] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 67252}
PM2.5(0)最佳损失：-1.7020955434465095
PM2.5(0)最好结果：{'pred_time': 5.432899962080286e-06, 'wall_clock_time': 58.09171390533447, 'metric_for_logging': {'pred_time': 5.432899962080286e-06}, 'val_loss': 2.7020955434465095, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 67252}, 'config/n_estimators': 13, 'config/max_leaves': 18, 'config/min_child_weight': 0.002212987346630184, 'config/learning_rate': 1.0, 'config/subsample': 0.9736629142498529, 'config/colsample_bylevel': 0.8717925041332704, 'config/colsample_bytree': 0.9857450191529009, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.04619111740498511, 'config/FLAML_sample_size': 67252, 'experiment_tag': 'exp', 'time_total_s': 13.296803951263428}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9060598173603353
PM2.5(0)的mse=16.36366014617429
PM2.5(0)的mae=2.696207166077013
PM2.5(0)的mar=0.18542201989008117
总共花费的时间为：72.37
中山市
1379A
3454A
[flaml.automl: 09-16 20:35:27] {2390} INFO - task = regression
[flaml.automl: 09-16 20:35:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:35:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:35:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:35:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:35:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:35:29] {3025} INFO - Estimated sufficient time budget=21669s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 20:35:29] {3072} INFO -  at 2.3s,	estimator xgboost's best error=11.2620,	best estimator xgboost's best error=11.2620
[flaml.automl: 09-16 20:35:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:35:33] {3072} INFO -  at 6.3s,	estimator xgboost's best error=5.4190,	best estimator xgboost's best error=5.4190
[flaml.automl: 09-16 20:35:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:35:35] {3072} INFO -  at 8.4s,	estimator xgboost's best error=5.4190,	best estimator xgboost's best error=5.4190
[flaml.automl: 09-16 20:35:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:35:53] {3072} INFO -  at 26.1s,	estimator xgboost's best error=5.4190,	best estimator xgboost's best error=5.4190
[flaml.automl: 09-16 20:35:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:35:55] {3072} INFO -  at 28.2s,	estimator xgboost's best error=3.6599,	best estimator xgboost's best error=3.6599
[flaml.automl: 09-16 20:35:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:35:58] {3072} INFO -  at 31.1s,	estimator xgboost's best error=3.5209,	best estimator xgboost's best error=3.5209
[flaml.automl: 09-16 20:35:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:36:01] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.0805,	best estimator xgboost's best error=3.0805
[flaml.automl: 09-16 20:36:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:36:06] {3072} INFO -  at 39.2s,	estimator xgboost's best error=3.0805,	best estimator xgboost's best error=3.0805
[flaml.automl: 09-16 20:36:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:36:09] {3072} INFO -  at 42.3s,	estimator xgboost's best error=3.0805,	best estimator xgboost's best error=3.0805
[flaml.automl: 09-16 20:36:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:36:13] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.0394,	best estimator xgboost's best error=3.0394
[flaml.automl: 09-16 20:36:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:36:15] {3072} INFO -  at 48.0s,	estimator xgboost's best error=3.0394,	best estimator xgboost's best error=3.0394
[flaml.automl: 09-16 20:36:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:36:16] {3072} INFO -  at 49.1s,	estimator xgboost's best error=3.0394,	best estimator xgboost's best error=3.0394
[flaml.automl: 09-16 20:36:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:36:26] {3072} INFO -  at 59.2s,	estimator xgboost's best error=2.7290,	best estimator xgboost's best error=2.7290
[flaml.automl: 09-16 20:36:38] {3335} INFO - retrain xgboost for 11.7s
[flaml.automl: 09-16 20:36:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 20:36:38] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:36:38] {2637} INFO - Time taken to find the best model: 59.23672080039978
[flaml.automl: 09-16 20:36:38] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-1.7290003453717868
PM2.5(0)最好结果：{'pred_time': 1.5723017083092182e-05, 'wall_clock_time': 59.23672080039978, 'metric_for_logging': {'pred_time': 1.5723017083092182e-05}, 'val_loss': 2.729000345371787, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 10.116617441177368}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9074432873177161
PM2.5(0)的mse=14.66832820779776
PM2.5(0)的mae=2.63502865501268
PM2.5(0)的mar=0.22981665774337984
总共花费的时间为：71.36
江门市
1386A
3449A
[flaml.automl: 09-16 20:43:28] {2390} INFO - task = regression
[flaml.automl: 09-16 20:43:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:43:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:43:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:43:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:43:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:43:30] {3025} INFO - Estimated sufficient time budget=21994s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 20:43:30] {3072} INFO -  at 2.3s,	estimator xgboost's best error=12.0062,	best estimator xgboost's best error=12.0062
[flaml.automl: 09-16 20:43:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:43:34] {3072} INFO -  at 6.2s,	estimator xgboost's best error=5.7897,	best estimator xgboost's best error=5.7897
[flaml.automl: 09-16 20:43:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:43:37] {3072} INFO -  at 8.5s,	estimator xgboost's best error=5.7897,	best estimator xgboost's best error=5.7897
[flaml.automl: 09-16 20:43:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:44:03] {3072} INFO -  at 34.9s,	estimator xgboost's best error=5.7897,	best estimator xgboost's best error=5.7897
[flaml.automl: 09-16 20:44:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:44:06] {3072} INFO -  at 38.3s,	estimator xgboost's best error=4.2595,	best estimator xgboost's best error=4.2595
[flaml.automl: 09-16 20:44:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:44:11] {3072} INFO -  at 42.7s,	estimator xgboost's best error=3.7754,	best estimator xgboost's best error=3.7754
[flaml.automl: 09-16 20:44:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:44:15] {3072} INFO -  at 47.4s,	estimator xgboost's best error=3.3393,	best estimator xgboost's best error=3.3393
[flaml.automl: 09-16 20:44:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:44:23] {3072} INFO -  at 55.1s,	estimator xgboost's best error=3.3393,	best estimator xgboost's best error=3.3393
[flaml.automl: 09-16 20:44:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:44:27] {3072} INFO -  at 58.5s,	estimator xgboost's best error=3.3393,	best estimator xgboost's best error=3.3393
[flaml.automl: 09-16 20:44:29] {3335} INFO - retrain xgboost for 2.7s
[flaml.automl: 09-16 20:44:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 20:44:29] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:44:29] {2637} INFO - Time taken to find the best model: 47.35666108131409
[flaml.automl: 09-16 20:44:29] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-2.339283901541394
PM2.5(0)最好结果：{'pred_time': 5.0305436431281455e-05, 'wall_clock_time': 47.35666108131409, 'metric_for_logging': {'pred_time': 5.0305436431281455e-05}, 'val_loss': 3.339283901541394, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.614432334899902}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.862633555684537
PM2.5(0)的mse=21.388062586850193
PM2.5(0)的mae=3.295606947316476
PM2.5(0)的mar=0.30478049134099017
总共花费的时间为：61.61
东莞市
1389A
1391A
3319A
3626A
3627A
[flaml.automl: 09-16 21:01:01] {2390} INFO - task = regression
[flaml.automl: 09-16 21:01:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:01:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:01:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:01:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:01:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:01:02] {3025} INFO - Estimated sufficient time budget=66514s. Estimated necessary time budget=67s.
[flaml.automl: 09-16 21:01:02] {3072} INFO -  at 1.5s,	estimator xgboost's best error=12.8060,	best estimator xgboost's best error=12.8060
[flaml.automl: 09-16 21:01:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:01:04] {3072} INFO -  at 3.6s,	estimator xgboost's best error=6.1022,	best estimator xgboost's best error=6.1022
[flaml.automl: 09-16 21:01:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:01:05] {3072} INFO -  at 4.8s,	estimator xgboost's best error=6.1022,	best estimator xgboost's best error=6.1022
[flaml.automl: 09-16 21:01:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:01:10] {3072} INFO -  at 9.6s,	estimator xgboost's best error=6.1022,	best estimator xgboost's best error=6.1022
[flaml.automl: 09-16 21:01:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:01:11] {3072} INFO -  at 10.7s,	estimator xgboost's best error=4.3341,	best estimator xgboost's best error=4.3341
[flaml.automl: 09-16 21:01:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:01:13] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.7406,	best estimator xgboost's best error=3.7406
[flaml.automl: 09-16 21:01:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:01:14] {3072} INFO -  at 13.9s,	estimator xgboost's best error=3.3212,	best estimator xgboost's best error=3.3212
[flaml.automl: 09-16 21:01:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:01:17] {3072} INFO -  at 16.6s,	estimator xgboost's best error=3.3212,	best estimator xgboost's best error=3.3212
[flaml.automl: 09-16 21:01:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:01:19] {3072} INFO -  at 18.2s,	estimator xgboost's best error=3.3212,	best estimator xgboost's best error=3.3212
[flaml.automl: 09-16 21:01:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:01:22] {3072} INFO -  at 21.3s,	estimator xgboost's best error=2.9418,	best estimator xgboost's best error=2.9418
[flaml.automl: 09-16 21:01:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:01:23] {3072} INFO -  at 22.9s,	estimator xgboost's best error=2.9418,	best estimator xgboost's best error=2.9418
[flaml.automl: 09-16 21:01:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:01:24] {3072} INFO -  at 24.0s,	estimator xgboost's best error=2.9418,	best estimator xgboost's best error=2.9418
[flaml.automl: 09-16 21:01:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:01:27] {3072} INFO -  at 26.9s,	estimator xgboost's best error=2.8459,	best estimator xgboost's best error=2.8459
[flaml.automl: 09-16 21:01:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:01:30] {3072} INFO -  at 29.8s,	estimator xgboost's best error=2.8459,	best estimator xgboost's best error=2.8459
[flaml.automl: 09-16 21:01:30] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:01:33] {3072} INFO -  at 32.3s,	estimator xgboost's best error=2.8279,	best estimator xgboost's best error=2.8279
[flaml.automl: 09-16 21:01:33] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 21:01:35] {3072} INFO -  at 34.6s,	estimator xgboost's best error=2.8279,	best estimator xgboost's best error=2.8279
[flaml.automl: 09-16 21:01:35] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 21:01:37] {3072} INFO -  at 36.1s,	estimator xgboost's best error=2.8279,	best estimator xgboost's best error=2.8279
[flaml.automl: 09-16 21:01:37] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 21:01:38] {3072} INFO -  at 37.8s,	estimator xgboost's best error=2.8279,	best estimator xgboost's best error=2.8279
[flaml.automl: 09-16 21:01:38] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 21:01:41] {3072} INFO -  at 40.6s,	estimator xgboost's best error=2.8279,	best estimator xgboost's best error=2.8279
[flaml.automl: 09-16 21:01:41] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 21:01:48] {3072} INFO -  at 48.0s,	estimator xgboost's best error=2.7070,	best estimator xgboost's best error=2.7070
[flaml.automl: 09-16 21:01:48] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 21:01:51] {3072} INFO -  at 50.7s,	estimator xgboost's best error=2.7070,	best estimator xgboost's best error=2.7070
[flaml.automl: 09-16 21:01:51] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 21:01:58] {3072} INFO -  at 57.8s,	estimator xgboost's best error=2.6105,	best estimator xgboost's best error=2.6105
[flaml.automl: 09-16 21:02:21] {3335} INFO - retrain xgboost for 22.9s
[flaml.automl: 09-16 21:02:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7869551594064775, colsample_bynode=1,
             colsample_bytree=0.7933397443475808, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=41, min_child_weight=0.009416638758491828,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009169417918441369, reg_lambda=0.1716204668378346,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:02:21] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:02:21] {2637} INFO - Time taken to find the best model: 57.75165247917175
[flaml.automl: 09-16 21:02:21] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 10, 'max_leaves': 41, 'min_child_weight': 0.009416638758491828, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7869551594064775, 'colsample_bytree': 0.7933397443475808, 'reg_alpha': 0.009169417918441369, 'reg_lambda': 0.1716204668378346, 'FLAML_sample_size': 55638}
PM2.5(0)最佳损失：-1.6104823677758113
PM2.5(0)最好结果：{'pred_time': 6.560791815341541e-06, 'wall_clock_time': 57.75165247917175, 'metric_for_logging': {'pred_time': 6.560791815341541e-06}, 'val_loss': 2.6104823677758113, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 41, 'min_child_weight': 0.009416638758491828, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7869551594064775, 'colsample_bytree': 0.7933397443475808, 'reg_alpha': 0.009169417918441369, 'reg_lambda': 0.1716204668378346, 'FLAML_sample_size': 55638}, 'config/n_estimators': 10, 'config/max_leaves': 41, 'config/min_child_weight': 0.009416638758491828, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7869551594064775, 'config/colsample_bytree': 0.7933397443475808, 'config/reg_alpha': 0.009169417918441369, 'config/reg_lambda': 0.1716204668378346, 'config/FLAML_sample_size': 55638, 'experiment_tag': 'exp', 'time_total_s': 7.056588888168335}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7869551594064775, colsample_bynode=1,
             colsample_bytree=0.7933397443475808, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=41, min_child_weight=0.009416638758491828,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009169417918441369, reg_lambda=0.1716204668378346,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9046732638027036
PM2.5(0)的mse=14.745043266740538
PM2.5(0)的mae=2.545870320906995
PM2.5(0)的mar=0.1708634389768689
总共花费的时间为：81.49
惠州市
1392A
1393A
1395A
1396A
3314A
3452A
[flaml.automl: 09-16 21:21:00] {2390} INFO - task = regression
[flaml.automl: 09-16 21:21:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:21:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:21:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:21:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:21:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:21:01] {3025} INFO - Estimated sufficient time budget=77302s. Estimated necessary time budget=77s.
[flaml.automl: 09-16 21:21:01] {3072} INFO -  at 1.5s,	estimator xgboost's best error=10.2586,	best estimator xgboost's best error=10.2586
[flaml.automl: 09-16 21:21:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:21:03] {3072} INFO -  at 3.6s,	estimator xgboost's best error=4.9720,	best estimator xgboost's best error=4.9720
[flaml.automl: 09-16 21:21:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:21:04] {3072} INFO -  at 4.8s,	estimator xgboost's best error=4.9720,	best estimator xgboost's best error=4.9720
[flaml.automl: 09-16 21:21:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:21:08] {3072} INFO -  at 8.5s,	estimator xgboost's best error=4.9720,	best estimator xgboost's best error=4.9720
[flaml.automl: 09-16 21:21:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:21:09] {3072} INFO -  at 9.7s,	estimator xgboost's best error=3.6381,	best estimator xgboost's best error=3.6381
[flaml.automl: 09-16 21:21:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:21:11] {3072} INFO -  at 11.3s,	estimator xgboost's best error=3.2163,	best estimator xgboost's best error=3.2163
[flaml.automl: 09-16 21:21:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:21:13] {3072} INFO -  at 12.9s,	estimator xgboost's best error=2.8819,	best estimator xgboost's best error=2.8819
[flaml.automl: 09-16 21:21:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:21:15] {3072} INFO -  at 15.6s,	estimator xgboost's best error=2.8819,	best estimator xgboost's best error=2.8819
[flaml.automl: 09-16 21:21:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:21:17] {3072} INFO -  at 17.2s,	estimator xgboost's best error=2.8819,	best estimator xgboost's best error=2.8819
[flaml.automl: 09-16 21:21:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:21:20] {3072} INFO -  at 20.2s,	estimator xgboost's best error=2.5309,	best estimator xgboost's best error=2.5309
[flaml.automl: 09-16 21:21:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:21:21] {3072} INFO -  at 21.8s,	estimator xgboost's best error=2.5309,	best estimator xgboost's best error=2.5309
[flaml.automl: 09-16 21:21:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:21:23] {3072} INFO -  at 23.0s,	estimator xgboost's best error=2.5309,	best estimator xgboost's best error=2.5309
[flaml.automl: 09-16 21:21:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:21:25] {3072} INFO -  at 24.9s,	estimator xgboost's best error=2.5309,	best estimator xgboost's best error=2.5309
[flaml.automl: 09-16 21:21:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:21:27] {3072} INFO -  at 27.5s,	estimator xgboost's best error=2.5309,	best estimator xgboost's best error=2.5309
[flaml.automl: 09-16 21:21:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:21:30] {3072} INFO -  at 30.6s,	estimator xgboost's best error=2.5233,	best estimator xgboost's best error=2.5233
[flaml.automl: 09-16 21:21:30] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 21:21:35] {3072} INFO -  at 35.6s,	estimator xgboost's best error=2.5233,	best estimator xgboost's best error=2.5233
[flaml.automl: 09-16 21:21:35] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 21:21:37] {3072} INFO -  at 37.5s,	estimator xgboost's best error=2.5176,	best estimator xgboost's best error=2.5176
[flaml.automl: 09-16 21:21:37] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 21:21:39] {3072} INFO -  at 39.1s,	estimator xgboost's best error=2.5176,	best estimator xgboost's best error=2.5176
[flaml.automl: 09-16 21:21:39] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 21:21:43] {3072} INFO -  at 43.5s,	estimator xgboost's best error=2.3265,	best estimator xgboost's best error=2.3265
[flaml.automl: 09-16 21:21:43] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 21:21:45] {3072} INFO -  at 45.2s,	estimator xgboost's best error=2.3265,	best estimator xgboost's best error=2.3265
[flaml.automl: 09-16 21:21:45] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 21:21:58] {3072} INFO -  at 58.6s,	estimator xgboost's best error=2.2677,	best estimator xgboost's best error=2.2677
[flaml.automl: 09-16 21:22:12] {3335} INFO - retrain xgboost for 13.3s
[flaml.automl: 09-16 21:22:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:22:12] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:22:12] {2637} INFO - Time taken to find the best model: 58.60319423675537
[flaml.automl: 09-16 21:22:12] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 65061}
PM2.5(0)最佳损失：-1.2676879527616634
PM2.5(0)最好结果：{'pred_time': 6.057711557728621e-06, 'wall_clock_time': 58.60319423675537, 'metric_for_logging': {'pred_time': 6.057711557728621e-06}, 'val_loss': 2.2676879527616634, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 65061}, 'config/n_estimators': 13, 'config/max_leaves': 18, 'config/min_child_weight': 0.002212987346630184, 'config/learning_rate': 1.0, 'config/subsample': 0.9736629142498529, 'config/colsample_bylevel': 0.8717925041332704, 'config/colsample_bytree': 0.9857450191529009, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.04619111740498511, 'config/FLAML_sample_size': 65061, 'experiment_tag': 'exp', 'time_total_s': 13.3955717086792}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8955330496893211
PM2.5(0)的mse=11.145005095607196
PM2.5(0)的mae=2.253154497870499
PM2.5(0)的mar=0.19910760488072085
总共花费的时间为：72.98
肇庆市
1397A
1398A
1400A
3451A
[flaml.automl: 09-16 21:35:08] {2390} INFO - task = regression
[flaml.automl: 09-16 21:35:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:35:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:35:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:35:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:35:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:35:09] {3025} INFO - Estimated sufficient time budget=50637s. Estimated necessary time budget=51s.
[flaml.automl: 09-16 21:35:09] {3072} INFO -  at 1.4s,	estimator xgboost's best error=12.7546,	best estimator xgboost's best error=12.7546
[flaml.automl: 09-16 21:35:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:35:11] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.1257,	best estimator xgboost's best error=6.1257
[flaml.automl: 09-16 21:35:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:35:13] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.1257,	best estimator xgboost's best error=6.1257
[flaml.automl: 09-16 21:35:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:35:19] {3072} INFO -  at 11.1s,	estimator xgboost's best error=6.1257,	best estimator xgboost's best error=6.1257
[flaml.automl: 09-16 21:35:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:35:20] {3072} INFO -  at 12.2s,	estimator xgboost's best error=3.9674,	best estimator xgboost's best error=3.9674
[flaml.automl: 09-16 21:35:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:35:22] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.8389,	best estimator xgboost's best error=3.8389
[flaml.automl: 09-16 21:35:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:35:23] {3072} INFO -  at 15.4s,	estimator xgboost's best error=3.1671,	best estimator xgboost's best error=3.1671
[flaml.automl: 09-16 21:35:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:35:26] {3072} INFO -  at 18.1s,	estimator xgboost's best error=3.1671,	best estimator xgboost's best error=3.1671
[flaml.automl: 09-16 21:35:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:35:28] {3072} INFO -  at 19.7s,	estimator xgboost's best error=3.1671,	best estimator xgboost's best error=3.1671
[flaml.automl: 09-16 21:35:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:35:31] {3072} INFO -  at 22.7s,	estimator xgboost's best error=2.8753,	best estimator xgboost's best error=2.8753
[flaml.automl: 09-16 21:35:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:35:32] {3072} INFO -  at 24.3s,	estimator xgboost's best error=2.8753,	best estimator xgboost's best error=2.8753
[flaml.automl: 09-16 21:35:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:35:33] {3072} INFO -  at 25.4s,	estimator xgboost's best error=2.8753,	best estimator xgboost's best error=2.8753
[flaml.automl: 09-16 21:35:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:35:37] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.8753,	best estimator xgboost's best error=2.8753
[flaml.automl: 09-16 21:35:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:35:40] {3072} INFO -  at 32.6s,	estimator xgboost's best error=2.8753,	best estimator xgboost's best error=2.8753
[flaml.automl: 09-16 21:35:40] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:35:43] {3072} INFO -  at 35.7s,	estimator xgboost's best error=2.8196,	best estimator xgboost's best error=2.8196
[flaml.automl: 09-16 21:35:43] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 21:35:48] {3072} INFO -  at 40.6s,	estimator xgboost's best error=2.8196,	best estimator xgboost's best error=2.8196
[flaml.automl: 09-16 21:35:48] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 21:35:50] {3072} INFO -  at 42.5s,	estimator xgboost's best error=2.7972,	best estimator xgboost's best error=2.7972
[flaml.automl: 09-16 21:35:50] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 21:35:52] {3072} INFO -  at 44.2s,	estimator xgboost's best error=2.7972,	best estimator xgboost's best error=2.7972
[flaml.automl: 09-16 21:35:52] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 21:35:56] {3072} INFO -  at 48.5s,	estimator xgboost's best error=2.6337,	best estimator xgboost's best error=2.6337
[flaml.automl: 09-16 21:35:56] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 21:35:58] {3072} INFO -  at 50.1s,	estimator xgboost's best error=2.6337,	best estimator xgboost's best error=2.6337
[flaml.automl: 09-16 21:35:58] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 21:36:07] {3072} INFO -  at 59.4s,	estimator xgboost's best error=2.5071,	best estimator xgboost's best error=2.5071
[flaml.automl: 09-16 21:36:21] {3335} INFO - retrain xgboost for 13.3s
[flaml.automl: 09-16 21:36:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:36:21] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:36:21] {2637} INFO - Time taken to find the best model: 59.42241621017456
[flaml.automl: 09-16 21:36:21] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 42232}
PM2.5(0)最佳损失：-1.5070802978226658
PM2.5(0)最好结果：{'pred_time': 8.531808599134307e-06, 'wall_clock_time': 59.42241621017456, 'metric_for_logging': {'pred_time': 8.531808599134307e-06}, 'val_loss': 2.507080297822666, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 42232}, 'config/n_estimators': 13, 'config/max_leaves': 18, 'config/min_child_weight': 0.002212987346630184, 'config/learning_rate': 1.0, 'config/subsample': 0.9736629142498529, 'config/colsample_bylevel': 0.8717925041332704, 'config/colsample_bytree': 0.9857450191529009, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.04619111740498511, 'config/FLAML_sample_size': 42232, 'experiment_tag': 'exp', 'time_total_s': 9.273716926574707}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8960323746613632
PM2.5(0)的mse=15.849106096247436
PM2.5(0)的mae=2.5738059098639043
PM2.5(0)的mar=0.1791236436728094
总共花费的时间为：73.35
南宁市
1401A
1408A
[flaml.automl: 09-16 21:43:31] {2390} INFO - task = regression
[flaml.automl: 09-16 21:43:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:43:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:43:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:43:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:43:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:43:33] {3025} INFO - Estimated sufficient time budget=22016s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 21:43:33] {3072} INFO -  at 2.3s,	estimator xgboost's best error=16.0210,	best estimator xgboost's best error=16.0210
[flaml.automl: 09-16 21:43:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:43:37] {3072} INFO -  at 6.3s,	estimator xgboost's best error=7.2955,	best estimator xgboost's best error=7.2955
[flaml.automl: 09-16 21:43:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:43:39] {3072} INFO -  at 8.6s,	estimator xgboost's best error=7.2955,	best estimator xgboost's best error=7.2955
[flaml.automl: 09-16 21:43:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:44:02] {3072} INFO -  at 31.6s,	estimator xgboost's best error=7.2955,	best estimator xgboost's best error=7.2955
[flaml.automl: 09-16 21:44:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:44:04] {3072} INFO -  at 33.3s,	estimator xgboost's best error=4.4288,	best estimator xgboost's best error=4.4288
[flaml.automl: 09-16 21:44:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:44:06] {3072} INFO -  at 35.8s,	estimator xgboost's best error=4.4288,	best estimator xgboost's best error=4.4288
[flaml.automl: 09-16 21:44:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:44:09] {3072} INFO -  at 38.1s,	estimator xgboost's best error=2.7158,	best estimator xgboost's best error=2.7158
[flaml.automl: 09-16 21:44:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:44:13] {3072} INFO -  at 42.3s,	estimator xgboost's best error=2.7158,	best estimator xgboost's best error=2.7158
[flaml.automl: 09-16 21:44:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:44:16] {3072} INFO -  at 45.0s,	estimator xgboost's best error=2.7158,	best estimator xgboost's best error=2.7158
[flaml.automl: 09-16 21:44:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:44:20] {3072} INFO -  at 49.8s,	estimator xgboost's best error=2.7158,	best estimator xgboost's best error=2.7158
[flaml.automl: 09-16 21:44:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:44:23] {3072} INFO -  at 52.7s,	estimator xgboost's best error=2.5468,	best estimator xgboost's best error=2.5468
[flaml.automl: 09-16 21:44:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:44:25] {3072} INFO -  at 54.6s,	estimator xgboost's best error=2.5468,	best estimator xgboost's best error=2.5468
[flaml.automl: 09-16 21:44:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:44:30] {3072} INFO -  at 59.6s,	estimator xgboost's best error=2.1723,	best estimator xgboost's best error=2.1723
[flaml.automl: 09-16 21:44:40] {3335} INFO - retrain xgboost for 9.5s
[flaml.automl: 09-16 21:44:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:44:40] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:44:40] {2637} INFO - Time taken to find the best model: 59.625348806381226
[flaml.automl: 09-16 21:44:40] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
PM2.5(0)最佳损失：-1.1722547230984772
PM2.5(0)最好结果：{'pred_time': 3.6118245499502836e-05, 'wall_clock_time': 59.625348806381226, 'metric_for_logging': {'pred_time': 3.6118245499502836e-05}, 'val_loss': 2.1722547230984772, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 5.057725667953491}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9483930024080491
PM2.5(0)的mse=11.332663720292635
PM2.5(0)的mae=2.13900028824895
PM2.5(0)的mar=0.0982359410606798
总共花费的时间为：69.75
海口市
1409A
1411A
1413A
3539A
[flaml.automl: 09-16 21:56:55] {2390} INFO - task = regression
[flaml.automl: 09-16 21:56:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:56:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:56:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:56:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:56:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:56:56] {3025} INFO - Estimated sufficient time budget=48643s. Estimated necessary time budget=49s.
[flaml.automl: 09-16 21:56:56] {3072} INFO -  at 1.4s,	estimator xgboost's best error=8.0269,	best estimator xgboost's best error=8.0269
[flaml.automl: 09-16 21:56:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:56:58] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.8511,	best estimator xgboost's best error=3.8511
[flaml.automl: 09-16 21:56:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:56:59] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.8511,	best estimator xgboost's best error=3.8511
[flaml.automl: 09-16 21:56:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:57:06] {3072} INFO -  at 11.1s,	estimator xgboost's best error=3.8511,	best estimator xgboost's best error=3.8511
[flaml.automl: 09-16 21:57:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:57:07] {3072} INFO -  at 12.2s,	estimator xgboost's best error=2.7664,	best estimator xgboost's best error=2.7664
[flaml.automl: 09-16 21:57:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:57:08] {3072} INFO -  at 13.8s,	estimator xgboost's best error=2.5213,	best estimator xgboost's best error=2.5213
[flaml.automl: 09-16 21:57:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:57:10] {3072} INFO -  at 15.4s,	estimator xgboost's best error=2.1603,	best estimator xgboost's best error=2.1603
[flaml.automl: 09-16 21:57:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:57:13] {3072} INFO -  at 18.1s,	estimator xgboost's best error=2.1603,	best estimator xgboost's best error=2.1603
[flaml.automl: 09-16 21:57:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:57:14] {3072} INFO -  at 19.7s,	estimator xgboost's best error=2.1603,	best estimator xgboost's best error=2.1603
[flaml.automl: 09-16 21:57:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:57:17] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.9733,	best estimator xgboost's best error=1.9733
[flaml.automl: 09-16 21:57:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:57:19] {3072} INFO -  at 24.4s,	estimator xgboost's best error=1.9733,	best estimator xgboost's best error=1.9733
[flaml.automl: 09-16 21:57:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:57:20] {3072} INFO -  at 25.5s,	estimator xgboost's best error=1.9733,	best estimator xgboost's best error=1.9733
[flaml.automl: 09-16 21:57:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:57:24] {3072} INFO -  at 29.3s,	estimator xgboost's best error=1.9424,	best estimator xgboost's best error=1.9424
[flaml.automl: 09-16 21:57:24] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:57:27] {3072} INFO -  at 32.8s,	estimator xgboost's best error=1.8755,	best estimator xgboost's best error=1.8755
[flaml.automl: 09-16 21:57:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:57:30] {3072} INFO -  at 35.4s,	estimator xgboost's best error=1.8755,	best estimator xgboost's best error=1.8755
[flaml.automl: 09-16 21:57:30] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 21:57:32] {3072} INFO -  at 37.9s,	estimator xgboost's best error=1.8755,	best estimator xgboost's best error=1.8755
[flaml.automl: 09-16 21:57:32] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 21:57:35] {3072} INFO -  at 40.5s,	estimator xgboost's best error=1.8450,	best estimator xgboost's best error=1.8450
[flaml.automl: 09-16 21:57:35] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 21:57:37] {3072} INFO -  at 42.6s,	estimator xgboost's best error=1.8450,	best estimator xgboost's best error=1.8450
[flaml.automl: 09-16 21:57:37] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 21:57:39] {3072} INFO -  at 44.6s,	estimator xgboost's best error=1.8450,	best estimator xgboost's best error=1.8450
[flaml.automl: 09-16 21:57:39] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 21:57:41] {3072} INFO -  at 46.2s,	estimator xgboost's best error=1.8450,	best estimator xgboost's best error=1.8450
[flaml.automl: 09-16 21:57:41] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 21:57:42] {3072} INFO -  at 47.1s,	estimator xgboost's best error=1.8450,	best estimator xgboost's best error=1.8450
[flaml.automl: 09-16 21:57:42] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 21:57:55] {3072} INFO -  at 59.9s,	estimator xgboost's best error=1.7910,	best estimator xgboost's best error=1.7910
[flaml.automl: 09-16 21:58:19] {3335} INFO - retrain xgboost for 24.4s
[flaml.automl: 09-16 21:58:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4512592128754277,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.003827601076117227, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010065608612606477, reg_lambda=0.020722552365849516,
             scale_pos_weight=1, subsample=0.749001334635897,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 21:58:19] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:58:19] {2637} INFO - Time taken to find the best model: 59.93044114112854
[flaml.automl: 09-16 21:58:19] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.003827601076117227, 'learning_rate': 0.4512592128754277, 'subsample': 0.749001334635897, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0010065608612606477, 'reg_lambda': 0.020722552365849516, 'FLAML_sample_size': 40483}
PM2.5(0)最佳损失：-0.7910230093649901
PM2.5(0)最好结果：{'pred_time': 1.6911132093481713e-05, 'wall_clock_time': 59.93044114112854, 'metric_for_logging': {'pred_time': 1.6911132093481713e-05}, 'val_loss': 1.7910230093649901, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.003827601076117227, 'learning_rate': 0.4512592128754277, 'subsample': 0.749001334635897, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0010065608612606477, 'reg_lambda': 0.020722552365849516, 'FLAML_sample_size': 40483}, 'config/n_estimators': 35, 'config/max_leaves': 7, 'config/min_child_weight': 0.003827601076117227, 'config/learning_rate': 0.4512592128754277, 'config/subsample': 0.749001334635897, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0010065608612606477, 'config/reg_lambda': 0.020722552365849516, 'config/FLAML_sample_size': 40483, 'experiment_tag': 'exp', 'time_total_s': 12.788275003433228}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4512592128754277,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.003827601076117227, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010065608612606477, reg_lambda=0.020722552365849516,
             scale_pos_weight=1, subsample=0.749001334635897,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8811635047778432
PM2.5(0)的mse=6.9776450014469384
PM2.5(0)的mae=1.7970306283953477
PM2.5(0)的mar=0.1808518007483596
总共花费的时间为：85.08
重庆市
1414A
1418A
1419A
1420A
1422A
1428A
1429A
3015A
3016A
3346A
3347A
3348A
3349A
3350A
3351A
3352A
3353A
3354A
3355A
3356A
3482A
3483A
3484A
3485A
3599A
3600A
3601A
3610A
[flaml.automl: 09-16 23:24:26] {2390} INFO - task = regression
[flaml.automl: 09-16 23:24:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 23:24:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 23:24:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 23:24:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 23:24:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 23:24:26] {3025} INFO - Estimated sufficient time budget=213142s. Estimated necessary time budget=213s.
[flaml.automl: 09-16 23:24:26] {3072} INFO -  at 2.3s,	estimator xgboost's best error=22.6537,	best estimator xgboost's best error=22.6537
[flaml.automl: 09-16 23:24:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 23:24:27] {3072} INFO -  at 3.0s,	estimator xgboost's best error=20.5695,	best estimator xgboost's best error=20.5695
[flaml.automl: 09-16 23:24:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 23:24:28] {3072} INFO -  at 3.7s,	estimator xgboost's best error=20.5695,	best estimator xgboost's best error=20.5695
[flaml.automl: 09-16 23:24:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 23:24:29] {3072} INFO -  at 4.4s,	estimator xgboost's best error=20.5695,	best estimator xgboost's best error=20.5695
[flaml.automl: 09-16 23:24:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 23:24:29] {3072} INFO -  at 5.0s,	estimator xgboost's best error=10.2963,	best estimator xgboost's best error=10.2963
[flaml.automl: 09-16 23:24:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 23:24:30] {3072} INFO -  at 5.6s,	estimator xgboost's best error=10.2963,	best estimator xgboost's best error=10.2963
[flaml.automl: 09-16 23:24:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 23:24:30] {3072} INFO -  at 6.3s,	estimator xgboost's best error=10.2963,	best estimator xgboost's best error=10.2963
[flaml.automl: 09-16 23:24:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 23:24:31] {3072} INFO -  at 6.9s,	estimator xgboost's best error=10.2963,	best estimator xgboost's best error=10.2963
[flaml.automl: 09-16 23:24:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 23:24:32] {3072} INFO -  at 7.5s,	estimator xgboost's best error=6.3873,	best estimator xgboost's best error=6.3873
[flaml.automl: 09-16 23:24:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 23:24:32] {3072} INFO -  at 8.1s,	estimator xgboost's best error=6.3873,	best estimator xgboost's best error=6.3873
[flaml.automl: 09-16 23:24:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 23:24:33] {3072} INFO -  at 8.8s,	estimator xgboost's best error=6.3873,	best estimator xgboost's best error=6.3873
[flaml.automl: 09-16 23:24:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 23:24:34] {3072} INFO -  at 9.5s,	estimator xgboost's best error=6.3873,	best estimator xgboost's best error=6.3873
[flaml.automl: 09-16 23:24:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 23:24:34] {3072} INFO -  at 10.2s,	estimator xgboost's best error=4.9713,	best estimator xgboost's best error=4.9713
[flaml.automl: 09-16 23:24:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 23:24:35] {3072} INFO -  at 10.7s,	estimator xgboost's best error=4.9713,	best estimator xgboost's best error=4.9713
[flaml.automl: 09-16 23:24:35] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 23:24:36] {3072} INFO -  at 11.6s,	estimator xgboost's best error=4.9713,	best estimator xgboost's best error=4.9713
[flaml.automl: 09-16 23:24:36] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 23:24:37] {3072} INFO -  at 12.4s,	estimator xgboost's best error=4.9713,	best estimator xgboost's best error=4.9713
[flaml.automl: 09-16 23:24:37] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 23:24:37] {3072} INFO -  at 12.9s,	estimator xgboost's best error=4.9713,	best estimator xgboost's best error=4.9713
[flaml.automl: 09-16 23:24:37] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 23:24:40] {3072} INFO -  at 15.7s,	estimator xgboost's best error=4.6244,	best estimator xgboost's best error=4.6244
[flaml.automl: 09-16 23:24:40] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 23:24:42] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.6244,	best estimator xgboost's best error=4.6244
[flaml.automl: 09-16 23:24:42] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 23:24:44] {3072} INFO -  at 20.1s,	estimator xgboost's best error=4.6244,	best estimator xgboost's best error=4.6244
[flaml.automl: 09-16 23:24:44] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 23:24:46] {3072} INFO -  at 21.8s,	estimator xgboost's best error=4.6244,	best estimator xgboost's best error=4.6244
[flaml.automl: 09-16 23:24:46] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 23:24:48] {3072} INFO -  at 24.0s,	estimator xgboost's best error=4.5527,	best estimator xgboost's best error=4.5527
[flaml.automl: 09-16 23:24:48] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 23:24:50] {3072} INFO -  at 26.1s,	estimator xgboost's best error=4.5527,	best estimator xgboost's best error=4.5527
[flaml.automl: 09-16 23:24:50] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-16 23:24:52] {3072} INFO -  at 28.0s,	estimator xgboost's best error=4.5527,	best estimator xgboost's best error=4.5527
[flaml.automl: 09-16 23:24:52] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-16 23:24:55] {3072} INFO -  at 30.6s,	estimator xgboost's best error=4.4888,	best estimator xgboost's best error=4.4888
[flaml.automl: 09-16 23:24:55] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-16 23:24:56] {3072} INFO -  at 31.8s,	estimator xgboost's best error=4.4888,	best estimator xgboost's best error=4.4888
[flaml.automl: 09-16 23:24:56] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-16 23:24:58] {3072} INFO -  at 33.5s,	estimator xgboost's best error=4.4888,	best estimator xgboost's best error=4.4888
[flaml.automl: 09-16 23:24:58] {2897} INFO - iteration 27, current learner xgboost
[flaml.automl: 09-16 23:25:06] {3072} INFO -  at 41.7s,	estimator xgboost's best error=4.4888,	best estimator xgboost's best error=4.4888
[flaml.automl: 09-16 23:25:06] {2897} INFO - iteration 28, current learner xgboost
[flaml.automl: 09-16 23:25:25] {3072} INFO -  at 60.3s,	estimator xgboost's best error=4.4888,	best estimator xgboost's best error=4.4888
[flaml.automl: 09-16 23:26:10] {3335} INFO - retrain xgboost for 45.8s
[flaml.automl: 09-16 23:26:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9550164783283043, colsample_bynode=1,
             colsample_bytree=0.590237575810337, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=43, min_child_weight=0.0238585843094576,
             missing=nan, monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6211844504060098, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 23:26:10] {2636} INFO - fit succeeded
[flaml.automl: 09-16 23:26:10] {2637} INFO - Time taken to find the best model: 30.5606369972229
PM2.5(0)最佳参数：{'n_estimators': 19, 'max_leaves': 43, 'min_child_weight': 0.0238585843094576, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.9550164783283043, 'colsample_bytree': 0.590237575810337, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6211844504060098, 'FLAML_sample_size': 40000}
PM2.5(0)最佳损失：-3.4888464860919575
PM2.5(0)最好结果：{'pred_time': 1.4075255076243162e-06, 'wall_clock_time': 30.5606369972229, 'metric_for_logging': {'pred_time': 1.4075255076243162e-06}, 'val_loss': 4.4888464860919575, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 43, 'min_child_weight': 0.0238585843094576, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.9550164783283043, 'colsample_bytree': 0.590237575810337, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6211844504060098, 'FLAML_sample_size': 40000}, 'config/n_estimators': 19, 'config/max_leaves': 43, 'config/min_child_weight': 0.0238585843094576, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9550164783283043, 'config/colsample_bytree': 0.590237575810337, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6211844504060098, 'config/FLAML_sample_size': 40000, 'experiment_tag': 'exp', 'time_total_s': 2.583665609359741}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9550164783283043, colsample_bynode=1,
             colsample_bytree=0.590237575810337, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=43, min_child_weight=0.0238585843094576,
             missing=nan, monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6211844504060098, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8931358971061403
PM2.5(0)的mse=40.693225464484236
PM2.5(0)的mae=4.3484143240736435
PM2.5(0)的mar=0.24216984725816582
总共花费的时间为：111.49
贵阳市
1440A
1442A
1443A
1444A
1445A
1446A
[flaml.automl: 09-16 23:44:08] {2390} INFO - task = regression
[flaml.automl: 09-16 23:44:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 23:44:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 23:44:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 23:44:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 23:44:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 23:44:10] {3025} INFO - Estimated sufficient time budget=135677s. Estimated necessary time budget=136s.
[flaml.automl: 09-16 23:44:10] {3072} INFO -  at 2.5s,	estimator xgboost's best error=13.8290,	best estimator xgboost's best error=13.8290
[flaml.automl: 09-16 23:44:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 23:44:13] {3072} INFO -  at 5.8s,	estimator xgboost's best error=6.8626,	best estimator xgboost's best error=6.8626
[flaml.automl: 09-16 23:44:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 23:44:15] {3072} INFO -  at 7.9s,	estimator xgboost's best error=6.8626,	best estimator xgboost's best error=6.8626
[flaml.automl: 09-16 23:44:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 23:44:18] {3072} INFO -  at 10.8s,	estimator xgboost's best error=6.8626,	best estimator xgboost's best error=6.8626
[flaml.automl: 09-16 23:44:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 23:44:20] {3072} INFO -  at 12.4s,	estimator xgboost's best error=5.2933,	best estimator xgboost's best error=5.2933
[flaml.automl: 09-16 23:44:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 23:44:22] {3072} INFO -  at 14.5s,	estimator xgboost's best error=5.2933,	best estimator xgboost's best error=5.2933
[flaml.automl: 09-16 23:44:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 23:44:25] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.0238,	best estimator xgboost's best error=4.0238
[flaml.automl: 09-16 23:44:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 23:44:27] {3072} INFO -  at 20.1s,	estimator xgboost's best error=4.0238,	best estimator xgboost's best error=4.0238
[flaml.automl: 09-16 23:44:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 23:44:30] {3072} INFO -  at 22.6s,	estimator xgboost's best error=4.0238,	best estimator xgboost's best error=4.0238
[flaml.automl: 09-16 23:44:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 23:44:32] {3072} INFO -  at 24.9s,	estimator xgboost's best error=4.0238,	best estimator xgboost's best error=4.0238
[flaml.automl: 09-16 23:44:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 23:44:34] {3072} INFO -  at 27.1s,	estimator xgboost's best error=4.0238,	best estimator xgboost's best error=4.0238
[flaml.automl: 09-16 23:44:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 23:44:37] {3072} INFO -  at 30.0s,	estimator xgboost's best error=4.0238,	best estimator xgboost's best error=4.0238
[flaml.automl: 09-16 23:44:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 23:44:39] {3072} INFO -  at 32.2s,	estimator xgboost's best error=4.0238,	best estimator xgboost's best error=4.0238
[flaml.automl: 09-16 23:44:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 23:44:50] {3072} INFO -  at 42.9s,	estimator xgboost's best error=3.6809,	best estimator xgboost's best error=3.6809
[flaml.automl: 09-16 23:44:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 23:45:07] {3072} INFO -  at 59.2s,	estimator xgboost's best error=3.6496,	best estimator xgboost's best error=3.6496
[flaml.automl: 09-16 23:45:28] {3335} INFO - retrain xgboost for 21.0s
[flaml.automl: 09-16 23:45:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 23:45:28] {2636} INFO - fit succeeded
[flaml.automl: 09-16 23:45:28] {2637} INFO - Time taken to find the best model: 59.24046754837036
[flaml.automl: 09-16 23:45:28] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 65600}
PM2.5(0)最佳损失：-2.6495902714930146
PM2.5(0)最好结果：{'pred_time': 9.365313369430686e-06, 'wall_clock_time': 59.24046754837036, 'metric_for_logging': {'pred_time': 9.365313369430686e-06}, 'val_loss': 3.6495902714930146, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 65600}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 65600, 'experiment_tag': 'exp', 'time_total_s': 16.32035255432129}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8625963075617697
PM2.5(0)的mse=25.231791818889345
PM2.5(0)的mae=3.6425014098554
PM2.5(0)的mar=0.26574925603985766
总共花费的时间为：81.64
昆明市
1452A
1453A
1455A
3179A
3375A
3550A
3551A
3552A
[flaml.automl: 09-17 00:10:18] {2390} INFO - task = regression
[flaml.automl: 09-17 00:10:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 00:10:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 00:10:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 00:10:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 00:10:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 00:10:19] {3025} INFO - Estimated sufficient time budget=104545s. Estimated necessary time budget=105s.
[flaml.automl: 09-17 00:10:19] {3072} INFO -  at 1.6s,	estimator xgboost's best error=13.5836,	best estimator xgboost's best error=13.5836
[flaml.automl: 09-17 00:10:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 00:10:21] {3072} INFO -  at 3.7s,	estimator xgboost's best error=6.5077,	best estimator xgboost's best error=6.5077
[flaml.automl: 09-17 00:10:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 00:10:22] {3072} INFO -  at 4.9s,	estimator xgboost's best error=6.5077,	best estimator xgboost's best error=6.5077
[flaml.automl: 09-17 00:10:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 00:10:25] {3072} INFO -  at 7.6s,	estimator xgboost's best error=6.5077,	best estimator xgboost's best error=6.5077
[flaml.automl: 09-17 00:10:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 00:10:26] {3072} INFO -  at 8.8s,	estimator xgboost's best error=4.5715,	best estimator xgboost's best error=4.5715
[flaml.automl: 09-17 00:10:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 00:10:28] {3072} INFO -  at 10.4s,	estimator xgboost's best error=4.0665,	best estimator xgboost's best error=4.0665
[flaml.automl: 09-17 00:10:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 00:10:29] {3072} INFO -  at 12.0s,	estimator xgboost's best error=3.7320,	best estimator xgboost's best error=3.7320
[flaml.automl: 09-17 00:10:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 00:10:32] {3072} INFO -  at 14.3s,	estimator xgboost's best error=3.7320,	best estimator xgboost's best error=3.7320
[flaml.automl: 09-17 00:10:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 00:10:33] {3072} INFO -  at 15.9s,	estimator xgboost's best error=3.6748,	best estimator xgboost's best error=3.6748
[flaml.automl: 09-17 00:10:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 00:10:35] {3072} INFO -  at 18.1s,	estimator xgboost's best error=3.6748,	best estimator xgboost's best error=3.6748
[flaml.automl: 09-17 00:10:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 00:10:37] {3072} INFO -  at 19.5s,	estimator xgboost's best error=3.6748,	best estimator xgboost's best error=3.6748
[flaml.automl: 09-17 00:10:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 00:10:38] {3072} INFO -  at 20.7s,	estimator xgboost's best error=3.6748,	best estimator xgboost's best error=3.6748
[flaml.automl: 09-17 00:10:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 00:10:40] {3072} INFO -  at 22.4s,	estimator xgboost's best error=3.2212,	best estimator xgboost's best error=3.2212
[flaml.automl: 09-17 00:10:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 00:10:41] {3072} INFO -  at 24.2s,	estimator xgboost's best error=3.1824,	best estimator xgboost's best error=3.1824
[flaml.automl: 09-17 00:10:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 00:10:43] {3072} INFO -  at 25.9s,	estimator xgboost's best error=3.1824,	best estimator xgboost's best error=3.1824
[flaml.automl: 09-17 00:10:43] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 00:10:45] {3072} INFO -  at 27.5s,	estimator xgboost's best error=3.1824,	best estimator xgboost's best error=3.1824
[flaml.automl: 09-17 00:10:45] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 00:10:46] {3072} INFO -  at 29.1s,	estimator xgboost's best error=3.1824,	best estimator xgboost's best error=3.1824
[flaml.automl: 09-17 00:10:46] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 00:10:48] {3072} INFO -  at 30.7s,	estimator xgboost's best error=3.1824,	best estimator xgboost's best error=3.1824
[flaml.automl: 09-17 00:10:48] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 00:11:01] {3072} INFO -  at 43.6s,	estimator xgboost's best error=2.8821,	best estimator xgboost's best error=2.8821
[flaml.automl: 09-17 00:11:01] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 00:11:06] {3072} INFO -  at 48.4s,	estimator xgboost's best error=2.8821,	best estimator xgboost's best error=2.8821
[flaml.automl: 09-17 00:11:18] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 00:11:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 00:11:18] {2636} INFO - fit succeeded
[flaml.automl: 09-17 00:11:18] {2637} INFO - Time taken to find the best model: 43.5722770690918
[flaml.automl: 09-17 00:11:18] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 88336}
PM2.5(0)最佳损失：-1.8820579372583217
PM2.5(0)最好结果：{'pred_time': 4.20948695436958e-06, 'wall_clock_time': 43.5722770690918, 'metric_for_logging': {'pred_time': 4.20948695436958e-06}, 'val_loss': 2.8820579372583217, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 88336}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'config/FLAML_sample_size': 88336, 'experiment_tag': 'exp', 'time_total_s': 12.85798978805542}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8966476686192077
PM2.5(0)的mse=19.618838170236177
PM2.5(0)的mae=2.9504241612926876
PM2.5(0)的mar=0.1926534286439567
总共花费的时间为：62.49
拉萨市
1456A
1457A
1458A
1461A
[flaml.automl: 09-17 00:23:30] {2390} INFO - task = regression
[flaml.automl: 09-17 00:23:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 00:23:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 00:23:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 00:23:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 00:23:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 00:23:31] {3025} INFO - Estimated sufficient time budget=49112s. Estimated necessary time budget=49s.
[flaml.automl: 09-17 00:23:31] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.9452,	best estimator xgboost's best error=4.9452
[flaml.automl: 09-17 00:23:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 00:23:33] {3072} INFO -  at 3.4s,	estimator xgboost's best error=3.0403,	best estimator xgboost's best error=3.0403
[flaml.automl: 09-17 00:23:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 00:23:34] {3072} INFO -  at 4.6s,	estimator xgboost's best error=3.0403,	best estimator xgboost's best error=3.0403
[flaml.automl: 09-17 00:23:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 00:23:41] {3072} INFO -  at 10.9s,	estimator xgboost's best error=3.0403,	best estimator xgboost's best error=3.0403
[flaml.automl: 09-17 00:23:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 00:23:42] {3072} INFO -  at 12.1s,	estimator xgboost's best error=2.6677,	best estimator xgboost's best error=2.6677
[flaml.automl: 09-17 00:23:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 00:23:43] {3072} INFO -  at 13.7s,	estimator xgboost's best error=2.6180,	best estimator xgboost's best error=2.6180
[flaml.automl: 09-17 00:23:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 00:23:45] {3072} INFO -  at 15.3s,	estimator xgboost's best error=2.6000,	best estimator xgboost's best error=2.6000
[flaml.automl: 09-17 00:23:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 00:23:48] {3072} INFO -  at 17.9s,	estimator xgboost's best error=2.6000,	best estimator xgboost's best error=2.6000
[flaml.automl: 09-17 00:23:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 00:23:49] {3072} INFO -  at 19.5s,	estimator xgboost's best error=2.6000,	best estimator xgboost's best error=2.6000
[flaml.automl: 09-17 00:23:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 00:23:52] {3072} INFO -  at 22.5s,	estimator xgboost's best error=2.5606,	best estimator xgboost's best error=2.5606
[flaml.automl: 09-17 00:23:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 00:23:54] {3072} INFO -  at 24.1s,	estimator xgboost's best error=2.5606,	best estimator xgboost's best error=2.5606
[flaml.automl: 09-17 00:23:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 00:23:55] {3072} INFO -  at 25.2s,	estimator xgboost's best error=2.5606,	best estimator xgboost's best error=2.5606
[flaml.automl: 09-17 00:23:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 00:23:59] {3072} INFO -  at 29.0s,	estimator xgboost's best error=2.5606,	best estimator xgboost's best error=2.5606
[flaml.automl: 09-17 00:23:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 00:24:02] {3072} INFO -  at 32.4s,	estimator xgboost's best error=2.5606,	best estimator xgboost's best error=2.5606
[flaml.automl: 09-17 00:24:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 00:24:05] {3072} INFO -  at 35.4s,	estimator xgboost's best error=2.5557,	best estimator xgboost's best error=2.5557
[flaml.automl: 09-17 00:24:05] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 00:24:10] {3072} INFO -  at 40.4s,	estimator xgboost's best error=2.5333,	best estimator xgboost's best error=2.5333
[flaml.automl: 09-17 00:24:10] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 00:24:13] {3072} INFO -  at 43.4s,	estimator xgboost's best error=2.5333,	best estimator xgboost's best error=2.5333
[flaml.automl: 09-17 00:24:13] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 00:24:15] {3072} INFO -  at 45.4s,	estimator xgboost's best error=2.5333,	best estimator xgboost's best error=2.5333
[flaml.automl: 09-17 00:24:15] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 00:24:27] {3072} INFO -  at 57.6s,	estimator xgboost's best error=2.5333,	best estimator xgboost's best error=2.5333
[flaml.automl: 09-17 00:24:32] {3335} INFO - retrain xgboost for 4.9s
[flaml.automl: 09-17 00:24:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.4880713514874074, colsample_bynode=1,
             colsample_bytree=0.8895456505363326, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.32947189008227545,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.49812101235598427, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0014708297882995176, reg_lambda=1.3838145026508042,
             scale_pos_weight=1, subsample=0.9251629318205657,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 00:24:32] {2636} INFO - fit succeeded
[flaml.automl: 09-17 00:24:32] {2637} INFO - Time taken to find the best model: 40.37346863746643
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 12, 'min_child_weight': 0.49812101235598427, 'learning_rate': 0.32947189008227545, 'subsample': 0.9251629318205657, 'colsample_bylevel': 0.4880713514874074, 'colsample_bytree': 0.8895456505363326, 'reg_alpha': 0.0014708297882995176, 'reg_lambda': 1.3838145026508042, 'FLAML_sample_size': 41338}
PM2.5(0)最佳损失：-1.533270259862575
PM2.5(0)最好结果：{'pred_time': 9.767480865996663e-06, 'wall_clock_time': 40.37346863746643, 'metric_for_logging': {'pred_time': 9.767480865996663e-06}, 'val_loss': 2.533270259862575, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 12, 'min_child_weight': 0.49812101235598427, 'learning_rate': 0.32947189008227545, 'subsample': 0.9251629318205657, 'colsample_bylevel': 0.4880713514874074, 'colsample_bytree': 0.8895456505363326, 'reg_alpha': 0.0014708297882995176, 'reg_lambda': 1.3838145026508042, 'FLAML_sample_size': 41338}, 'config/n_estimators': 7, 'config/max_leaves': 12, 'config/min_child_weight': 0.49812101235598427, 'config/learning_rate': 0.32947189008227545, 'config/subsample': 0.9251629318205657, 'config/colsample_bylevel': 0.4880713514874074, 'config/colsample_bytree': 0.8895456505363326, 'config/reg_alpha': 0.0014708297882995176, 'config/reg_lambda': 1.3838145026508042, 'config/FLAML_sample_size': 41338, 'experiment_tag': 'exp', 'time_total_s': 4.925345182418823}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.4880713514874074, colsample_bynode=1,
             colsample_bytree=0.8895456505363326, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.32947189008227545,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.49812101235598427, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0014708297882995176, reg_lambda=1.3838145026508042,
             scale_pos_weight=1, subsample=0.9251629318205657,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.20158029353815932
PM2.5(0)的mse=13.831182460607705
PM2.5(0)的mae=2.523574660470867
PM2.5(0)的mar=0.47824482197648144
总共花费的时间为：63.20
西安市
1462A
1463A
1464A
1465A
1466A
1468A
1474A
3524A
3605A
[flaml.automl: 09-17 00:51:09] {2390} INFO - task = regression
[flaml.automl: 09-17 00:51:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 00:51:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 00:51:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 00:51:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 00:51:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 00:51:10] {3025} INFO - Estimated sufficient time budget=117323s. Estimated necessary time budget=117s.
[flaml.automl: 09-17 00:51:10] {3072} INFO -  at 1.7s,	estimator xgboost's best error=27.0680,	best estimator xgboost's best error=27.0680
[flaml.automl: 09-17 00:51:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 00:51:12] {3072} INFO -  at 3.8s,	estimator xgboost's best error=12.8050,	best estimator xgboost's best error=12.8050
[flaml.automl: 09-17 00:51:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 00:51:14] {3072} INFO -  at 5.0s,	estimator xgboost's best error=12.8050,	best estimator xgboost's best error=12.8050
[flaml.automl: 09-17 00:51:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 00:51:16] {3072} INFO -  at 7.3s,	estimator xgboost's best error=12.8050,	best estimator xgboost's best error=12.8050
[flaml.automl: 09-17 00:51:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 00:51:17] {3072} INFO -  at 8.4s,	estimator xgboost's best error=9.3337,	best estimator xgboost's best error=9.3337
[flaml.automl: 09-17 00:51:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 00:51:19] {3072} INFO -  at 10.0s,	estimator xgboost's best error=9.3337,	best estimator xgboost's best error=9.3337
[flaml.automl: 09-17 00:51:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 00:51:20] {3072} INFO -  at 11.7s,	estimator xgboost's best error=5.6071,	best estimator xgboost's best error=5.6071
[flaml.automl: 09-17 00:51:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 00:51:22] {3072} INFO -  at 14.0s,	estimator xgboost's best error=5.6071,	best estimator xgboost's best error=5.6071
[flaml.automl: 09-17 00:51:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 00:51:24] {3072} INFO -  at 15.6s,	estimator xgboost's best error=5.6071,	best estimator xgboost's best error=5.6071
[flaml.automl: 09-17 00:51:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 00:51:26] {3072} INFO -  at 17.4s,	estimator xgboost's best error=5.6071,	best estimator xgboost's best error=5.6071
[flaml.automl: 09-17 00:51:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 00:51:27] {3072} INFO -  at 18.8s,	estimator xgboost's best error=5.6071,	best estimator xgboost's best error=5.6071
[flaml.automl: 09-17 00:51:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 00:51:29] {3072} INFO -  at 20.6s,	estimator xgboost's best error=5.5441,	best estimator xgboost's best error=5.5441
[flaml.automl: 09-17 00:51:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 00:51:30] {3072} INFO -  at 21.8s,	estimator xgboost's best error=5.5441,	best estimator xgboost's best error=5.5441
[flaml.automl: 09-17 00:51:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 00:51:37] {3072} INFO -  at 29.0s,	estimator xgboost's best error=4.4690,	best estimator xgboost's best error=4.4690
[flaml.automl: 09-17 00:51:37] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 00:51:50] {3072} INFO -  at 41.9s,	estimator xgboost's best error=4.2791,	best estimator xgboost's best error=4.2791
[flaml.automl: 09-17 00:51:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 00:51:58] {3072} INFO -  at 49.0s,	estimator xgboost's best error=4.2791,	best estimator xgboost's best error=4.2791
[flaml.automl: 09-17 00:52:10] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 00:52:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 00:52:10] {2636} INFO - fit succeeded
[flaml.automl: 09-17 00:52:10] {2637} INFO - Time taken to find the best model: 41.89144515991211
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 97861}
PM2.5(0)最佳损失：-3.279104720533212
PM2.5(0)最好结果：{'pred_time': 4.36467810415577e-06, 'wall_clock_time': 41.89144515991211, 'metric_for_logging': {'pred_time': 4.36467810415577e-06}, 'val_loss': 4.279104720533212, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 97861}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 97861, 'experiment_tag': 'exp', 'time_total_s': 12.915691375732422}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9539764693579464
PM2.5(0)的mse=48.872169145102205
PM2.5(0)的mae=4.316593473048582
PM2.5(0)的mar=0.1358078076467593
总共花费的时间为：63.37
兰州市
1478A
3186A
3241A
3242A
[flaml.automl: 09-17 01:04:35] {2390} INFO - task = regression
[flaml.automl: 09-17 01:04:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:04:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:04:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:04:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:04:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:04:37] {3025} INFO - Estimated sufficient time budget=52978s. Estimated necessary time budget=53s.
[flaml.automl: 09-17 01:04:37] {3072} INFO -  at 1.4s,	estimator xgboost's best error=20.7992,	best estimator xgboost's best error=20.7992
[flaml.automl: 09-17 01:04:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:04:39] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.9272,	best estimator xgboost's best error=9.9272
[flaml.automl: 09-17 01:04:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:04:40] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.9272,	best estimator xgboost's best error=9.9272
[flaml.automl: 09-17 01:04:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:04:46] {3072} INFO -  at 10.5s,	estimator xgboost's best error=9.9272,	best estimator xgboost's best error=9.9272
[flaml.automl: 09-17 01:04:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:04:47] {3072} INFO -  at 11.7s,	estimator xgboost's best error=6.7737,	best estimator xgboost's best error=6.7737
[flaml.automl: 09-17 01:04:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:04:48] {3072} INFO -  at 13.2s,	estimator xgboost's best error=6.7737,	best estimator xgboost's best error=6.7737
[flaml.automl: 09-17 01:04:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:04:50] {3072} INFO -  at 14.9s,	estimator xgboost's best error=5.3833,	best estimator xgboost's best error=5.3833
[flaml.automl: 09-17 01:04:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:04:53] {3072} INFO -  at 17.6s,	estimator xgboost's best error=5.3833,	best estimator xgboost's best error=5.3833
[flaml.automl: 09-17 01:04:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:04:54] {3072} INFO -  at 19.2s,	estimator xgboost's best error=5.3833,	best estimator xgboost's best error=5.3833
[flaml.automl: 09-17 01:04:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:04:57] {3072} INFO -  at 22.2s,	estimator xgboost's best error=5.3833,	best estimator xgboost's best error=5.3833
[flaml.automl: 09-17 01:04:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:04:59] {3072} INFO -  at 23.7s,	estimator xgboost's best error=5.3833,	best estimator xgboost's best error=5.3833
[flaml.automl: 09-17 01:04:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:05:01] {3072} INFO -  at 25.4s,	estimator xgboost's best error=5.3211,	best estimator xgboost's best error=5.3211
[flaml.automl: 09-17 01:05:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:05:02] {3072} INFO -  at 26.6s,	estimator xgboost's best error=5.3211,	best estimator xgboost's best error=5.3211
[flaml.automl: 09-17 01:05:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:05:09] {3072} INFO -  at 33.6s,	estimator xgboost's best error=4.8996,	best estimator xgboost's best error=4.8996
[flaml.automl: 09-17 01:05:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:05:22] {3072} INFO -  at 46.4s,	estimator xgboost's best error=4.8471,	best estimator xgboost's best error=4.8471
[flaml.automl: 09-17 01:05:22] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 01:05:29] {3072} INFO -  at 53.5s,	estimator xgboost's best error=4.8471,	best estimator xgboost's best error=4.8471
[flaml.automl: 09-17 01:05:41] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 01:05:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:05:41] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:05:41] {2637} INFO - Time taken to find the best model: 46.41368079185486
[flaml.automl: 09-17 01:05:41] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 44144}
PM2.5(0)最佳损失：-3.847071974479216
PM2.5(0)最好结果：{'pred_time': 8.346157093417511e-06, 'wall_clock_time': 46.41368079185486, 'metric_for_logging': {'pred_time': 8.346157093417511e-06}, 'val_loss': 4.847071974479216, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 44144}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 44144, 'experiment_tag': 'exp', 'time_total_s': 12.80901837348938}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8506908600488855
PM2.5(0)的mse=51.22281254455301
PM2.5(0)的mae=4.7859753609797275
PM2.5(0)的mar=0.18672182201374612
总共花费的时间为：66.89
西宁市
3629A
3630A
[flaml.automl: 09-17 01:12:54] {2390} INFO - task = regression
[flaml.automl: 09-17 01:12:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:12:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:12:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:12:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:12:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:12:55] {3025} INFO - Estimated sufficient time budget=12056s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 01:12:55] {3072} INFO -  at 1.3s,	estimator xgboost's best error=21.1046,	best estimator xgboost's best error=21.1046
[flaml.automl: 09-17 01:12:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:12:57] {3072} INFO -  at 3.4s,	estimator xgboost's best error=10.1417,	best estimator xgboost's best error=10.1417
[flaml.automl: 09-17 01:12:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:12:59] {3072} INFO -  at 4.6s,	estimator xgboost's best error=10.1417,	best estimator xgboost's best error=10.1417
[flaml.automl: 09-17 01:12:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:13:08] {3072} INFO -  at 14.0s,	estimator xgboost's best error=10.1417,	best estimator xgboost's best error=10.1417
[flaml.automl: 09-17 01:13:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:13:09] {3072} INFO -  at 15.2s,	estimator xgboost's best error=6.8287,	best estimator xgboost's best error=6.8287
[flaml.automl: 09-17 01:13:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:13:11] {3072} INFO -  at 16.9s,	estimator xgboost's best error=6.2176,	best estimator xgboost's best error=6.2176
[flaml.automl: 09-17 01:13:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:13:14] {3072} INFO -  at 19.9s,	estimator xgboost's best error=5.8374,	best estimator xgboost's best error=5.8374
[flaml.automl: 09-17 01:13:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:13:18] {3072} INFO -  at 23.8s,	estimator xgboost's best error=5.8374,	best estimator xgboost's best error=5.8374
[flaml.automl: 09-17 01:13:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:13:20] {3072} INFO -  at 26.5s,	estimator xgboost's best error=5.7298,	best estimator xgboost's best error=5.7298
[flaml.automl: 09-17 01:13:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:13:25] {3072} INFO -  at 31.4s,	estimator xgboost's best error=5.7298,	best estimator xgboost's best error=5.7298
[flaml.automl: 09-17 01:13:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:13:28] {3072} INFO -  at 34.3s,	estimator xgboost's best error=5.7298,	best estimator xgboost's best error=5.7298
[flaml.automl: 09-17 01:13:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:13:30] {3072} INFO -  at 36.2s,	estimator xgboost's best error=5.7298,	best estimator xgboost's best error=5.7298
[flaml.automl: 09-17 01:13:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:13:43] {3072} INFO -  at 49.3s,	estimator xgboost's best error=5.1149,	best estimator xgboost's best error=5.1149
[flaml.automl: 09-17 01:14:01] {3335} INFO - retrain xgboost for 17.7s
[flaml.automl: 09-17 01:14:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466852,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 01:14:01] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:14:01] {2637} INFO - Time taken to find the best model: 49.256754875183105
[flaml.automl: 09-17 01:14:01] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466852}
PM2.5(0)最佳损失：-4.114943911377395
PM2.5(0)最好结果：{'pred_time': 5.7852503533425426e-05, 'wall_clock_time': 49.256754875183105, 'metric_for_logging': {'pred_time': 5.7852503533425426e-05}, 'val_loss': 5.114943911377395, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466852}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609401, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466852, 'experiment_tag': 'exp', 'time_total_s': 13.049630880355835}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466852,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8063427901144871
PM2.5(0)的mse=63.513355993990956
PM2.5(0)的mae=5.436219007531727
PM2.5(0)的mar=0.2243080530169772
总共花费的时间为：67.45
银川市
1484A
1488A
2925A
2926A
3523A
[flaml.automl: 09-17 01:29:41] {2390} INFO - task = regression
[flaml.automl: 09-17 01:29:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:29:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:29:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:29:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:29:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:29:43] {3025} INFO - Estimated sufficient time budget=121209s. Estimated necessary time budget=121s.
[flaml.automl: 09-17 01:29:43] {3072} INFO -  at 2.6s,	estimator xgboost's best error=17.8200,	best estimator xgboost's best error=17.8200
[flaml.automl: 09-17 01:29:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:29:47] {3072} INFO -  at 6.2s,	estimator xgboost's best error=8.6755,	best estimator xgboost's best error=8.6755
[flaml.automl: 09-17 01:29:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:29:48] {3072} INFO -  at 7.4s,	estimator xgboost's best error=8.6755,	best estimator xgboost's best error=8.6755
[flaml.automl: 09-17 01:29:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:29:53] {3072} INFO -  at 12.2s,	estimator xgboost's best error=8.6755,	best estimator xgboost's best error=8.6755
[flaml.automl: 09-17 01:29:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:29:54] {3072} INFO -  at 13.3s,	estimator xgboost's best error=6.8051,	best estimator xgboost's best error=6.8051
[flaml.automl: 09-17 01:29:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:29:55] {3072} INFO -  at 14.9s,	estimator xgboost's best error=6.8051,	best estimator xgboost's best error=6.8051
[flaml.automl: 09-17 01:29:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:29:57] {3072} INFO -  at 16.6s,	estimator xgboost's best error=5.1713,	best estimator xgboost's best error=5.1713
[flaml.automl: 09-17 01:29:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:30:00] {3072} INFO -  at 19.3s,	estimator xgboost's best error=5.1713,	best estimator xgboost's best error=5.1713
[flaml.automl: 09-17 01:30:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:30:01] {3072} INFO -  at 20.9s,	estimator xgboost's best error=5.1713,	best estimator xgboost's best error=5.1713
[flaml.automl: 09-17 01:30:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:30:04] {3072} INFO -  at 23.9s,	estimator xgboost's best error=5.1713,	best estimator xgboost's best error=5.1713
[flaml.automl: 09-17 01:30:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:30:06] {3072} INFO -  at 25.4s,	estimator xgboost's best error=5.1713,	best estimator xgboost's best error=5.1713
[flaml.automl: 09-17 01:30:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:30:08] {3072} INFO -  at 27.1s,	estimator xgboost's best error=5.1208,	best estimator xgboost's best error=5.1208
[flaml.automl: 09-17 01:30:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:30:09] {3072} INFO -  at 28.3s,	estimator xgboost's best error=5.1208,	best estimator xgboost's best error=5.1208
[flaml.automl: 09-17 01:30:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:30:16] {3072} INFO -  at 35.3s,	estimator xgboost's best error=4.8277,	best estimator xgboost's best error=4.8277
[flaml.automl: 09-17 01:30:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:30:29] {3072} INFO -  at 48.2s,	estimator xgboost's best error=4.7187,	best estimator xgboost's best error=4.7187
[flaml.automl: 09-17 01:30:41] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 01:30:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:30:41] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:30:41] {2637} INFO - Time taken to find the best model: 48.16795468330383
[flaml.automl: 09-17 01:30:41] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 51579}
PM2.5(0)最佳损失：-3.718733673401837
PM2.5(0)最好结果：{'pred_time': 8.168331992767611e-06, 'wall_clock_time': 48.16795468330383, 'metric_for_logging': {'pred_time': 8.168331992767611e-06}, 'val_loss': 4.718733673401837, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 51579}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 51579, 'experiment_tag': 'exp', 'time_total_s': 12.845567464828491}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.853805673572777
PM2.5(0)的mse=64.34503742018619
PM2.5(0)的mae=4.690385892912371
PM2.5(0)的mar=0.20659289775941464
总共花费的时间为：61.86
乌鲁木齐市
1491A
3033A
3437A
3438A
3439A
3440A
[flaml.automl: 09-17 01:49:01] {2390} INFO - task = regression
[flaml.automl: 09-17 01:49:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:49:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:49:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:49:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:49:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:49:02] {3025} INFO - Estimated sufficient time budget=75459s. Estimated necessary time budget=75s.
[flaml.automl: 09-17 01:49:02] {3072} INFO -  at 1.5s,	estimator xgboost's best error=18.8582,	best estimator xgboost's best error=18.8582
[flaml.automl: 09-17 01:49:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:49:04] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.9677,	best estimator xgboost's best error=9.9677
[flaml.automl: 09-17 01:49:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:49:05] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.9677,	best estimator xgboost's best error=9.9677
[flaml.automl: 09-17 01:49:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:49:10] {3072} INFO -  at 9.0s,	estimator xgboost's best error=9.9677,	best estimator xgboost's best error=9.9677
[flaml.automl: 09-17 01:49:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:49:11] {3072} INFO -  at 10.1s,	estimator xgboost's best error=8.6259,	best estimator xgboost's best error=8.6259
[flaml.automl: 09-17 01:49:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:49:12] {3072} INFO -  at 11.7s,	estimator xgboost's best error=7.2588,	best estimator xgboost's best error=7.2588
[flaml.automl: 09-17 01:49:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:49:14] {3072} INFO -  at 13.3s,	estimator xgboost's best error=6.9468,	best estimator xgboost's best error=6.9468
[flaml.automl: 09-17 01:49:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:49:17] {3072} INFO -  at 15.9s,	estimator xgboost's best error=6.9468,	best estimator xgboost's best error=6.9468
[flaml.automl: 09-17 01:49:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:49:18] {3072} INFO -  at 17.5s,	estimator xgboost's best error=6.9468,	best estimator xgboost's best error=6.9468
[flaml.automl: 09-17 01:49:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:49:21] {3072} INFO -  at 20.5s,	estimator xgboost's best error=5.7167,	best estimator xgboost's best error=5.7167
[flaml.automl: 09-17 01:49:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:49:23] {3072} INFO -  at 22.1s,	estimator xgboost's best error=5.7167,	best estimator xgboost's best error=5.7167
[flaml.automl: 09-17 01:49:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:49:24] {3072} INFO -  at 23.3s,	estimator xgboost's best error=5.7167,	best estimator xgboost's best error=5.7167
[flaml.automl: 09-17 01:49:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:49:26] {3072} INFO -  at 25.2s,	estimator xgboost's best error=5.7167,	best estimator xgboost's best error=5.7167
[flaml.automl: 09-17 01:49:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:49:29] {3072} INFO -  at 27.8s,	estimator xgboost's best error=5.7167,	best estimator xgboost's best error=5.7167
[flaml.automl: 09-17 01:49:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:49:32] {3072} INFO -  at 30.9s,	estimator xgboost's best error=5.6140,	best estimator xgboost's best error=5.6140
[flaml.automl: 09-17 01:49:32] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 01:49:37] {3072} INFO -  at 35.9s,	estimator xgboost's best error=5.6140,	best estimator xgboost's best error=5.6140
[flaml.automl: 09-17 01:49:37] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 01:49:39] {3072} INFO -  at 37.8s,	estimator xgboost's best error=5.6140,	best estimator xgboost's best error=5.6140
[flaml.automl: 09-17 01:49:39] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 01:49:40] {3072} INFO -  at 39.5s,	estimator xgboost's best error=5.6140,	best estimator xgboost's best error=5.6140
[flaml.automl: 09-17 01:49:40] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 01:49:48] {3072} INFO -  at 46.9s,	estimator xgboost's best error=5.5162,	best estimator xgboost's best error=5.5162
[flaml.automl: 09-17 01:49:48] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 01:49:52] {3072} INFO -  at 51.1s,	estimator xgboost's best error=5.5162,	best estimator xgboost's best error=5.5162
[flaml.automl: 09-17 01:49:52] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 01:49:59] {3072} INFO -  at 58.2s,	estimator xgboost's best error=5.5162,	best estimator xgboost's best error=5.5162
[flaml.automl: 09-17 01:50:13] {3335} INFO - retrain xgboost for 13.7s
[flaml.automl: 09-17 01:50:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5492977310397356, colsample_bynode=1,
             colsample_bytree=0.9510761842589558, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.04321599195729943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5415707634193446, scale_pos_weight=1,
             subsample=0.9351529901519405, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:50:13] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:50:13] {2637} INFO - Time taken to find the best model: 46.88231897354126
[flaml.automl: 09-17 01:50:13] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.04321599195729943, 'learning_rate': 1.0, 'subsample': 0.9351529901519405, 'colsample_bylevel': 0.5492977310397356, 'colsample_bytree': 0.9510761842589558, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5415707634193446, 'FLAML_sample_size': 63324}
PM2.5(0)最佳损失：-4.516218257681459
PM2.5(0)最好结果：{'pred_time': 5.701274069416855e-06, 'wall_clock_time': 46.88231897354126, 'metric_for_logging': {'pred_time': 5.701274069416855e-06}, 'val_loss': 5.516218257681459, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.04321599195729943, 'learning_rate': 1.0, 'subsample': 0.9351529901519405, 'colsample_bylevel': 0.5492977310397356, 'colsample_bytree': 0.9510761842589558, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5415707634193446, 'FLAML_sample_size': 63324}, 'config/n_estimators': 8, 'config/max_leaves': 16, 'config/min_child_weight': 0.04321599195729943, 'config/learning_rate': 1.0, 'config/subsample': 0.9351529901519405, 'config/colsample_bylevel': 0.5492977310397356, 'config/colsample_bytree': 0.9510761842589558, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.5415707634193446, 'config/FLAML_sample_size': 63324, 'experiment_tag': 'exp', 'time_total_s': 7.428271770477295}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5492977310397356, colsample_bynode=1,
             colsample_bytree=0.9510761842589558, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.04321599195729943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5415707634193446, scale_pos_weight=1,
             subsample=0.9351529901519405, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.930647615296819
PM2.5(0)的mse=79.77097229648476
PM2.5(0)的mae=5.4173973213531506
PM2.5(0)的mar=0.36299628132863115
总共花费的时间为：72.99
湘潭市
1508A
1511A
1512A
1513A
1514A
1564A
[flaml.automl: 09-17 02:08:06] {2390} INFO - task = regression
[flaml.automl: 09-17 02:08:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:08:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:08:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:08:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:08:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:08:08] {3025} INFO - Estimated sufficient time budget=74857s. Estimated necessary time budget=75s.
[flaml.automl: 09-17 02:08:08] {3072} INFO -  at 1.4s,	estimator xgboost's best error=23.0786,	best estimator xgboost's best error=23.0786
[flaml.automl: 09-17 02:08:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:08:10] {3072} INFO -  at 3.6s,	estimator xgboost's best error=10.9333,	best estimator xgboost's best error=10.9333
[flaml.automl: 09-17 02:08:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:08:11] {3072} INFO -  at 4.8s,	estimator xgboost's best error=10.9333,	best estimator xgboost's best error=10.9333
[flaml.automl: 09-17 02:08:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:08:15] {3072} INFO -  at 9.0s,	estimator xgboost's best error=10.9333,	best estimator xgboost's best error=10.9333
[flaml.automl: 09-17 02:08:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:08:16] {3072} INFO -  at 10.2s,	estimator xgboost's best error=7.9390,	best estimator xgboost's best error=7.9390
[flaml.automl: 09-17 02:08:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:08:18] {3072} INFO -  at 11.8s,	estimator xgboost's best error=6.7742,	best estimator xgboost's best error=6.7742
[flaml.automl: 09-17 02:08:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:08:20] {3072} INFO -  at 13.4s,	estimator xgboost's best error=5.9088,	best estimator xgboost's best error=5.9088
[flaml.automl: 09-17 02:08:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:08:22] {3072} INFO -  at 16.0s,	estimator xgboost's best error=5.9088,	best estimator xgboost's best error=5.9088
[flaml.automl: 09-17 02:08:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:08:24] {3072} INFO -  at 17.7s,	estimator xgboost's best error=5.9088,	best estimator xgboost's best error=5.9088
[flaml.automl: 09-17 02:08:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:08:27] {3072} INFO -  at 20.7s,	estimator xgboost's best error=5.9088,	best estimator xgboost's best error=5.9088
[flaml.automl: 09-17 02:08:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:08:28] {3072} INFO -  at 22.1s,	estimator xgboost's best error=5.9088,	best estimator xgboost's best error=5.9088
[flaml.automl: 09-17 02:08:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:08:30] {3072} INFO -  at 23.8s,	estimator xgboost's best error=5.7856,	best estimator xgboost's best error=5.7856
[flaml.automl: 09-17 02:08:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:08:31] {3072} INFO -  at 24.9s,	estimator xgboost's best error=5.7856,	best estimator xgboost's best error=5.7856
[flaml.automl: 09-17 02:08:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:08:38] {3072} INFO -  at 31.9s,	estimator xgboost's best error=4.0914,	best estimator xgboost's best error=4.0914
[flaml.automl: 09-17 02:08:38] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 02:08:51] {3072} INFO -  at 44.7s,	estimator xgboost's best error=3.9955,	best estimator xgboost's best error=3.9955
[flaml.automl: 09-17 02:08:51] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 02:08:58] {3072} INFO -  at 51.7s,	estimator xgboost's best error=3.9955,	best estimator xgboost's best error=3.9955
[flaml.automl: 09-17 02:09:11] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 02:09:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:09:11] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:09:11] {2637} INFO - Time taken to find the best model: 44.73831248283386
[flaml.automl: 09-17 02:09:11] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 62712}
PM2.5(0)最佳损失：-2.9955489280132146
PM2.5(0)最好结果：{'pred_time': 5.871433888729897e-06, 'wall_clock_time': 44.73831248283386, 'metric_for_logging': {'pred_time': 5.871433888729897e-06}, 'val_loss': 3.9955489280132146, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 62712}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 62712, 'experiment_tag': 'exp', 'time_total_s': 12.830302953720093}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9389150213854548
PM2.5(0)的mse=36.33108704199104
PM2.5(0)的mae=4.002595566908826
PM2.5(0)的mar=0.1693020895520459
总共花费的时间为：65.50
株洲市
1515A
1518A
1520A
1524A
1559A
2031A
[flaml.automl: 09-17 02:27:18] {2390} INFO - task = regression
[flaml.automl: 09-17 02:27:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:27:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:27:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:27:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:27:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:27:19] {3025} INFO - Estimated sufficient time budget=74471s. Estimated necessary time budget=74s.
[flaml.automl: 09-17 02:27:19] {3072} INFO -  at 1.5s,	estimator xgboost's best error=21.6323,	best estimator xgboost's best error=21.6323
[flaml.automl: 09-17 02:27:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:27:21] {3072} INFO -  at 3.6s,	estimator xgboost's best error=10.1795,	best estimator xgboost's best error=10.1795
[flaml.automl: 09-17 02:27:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:27:23] {3072} INFO -  at 4.8s,	estimator xgboost's best error=10.1795,	best estimator xgboost's best error=10.1795
[flaml.automl: 09-17 02:27:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:27:27] {3072} INFO -  at 9.1s,	estimator xgboost's best error=10.1795,	best estimator xgboost's best error=10.1795
[flaml.automl: 09-17 02:27:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:27:28] {3072} INFO -  at 10.2s,	estimator xgboost's best error=7.0939,	best estimator xgboost's best error=7.0939
[flaml.automl: 09-17 02:27:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:27:30] {3072} INFO -  at 11.8s,	estimator xgboost's best error=6.0122,	best estimator xgboost's best error=6.0122
[flaml.automl: 09-17 02:27:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:27:31] {3072} INFO -  at 13.4s,	estimator xgboost's best error=5.3791,	best estimator xgboost's best error=5.3791
[flaml.automl: 09-17 02:27:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:27:34] {3072} INFO -  at 16.1s,	estimator xgboost's best error=5.3791,	best estimator xgboost's best error=5.3791
[flaml.automl: 09-17 02:27:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:27:36] {3072} INFO -  at 17.7s,	estimator xgboost's best error=5.3791,	best estimator xgboost's best error=5.3791
[flaml.automl: 09-17 02:27:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:27:39] {3072} INFO -  at 20.8s,	estimator xgboost's best error=4.2809,	best estimator xgboost's best error=4.2809
[flaml.automl: 09-17 02:27:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:27:40] {3072} INFO -  at 22.4s,	estimator xgboost's best error=4.2809,	best estimator xgboost's best error=4.2809
[flaml.automl: 09-17 02:27:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:27:41] {3072} INFO -  at 23.5s,	estimator xgboost's best error=4.2809,	best estimator xgboost's best error=4.2809
[flaml.automl: 09-17 02:27:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:27:43] {3072} INFO -  at 25.5s,	estimator xgboost's best error=4.2809,	best estimator xgboost's best error=4.2809
[flaml.automl: 09-17 02:27:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:27:46] {3072} INFO -  at 28.0s,	estimator xgboost's best error=4.2809,	best estimator xgboost's best error=4.2809
[flaml.automl: 09-17 02:27:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 02:27:49] {3072} INFO -  at 31.1s,	estimator xgboost's best error=4.2809,	best estimator xgboost's best error=4.2809
[flaml.automl: 09-17 02:27:49] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 02:27:54] {3072} INFO -  at 36.1s,	estimator xgboost's best error=4.2809,	best estimator xgboost's best error=4.2809
[flaml.automl: 09-17 02:27:54] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 02:27:56] {3072} INFO -  at 38.0s,	estimator xgboost's best error=4.2328,	best estimator xgboost's best error=4.2328
[flaml.automl: 09-17 02:27:56] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 02:27:58] {3072} INFO -  at 39.7s,	estimator xgboost's best error=4.2328,	best estimator xgboost's best error=4.2328
[flaml.automl: 09-17 02:27:58] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 02:28:02] {3072} INFO -  at 44.0s,	estimator xgboost's best error=3.9260,	best estimator xgboost's best error=3.9260
[flaml.automl: 09-17 02:28:02] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 02:28:04] {3072} INFO -  at 45.7s,	estimator xgboost's best error=3.9260,	best estimator xgboost's best error=3.9260
[flaml.automl: 09-17 02:28:04] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 02:28:17] {3072} INFO -  at 59.1s,	estimator xgboost's best error=3.4986,	best estimator xgboost's best error=3.4986
[flaml.automl: 09-17 02:28:34] {3335} INFO - retrain xgboost for 16.6s
[flaml.automl: 09-17 02:28:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:28:34] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:28:34] {2637} INFO - Time taken to find the best model: 59.09825348854065
[flaml.automl: 09-17 02:28:34] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 62737}
PM2.5(0)最佳损失：-2.4985842080595777
PM2.5(0)最好结果：{'pred_time': 6.168648940455604e-06, 'wall_clock_time': 59.09825348854065, 'metric_for_logging': {'pred_time': 6.168648940455604e-06}, 'val_loss': 3.4985842080595777, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 62737}, 'config/n_estimators': 13, 'config/max_leaves': 18, 'config/min_child_weight': 0.002212987346630184, 'config/learning_rate': 1.0, 'config/subsample': 0.9736629142498529, 'config/colsample_bylevel': 0.8717925041332704, 'config/colsample_bytree': 0.9857450191529009, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.04619111740498511, 'config/FLAML_sample_size': 62737, 'experiment_tag': 'exp', 'time_total_s': 13.35209608078003}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9455155178620893
PM2.5(0)的mse=30.048414429400754
PM2.5(0)的mae=3.4224416421899
PM2.5(0)的mar=0.13389946128879004
总共花费的时间为：76.71
包头市
1585A
3283A
3419A
3683A
[flaml.automl: 09-17 02:41:24] {2390} INFO - task = regression
[flaml.automl: 09-17 02:41:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:41:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:41:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:41:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:41:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:41:26] {3025} INFO - Estimated sufficient time budget=53131s. Estimated necessary time budget=53s.
[flaml.automl: 09-17 02:41:26] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.4181,	best estimator xgboost's best error=18.4181
[flaml.automl: 09-17 02:41:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:41:28] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.4110,	best estimator xgboost's best error=9.4110
[flaml.automl: 09-17 02:41:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:41:29] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.4110,	best estimator xgboost's best error=9.4110
[flaml.automl: 09-17 02:41:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:41:35] {3072} INFO -  at 10.5s,	estimator xgboost's best error=9.4110,	best estimator xgboost's best error=9.4110
[flaml.automl: 09-17 02:41:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:41:36] {3072} INFO -  at 11.7s,	estimator xgboost's best error=7.9080,	best estimator xgboost's best error=7.9080
[flaml.automl: 09-17 02:41:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:41:37] {3072} INFO -  at 13.3s,	estimator xgboost's best error=7.9080,	best estimator xgboost's best error=7.9080
[flaml.automl: 09-17 02:41:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:41:39] {3072} INFO -  at 14.9s,	estimator xgboost's best error=5.9320,	best estimator xgboost's best error=5.9320
[flaml.automl: 09-17 02:41:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:41:42] {3072} INFO -  at 17.6s,	estimator xgboost's best error=5.9320,	best estimator xgboost's best error=5.9320
[flaml.automl: 09-17 02:41:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:41:44] {3072} INFO -  at 19.3s,	estimator xgboost's best error=5.9320,	best estimator xgboost's best error=5.9320
[flaml.automl: 09-17 02:41:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:41:47] {3072} INFO -  at 22.3s,	estimator xgboost's best error=5.9320,	best estimator xgboost's best error=5.9320
[flaml.automl: 09-17 02:41:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:41:48] {3072} INFO -  at 23.7s,	estimator xgboost's best error=5.9320,	best estimator xgboost's best error=5.9320
[flaml.automl: 09-17 02:41:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:41:50] {3072} INFO -  at 25.6s,	estimator xgboost's best error=5.9320,	best estimator xgboost's best error=5.9320
[flaml.automl: 09-17 02:41:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:41:52] {3072} INFO -  at 27.8s,	estimator xgboost's best error=5.9320,	best estimator xgboost's best error=5.9320
[flaml.automl: 09-17 02:41:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:42:05] {3072} INFO -  at 41.0s,	estimator xgboost's best error=5.5943,	best estimator xgboost's best error=5.5943
[flaml.automl: 09-17 02:42:05] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 02:42:24] {3072} INFO -  at 59.5s,	estimator xgboost's best error=5.4745,	best estimator xgboost's best error=5.4745
[flaml.automl: 09-17 02:42:47] {3335} INFO - retrain xgboost for 23.6s
[flaml.automl: 09-17 02:42:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:42:47] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:42:47] {2637} INFO - Time taken to find the best model: 59.536876916885376
[flaml.automl: 09-17 02:42:47] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 44475}
PM2.5(0)最佳损失：-4.474520101763371
PM2.5(0)最好结果：{'pred_time': 1.6935293320362888e-05, 'wall_clock_time': 59.536876916885376, 'metric_for_logging': {'pred_time': 1.6935293320362888e-05}, 'val_loss': 5.474520101763371, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 44475}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 44475, 'experiment_tag': 'exp', 'time_total_s': 18.53548502922058}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8679956635712888
PM2.5(0)的mse=67.5328977544143
PM2.5(0)的mae=5.363480499024774
PM2.5(0)的mar=0.32717384915156117
总共花费的时间为：84.03
鄂尔多斯市
1591A
1592A
1594A
1595A
[flaml.automl: 09-17 02:55:10] {2390} INFO - task = regression
[flaml.automl: 09-17 02:55:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:55:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:55:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:55:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:55:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:55:12] {3025} INFO - Estimated sufficient time budget=52319s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 02:55:12] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.0746,	best estimator xgboost's best error=15.0746
[flaml.automl: 09-17 02:55:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:55:14] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.9284,	best estimator xgboost's best error=7.9284
[flaml.automl: 09-17 02:55:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:55:15] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.9284,	best estimator xgboost's best error=7.9284
[flaml.automl: 09-17 02:55:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:55:21] {3072} INFO -  at 10.5s,	estimator xgboost's best error=7.9284,	best estimator xgboost's best error=7.9284
[flaml.automl: 09-17 02:55:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:55:22] {3072} INFO -  at 11.6s,	estimator xgboost's best error=6.3380,	best estimator xgboost's best error=6.3380
[flaml.automl: 09-17 02:55:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:55:23] {3072} INFO -  at 13.2s,	estimator xgboost's best error=6.3380,	best estimator xgboost's best error=6.3380
[flaml.automl: 09-17 02:55:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:55:25] {3072} INFO -  at 14.9s,	estimator xgboost's best error=5.4468,	best estimator xgboost's best error=5.4468
[flaml.automl: 09-17 02:55:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:55:28] {3072} INFO -  at 17.5s,	estimator xgboost's best error=5.4468,	best estimator xgboost's best error=5.4468
[flaml.automl: 09-17 02:55:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:55:29] {3072} INFO -  at 19.1s,	estimator xgboost's best error=5.4468,	best estimator xgboost's best error=5.4468
[flaml.automl: 09-17 02:55:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:55:32] {3072} INFO -  at 22.2s,	estimator xgboost's best error=5.4468,	best estimator xgboost's best error=5.4468
[flaml.automl: 09-17 02:55:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:55:34] {3072} INFO -  at 23.6s,	estimator xgboost's best error=5.4468,	best estimator xgboost's best error=5.4468
[flaml.automl: 09-17 02:55:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:55:36] {3072} INFO -  at 25.3s,	estimator xgboost's best error=5.4468,	best estimator xgboost's best error=5.4468
[flaml.automl: 09-17 02:55:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:55:37] {3072} INFO -  at 26.8s,	estimator xgboost's best error=5.4468,	best estimator xgboost's best error=5.4468
[flaml.automl: 09-17 02:55:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:55:50] {3072} INFO -  at 39.9s,	estimator xgboost's best error=5.2023,	best estimator xgboost's best error=5.2023
[flaml.automl: 09-17 02:55:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 02:56:10] {3072} INFO -  at 59.7s,	estimator xgboost's best error=5.1381,	best estimator xgboost's best error=5.1381
[flaml.automl: 09-17 02:56:33] {3335} INFO - retrain xgboost for 23.5s
[flaml.automl: 09-17 02:56:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:56:33] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:56:33] {2637} INFO - Time taken to find the best model: 59.713263750076294
[flaml.automl: 09-17 02:56:33] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43754}
PM2.5(0)最佳损失：-4.138125913322634
PM2.5(0)最好结果：{'pred_time': 1.7796461104562085e-05, 'wall_clock_time': 59.713263750076294, 'metric_for_logging': {'pred_time': 1.7796461104562085e-05}, 'val_loss': 5.138125913322634, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43754}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 43754, 'experiment_tag': 'exp', 'time_total_s': 19.764076232910156}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.7952939052919872
PM2.5(0)的mse=56.793582127000995
PM2.5(0)的mae=4.95289855460598
PM2.5(0)的mar=0.3200244574694869
总共花费的时间为：83.96
营口市
1598A
3378A
3379A
3866A
[flaml.automl: 09-17 03:10:25] {2390} INFO - task = regression
[flaml.automl: 09-17 03:10:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:10:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:10:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:10:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:10:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:10:26] {3025} INFO - Estimated sufficient time budget=12047s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:10:26] {3072} INFO -  at 1.4s,	estimator xgboost's best error=19.1844,	best estimator xgboost's best error=19.1844
[flaml.automl: 09-17 03:10:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:10:28] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.7008,	best estimator xgboost's best error=9.7008
[flaml.automl: 09-17 03:10:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:10:29] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.7008,	best estimator xgboost's best error=9.7008
[flaml.automl: 09-17 03:10:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:10:42] {3072} INFO -  at 18.0s,	estimator xgboost's best error=9.7008,	best estimator xgboost's best error=9.7008
[flaml.automl: 09-17 03:10:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:10:45] {3072} INFO -  at 20.2s,	estimator xgboost's best error=7.4540,	best estimator xgboost's best error=7.4540
[flaml.automl: 09-17 03:10:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:10:48] {3072} INFO -  at 23.2s,	estimator xgboost's best error=7.4540,	best estimator xgboost's best error=7.4540
[flaml.automl: 09-17 03:10:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:10:51] {3072} INFO -  at 26.4s,	estimator xgboost's best error=6.0767,	best estimator xgboost's best error=6.0767
[flaml.automl: 09-17 03:10:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:10:56] {3072} INFO -  at 31.5s,	estimator xgboost's best error=6.0767,	best estimator xgboost's best error=6.0767
[flaml.automl: 09-17 03:10:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:10:59] {3072} INFO -  at 34.4s,	estimator xgboost's best error=6.0767,	best estimator xgboost's best error=6.0767
[flaml.automl: 09-17 03:10:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:11:04] {3072} INFO -  at 39.9s,	estimator xgboost's best error=6.0767,	best estimator xgboost's best error=6.0767
[flaml.automl: 09-17 03:11:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:11:07] {3072} INFO -  at 42.6s,	estimator xgboost's best error=6.0767,	best estimator xgboost's best error=6.0767
[flaml.automl: 09-17 03:11:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:11:09] {3072} INFO -  at 44.7s,	estimator xgboost's best error=6.0767,	best estimator xgboost's best error=6.0767
[flaml.automl: 09-17 03:11:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:11:22] {3072} INFO -  at 57.8s,	estimator xgboost's best error=5.6636,	best estimator xgboost's best error=5.6636
[flaml.automl: 09-17 03:11:35] {3335} INFO - retrain xgboost for 13.0s
[flaml.automl: 09-17 03:11:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:11:35] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:11:35] {2637} INFO - Time taken to find the best model: 57.76372766494751
[flaml.automl: 09-17 03:11:35] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
PM2.5(0)最佳损失：-4.663627965430276
PM2.5(0)最好结果：{'pred_time': 1.897331622007082e-05, 'wall_clock_time': 57.76372766494751, 'metric_for_logging': {'pred_time': 1.897331622007082e-05}, 'val_loss': 5.663627965430276, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 13.074847936630249}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8703632566492779
PM2.5(0)的mse=69.38870439740612
PM2.5(0)的mae=5.584686163818504
PM2.5(0)的mar=0.25264849183370497
总共花费的时间为：71.50
丹东市
1600A
1602A
1603A
[flaml.automl: 09-17 03:22:02] {2390} INFO - task = regression
[flaml.automl: 09-17 03:22:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:22:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:22:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:22:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:22:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:22:04] {3025} INFO - Estimated sufficient time budget=20886s. Estimated necessary time budget=21s.
[flaml.automl: 09-17 03:22:04] {3072} INFO -  at 2.2s,	estimator xgboost's best error=15.3038,	best estimator xgboost's best error=15.3038
[flaml.automl: 09-17 03:22:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:22:08] {3072} INFO -  at 6.0s,	estimator xgboost's best error=7.5322,	best estimator xgboost's best error=7.5322
[flaml.automl: 09-17 03:22:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:22:10] {3072} INFO -  at 8.2s,	estimator xgboost's best error=7.5322,	best estimator xgboost's best error=7.5322
[flaml.automl: 09-17 03:22:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:22:37] {3072} INFO -  at 35.1s,	estimator xgboost's best error=7.5322,	best estimator xgboost's best error=7.5322
[flaml.automl: 09-17 03:22:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:22:40] {3072} INFO -  at 38.5s,	estimator xgboost's best error=5.9902,	best estimator xgboost's best error=5.9902
[flaml.automl: 09-17 03:22:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:22:45] {3072} INFO -  at 42.8s,	estimator xgboost's best error=5.3806,	best estimator xgboost's best error=5.3806
[flaml.automl: 09-17 03:22:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:22:49] {3072} INFO -  at 47.3s,	estimator xgboost's best error=4.9172,	best estimator xgboost's best error=4.9172
[flaml.automl: 09-17 03:22:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:22:57] {3072} INFO -  at 55.0s,	estimator xgboost's best error=4.9172,	best estimator xgboost's best error=4.9172
[flaml.automl: 09-17 03:22:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:23:00] {3072} INFO -  at 58.1s,	estimator xgboost's best error=4.9172,	best estimator xgboost's best error=4.9172
[flaml.automl: 09-17 03:23:03] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-17 03:23:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:23:03] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:23:03] {2637} INFO - Time taken to find the best model: 47.338809967041016
[flaml.automl: 09-17 03:23:03] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-3.917226433103971
PM2.5(0)最好结果：{'pred_time': 3.6739731042959696e-05, 'wall_clock_time': 47.338809967041016, 'metric_for_logging': {'pred_time': 3.6739731042959696e-05}, 'val_loss': 4.917226433103971, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.555882215499878}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.7788489037225542
PM2.5(0)的mse=58.267480123799686
PM2.5(0)的mae=4.942610167544537
PM2.5(0)的mar=0.2965937917148081
总共花费的时间为：61.67
盘锦市
1604A
1605A
[flaml.automl: 09-17 03:29:29] {2390} INFO - task = regression
[flaml.automl: 09-17 03:29:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:29:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:29:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:29:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:29:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:29:30] {3025} INFO - Estimated sufficient time budget=12043s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:29:30] {3072} INFO -  at 1.3s,	estimator xgboost's best error=18.0515,	best estimator xgboost's best error=18.0515
[flaml.automl: 09-17 03:29:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:29:32] {3072} INFO -  at 3.4s,	estimator xgboost's best error=9.2048,	best estimator xgboost's best error=9.2048
[flaml.automl: 09-17 03:29:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:29:34] {3072} INFO -  at 4.6s,	estimator xgboost's best error=9.2048,	best estimator xgboost's best error=9.2048
[flaml.automl: 09-17 03:29:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:29:45] {3072} INFO -  at 15.8s,	estimator xgboost's best error=9.2048,	best estimator xgboost's best error=9.2048
[flaml.automl: 09-17 03:29:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:29:47] {3072} INFO -  at 17.8s,	estimator xgboost's best error=7.1095,	best estimator xgboost's best error=7.1095
[flaml.automl: 09-17 03:29:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:29:50] {3072} INFO -  at 20.7s,	estimator xgboost's best error=6.9275,	best estimator xgboost's best error=6.9275
[flaml.automl: 09-17 03:29:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:29:53] {3072} INFO -  at 23.8s,	estimator xgboost's best error=6.0481,	best estimator xgboost's best error=6.0481
[flaml.automl: 09-17 03:29:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:29:58] {3072} INFO -  at 28.8s,	estimator xgboost's best error=6.0481,	best estimator xgboost's best error=6.0481
[flaml.automl: 09-17 03:29:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:30:01] {3072} INFO -  at 31.8s,	estimator xgboost's best error=6.0481,	best estimator xgboost's best error=6.0481
[flaml.automl: 09-17 03:30:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:30:06] {3072} INFO -  at 37.5s,	estimator xgboost's best error=5.8247,	best estimator xgboost's best error=5.8247
[flaml.automl: 09-17 03:30:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:30:09] {3072} INFO -  at 40.4s,	estimator xgboost's best error=5.8247,	best estimator xgboost's best error=5.8247
[flaml.automl: 09-17 03:30:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:30:12] {3072} INFO -  at 42.6s,	estimator xgboost's best error=5.8247,	best estimator xgboost's best error=5.8247
[flaml.automl: 09-17 03:30:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:30:27] {3072} INFO -  at 58.5s,	estimator xgboost's best error=5.5742,	best estimator xgboost's best error=5.5742
[flaml.automl: 09-17 03:30:50] {3335} INFO - retrain xgboost for 22.0s
[flaml.automl: 09-17 03:30:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 03:30:50] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:30:50] {2637} INFO - Time taken to find the best model: 58.486382484436035
[flaml.automl: 09-17 03:30:50] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-4.574246895234479
PM2.5(0)最好结果：{'pred_time': 3.446788149695414e-05, 'wall_clock_time': 58.486382484436035, 'metric_for_logging': {'pred_time': 3.446788149695414e-05}, 'val_loss': 5.574246895234479, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 15.902122259140015}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8506616975425634
PM2.5(0)的mse=84.57502899416579
PM2.5(0)的mae=5.51575490892155
PM2.5(0)的mar=0.2673909366089826
总共花费的时间为：80.97
葫芦岛市
1607A
[flaml.automl: 09-17 03:34:41] {2390} INFO - task = regression
[flaml.automl: 09-17 03:34:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:34:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:34:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:34:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:34:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:34:43] {3025} INFO - Estimated sufficient time budget=11955s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:34:43] {3072} INFO -  at 1.2s,	estimator xgboost's best error=20.5824,	best estimator xgboost's best error=20.5824
[flaml.automl: 09-17 03:34:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:34:45] {3072} INFO -  at 3.4s,	estimator xgboost's best error=11.8929,	best estimator xgboost's best error=11.8929
[flaml.automl: 09-17 03:34:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:34:47] {3072} INFO -  at 5.6s,	estimator xgboost's best error=11.8929,	best estimator xgboost's best error=11.8929
[flaml.automl: 09-17 03:34:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:35:00] {3072} INFO -  at 18.6s,	estimator xgboost's best error=11.8929,	best estimator xgboost's best error=11.8929
[flaml.automl: 09-17 03:35:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:35:02] {3072} INFO -  at 20.7s,	estimator xgboost's best error=8.6606,	best estimator xgboost's best error=8.6606
[flaml.automl: 09-17 03:35:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:35:05] {3072} INFO -  at 23.7s,	estimator xgboost's best error=8.1193,	best estimator xgboost's best error=8.1193
[flaml.automl: 09-17 03:35:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:35:08] {3072} INFO -  at 26.8s,	estimator xgboost's best error=8.1193,	best estimator xgboost's best error=8.1193
[flaml.automl: 09-17 03:35:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:35:12] {3072} INFO -  at 30.7s,	estimator xgboost's best error=8.1193,	best estimator xgboost's best error=8.1193
[flaml.automl: 09-17 03:35:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:35:14] {3072} INFO -  at 32.9s,	estimator xgboost's best error=8.0138,	best estimator xgboost's best error=8.0138
[flaml.automl: 09-17 03:35:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:35:18] {3072} INFO -  at 36.3s,	estimator xgboost's best error=7.8121,	best estimator xgboost's best error=7.8121
[flaml.automl: 09-17 03:35:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:35:20] {3072} INFO -  at 38.4s,	estimator xgboost's best error=7.8121,	best estimator xgboost's best error=7.8121
[flaml.automl: 09-17 03:35:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:35:22] {3072} INFO -  at 40.5s,	estimator xgboost's best error=7.8121,	best estimator xgboost's best error=7.8121
[flaml.automl: 09-17 03:35:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:35:34] {3072} INFO -  at 52.5s,	estimator xgboost's best error=7.6513,	best estimator xgboost's best error=7.6513
[flaml.automl: 09-17 03:35:51] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-17 03:35:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7321477488407947, colsample_bynode=1,
             colsample_bytree=0.523235031572675, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.006178490416939562,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.026483886059738067, reg_lambda=15.135470774428443,
             scale_pos_weight=1, subsample=0.7583121251176891,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 03:35:51] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:35:51] {2637} INFO - Time taken to find the best model: 52.484028339385986
[flaml.automl: 09-17 03:35:51] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.006178490416939562, 'learning_rate': 1.0, 'subsample': 0.7583121251176891, 'colsample_bylevel': 0.7321477488407947, 'colsample_bytree': 0.523235031572675, 'reg_alpha': 0.026483886059738067, 'reg_lambda': 15.135470774428443}
PM2.5(0)最佳损失：-6.651346521525451
PM2.5(0)最好结果：{'pred_time': 6.0333490171856175e-05, 'wall_clock_time': 52.484028339385986, 'metric_for_logging': {'pred_time': 6.0333490171856175e-05}, 'val_loss': 7.651346521525451, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.006178490416939562, 'learning_rate': 1.0, 'subsample': 0.7583121251176891, 'colsample_bylevel': 0.7321477488407947, 'colsample_bytree': 0.523235031572675, 'reg_alpha': 0.026483886059738067, 'reg_lambda': 15.135470774428443}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.006178490416939562, 'config/learning_rate': 1.0, 'config/subsample': 0.7583121251176891, 'config/colsample_bylevel': 0.7321477488407947, 'config/colsample_bytree': 0.523235031572675, 'config/reg_alpha': 0.026483886059738067, 'config/reg_lambda': 15.135470774428443, 'experiment_tag': 'exp', 'time_total_s': 11.941752195358276}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7321477488407947, colsample_bynode=1,
             colsample_bytree=0.523235031572675, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.006178490416939562,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.026483886059738067, reg_lambda=15.135470774428443,
             scale_pos_weight=1, subsample=0.7583121251176891,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8381756145212462
PM2.5(0)的mse=135.15776037469305
PM2.5(0)的mae=7.074037697250487
PM2.5(0)的mar=0.33508337110212333
总共花费的时间为：70.07
泉州市
1614A
3529A
[flaml.automl: 09-17 03:42:53] {2390} INFO - task = regression
[flaml.automl: 09-17 03:42:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:42:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:42:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:42:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:42:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:42:54] {3025} INFO - Estimated sufficient time budget=17269s. Estimated necessary time budget=17s.
[flaml.automl: 09-17 03:42:54] {3072} INFO -  at 1.8s,	estimator xgboost's best error=11.3065,	best estimator xgboost's best error=11.3065
[flaml.automl: 09-17 03:42:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:42:58] {3072} INFO -  at 5.3s,	estimator xgboost's best error=6.2380,	best estimator xgboost's best error=6.2380
[flaml.automl: 09-17 03:42:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:43:00] {3072} INFO -  at 7.5s,	estimator xgboost's best error=6.2380,	best estimator xgboost's best error=6.2380
[flaml.automl: 09-17 03:43:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:43:18] {3072} INFO -  at 25.0s,	estimator xgboost's best error=6.2380,	best estimator xgboost's best error=6.2380
[flaml.automl: 09-17 03:43:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:43:21] {3072} INFO -  at 28.2s,	estimator xgboost's best error=3.9108,	best estimator xgboost's best error=3.9108
[flaml.automl: 09-17 03:43:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:43:25] {3072} INFO -  at 32.4s,	estimator xgboost's best error=3.9108,	best estimator xgboost's best error=3.9108
[flaml.automl: 09-17 03:43:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:43:30] {3072} INFO -  at 36.9s,	estimator xgboost's best error=3.1734,	best estimator xgboost's best error=3.1734
[flaml.automl: 09-17 03:43:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:43:36] {3072} INFO -  at 43.3s,	estimator xgboost's best error=3.1734,	best estimator xgboost's best error=3.1734
[flaml.automl: 09-17 03:43:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:43:40] {3072} INFO -  at 47.6s,	estimator xgboost's best error=3.1734,	best estimator xgboost's best error=3.1734
[flaml.automl: 09-17 03:43:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:43:48] {3072} INFO -  at 55.7s,	estimator xgboost's best error=3.1734,	best estimator xgboost's best error=3.1734
[flaml.automl: 09-17 03:43:53] {3335} INFO - retrain xgboost for 4.4s
[flaml.automl: 09-17 03:43:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 03:43:53] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:43:53] {2637} INFO - Time taken to find the best model: 36.94717216491699
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-2.1733829936569915
PM2.5(0)最好结果：{'pred_time': 4.7940663785840196e-05, 'wall_clock_time': 36.94717216491699, 'metric_for_logging': {'pred_time': 4.7940663785840196e-05}, 'val_loss': 3.1733829936569915, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.566134691238403}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7663935623983267
PM2.5(0)的mse=21.971909458892792
PM2.5(0)的mae=3.1121779454173804
PM2.5(0)的mar=0.23286532043745894
总共花费的时间为：60.69
临沂市
1618A
1619A
1620A
3496A
3860A
[flaml.automl: 09-17 04:00:03] {2390} INFO - task = regression
[flaml.automl: 09-17 04:00:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:00:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:00:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:00:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:00:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:00:04] {3025} INFO - Estimated sufficient time budget=58050s. Estimated necessary time budget=58s.
[flaml.automl: 09-17 04:00:04] {3072} INFO -  at 1.4s,	estimator xgboost's best error=25.7083,	best estimator xgboost's best error=25.7083
[flaml.automl: 09-17 04:00:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:00:06] {3072} INFO -  at 3.5s,	estimator xgboost's best error=12.4196,	best estimator xgboost's best error=12.4196
[flaml.automl: 09-17 04:00:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:00:07] {3072} INFO -  at 4.7s,	estimator xgboost's best error=12.4196,	best estimator xgboost's best error=12.4196
[flaml.automl: 09-17 04:00:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:00:13] {3072} INFO -  at 10.0s,	estimator xgboost's best error=12.4196,	best estimator xgboost's best error=12.4196
[flaml.automl: 09-17 04:00:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:00:14] {3072} INFO -  at 11.1s,	estimator xgboost's best error=9.2449,	best estimator xgboost's best error=9.2449
[flaml.automl: 09-17 04:00:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:00:15] {3072} INFO -  at 12.7s,	estimator xgboost's best error=9.2449,	best estimator xgboost's best error=9.2449
[flaml.automl: 09-17 04:00:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:00:17] {3072} INFO -  at 14.4s,	estimator xgboost's best error=5.9695,	best estimator xgboost's best error=5.9695
[flaml.automl: 09-17 04:00:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:00:20] {3072} INFO -  at 17.1s,	estimator xgboost's best error=5.9695,	best estimator xgboost's best error=5.9695
[flaml.automl: 09-17 04:00:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:00:21] {3072} INFO -  at 18.7s,	estimator xgboost's best error=5.9695,	best estimator xgboost's best error=5.9695
[flaml.automl: 09-17 04:00:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:00:24] {3072} INFO -  at 21.7s,	estimator xgboost's best error=5.9695,	best estimator xgboost's best error=5.9695
[flaml.automl: 09-17 04:00:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:00:26] {3072} INFO -  at 23.2s,	estimator xgboost's best error=5.9695,	best estimator xgboost's best error=5.9695
[flaml.automl: 09-17 04:00:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:00:28] {3072} INFO -  at 24.9s,	estimator xgboost's best error=5.8462,	best estimator xgboost's best error=5.8462
[flaml.automl: 09-17 04:00:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:00:29] {3072} INFO -  at 26.0s,	estimator xgboost's best error=5.8462,	best estimator xgboost's best error=5.8462
[flaml.automl: 09-17 04:00:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:00:36] {3072} INFO -  at 33.1s,	estimator xgboost's best error=4.7845,	best estimator xgboost's best error=4.7845
[flaml.automl: 09-17 04:00:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 04:00:48] {3072} INFO -  at 45.8s,	estimator xgboost's best error=4.6526,	best estimator xgboost's best error=4.6526
[flaml.automl: 09-17 04:00:48] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 04:00:56] {3072} INFO -  at 52.9s,	estimator xgboost's best error=4.6526,	best estimator xgboost's best error=4.6526
[flaml.automl: 09-17 04:01:10] {3335} INFO - retrain xgboost for 14.5s
[flaml.automl: 09-17 04:01:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:01:10] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:01:10] {2637} INFO - Time taken to find the best model: 45.84606909751892
[flaml.automl: 09-17 04:01:10] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 48462}
PM2.5(0)最佳损失：-3.6525700624053306
PM2.5(0)最好结果：{'pred_time': 7.801091327862045e-06, 'wall_clock_time': 45.84606909751892, 'metric_for_logging': {'pred_time': 7.801091327862045e-06}, 'val_loss': 4.6525700624053306, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 48462}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 48462, 'experiment_tag': 'exp', 'time_total_s': 12.769295930862427}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9464663834660917
PM2.5(0)的mse=47.71500812666846
PM2.5(0)的mae=4.612404299643094
PM2.5(0)的mar=0.17262454899971422
总共花费的时间为：68.43
德州市
3066A
3372A
3511A
[flaml.automl: 09-17 04:10:15] {2390} INFO - task = regression
[flaml.automl: 09-17 04:10:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:10:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:10:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:10:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:10:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:10:18] {3025} INFO - Estimated sufficient time budget=28628s. Estimated necessary time budget=29s.
[flaml.automl: 09-17 04:10:18] {3072} INFO -  at 3.0s,	estimator xgboost's best error=26.0722,	best estimator xgboost's best error=26.0722
[flaml.automl: 09-17 04:10:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:10:23] {3072} INFO -  at 7.7s,	estimator xgboost's best error=12.7713,	best estimator xgboost's best error=12.7713
[flaml.automl: 09-17 04:10:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:10:26] {3072} INFO -  at 10.5s,	estimator xgboost's best error=12.7713,	best estimator xgboost's best error=12.7713
[flaml.automl: 09-17 04:10:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:10:42] {3072} INFO -  at 27.0s,	estimator xgboost's best error=12.7713,	best estimator xgboost's best error=12.7713
[flaml.automl: 09-17 04:10:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:10:43] {3072} INFO -  at 28.2s,	estimator xgboost's best error=9.7897,	best estimator xgboost's best error=9.7897
[flaml.automl: 09-17 04:10:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:10:45] {3072} INFO -  at 29.8s,	estimator xgboost's best error=8.4561,	best estimator xgboost's best error=8.4561
[flaml.automl: 09-17 04:10:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:10:47] {3072} INFO -  at 31.4s,	estimator xgboost's best error=7.5111,	best estimator xgboost's best error=7.5111
[flaml.automl: 09-17 04:10:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:10:49] {3072} INFO -  at 34.1s,	estimator xgboost's best error=7.5111,	best estimator xgboost's best error=7.5111
[flaml.automl: 09-17 04:10:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:10:51] {3072} INFO -  at 35.7s,	estimator xgboost's best error=6.8352,	best estimator xgboost's best error=6.8352
[flaml.automl: 09-17 04:10:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:10:54] {3072} INFO -  at 38.7s,	estimator xgboost's best error=6.6337,	best estimator xgboost's best error=6.6337
[flaml.automl: 09-17 04:10:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:10:55] {3072} INFO -  at 39.9s,	estimator xgboost's best error=6.6337,	best estimator xgboost's best error=6.6337
[flaml.automl: 09-17 04:10:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:11:09] {3072} INFO -  at 53.5s,	estimator xgboost's best error=5.8501,	best estimator xgboost's best error=5.8501
[flaml.automl: 09-17 04:11:22] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-17 04:11:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:11:22] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:11:22] {2637} INFO - Time taken to find the best model: 53.52180099487305
[flaml.automl: 09-17 04:11:22] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
PM2.5(0)最佳损失：-4.850076583493126
PM2.5(0)最好结果：{'pred_time': 1.1393021687205458e-05, 'wall_clock_time': 53.52180099487305, 'metric_for_logging': {'pred_time': 1.1393021687205458e-05}, 'val_loss': 5.850076583493126, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 13.637557983398438}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9178526295805965
PM2.5(0)的mse=66.38996663824327
PM2.5(0)的mae=5.801951405964406
PM2.5(0)的mar=0.24562275568632655
总共花费的时间为：67.84
聊城市
1625A
3513A
[flaml.automl: 09-17 04:17:52] {2390} INFO - task = regression
[flaml.automl: 09-17 04:17:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:17:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:17:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:17:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:17:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:17:53] {3025} INFO - Estimated sufficient time budget=11942s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 04:17:53] {3072} INFO -  at 1.3s,	estimator xgboost's best error=26.5326,	best estimator xgboost's best error=26.5326
[flaml.automl: 09-17 04:17:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:17:55] {3072} INFO -  at 3.2s,	estimator xgboost's best error=14.6334,	best estimator xgboost's best error=14.6334
[flaml.automl: 09-17 04:17:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:17:56] {3072} INFO -  at 4.4s,	estimator xgboost's best error=14.6334,	best estimator xgboost's best error=14.6334
[flaml.automl: 09-17 04:17:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:18:05] {3072} INFO -  at 12.8s,	estimator xgboost's best error=14.6334,	best estimator xgboost's best error=14.6334
[flaml.automl: 09-17 04:18:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:18:06] {3072} INFO -  at 14.0s,	estimator xgboost's best error=9.8487,	best estimator xgboost's best error=9.8487
[flaml.automl: 09-17 04:18:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:18:07] {3072} INFO -  at 15.6s,	estimator xgboost's best error=8.1062,	best estimator xgboost's best error=8.1062
[flaml.automl: 09-17 04:18:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:18:09] {3072} INFO -  at 17.2s,	estimator xgboost's best error=7.4798,	best estimator xgboost's best error=7.4798
[flaml.automl: 09-17 04:18:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:18:11] {3072} INFO -  at 19.5s,	estimator xgboost's best error=7.4798,	best estimator xgboost's best error=7.4798
[flaml.automl: 09-17 04:18:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:18:13] {3072} INFO -  at 21.2s,	estimator xgboost's best error=7.4306,	best estimator xgboost's best error=7.4306
[flaml.automl: 09-17 04:18:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:18:16] {3072} INFO -  at 24.2s,	estimator xgboost's best error=6.5528,	best estimator xgboost's best error=6.5528
[flaml.automl: 09-17 04:18:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:18:17] {3072} INFO -  at 25.3s,	estimator xgboost's best error=6.5528,	best estimator xgboost's best error=6.5528
[flaml.automl: 09-17 04:18:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:18:29] {3072} INFO -  at 37.3s,	estimator xgboost's best error=5.7302,	best estimator xgboost's best error=5.7302
[flaml.automl: 09-17 04:18:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:18:51] {3072} INFO -  at 59.3s,	estimator xgboost's best error=5.5659,	best estimator xgboost's best error=5.5659
[flaml.automl: 09-17 04:19:16] {3335} INFO - retrain xgboost for 25.1s
[flaml.automl: 09-17 04:19:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 04:19:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:19:16] {2637} INFO - Time taken to find the best model: 59.29629063606262
[flaml.automl: 09-17 04:19:16] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}
PM2.5(0)最佳损失：-4.565907827249688
PM2.5(0)最好结果：{'pred_time': 1.8861812093983526e-05, 'wall_clock_time': 59.29629063606262, 'metric_for_logging': {'pred_time': 1.8861812093983526e-05}, 'val_loss': 5.565907827249688, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.006967902231194494, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7520708370056695, 'config/colsample_bylevel': 0.948506134666318, 'config/colsample_bytree': 0.8223167301060249, 'config/reg_alpha': 0.0038027739156058313, 'config/reg_lambda': 0.5403518701157697, 'experiment_tag': 'exp', 'time_total_s': 21.964834690093994}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9353156127157776
PM2.5(0)的mse=55.154230396974086
PM2.5(0)的mae=5.317164896985556
PM2.5(0)的mar=0.20231749876374366
总共花费的时间为：84.81
滨州市
1629A
1630A
3514A
3515A
3516A
[flaml.automl: 09-17 04:34:35] {2390} INFO - task = regression
[flaml.automl: 09-17 04:34:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:34:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:34:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:34:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:34:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:34:37] {3025} INFO - Estimated sufficient time budget=63098s. Estimated necessary time budget=63s.
[flaml.automl: 09-17 04:34:37] {3072} INFO -  at 1.4s,	estimator xgboost's best error=24.1297,	best estimator xgboost's best error=24.1297
[flaml.automl: 09-17 04:34:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:34:39] {3072} INFO -  at 3.5s,	estimator xgboost's best error=12.1071,	best estimator xgboost's best error=12.1071
[flaml.automl: 09-17 04:34:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:34:40] {3072} INFO -  at 4.7s,	estimator xgboost's best error=12.1071,	best estimator xgboost's best error=12.1071
[flaml.automl: 09-17 04:34:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:34:45] {3072} INFO -  at 9.6s,	estimator xgboost's best error=12.1071,	best estimator xgboost's best error=12.1071
[flaml.automl: 09-17 04:34:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:34:46] {3072} INFO -  at 10.7s,	estimator xgboost's best error=8.8452,	best estimator xgboost's best error=8.8452
[flaml.automl: 09-17 04:34:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:34:47] {3072} INFO -  at 12.3s,	estimator xgboost's best error=8.8452,	best estimator xgboost's best error=8.8452
[flaml.automl: 09-17 04:34:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:34:49] {3072} INFO -  at 14.0s,	estimator xgboost's best error=7.1903,	best estimator xgboost's best error=7.1903
[flaml.automl: 09-17 04:34:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:34:52] {3072} INFO -  at 16.7s,	estimator xgboost's best error=7.1903,	best estimator xgboost's best error=7.1903
[flaml.automl: 09-17 04:34:52] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:34:53] {3072} INFO -  at 18.3s,	estimator xgboost's best error=7.1903,	best estimator xgboost's best error=7.1903
[flaml.automl: 09-17 04:34:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:34:56] {3072} INFO -  at 21.3s,	estimator xgboost's best error=7.1903,	best estimator xgboost's best error=7.1903
[flaml.automl: 09-17 04:34:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:34:58] {3072} INFO -  at 22.8s,	estimator xgboost's best error=7.1903,	best estimator xgboost's best error=7.1903
[flaml.automl: 09-17 04:34:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:35:00] {3072} INFO -  at 24.5s,	estimator xgboost's best error=7.1567,	best estimator xgboost's best error=7.1567
[flaml.automl: 09-17 04:35:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:35:01] {3072} INFO -  at 25.7s,	estimator xgboost's best error=7.1567,	best estimator xgboost's best error=7.1567
[flaml.automl: 09-17 04:35:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:35:08] {3072} INFO -  at 32.7s,	estimator xgboost's best error=6.3079,	best estimator xgboost's best error=6.3079
[flaml.automl: 09-17 04:35:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 04:35:21] {3072} INFO -  at 45.5s,	estimator xgboost's best error=6.1861,	best estimator xgboost's best error=6.1861
[flaml.automl: 09-17 04:35:21] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 04:35:28] {3072} INFO -  at 52.5s,	estimator xgboost's best error=6.1861,	best estimator xgboost's best error=6.1861
[flaml.automl: 09-17 04:35:40] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 04:35:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:35:40] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:35:40] {2637} INFO - Time taken to find the best model: 45.460522174835205
[flaml.automl: 09-17 04:35:40] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52781}
PM2.5(0)最佳损失：-5.186121002111491
PM2.5(0)最好结果：{'pred_time': 6.974993459403972e-06, 'wall_clock_time': 45.460522174835205, 'metric_for_logging': {'pred_time': 6.974993459403972e-06}, 'val_loss': 6.186121002111491, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52781}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 52781, 'experiment_tag': 'exp', 'time_total_s': 12.75489330291748}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.886338205829435
PM2.5(0)的mse=77.27050491988564
PM2.5(0)的mae=6.079297707090719
PM2.5(0)的mar=0.2849080351663003
总共花费的时间为：66.11
淄博市
1631A
3363A
3644A
3645A
[flaml.automl: 09-17 04:48:40] {2390} INFO - task = regression
[flaml.automl: 09-17 04:48:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:48:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:48:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:48:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:48:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:48:42] {3025} INFO - Estimated sufficient time budget=51130s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 04:48:42] {3072} INFO -  at 1.4s,	estimator xgboost's best error=27.0898,	best estimator xgboost's best error=27.0898
[flaml.automl: 09-17 04:48:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:48:44] {3072} INFO -  at 3.5s,	estimator xgboost's best error=13.4177,	best estimator xgboost's best error=13.4177
[flaml.automl: 09-17 04:48:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:48:45] {3072} INFO -  at 4.7s,	estimator xgboost's best error=13.4177,	best estimator xgboost's best error=13.4177
[flaml.automl: 09-17 04:48:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:48:51] {3072} INFO -  at 11.0s,	estimator xgboost's best error=13.4177,	best estimator xgboost's best error=13.4177
[flaml.automl: 09-17 04:48:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:48:52] {3072} INFO -  at 12.1s,	estimator xgboost's best error=10.3729,	best estimator xgboost's best error=10.3729
[flaml.automl: 09-17 04:48:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:48:54] {3072} INFO -  at 13.7s,	estimator xgboost's best error=10.3729,	best estimator xgboost's best error=10.3729
[flaml.automl: 09-17 04:48:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:48:56] {3072} INFO -  at 15.4s,	estimator xgboost's best error=7.4411,	best estimator xgboost's best error=7.4411
[flaml.automl: 09-17 04:48:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:48:58] {3072} INFO -  at 18.0s,	estimator xgboost's best error=7.4411,	best estimator xgboost's best error=7.4411
[flaml.automl: 09-17 04:48:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:49:00] {3072} INFO -  at 19.6s,	estimator xgboost's best error=7.4411,	best estimator xgboost's best error=7.4411
[flaml.automl: 09-17 04:49:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:49:03] {3072} INFO -  at 22.6s,	estimator xgboost's best error=7.4411,	best estimator xgboost's best error=7.4411
[flaml.automl: 09-17 04:49:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:49:04] {3072} INFO -  at 24.1s,	estimator xgboost's best error=7.3708,	best estimator xgboost's best error=7.3708
[flaml.automl: 09-17 04:49:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:49:06] {3072} INFO -  at 25.2s,	estimator xgboost's best error=7.3708,	best estimator xgboost's best error=7.3708
[flaml.automl: 09-17 04:49:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:49:09] {3072} INFO -  at 28.9s,	estimator xgboost's best error=6.8840,	best estimator xgboost's best error=6.8840
[flaml.automl: 09-17 04:49:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:49:12] {3072} INFO -  at 32.2s,	estimator xgboost's best error=6.8840,	best estimator xgboost's best error=6.8840
[flaml.automl: 09-17 04:49:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 04:49:15] {3072} INFO -  at 34.6s,	estimator xgboost's best error=6.8840,	best estimator xgboost's best error=6.8840
[flaml.automl: 09-17 04:49:15] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 04:49:17] {3072} INFO -  at 36.2s,	estimator xgboost's best error=6.8840,	best estimator xgboost's best error=6.8840
[flaml.automl: 09-17 04:49:17] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 04:49:19] {3072} INFO -  at 38.5s,	estimator xgboost's best error=6.8840,	best estimator xgboost's best error=6.8840
[flaml.automl: 09-17 04:49:19] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 04:49:29] {3072} INFO -  at 48.8s,	estimator xgboost's best error=6.4278,	best estimator xgboost's best error=6.4278
[flaml.automl: 09-17 04:49:29] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 04:49:33] {3072} INFO -  at 52.8s,	estimator xgboost's best error=6.4278,	best estimator xgboost's best error=6.4278
[flaml.automl: 09-17 04:49:43] {3335} INFO - retrain xgboost for 9.8s
[flaml.automl: 09-17 04:49:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:49:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:49:43] {2637} INFO - Time taken to find the best model: 48.75656270980835
[flaml.automl: 09-17 04:49:43] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 42734}
PM2.5(0)最佳损失：-5.427778330118336
PM2.5(0)最好结果：{'pred_time': 1.8843150585670173e-05, 'wall_clock_time': 48.75656270980835, 'metric_for_logging': {'pred_time': 1.8843150585670173e-05}, 'val_loss': 6.427778330118336, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 42734}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'config/FLAML_sample_size': 42734, 'experiment_tag': 'exp', 'time_total_s': 10.282123804092407}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8883296778312885
PM2.5(0)的mse=89.53472949788808
PM2.5(0)的mae=6.5923839977422025
PM2.5(0)的mar=0.2474138487071651
总共花费的时间为：63.32
枣庄市
1637A
1638A
1639A
1640A
3364A
[flaml.automl: 09-17 05:05:14] {2390} INFO - task = regression
[flaml.automl: 09-17 05:05:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:05:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:05:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:05:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:05:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:05:15] {3025} INFO - Estimated sufficient time budget=64407s. Estimated necessary time budget=64s.
[flaml.automl: 09-17 05:05:15] {3072} INFO -  at 1.4s,	estimator xgboost's best error=26.5183,	best estimator xgboost's best error=26.5183
[flaml.automl: 09-17 05:05:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:05:17] {3072} INFO -  at 3.5s,	estimator xgboost's best error=12.8037,	best estimator xgboost's best error=12.8037
[flaml.automl: 09-17 05:05:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:05:18] {3072} INFO -  at 4.7s,	estimator xgboost's best error=12.8037,	best estimator xgboost's best error=12.8037
[flaml.automl: 09-17 05:05:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:05:23] {3072} INFO -  at 9.5s,	estimator xgboost's best error=12.8037,	best estimator xgboost's best error=12.8037
[flaml.automl: 09-17 05:05:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:05:24] {3072} INFO -  at 10.7s,	estimator xgboost's best error=9.3619,	best estimator xgboost's best error=9.3619
[flaml.automl: 09-17 05:05:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:05:26] {3072} INFO -  at 12.2s,	estimator xgboost's best error=8.3016,	best estimator xgboost's best error=8.3016
[flaml.automl: 09-17 05:05:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:05:27] {3072} INFO -  at 13.9s,	estimator xgboost's best error=7.5668,	best estimator xgboost's best error=7.5668
[flaml.automl: 09-17 05:05:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:05:30] {3072} INFO -  at 16.5s,	estimator xgboost's best error=7.5668,	best estimator xgboost's best error=7.5668
[flaml.automl: 09-17 05:05:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:05:32] {3072} INFO -  at 18.2s,	estimator xgboost's best error=7.5668,	best estimator xgboost's best error=7.5668
[flaml.automl: 09-17 05:05:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:05:35] {3072} INFO -  at 21.2s,	estimator xgboost's best error=6.7418,	best estimator xgboost's best error=6.7418
[flaml.automl: 09-17 05:05:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:05:36] {3072} INFO -  at 22.8s,	estimator xgboost's best error=6.7418,	best estimator xgboost's best error=6.7418
[flaml.automl: 09-17 05:05:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:05:37] {3072} INFO -  at 23.9s,	estimator xgboost's best error=6.7418,	best estimator xgboost's best error=6.7418
[flaml.automl: 09-17 05:05:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:05:40] {3072} INFO -  at 26.8s,	estimator xgboost's best error=6.5587,	best estimator xgboost's best error=6.5587
[flaml.automl: 09-17 05:05:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:05:43] {3072} INFO -  at 29.7s,	estimator xgboost's best error=6.5587,	best estimator xgboost's best error=6.5587
[flaml.automl: 09-17 05:05:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 05:05:46] {3072} INFO -  at 32.2s,	estimator xgboost's best error=6.4295,	best estimator xgboost's best error=6.4295
[flaml.automl: 09-17 05:05:46] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 05:05:48] {3072} INFO -  at 34.5s,	estimator xgboost's best error=6.4295,	best estimator xgboost's best error=6.4295
[flaml.automl: 09-17 05:05:48] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 05:05:50] {3072} INFO -  at 36.7s,	estimator xgboost's best error=6.3382,	best estimator xgboost's best error=6.3382
[flaml.automl: 09-17 05:05:50] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 05:05:52] {3072} INFO -  at 38.4s,	estimator xgboost's best error=6.3382,	best estimator xgboost's best error=6.3382
[flaml.automl: 09-17 05:05:52] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 05:05:54] {3072} INFO -  at 40.1s,	estimator xgboost's best error=6.3382,	best estimator xgboost's best error=6.3382
[flaml.automl: 09-17 05:05:54] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 05:05:55] {3072} INFO -  at 41.6s,	estimator xgboost's best error=6.3382,	best estimator xgboost's best error=6.3382
[flaml.automl: 09-17 05:05:55] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 05:05:57] {3072} INFO -  at 43.2s,	estimator xgboost's best error=6.2541,	best estimator xgboost's best error=6.2541
[flaml.automl: 09-17 05:05:57] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 05:05:59] {3072} INFO -  at 45.8s,	estimator xgboost's best error=6.2541,	best estimator xgboost's best error=6.2541
[flaml.automl: 09-17 05:05:59] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-17 05:06:00] {3072} INFO -  at 46.7s,	estimator xgboost's best error=6.2541,	best estimator xgboost's best error=6.2541
[flaml.automl: 09-17 05:06:00] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-17 05:06:04] {3072} INFO -  at 50.0s,	estimator xgboost's best error=6.2334,	best estimator xgboost's best error=6.2334
[flaml.automl: 09-17 05:06:04] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-17 05:06:05] {3072} INFO -  at 51.5s,	estimator xgboost's best error=6.2334,	best estimator xgboost's best error=6.2334
[flaml.automl: 09-17 05:06:05] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-17 05:06:06] {3072} INFO -  at 52.7s,	estimator xgboost's best error=6.2334,	best estimator xgboost's best error=6.2334
[flaml.automl: 09-17 05:06:06] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-17 05:06:17] {3072} INFO -  at 63.4s,	estimator xgboost's best error=6.2334,	best estimator xgboost's best error=6.2334
[flaml.automl: 09-17 05:07:12] {3335} INFO - retrain xgboost for 55.2s
[flaml.automl: 09-17 05:07:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 05:07:12] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:07:12] {2637} INFO - Time taken to find the best model: 50.02265238761902
[flaml.automl: 09-17 05:07:12] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}
PM2.5(0)最佳损失：-5.233382881587071
PM2.5(0)最好结果：{'pred_time': 7.486248506556457e-06, 'wall_clock_time': 50.02265238761902, 'metric_for_logging': {'pred_time': 7.486248506556457e-06}, 'val_loss': 6.233382881587071, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}, 'config/n_estimators': 15, 'config/max_leaves': 57, 'config/min_child_weight': 0.0011006053677836264, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532848210950454, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.0763912614998318, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.3547112941741943}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8970240087874224
PM2.5(0)的mse=80.86591026050141
PM2.5(0)的mae=6.132980608509612
PM2.5(0)的mar=0.21808272677004395
总共花费的时间为：119.44
烟台市
1642A
1643A
1644A
1646A
3366A
[flaml.automl: 09-17 05:22:49] {2390} INFO - task = regression
[flaml.automl: 09-17 05:22:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:22:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:22:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:22:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:22:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:22:50] {3025} INFO - Estimated sufficient time budget=63079s. Estimated necessary time budget=63s.
[flaml.automl: 09-17 05:22:50] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.5582,	best estimator xgboost's best error=15.5582
[flaml.automl: 09-17 05:22:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:22:52] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.2562,	best estimator xgboost's best error=8.2562
[flaml.automl: 09-17 05:22:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:22:54] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.2562,	best estimator xgboost's best error=8.2562
[flaml.automl: 09-17 05:22:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:22:58] {3072} INFO -  at 9.5s,	estimator xgboost's best error=8.2562,	best estimator xgboost's best error=8.2562
[flaml.automl: 09-17 05:22:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:22:59] {3072} INFO -  at 10.6s,	estimator xgboost's best error=6.5440,	best estimator xgboost's best error=6.5440
[flaml.automl: 09-17 05:22:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:23:01] {3072} INFO -  at 12.2s,	estimator xgboost's best error=6.5440,	best estimator xgboost's best error=6.5440
[flaml.automl: 09-17 05:23:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:23:03] {3072} INFO -  at 13.9s,	estimator xgboost's best error=5.2146,	best estimator xgboost's best error=5.2146
[flaml.automl: 09-17 05:23:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:23:05] {3072} INFO -  at 16.6s,	estimator xgboost's best error=5.2146,	best estimator xgboost's best error=5.2146
[flaml.automl: 09-17 05:23:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:23:07] {3072} INFO -  at 18.2s,	estimator xgboost's best error=5.2146,	best estimator xgboost's best error=5.2146
[flaml.automl: 09-17 05:23:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:23:10] {3072} INFO -  at 21.2s,	estimator xgboost's best error=5.2146,	best estimator xgboost's best error=5.2146
[flaml.automl: 09-17 05:23:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:23:12] {3072} INFO -  at 22.7s,	estimator xgboost's best error=5.2146,	best estimator xgboost's best error=5.2146
[flaml.automl: 09-17 05:23:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:23:13] {3072} INFO -  at 24.4s,	estimator xgboost's best error=5.1657,	best estimator xgboost's best error=5.1657
[flaml.automl: 09-17 05:23:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:23:14] {3072} INFO -  at 25.6s,	estimator xgboost's best error=5.1657,	best estimator xgboost's best error=5.1657
[flaml.automl: 09-17 05:23:14] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:23:24] {3072} INFO -  at 35.3s,	estimator xgboost's best error=4.7892,	best estimator xgboost's best error=4.7892
[flaml.automl: 09-17 05:23:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 05:23:48] {3072} INFO -  at 59.1s,	estimator xgboost's best error=4.7017,	best estimator xgboost's best error=4.7017
[flaml.automl: 09-17 05:24:11] {3335} INFO - retrain xgboost for 23.5s
[flaml.automl: 09-17 05:24:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:24:11] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:24:11] {2637} INFO - Time taken to find the best model: 59.066002368927
[flaml.automl: 09-17 05:24:11] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52907}
PM2.5(0)最佳损失：-3.7017013223261674
PM2.5(0)最好结果：{'pred_time': 1.430965682482472e-05, 'wall_clock_time': 59.066002368927, 'metric_for_logging': {'pred_time': 1.430965682482472e-05}, 'val_loss': 4.701701322326167, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52907}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 52907, 'experiment_tag': 'exp', 'time_total_s': 23.789310932159424}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8853512579273525
PM2.5(0)的mse=46.86834649202334
PM2.5(0)的mae=4.684830124930816
PM2.5(0)的mar=0.3416508516098445
总共花费的时间为：83.59
潍坊市
3178A
3368A
3416A
3861A
[flaml.automl: 09-17 05:37:19] {2390} INFO - task = regression
[flaml.automl: 09-17 05:37:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:37:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:37:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:37:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:37:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:37:20] {3025} INFO - Estimated sufficient time budget=13094s. Estimated necessary time budget=13s.
[flaml.automl: 09-17 05:37:20] {3072} INFO -  at 1.5s,	estimator xgboost's best error=20.9568,	best estimator xgboost's best error=20.9568
[flaml.automl: 09-17 05:37:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:37:22] {3072} INFO -  at 3.6s,	estimator xgboost's best error=10.3714,	best estimator xgboost's best error=10.3714
[flaml.automl: 09-17 05:37:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:37:23] {3072} INFO -  at 4.8s,	estimator xgboost's best error=10.3714,	best estimator xgboost's best error=10.3714
[flaml.automl: 09-17 05:37:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:37:33] {3072} INFO -  at 14.8s,	estimator xgboost's best error=10.3714,	best estimator xgboost's best error=10.3714
[flaml.automl: 09-17 05:37:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:37:34] {3072} INFO -  at 15.9s,	estimator xgboost's best error=7.4408,	best estimator xgboost's best error=7.4408
[flaml.automl: 09-17 05:37:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:37:37] {3072} INFO -  at 18.1s,	estimator xgboost's best error=7.4408,	best estimator xgboost's best error=7.4408
[flaml.automl: 09-17 05:37:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:37:40] {3072} INFO -  at 21.3s,	estimator xgboost's best error=5.8919,	best estimator xgboost's best error=5.8919
[flaml.automl: 09-17 05:37:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:37:45] {3072} INFO -  at 26.3s,	estimator xgboost's best error=5.8919,	best estimator xgboost's best error=5.8919
[flaml.automl: 09-17 05:37:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:37:48] {3072} INFO -  at 29.3s,	estimator xgboost's best error=5.8919,	best estimator xgboost's best error=5.8919
[flaml.automl: 09-17 05:37:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:37:53] {3072} INFO -  at 35.0s,	estimator xgboost's best error=5.8919,	best estimator xgboost's best error=5.8919
[flaml.automl: 09-17 05:37:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:37:56] {3072} INFO -  at 37.7s,	estimator xgboost's best error=5.8858,	best estimator xgboost's best error=5.8858
[flaml.automl: 09-17 05:37:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:37:58] {3072} INFO -  at 39.8s,	estimator xgboost's best error=5.8858,	best estimator xgboost's best error=5.8858
[flaml.automl: 09-17 05:37:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:38:13] {3072} INFO -  at 54.6s,	estimator xgboost's best error=5.2691,	best estimator xgboost's best error=5.2691
[flaml.automl: 09-17 05:38:29] {3335} INFO - retrain xgboost for 15.7s
[flaml.automl: 09-17 05:38:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:38:29] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:38:29] {2637} INFO - Time taken to find the best model: 54.59355092048645
[flaml.automl: 09-17 05:38:29] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
PM2.5(0)最佳损失：-4.269050770422488
PM2.5(0)最好结果：{'pred_time': 2.719527711456462e-05, 'wall_clock_time': 54.59355092048645, 'metric_for_logging': {'pred_time': 2.719527711456462e-05}, 'val_loss': 5.269050770422488, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 14.76328992843628}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9041307832639758
PM2.5(0)的mse=57.05675787905709
PM2.5(0)的mae=5.199834752343107
PM2.5(0)的mar=0.2714380038077313
总共花费的时间为：71.08
济宁市
1653A
3501A
3678A
[flaml.automl: 09-17 05:47:33] {2390} INFO - task = regression
[flaml.automl: 09-17 05:47:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:47:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:47:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:47:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:47:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:47:34] {3025} INFO - Estimated sufficient time budget=12035s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 05:47:34] {3072} INFO -  at 1.3s,	estimator xgboost's best error=27.2510,	best estimator xgboost's best error=27.2510
[flaml.automl: 09-17 05:47:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:47:36] {3072} INFO -  at 3.5s,	estimator xgboost's best error=13.0503,	best estimator xgboost's best error=13.0503
[flaml.automl: 09-17 05:47:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:47:37] {3072} INFO -  at 4.7s,	estimator xgboost's best error=13.0503,	best estimator xgboost's best error=13.0503
[flaml.automl: 09-17 05:47:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:47:47] {3072} INFO -  at 14.6s,	estimator xgboost's best error=13.0503,	best estimator xgboost's best error=13.0503
[flaml.automl: 09-17 05:47:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:47:48] {3072} INFO -  at 15.8s,	estimator xgboost's best error=9.5358,	best estimator xgboost's best error=9.5358
[flaml.automl: 09-17 05:47:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:47:50] {3072} INFO -  at 17.4s,	estimator xgboost's best error=9.5358,	best estimator xgboost's best error=9.5358
[flaml.automl: 09-17 05:47:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:47:52] {3072} INFO -  at 19.1s,	estimator xgboost's best error=6.0945,	best estimator xgboost's best error=6.0945
[flaml.automl: 09-17 05:47:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:47:54] {3072} INFO -  at 21.8s,	estimator xgboost's best error=6.0945,	best estimator xgboost's best error=6.0945
[flaml.automl: 09-17 05:47:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:47:56] {3072} INFO -  at 23.5s,	estimator xgboost's best error=6.0945,	best estimator xgboost's best error=6.0945
[flaml.automl: 09-17 05:47:56] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:47:59] {3072} INFO -  at 26.5s,	estimator xgboost's best error=6.0945,	best estimator xgboost's best error=6.0945
[flaml.automl: 09-17 05:47:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:48:00] {3072} INFO -  at 28.0s,	estimator xgboost's best error=6.0945,	best estimator xgboost's best error=6.0945
[flaml.automl: 09-17 05:48:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:48:02] {3072} INFO -  at 29.1s,	estimator xgboost's best error=6.0945,	best estimator xgboost's best error=6.0945
[flaml.automl: 09-17 05:48:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:48:14] {3072} INFO -  at 41.2s,	estimator xgboost's best error=5.0507,	best estimator xgboost's best error=5.0507
[flaml.automl: 09-17 05:48:14] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:48:32] {3072} INFO -  at 59.4s,	estimator xgboost's best error=4.9321,	best estimator xgboost's best error=4.9321
[flaml.automl: 09-17 05:49:05] {3335} INFO - retrain xgboost for 33.7s
[flaml.automl: 09-17 05:49:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:49:05] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:49:05] {2637} INFO - Time taken to find the best model: 59.368271350860596
[flaml.automl: 09-17 05:49:05] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-3.9321496774479625
PM2.5(0)最好结果：{'pred_time': 2.392539142692176e-05, 'wall_clock_time': 59.368271350860596, 'metric_for_logging': {'pred_time': 2.392539142692176e-05}, 'val_loss': 4.9321496774479625, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 18.203933000564575}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.951928991290885
PM2.5(0)的mse=42.07692446316537
PM2.5(0)的mae=4.64057232027919
PM2.5(0)的mar=0.17689365126540982
总共花费的时间为：93.66
泰安市
3502A
3503A
3504A
[flaml.automl: 09-17 05:59:01] {2390} INFO - task = regression
[flaml.automl: 09-17 05:59:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:59:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:59:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:59:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:59:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:59:04] {3025} INFO - Estimated sufficient time budget=23075s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 05:59:04] {3072} INFO -  at 2.5s,	estimator xgboost's best error=24.1153,	best estimator xgboost's best error=24.1153
[flaml.automl: 09-17 05:59:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:59:08] {3072} INFO -  at 6.4s,	estimator xgboost's best error=11.3970,	best estimator xgboost's best error=11.3970
[flaml.automl: 09-17 05:59:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:59:10] {3072} INFO -  at 8.7s,	estimator xgboost's best error=11.3970,	best estimator xgboost's best error=11.3970
[flaml.automl: 09-17 05:59:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:59:38] {3072} INFO -  at 37.3s,	estimator xgboost's best error=11.3970,	best estimator xgboost's best error=11.3970
[flaml.automl: 09-17 05:59:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:59:43] {3072} INFO -  at 42.0s,	estimator xgboost's best error=8.1816,	best estimator xgboost's best error=8.1816
[flaml.automl: 09-17 05:59:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:59:50] {3072} INFO -  at 48.4s,	estimator xgboost's best error=8.1816,	best estimator xgboost's best error=8.1816
[flaml.automl: 09-17 05:59:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:59:56] {3072} INFO -  at 55.4s,	estimator xgboost's best error=4.9407,	best estimator xgboost's best error=4.9407
[flaml.automl: 09-17 06:00:03] {3335} INFO - retrain xgboost for 6.5s
[flaml.automl: 09-17 06:00:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 06:00:03] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:00:03] {2637} INFO - Time taken to find the best model: 55.3502836227417
[flaml.automl: 09-17 06:00:03] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-3.940659098652059
PM2.5(0)最好结果：{'pred_time': 4.3435748837815304e-05, 'wall_clock_time': 55.3502836227417, 'metric_for_logging': {'pred_time': 4.3435748837815304e-05}, 'val_loss': 4.940659098652059, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 6.933697938919067}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9028561075922212
PM2.5(0)的mse=55.075376256295435
PM2.5(0)的mae=5.0657473980139365
PM2.5(0)的mar=0.21961534406972472
总共花费的时间为：62.54
日照市
1659A
1661A
3507A
3604A
[flaml.automl: 09-17 06:11:44] {2390} INFO - task = regression
[flaml.automl: 09-17 06:11:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:11:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:11:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:11:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:11:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:11:46] {3025} INFO - Estimated sufficient time budget=78395s. Estimated necessary time budget=78s.
[flaml.automl: 09-17 06:11:46] {3072} INFO -  at 2.1s,	estimator xgboost's best error=18.8754,	best estimator xgboost's best error=18.8754
[flaml.automl: 09-17 06:11:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:11:50] {3072} INFO -  at 5.6s,	estimator xgboost's best error=9.3637,	best estimator xgboost's best error=9.3637
[flaml.automl: 09-17 06:11:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:11:52] {3072} INFO -  at 7.7s,	estimator xgboost's best error=9.3637,	best estimator xgboost's best error=9.3637
[flaml.automl: 09-17 06:11:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:11:57] {3072} INFO -  at 12.9s,	estimator xgboost's best error=9.3637,	best estimator xgboost's best error=9.3637
[flaml.automl: 09-17 06:11:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:11:59] {3072} INFO -  at 14.9s,	estimator xgboost's best error=6.8213,	best estimator xgboost's best error=6.8213
[flaml.automl: 09-17 06:11:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:12:01] {3072} INFO -  at 17.2s,	estimator xgboost's best error=6.6690,	best estimator xgboost's best error=6.6690
[flaml.automl: 09-17 06:12:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:12:04] {3072} INFO -  at 20.2s,	estimator xgboost's best error=5.8012,	best estimator xgboost's best error=5.8012
[flaml.automl: 09-17 06:12:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:12:09] {3072} INFO -  at 24.3s,	estimator xgboost's best error=5.8012,	best estimator xgboost's best error=5.8012
[flaml.automl: 09-17 06:12:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:12:10] {3072} INFO -  at 26.2s,	estimator xgboost's best error=5.8012,	best estimator xgboost's best error=5.8012
[flaml.automl: 09-17 06:12:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:12:13] {3072} INFO -  at 29.2s,	estimator xgboost's best error=5.3886,	best estimator xgboost's best error=5.3886
[flaml.automl: 09-17 06:12:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:12:15] {3072} INFO -  at 30.8s,	estimator xgboost's best error=5.3886,	best estimator xgboost's best error=5.3886
[flaml.automl: 09-17 06:12:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:12:16] {3072} INFO -  at 31.9s,	estimator xgboost's best error=5.3886,	best estimator xgboost's best error=5.3886
[flaml.automl: 09-17 06:12:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:12:19] {3072} INFO -  at 34.8s,	estimator xgboost's best error=5.2147,	best estimator xgboost's best error=5.2147
[flaml.automl: 09-17 06:12:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:12:21] {3072} INFO -  at 37.0s,	estimator xgboost's best error=5.2147,	best estimator xgboost's best error=5.2147
[flaml.automl: 09-17 06:12:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 06:12:24] {3072} INFO -  at 39.5s,	estimator xgboost's best error=5.2147,	best estimator xgboost's best error=5.2147
[flaml.automl: 09-17 06:12:24] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 06:12:26] {3072} INFO -  at 41.3s,	estimator xgboost's best error=5.2147,	best estimator xgboost's best error=5.2147
[flaml.automl: 09-17 06:12:26] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 06:12:27] {3072} INFO -  at 43.0s,	estimator xgboost's best error=5.2147,	best estimator xgboost's best error=5.2147
[flaml.automl: 09-17 06:12:27] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 06:12:41] {3072} INFO -  at 56.6s,	estimator xgboost's best error=5.0893,	best estimator xgboost's best error=5.0893
[flaml.automl: 09-17 06:12:55] {3335} INFO - retrain xgboost for 13.7s
[flaml.automl: 09-17 06:12:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 06:12:55] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:12:55] {2637} INFO - Time taken to find the best model: 56.63728070259094
[flaml.automl: 09-17 06:12:55] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007, 'FLAML_sample_size': 43344}
PM2.5(0)最佳损失：-4.0893319086775035
PM2.5(0)最好结果：{'pred_time': 8.438314710344588e-06, 'wall_clock_time': 56.63728070259094, 'metric_for_logging': {'pred_time': 8.438314710344588e-06}, 'val_loss': 5.0893319086775035, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007, 'FLAML_sample_size': 43344}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'config/FLAML_sample_size': 43344, 'experiment_tag': 'exp', 'time_total_s': 13.647968530654907}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8865766812270129
PM2.5(0)的mse=56.15927862302543
PM2.5(0)的mae=5.202970513647982
PM2.5(0)的mar=0.3010265166001053
总共花费的时间为：71.04
威海市
1662A
1664A
1982A
3505A
[flaml.automl: 09-17 06:25:38] {2390} INFO - task = regression
[flaml.automl: 09-17 06:25:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:25:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:25:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:25:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:25:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:25:39] {3025} INFO - Estimated sufficient time budget=50495s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 06:25:39] {3072} INFO -  at 1.4s,	estimator xgboost's best error=14.2276,	best estimator xgboost's best error=14.2276
[flaml.automl: 09-17 06:25:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:25:41] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.2090,	best estimator xgboost's best error=7.2090
[flaml.automl: 09-17 06:25:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:25:42] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.2090,	best estimator xgboost's best error=7.2090
[flaml.automl: 09-17 06:25:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:25:49] {3072} INFO -  at 11.0s,	estimator xgboost's best error=7.2090,	best estimator xgboost's best error=7.2090
[flaml.automl: 09-17 06:25:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:25:50] {3072} INFO -  at 12.2s,	estimator xgboost's best error=5.4169,	best estimator xgboost's best error=5.4169
[flaml.automl: 09-17 06:25:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:25:51] {3072} INFO -  at 13.7s,	estimator xgboost's best error=5.1805,	best estimator xgboost's best error=5.1805
[flaml.automl: 09-17 06:25:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:25:53] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.6653,	best estimator xgboost's best error=4.6653
[flaml.automl: 09-17 06:25:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:25:56] {3072} INFO -  at 18.0s,	estimator xgboost's best error=4.6653,	best estimator xgboost's best error=4.6653
[flaml.automl: 09-17 06:25:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:25:57] {3072} INFO -  at 19.7s,	estimator xgboost's best error=4.6653,	best estimator xgboost's best error=4.6653
[flaml.automl: 09-17 06:25:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:26:00] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.6653,	best estimator xgboost's best error=4.6653
[flaml.automl: 09-17 06:26:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:26:02] {3072} INFO -  at 24.1s,	estimator xgboost's best error=4.5367,	best estimator xgboost's best error=4.5367
[flaml.automl: 09-17 06:26:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:26:03] {3072} INFO -  at 25.2s,	estimator xgboost's best error=4.5367,	best estimator xgboost's best error=4.5367
[flaml.automl: 09-17 06:26:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:26:07] {3072} INFO -  at 29.1s,	estimator xgboost's best error=4.2890,	best estimator xgboost's best error=4.2890
[flaml.automl: 09-17 06:26:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:26:10] {3072} INFO -  at 32.5s,	estimator xgboost's best error=4.2059,	best estimator xgboost's best error=4.2059
[flaml.automl: 09-17 06:26:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 06:26:13] {3072} INFO -  at 35.3s,	estimator xgboost's best error=4.2059,	best estimator xgboost's best error=4.2059
[flaml.automl: 09-17 06:26:13] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 06:26:15] {3072} INFO -  at 37.6s,	estimator xgboost's best error=4.2059,	best estimator xgboost's best error=4.2059
[flaml.automl: 09-17 06:26:15] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 06:26:17] {3072} INFO -  at 39.9s,	estimator xgboost's best error=4.2059,	best estimator xgboost's best error=4.2059
[flaml.automl: 09-17 06:26:17] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 06:26:20] {3072} INFO -  at 42.0s,	estimator xgboost's best error=4.2059,	best estimator xgboost's best error=4.2059
[flaml.automl: 09-17 06:26:20] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 06:26:30] {3072} INFO -  at 52.8s,	estimator xgboost's best error=4.1060,	best estimator xgboost's best error=4.1060
[flaml.automl: 09-17 06:26:41] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-17 06:26:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:26:41] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:26:41] {2637} INFO - Time taken to find the best model: 52.84085011482239
[flaml.automl: 09-17 06:26:41] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463, 'FLAML_sample_size': 42129}
PM2.5(0)最佳损失：-3.1060015389206725
PM2.5(0)最好结果：{'pred_time': 9.070449190194778e-06, 'wall_clock_time': 52.84085011482239, 'metric_for_logging': {'pred_time': 9.070449190194778e-06}, 'val_loss': 4.1060015389206725, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463, 'FLAML_sample_size': 42129}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 6.532042959250319, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8835265175628242, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.10065328594609463, 'config/FLAML_sample_size': 42129, 'experiment_tag': 'exp', 'time_total_s': 10.823179960250854}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8497957097643607
PM2.5(0)的mse=33.276818518417166
PM2.5(0)的mae=4.1017613107999535
PM2.5(0)的mar=0.276154194860219
总共花费的时间为：64.24
东营市
3365A
3498A
3734A
[flaml.automl: 09-17 06:36:06] {2390} INFO - task = regression
[flaml.automl: 09-17 06:36:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:36:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:36:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:36:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:36:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:36:07] {3025} INFO - Estimated sufficient time budget=12134s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 06:36:07] {3072} INFO -  at 1.4s,	estimator xgboost's best error=20.2474,	best estimator xgboost's best error=20.2474
[flaml.automl: 09-17 06:36:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:36:09] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.3259,	best estimator xgboost's best error=10.3259
[flaml.automl: 09-17 06:36:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:36:11] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.3259,	best estimator xgboost's best error=10.3259
[flaml.automl: 09-17 06:36:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:36:20] {3072} INFO -  at 14.6s,	estimator xgboost's best error=10.3259,	best estimator xgboost's best error=10.3259
[flaml.automl: 09-17 06:36:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:36:22] {3072} INFO -  at 15.8s,	estimator xgboost's best error=8.4803,	best estimator xgboost's best error=8.4803
[flaml.automl: 09-17 06:36:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:36:23] {3072} INFO -  at 17.3s,	estimator xgboost's best error=7.4769,	best estimator xgboost's best error=7.4769
[flaml.automl: 09-17 06:36:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:36:25] {3072} INFO -  at 18.9s,	estimator xgboost's best error=6.8894,	best estimator xgboost's best error=6.8894
[flaml.automl: 09-17 06:36:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:36:27] {3072} INFO -  at 21.6s,	estimator xgboost's best error=6.8894,	best estimator xgboost's best error=6.8894
[flaml.automl: 09-17 06:36:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:36:29] {3072} INFO -  at 23.2s,	estimator xgboost's best error=6.5936,	best estimator xgboost's best error=6.5936
[flaml.automl: 09-17 06:36:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:36:32] {3072} INFO -  at 26.2s,	estimator xgboost's best error=6.2677,	best estimator xgboost's best error=6.2677
[flaml.automl: 09-17 06:36:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:36:33] {3072} INFO -  at 27.4s,	estimator xgboost's best error=6.2677,	best estimator xgboost's best error=6.2677
[flaml.automl: 09-17 06:36:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:36:59] {3072} INFO -  at 52.8s,	estimator xgboost's best error=5.7336,	best estimator xgboost's best error=5.7336
[flaml.automl: 09-17 06:37:29] {3335} INFO - retrain xgboost for 30.3s
[flaml.automl: 09-17 06:37:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:37:29] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:37:29] {2637} INFO - Time taken to find the best model: 52.78751587867737
[flaml.automl: 09-17 06:37:29] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
PM2.5(0)最佳损失：-4.733634623064088
PM2.5(0)最好结果：{'pred_time': 3.224972117642116e-05, 'wall_clock_time': 52.78751587867737, 'metric_for_logging': {'pred_time': 3.224972117642116e-05}, 'val_loss': 5.733634623064088, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 25.40360116958618}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8835184337911046
PM2.5(0)的mse=66.93207463561278
PM2.5(0)的mae=5.643482844374405
PM2.5(0)的mar=0.3150904003877261
总共花费的时间为：83.70
韶关市
1669A
1673A
3622A
[flaml.automl: 09-17 06:47:17] {2390} INFO - task = regression
[flaml.automl: 09-17 06:47:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:47:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:47:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:47:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:47:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:47:18] {3025} INFO - Estimated sufficient time budget=12128s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 06:47:18] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.4314,	best estimator xgboost's best error=13.4314
[flaml.automl: 09-17 06:47:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:47:20] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.3362,	best estimator xgboost's best error=6.3362
[flaml.automl: 09-17 06:47:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:47:22] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.3362,	best estimator xgboost's best error=6.3362
[flaml.automl: 09-17 06:47:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:47:32] {3072} INFO -  at 14.7s,	estimator xgboost's best error=6.3362,	best estimator xgboost's best error=6.3362
[flaml.automl: 09-17 06:47:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:47:33] {3072} INFO -  at 15.9s,	estimator xgboost's best error=4.4688,	best estimator xgboost's best error=4.4688
[flaml.automl: 09-17 06:47:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:47:34] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.4688,	best estimator xgboost's best error=4.4688
[flaml.automl: 09-17 06:47:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:47:36] {3072} INFO -  at 19.2s,	estimator xgboost's best error=2.9095,	best estimator xgboost's best error=2.9095
[flaml.automl: 09-17 06:47:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:47:39] {3072} INFO -  at 21.9s,	estimator xgboost's best error=2.9095,	best estimator xgboost's best error=2.9095
[flaml.automl: 09-17 06:47:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:47:40] {3072} INFO -  at 23.5s,	estimator xgboost's best error=2.9095,	best estimator xgboost's best error=2.9095
[flaml.automl: 09-17 06:47:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:47:43] {3072} INFO -  at 26.5s,	estimator xgboost's best error=2.9095,	best estimator xgboost's best error=2.9095
[flaml.automl: 09-17 06:47:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:47:45] {3072} INFO -  at 28.0s,	estimator xgboost's best error=2.9095,	best estimator xgboost's best error=2.9095
[flaml.automl: 09-17 06:47:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:47:46] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.9095,	best estimator xgboost's best error=2.9095
[flaml.automl: 09-17 06:47:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:47:53] {3072} INFO -  at 36.2s,	estimator xgboost's best error=2.5165,	best estimator xgboost's best error=2.5165
[flaml.automl: 09-17 06:47:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:48:07] {3072} INFO -  at 50.3s,	estimator xgboost's best error=2.3821,	best estimator xgboost's best error=2.3821
[flaml.automl: 09-17 06:48:29] {3335} INFO - retrain xgboost for 22.2s
[flaml.automl: 09-17 06:48:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:48:29] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:48:29] {2637} INFO - Time taken to find the best model: 50.29311156272888
[flaml.automl: 09-17 06:48:29] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-1.3821103592421884
PM2.5(0)最好结果：{'pred_time': 1.4189948067964317e-05, 'wall_clock_time': 50.29311156272888, 'metric_for_logging': {'pred_time': 1.4189948067964317e-05}, 'val_loss': 2.3821103592421884, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 14.081137418746948}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9273032394840113
PM2.5(0)的mse=13.38788685600701
PM2.5(0)的mae=2.410932994095129
PM2.5(0)的mar=0.15917100817944974
总共花费的时间为：73.08
汕头市
1674A
1675A
3624A
[flaml.automl: 09-17 06:57:47] {2390} INFO - task = regression
[flaml.automl: 09-17 06:57:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:57:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:57:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:57:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:57:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:57:49] {3025} INFO - Estimated sufficient time budget=12120s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 06:57:49] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.8984,	best estimator xgboost's best error=11.8984
[flaml.automl: 09-17 06:57:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:57:51] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.7880,	best estimator xgboost's best error=5.7880
[flaml.automl: 09-17 06:57:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:57:52] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.7880,	best estimator xgboost's best error=5.7880
[flaml.automl: 09-17 06:57:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:58:02] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.7880,	best estimator xgboost's best error=5.7880
[flaml.automl: 09-17 06:58:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:58:03] {3072} INFO -  at 15.8s,	estimator xgboost's best error=4.0836,	best estimator xgboost's best error=4.0836
[flaml.automl: 09-17 06:58:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:58:05] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.9513,	best estimator xgboost's best error=3.9513
[flaml.automl: 09-17 06:58:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:58:06] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.4510,	best estimator xgboost's best error=3.4510
[flaml.automl: 09-17 06:58:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:58:09] {3072} INFO -  at 21.8s,	estimator xgboost's best error=3.4510,	best estimator xgboost's best error=3.4510
[flaml.automl: 09-17 06:58:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:58:11] {3072} INFO -  at 23.4s,	estimator xgboost's best error=3.4510,	best estimator xgboost's best error=3.4510
[flaml.automl: 09-17 06:58:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:58:14] {3072} INFO -  at 26.5s,	estimator xgboost's best error=3.3510,	best estimator xgboost's best error=3.3510
[flaml.automl: 09-17 06:58:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:58:15] {3072} INFO -  at 28.1s,	estimator xgboost's best error=3.3510,	best estimator xgboost's best error=3.3510
[flaml.automl: 09-17 06:58:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:58:17] {3072} INFO -  at 29.2s,	estimator xgboost's best error=3.3510,	best estimator xgboost's best error=3.3510
[flaml.automl: 09-17 06:58:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:58:30] {3072} INFO -  at 42.8s,	estimator xgboost's best error=2.9462,	best estimator xgboost's best error=2.9462
[flaml.automl: 09-17 06:58:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:58:47] {3072} INFO -  at 59.4s,	estimator xgboost's best error=2.8859,	best estimator xgboost's best error=2.8859
[flaml.automl: 09-17 06:59:11] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-17 06:59:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:59:11] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:59:11] {2637} INFO - Time taken to find the best model: 59.35376596450806
[flaml.automl: 09-17 06:59:11] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-1.8858679698729892
PM2.5(0)最好结果：{'pred_time': 1.1319474459955229e-05, 'wall_clock_time': 59.35376596450806, 'metric_for_logging': {'pred_time': 1.1319474459955229e-05}, 'val_loss': 2.885867969872989, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.522964239120483}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8642268973525237
PM2.5(0)的mse=20.545919289444686
PM2.5(0)的mae=2.931988212084006
PM2.5(0)的mar=0.20351633946781833
总共花费的时间为：83.74
湛江市
1680A
1681A
1682A
1684A
1685A
[flaml.automl: 09-17 07:14:35] {2390} INFO - task = regression
[flaml.automl: 09-17 07:14:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:14:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:14:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:14:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:14:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:14:36] {3025} INFO - Estimated sufficient time budget=67620s. Estimated necessary time budget=68s.
[flaml.automl: 09-17 07:14:36] {3072} INFO -  at 1.5s,	estimator xgboost's best error=11.9882,	best estimator xgboost's best error=11.9882
[flaml.automl: 09-17 07:14:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:14:38] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.7165,	best estimator xgboost's best error=5.7165
[flaml.automl: 09-17 07:14:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:14:39] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.7165,	best estimator xgboost's best error=5.7165
[flaml.automl: 09-17 07:14:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:14:44] {3072} INFO -  at 9.6s,	estimator xgboost's best error=5.7165,	best estimator xgboost's best error=5.7165
[flaml.automl: 09-17 07:14:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:14:45] {3072} INFO -  at 10.8s,	estimator xgboost's best error=4.3449,	best estimator xgboost's best error=4.3449
[flaml.automl: 09-17 07:14:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:14:47] {3072} INFO -  at 12.3s,	estimator xgboost's best error=4.3449,	best estimator xgboost's best error=4.3449
[flaml.automl: 09-17 07:14:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:14:48] {3072} INFO -  at 14.0s,	estimator xgboost's best error=3.0676,	best estimator xgboost's best error=3.0676
[flaml.automl: 09-17 07:14:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:14:51] {3072} INFO -  at 16.7s,	estimator xgboost's best error=3.0676,	best estimator xgboost's best error=3.0676
[flaml.automl: 09-17 07:14:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:14:53] {3072} INFO -  at 18.4s,	estimator xgboost's best error=3.0676,	best estimator xgboost's best error=3.0676
[flaml.automl: 09-17 07:14:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:14:56] {3072} INFO -  at 21.4s,	estimator xgboost's best error=3.0676,	best estimator xgboost's best error=3.0676
[flaml.automl: 09-17 07:14:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:14:57] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.0676,	best estimator xgboost's best error=3.0676
[flaml.automl: 09-17 07:14:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:14:59] {3072} INFO -  at 24.5s,	estimator xgboost's best error=3.0527,	best estimator xgboost's best error=3.0527
[flaml.automl: 09-17 07:14:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:15:00] {3072} INFO -  at 25.7s,	estimator xgboost's best error=3.0527,	best estimator xgboost's best error=3.0527
[flaml.automl: 09-17 07:15:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:15:07] {3072} INFO -  at 32.8s,	estimator xgboost's best error=2.7814,	best estimator xgboost's best error=2.7814
[flaml.automl: 09-17 07:15:07] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 07:15:20] {3072} INFO -  at 45.6s,	estimator xgboost's best error=2.6875,	best estimator xgboost's best error=2.6875
[flaml.automl: 09-17 07:15:20] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 07:15:27] {3072} INFO -  at 52.7s,	estimator xgboost's best error=2.6875,	best estimator xgboost's best error=2.6875
[flaml.automl: 09-17 07:15:40] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 07:15:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:15:40] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:15:40] {2637} INFO - Time taken to find the best model: 45.58021807670593
[flaml.automl: 09-17 07:15:40] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 56190}
PM2.5(0)最佳损失：-1.6874752840615481
PM2.5(0)最好结果：{'pred_time': 6.544177613145338e-06, 'wall_clock_time': 45.58021807670593, 'metric_for_logging': {'pred_time': 6.544177613145338e-06}, 'val_loss': 2.687475284061548, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 56190}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 56190, 'experiment_tag': 'exp', 'time_total_s': 12.802934169769287}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.899586686802522
PM2.5(0)的mse=17.64131622686384
PM2.5(0)的mae=2.6839991536632537
PM2.5(0)的mar=0.17641548563300286
总共花费的时间为：66.29
茂名市
1686A
1688A
1689A
3450A
[flaml.automl: 09-17 07:28:09] {2390} INFO - task = regression
[flaml.automl: 09-17 07:28:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:28:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:28:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:28:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:28:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:28:11] {3025} INFO - Estimated sufficient time budget=89557s. Estimated necessary time budget=90s.
[flaml.automl: 09-17 07:28:11] {3072} INFO -  at 2.3s,	estimator xgboost's best error=11.2530,	best estimator xgboost's best error=11.2530
[flaml.automl: 09-17 07:28:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:28:15] {3072} INFO -  at 6.0s,	estimator xgboost's best error=5.9181,	best estimator xgboost's best error=5.9181
[flaml.automl: 09-17 07:28:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:28:17] {3072} INFO -  at 8.2s,	estimator xgboost's best error=5.9181,	best estimator xgboost's best error=5.9181
[flaml.automl: 09-17 07:28:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:28:22] {3072} INFO -  at 13.3s,	estimator xgboost's best error=5.9181,	best estimator xgboost's best error=5.9181
[flaml.automl: 09-17 07:28:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:28:24] {3072} INFO -  at 15.3s,	estimator xgboost's best error=4.6944,	best estimator xgboost's best error=4.6944
[flaml.automl: 09-17 07:28:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:28:27] {3072} INFO -  at 18.1s,	estimator xgboost's best error=4.2972,	best estimator xgboost's best error=4.2972
[flaml.automl: 09-17 07:28:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:28:30] {3072} INFO -  at 20.9s,	estimator xgboost's best error=4.1251,	best estimator xgboost's best error=4.1251
[flaml.automl: 09-17 07:28:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:28:34] {3072} INFO -  at 24.9s,	estimator xgboost's best error=4.1251,	best estimator xgboost's best error=4.1251
[flaml.automl: 09-17 07:28:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:28:37] {3072} INFO -  at 27.8s,	estimator xgboost's best error=4.1251,	best estimator xgboost's best error=4.1251
[flaml.automl: 09-17 07:28:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:28:39] {3072} INFO -  at 30.7s,	estimator xgboost's best error=4.1251,	best estimator xgboost's best error=4.1251
[flaml.automl: 09-17 07:28:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:28:41] {3072} INFO -  at 32.1s,	estimator xgboost's best error=4.1199,	best estimator xgboost's best error=4.1199
[flaml.automl: 09-17 07:28:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:28:42] {3072} INFO -  at 33.3s,	estimator xgboost's best error=4.1199,	best estimator xgboost's best error=4.1199
[flaml.automl: 09-17 07:28:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:28:45] {3072} INFO -  at 36.1s,	estimator xgboost's best error=3.7271,	best estimator xgboost's best error=3.7271
[flaml.automl: 09-17 07:28:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:28:48] {3072} INFO -  at 38.7s,	estimator xgboost's best error=3.6442,	best estimator xgboost's best error=3.6442
[flaml.automl: 09-17 07:28:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 07:28:50] {3072} INFO -  at 41.0s,	estimator xgboost's best error=3.6442,	best estimator xgboost's best error=3.6442
[flaml.automl: 09-17 07:28:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 07:28:51] {3072} INFO -  at 42.5s,	estimator xgboost's best error=3.6442,	best estimator xgboost's best error=3.6442
[flaml.automl: 09-17 07:28:51] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 07:28:53] {3072} INFO -  at 44.4s,	estimator xgboost's best error=3.6442,	best estimator xgboost's best error=3.6442
[flaml.automl: 09-17 07:28:53] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 07:28:55] {3072} INFO -  at 46.0s,	estimator xgboost's best error=3.6442,	best estimator xgboost's best error=3.6442
[flaml.automl: 09-17 07:28:55] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 07:29:06] {3072} INFO -  at 56.8s,	estimator xgboost's best error=3.5472,	best estimator xgboost's best error=3.5472
[flaml.automl: 09-17 07:29:16] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-17 07:29:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:29:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:29:16] {2637} INFO - Time taken to find the best model: 56.7644407749176
[flaml.automl: 09-17 07:29:16] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463, 'FLAML_sample_size': 43497}
PM2.5(0)最佳损失：-2.5472213963449715
PM2.5(0)最好结果：{'pred_time': 8.387510373072532e-06, 'wall_clock_time': 56.7644407749176, 'metric_for_logging': {'pred_time': 8.387510373072532e-06}, 'val_loss': 3.5472213963449715, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463, 'FLAML_sample_size': 43497}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 6.532042959250319, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8835265175628242, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.10065328594609463, 'config/FLAML_sample_size': 43497, 'experiment_tag': 'exp', 'time_total_s': 10.766878366470337}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8440048833601532
PM2.5(0)的mse=25.9125529749041
PM2.5(0)的mae=3.5699631694064107
PM2.5(0)的mar=0.3533233630532882
总共花费的时间为：68.22
梅州市
1690A
1692A
3315A
[flaml.automl: 09-17 07:38:46] {2390} INFO - task = regression
[flaml.automl: 09-17 07:38:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:38:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:38:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:38:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:38:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:38:47] {3025} INFO - Estimated sufficient time budget=12170s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 07:38:47] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.1236,	best estimator xgboost's best error=11.1236
[flaml.automl: 09-17 07:38:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:38:50] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.3583,	best estimator xgboost's best error=5.3583
[flaml.automl: 09-17 07:38:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:38:51] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.3583,	best estimator xgboost's best error=5.3583
[flaml.automl: 09-17 07:38:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:39:01] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.3583,	best estimator xgboost's best error=5.3583
[flaml.automl: 09-17 07:39:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:39:02] {3072} INFO -  at 15.8s,	estimator xgboost's best error=3.8489,	best estimator xgboost's best error=3.8489
[flaml.automl: 09-17 07:39:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:39:04] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.3541,	best estimator xgboost's best error=3.3541
[flaml.automl: 09-17 07:39:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:39:05] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.0830,	best estimator xgboost's best error=3.0830
[flaml.automl: 09-17 07:39:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:39:08] {3072} INFO -  at 21.8s,	estimator xgboost's best error=3.0830,	best estimator xgboost's best error=3.0830
[flaml.automl: 09-17 07:39:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:39:10] {3072} INFO -  at 23.4s,	estimator xgboost's best error=3.0830,	best estimator xgboost's best error=3.0830
[flaml.automl: 09-17 07:39:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:39:13] {3072} INFO -  at 26.5s,	estimator xgboost's best error=2.7609,	best estimator xgboost's best error=2.7609
[flaml.automl: 09-17 07:39:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:39:14] {3072} INFO -  at 28.1s,	estimator xgboost's best error=2.7609,	best estimator xgboost's best error=2.7609
[flaml.automl: 09-17 07:39:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:39:15] {3072} INFO -  at 29.3s,	estimator xgboost's best error=2.7609,	best estimator xgboost's best error=2.7609
[flaml.automl: 09-17 07:39:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:39:29] {3072} INFO -  at 42.9s,	estimator xgboost's best error=2.5510,	best estimator xgboost's best error=2.5510
[flaml.automl: 09-17 07:39:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:39:46] {3072} INFO -  at 59.4s,	estimator xgboost's best error=2.4769,	best estimator xgboost's best error=2.4769
[flaml.automl: 09-17 07:40:09] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-17 07:40:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:40:09] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:40:09] {2637} INFO - Time taken to find the best model: 59.43678259849548
[flaml.automl: 09-17 07:40:09] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-1.4768994651960359
PM2.5(0)最好结果：{'pred_time': 1.1364445557606003e-05, 'wall_clock_time': 59.43678259849548, 'metric_for_logging': {'pred_time': 1.1364445557606003e-05}, 'val_loss': 2.476899465196036, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.509634733200073}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8878540453866066
PM2.5(0)的mse=13.718354063612159
PM2.5(0)的mae=2.4507802906796976
PM2.5(0)的mar=0.20342619352520735
总共花费的时间为：83.88
汕尾市
1694A
1695A
[flaml.automl: 09-17 07:46:28] {2390} INFO - task = regression
[flaml.automl: 09-17 07:46:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:46:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:46:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:46:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:46:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:46:30] {3025} INFO - Estimated sufficient time budget=12136s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 07:46:30] {3072} INFO -  at 1.3s,	estimator xgboost's best error=9.9845,	best estimator xgboost's best error=9.9845
[flaml.automl: 09-17 07:46:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:46:32] {3072} INFO -  at 3.4s,	estimator xgboost's best error=4.8389,	best estimator xgboost's best error=4.8389
[flaml.automl: 09-17 07:46:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:46:33] {3072} INFO -  at 4.6s,	estimator xgboost's best error=4.8389,	best estimator xgboost's best error=4.8389
[flaml.automl: 09-17 07:46:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:46:42] {3072} INFO -  at 14.1s,	estimator xgboost's best error=4.8389,	best estimator xgboost's best error=4.8389
[flaml.automl: 09-17 07:46:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:46:44] {3072} INFO -  at 15.3s,	estimator xgboost's best error=3.5432,	best estimator xgboost's best error=3.5432
[flaml.automl: 09-17 07:46:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:46:45] {3072} INFO -  at 16.9s,	estimator xgboost's best error=3.1319,	best estimator xgboost's best error=3.1319
[flaml.automl: 09-17 07:46:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:46:47] {3072} INFO -  at 18.5s,	estimator xgboost's best error=2.8945,	best estimator xgboost's best error=2.8945
[flaml.automl: 09-17 07:46:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:46:50] {3072} INFO -  at 21.2s,	estimator xgboost's best error=2.8945,	best estimator xgboost's best error=2.8945
[flaml.automl: 09-17 07:46:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:46:51] {3072} INFO -  at 22.8s,	estimator xgboost's best error=2.8945,	best estimator xgboost's best error=2.8945
[flaml.automl: 09-17 07:46:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:46:54] {3072} INFO -  at 25.9s,	estimator xgboost's best error=2.5208,	best estimator xgboost's best error=2.5208
[flaml.automl: 09-17 07:46:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:46:56] {3072} INFO -  at 27.5s,	estimator xgboost's best error=2.5208,	best estimator xgboost's best error=2.5208
[flaml.automl: 09-17 07:46:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:46:57] {3072} INFO -  at 28.6s,	estimator xgboost's best error=2.5208,	best estimator xgboost's best error=2.5208
[flaml.automl: 09-17 07:46:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:47:09] {3072} INFO -  at 40.7s,	estimator xgboost's best error=2.3609,	best estimator xgboost's best error=2.3609
[flaml.automl: 09-17 07:47:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:47:28] {3072} INFO -  at 59.8s,	estimator xgboost's best error=2.2759,	best estimator xgboost's best error=2.2759
[flaml.automl: 09-17 07:47:52] {3335} INFO - retrain xgboost for 23.7s
[flaml.automl: 09-17 07:47:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:47:52] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:47:52] {2637} INFO - Time taken to find the best model: 59.83756756782532
[flaml.automl: 09-17 07:47:52] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-1.2758767160435314
PM2.5(0)最好结果：{'pred_time': 1.6549887116422358e-05, 'wall_clock_time': 59.83756756782532, 'metric_for_logging': {'pred_time': 1.6549887116422358e-05}, 'val_loss': 2.2758767160435314, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 19.180161237716675}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8926797517263444
PM2.5(0)的mse=11.546188075727157
PM2.5(0)的mae=2.2070794506434077
PM2.5(0)的mar=0.18108525369682896
总共花费的时间为：83.92
河源市
1696A
1697A
[flaml.automl: 09-17 07:54:21] {2390} INFO - task = regression
[flaml.automl: 09-17 07:54:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:54:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:54:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:54:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:54:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:54:22] {3025} INFO - Estimated sufficient time budget=12034s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 07:54:22] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.9169,	best estimator xgboost's best error=11.9169
[flaml.automl: 09-17 07:54:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:54:24] {3072} INFO -  at 3.2s,	estimator xgboost's best error=6.4150,	best estimator xgboost's best error=6.4150
[flaml.automl: 09-17 07:54:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:54:25] {3072} INFO -  at 4.4s,	estimator xgboost's best error=6.4150,	best estimator xgboost's best error=6.4150
[flaml.automl: 09-17 07:54:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:54:34] {3072} INFO -  at 12.8s,	estimator xgboost's best error=6.4150,	best estimator xgboost's best error=6.4150
[flaml.automl: 09-17 07:54:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:54:35] {3072} INFO -  at 14.0s,	estimator xgboost's best error=3.7757,	best estimator xgboost's best error=3.7757
[flaml.automl: 09-17 07:54:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:54:36] {3072} INFO -  at 15.6s,	estimator xgboost's best error=3.2562,	best estimator xgboost's best error=3.2562
[flaml.automl: 09-17 07:54:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:54:38] {3072} INFO -  at 17.2s,	estimator xgboost's best error=2.9112,	best estimator xgboost's best error=2.9112
[flaml.automl: 09-17 07:54:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:54:40] {3072} INFO -  at 19.5s,	estimator xgboost's best error=2.9112,	best estimator xgboost's best error=2.9112
[flaml.automl: 09-17 07:54:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:54:42] {3072} INFO -  at 21.2s,	estimator xgboost's best error=2.9112,	best estimator xgboost's best error=2.9112
[flaml.automl: 09-17 07:54:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:54:45] {3072} INFO -  at 24.2s,	estimator xgboost's best error=2.6554,	best estimator xgboost's best error=2.6554
[flaml.automl: 09-17 07:54:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:54:47] {3072} INFO -  at 25.8s,	estimator xgboost's best error=2.6554,	best estimator xgboost's best error=2.6554
[flaml.automl: 09-17 07:54:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:54:48] {3072} INFO -  at 27.0s,	estimator xgboost's best error=2.6554,	best estimator xgboost's best error=2.6554
[flaml.automl: 09-17 07:54:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:55:00] {3072} INFO -  at 39.0s,	estimator xgboost's best error=2.4035,	best estimator xgboost's best error=2.4035
[flaml.automl: 09-17 07:55:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:55:20] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.2327,	best estimator xgboost's best error=2.2327
[flaml.automl: 09-17 07:56:03] {3335} INFO - retrain xgboost for 42.7s
[flaml.automl: 09-17 07:56:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:56:03] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:56:03] {2637} INFO - Time taken to find the best model: 59.54722595214844
[flaml.automl: 09-17 07:56:03] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-1.2327083027157477
PM2.5(0)最好结果：{'pred_time': 1.919107445779975e-05, 'wall_clock_time': 59.54722595214844, 'metric_for_logging': {'pred_time': 1.919107445779975e-05}, 'val_loss': 2.2327083027157477, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 20.568090438842773}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.891963316790778
PM2.5(0)的mse=12.599343223331527
PM2.5(0)的mae=2.240369713000014
PM2.5(0)的mar=0.14264096885935348
总共花费的时间为：102.72
阳江市
1699A
3453A
[flaml.automl: 09-17 08:02:31] {2390} INFO - task = regression
[flaml.automl: 09-17 08:02:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:02:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:02:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:02:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:02:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:02:32] {3025} INFO - Estimated sufficient time budget=12116s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 08:02:32] {3072} INFO -  at 1.3s,	estimator xgboost's best error=13.8139,	best estimator xgboost's best error=13.8139
[flaml.automl: 09-17 08:02:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:02:34] {3072} INFO -  at 3.2s,	estimator xgboost's best error=7.5754,	best estimator xgboost's best error=7.5754
[flaml.automl: 09-17 08:02:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:02:36] {3072} INFO -  at 4.4s,	estimator xgboost's best error=7.5754,	best estimator xgboost's best error=7.5754
[flaml.automl: 09-17 08:02:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:02:44] {3072} INFO -  at 12.8s,	estimator xgboost's best error=7.5754,	best estimator xgboost's best error=7.5754
[flaml.automl: 09-17 08:02:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:02:45] {3072} INFO -  at 13.9s,	estimator xgboost's best error=4.9251,	best estimator xgboost's best error=4.9251
[flaml.automl: 09-17 08:02:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:02:47] {3072} INFO -  at 15.5s,	estimator xgboost's best error=4.9251,	best estimator xgboost's best error=4.9251
[flaml.automl: 09-17 08:02:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:02:48] {3072} INFO -  at 17.2s,	estimator xgboost's best error=3.3720,	best estimator xgboost's best error=3.3720
[flaml.automl: 09-17 08:02:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:02:51] {3072} INFO -  at 19.5s,	estimator xgboost's best error=3.3720,	best estimator xgboost's best error=3.3720
[flaml.automl: 09-17 08:02:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:02:52] {3072} INFO -  at 21.2s,	estimator xgboost's best error=3.3720,	best estimator xgboost's best error=3.3720
[flaml.automl: 09-17 08:02:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:02:55] {3072} INFO -  at 24.2s,	estimator xgboost's best error=3.3720,	best estimator xgboost's best error=3.3720
[flaml.automl: 09-17 08:02:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:02:57] {3072} INFO -  at 25.9s,	estimator xgboost's best error=3.3473,	best estimator xgboost's best error=3.3473
[flaml.automl: 09-17 08:02:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:02:58] {3072} INFO -  at 27.1s,	estimator xgboost's best error=3.3473,	best estimator xgboost's best error=3.3473
[flaml.automl: 09-17 08:02:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:03:05] {3072} INFO -  at 33.6s,	estimator xgboost's best error=2.9093,	best estimator xgboost's best error=2.9093
[flaml.automl: 09-17 08:03:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:03:16] {3072} INFO -  at 44.7s,	estimator xgboost's best error=2.8653,	best estimator xgboost's best error=2.8653
[flaml.automl: 09-17 08:03:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 08:03:22] {3072} INFO -  at 51.3s,	estimator xgboost's best error=2.8653,	best estimator xgboost's best error=2.8653
[flaml.automl: 09-17 08:03:37] {3335} INFO - retrain xgboost for 15.1s
[flaml.automl: 09-17 08:03:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:03:37] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:03:37] {2637} INFO - Time taken to find the best model: 44.72938799858093
[flaml.automl: 09-17 08:03:37] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-1.865330899833717
PM2.5(0)最好结果：{'pred_time': 1.7848210910085203e-05, 'wall_clock_time': 44.72938799858093, 'metric_for_logging': {'pred_time': 1.7848210910085203e-05}, 'val_loss': 2.865330899833717, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 11.160563707351685}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9221033512497286
PM2.5(0)的mse=18.820902423668993
PM2.5(0)的mae=2.869004041501883
PM2.5(0)的mar=0.18006173679817733
总共花费的时间为：66.84
清远市
1702A
3318A
3455A
[flaml.automl: 09-17 08:13:30] {2390} INFO - task = regression
[flaml.automl: 09-17 08:13:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:13:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:13:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:13:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:13:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:13:33] {3025} INFO - Estimated sufficient time budget=23416s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 08:13:33] {3072} INFO -  at 2.5s,	estimator xgboost's best error=12.8711,	best estimator xgboost's best error=12.8711
[flaml.automl: 09-17 08:13:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:13:37] {3072} INFO -  at 6.5s,	estimator xgboost's best error=6.0388,	best estimator xgboost's best error=6.0388
[flaml.automl: 09-17 08:13:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:13:39] {3072} INFO -  at 8.6s,	estimator xgboost's best error=6.0388,	best estimator xgboost's best error=6.0388
[flaml.automl: 09-17 08:13:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:13:56] {3072} INFO -  at 26.0s,	estimator xgboost's best error=6.0388,	best estimator xgboost's best error=6.0388
[flaml.automl: 09-17 08:13:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:13:58] {3072} INFO -  at 28.1s,	estimator xgboost's best error=3.8274,	best estimator xgboost's best error=3.8274
[flaml.automl: 09-17 08:13:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:14:01] {3072} INFO -  at 30.8s,	estimator xgboost's best error=3.8274,	best estimator xgboost's best error=3.8274
[flaml.automl: 09-17 08:14:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:14:04] {3072} INFO -  at 34.0s,	estimator xgboost's best error=2.7146,	best estimator xgboost's best error=2.7146
[flaml.automl: 09-17 08:14:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:14:09] {3072} INFO -  at 38.7s,	estimator xgboost's best error=2.7146,	best estimator xgboost's best error=2.7146
[flaml.automl: 09-17 08:14:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:14:13] {3072} INFO -  at 42.7s,	estimator xgboost's best error=2.7146,	best estimator xgboost's best error=2.7146
[flaml.automl: 09-17 08:14:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:14:18] {3072} INFO -  at 48.2s,	estimator xgboost's best error=2.7146,	best estimator xgboost's best error=2.7146
[flaml.automl: 09-17 08:14:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:14:21] {3072} INFO -  at 50.9s,	estimator xgboost's best error=2.7146,	best estimator xgboost's best error=2.7146
[flaml.automl: 09-17 08:14:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:14:23] {3072} INFO -  at 53.0s,	estimator xgboost's best error=2.7146,	best estimator xgboost's best error=2.7146
[flaml.automl: 09-17 08:14:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:14:29] {3072} INFO -  at 59.3s,	estimator xgboost's best error=2.3780,	best estimator xgboost's best error=2.3780
[flaml.automl: 09-17 08:14:43] {3335} INFO - retrain xgboost for 13.1s
[flaml.automl: 09-17 08:14:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:14:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:14:43] {2637} INFO - Time taken to find the best model: 59.26032042503357
[flaml.automl: 09-17 08:14:43] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
PM2.5(0)最佳损失：-1.377963114413126
PM2.5(0)最好结果：{'pred_time': 2.3790962699566194e-05, 'wall_clock_time': 59.26032042503357, 'metric_for_logging': {'pred_time': 2.3790962699566194e-05}, 'val_loss': 2.377963114413126, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 6.275525808334351}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9217346152734909
PM2.5(0)的mse=13.454332017778237
PM2.5(0)的mae=2.3882810032886006
PM2.5(0)的mar=0.16599898777009722
总共花费的时间为：73.12
潮州市
1705A
1706A
3026A
[flaml.automl: 09-17 08:24:21] {2390} INFO - task = regression
[flaml.automl: 09-17 08:24:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:24:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:24:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:24:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:24:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:24:23] {3025} INFO - Estimated sufficient time budget=22406s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 08:24:23] {3072} INFO -  at 2.4s,	estimator xgboost's best error=13.3512,	best estimator xgboost's best error=13.3512
[flaml.automl: 09-17 08:24:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:24:27] {3072} INFO -  at 6.3s,	estimator xgboost's best error=6.2413,	best estimator xgboost's best error=6.2413
[flaml.automl: 09-17 08:24:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:24:29] {3072} INFO -  at 8.3s,	estimator xgboost's best error=6.2413,	best estimator xgboost's best error=6.2413
[flaml.automl: 09-17 08:24:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:24:47] {3072} INFO -  at 26.4s,	estimator xgboost's best error=6.2413,	best estimator xgboost's best error=6.2413
[flaml.automl: 09-17 08:24:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:24:49] {3072} INFO -  at 28.0s,	estimator xgboost's best error=4.4262,	best estimator xgboost's best error=4.4262
[flaml.automl: 09-17 08:24:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:24:50] {3072} INFO -  at 29.6s,	estimator xgboost's best error=4.4262,	best estimator xgboost's best error=4.4262
[flaml.automl: 09-17 08:24:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:24:52] {3072} INFO -  at 31.3s,	estimator xgboost's best error=3.1240,	best estimator xgboost's best error=3.1240
[flaml.automl: 09-17 08:24:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:24:57] {3072} INFO -  at 35.8s,	estimator xgboost's best error=3.1240,	best estimator xgboost's best error=3.1240
[flaml.automl: 09-17 08:24:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:25:00] {3072} INFO -  at 38.8s,	estimator xgboost's best error=3.1240,	best estimator xgboost's best error=3.1240
[flaml.automl: 09-17 08:25:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:25:05] {3072} INFO -  at 44.4s,	estimator xgboost's best error=3.1240,	best estimator xgboost's best error=3.1240
[flaml.automl: 09-17 08:25:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:25:08] {3072} INFO -  at 47.1s,	estimator xgboost's best error=3.1240,	best estimator xgboost's best error=3.1240
[flaml.automl: 09-17 08:25:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:25:10] {3072} INFO -  at 49.2s,	estimator xgboost's best error=3.1240,	best estimator xgboost's best error=3.1240
[flaml.automl: 09-17 08:25:10] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:25:20] {3072} INFO -  at 59.2s,	estimator xgboost's best error=2.8582,	best estimator xgboost's best error=2.8582
[flaml.automl: 09-17 08:25:33] {3335} INFO - retrain xgboost for 12.6s
[flaml.automl: 09-17 08:25:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:25:33] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:25:33] {2637} INFO - Time taken to find the best model: 59.23775768280029
[flaml.automl: 09-17 08:25:33] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
PM2.5(0)最佳损失：-1.8581951697390533
PM2.5(0)最好结果：{'pred_time': 2.2825325269178507e-05, 'wall_clock_time': 59.23775768280029, 'metric_for_logging': {'pred_time': 2.2825325269178507e-05}, 'val_loss': 2.8581951697390533, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 9.998165369033813}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8886037385358025
PM2.5(0)的mse=18.47975949159275
PM2.5(0)的mae=2.8274444838624566
PM2.5(0)的mar=0.16814489910859315
总共花费的时间为：72.46
揭阳市
1708A
1709A
1710A
3320A
[flaml.automl: 09-17 08:37:49] {2390} INFO - task = regression
[flaml.automl: 09-17 08:37:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:37:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:37:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:37:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:37:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:37:51] {3025} INFO - Estimated sufficient time budget=52937s. Estimated necessary time budget=53s.
[flaml.automl: 09-17 08:37:51] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.2908,	best estimator xgboost's best error=15.2908
[flaml.automl: 09-17 08:37:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:37:53] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.3917,	best estimator xgboost's best error=7.3917
[flaml.automl: 09-17 08:37:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:37:54] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.3917,	best estimator xgboost's best error=7.3917
[flaml.automl: 09-17 08:37:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:38:00] {3072} INFO -  at 10.6s,	estimator xgboost's best error=7.3917,	best estimator xgboost's best error=7.3917
[flaml.automl: 09-17 08:38:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:38:01] {3072} INFO -  at 11.7s,	estimator xgboost's best error=5.4319,	best estimator xgboost's best error=5.4319
[flaml.automl: 09-17 08:38:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:38:03] {3072} INFO -  at 13.3s,	estimator xgboost's best error=4.8099,	best estimator xgboost's best error=4.8099
[flaml.automl: 09-17 08:38:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:38:04] {3072} INFO -  at 14.9s,	estimator xgboost's best error=4.4483,	best estimator xgboost's best error=4.4483
[flaml.automl: 09-17 08:38:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:38:07] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.4483,	best estimator xgboost's best error=4.4483
[flaml.automl: 09-17 08:38:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:38:08] {3072} INFO -  at 19.2s,	estimator xgboost's best error=4.3916,	best estimator xgboost's best error=4.3916
[flaml.automl: 09-17 08:38:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:38:11] {3072} INFO -  at 22.2s,	estimator xgboost's best error=4.2195,	best estimator xgboost's best error=4.2195
[flaml.automl: 09-17 08:38:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:38:13] {3072} INFO -  at 23.6s,	estimator xgboost's best error=4.2195,	best estimator xgboost's best error=4.2195
[flaml.automl: 09-17 08:38:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:38:17] {3072} INFO -  at 27.3s,	estimator xgboost's best error=4.2195,	best estimator xgboost's best error=4.2195
[flaml.automl: 09-17 08:38:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:38:20] {3072} INFO -  at 30.8s,	estimator xgboost's best error=4.2195,	best estimator xgboost's best error=4.2195
[flaml.automl: 09-17 08:38:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:38:26] {3072} INFO -  at 36.4s,	estimator xgboost's best error=4.2195,	best estimator xgboost's best error=4.2195
[flaml.automl: 09-17 08:38:26] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 08:38:35] {3072} INFO -  at 45.5s,	estimator xgboost's best error=4.1444,	best estimator xgboost's best error=4.1444
[flaml.automl: 09-17 08:38:35] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 08:38:40] {3072} INFO -  at 51.1s,	estimator xgboost's best error=4.1444,	best estimator xgboost's best error=4.1444
[flaml.automl: 09-17 08:38:50] {3335} INFO - retrain xgboost for 9.2s
[flaml.automl: 09-17 08:38:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.4133599162347451, colsample_bynode=1,
             colsample_bytree=0.7592639642076335, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6172511938767024,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.22715509478483517, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007128224391292669, reg_lambda=18.560362963807897,
             scale_pos_weight=1, subsample=0.8750838741026672,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 08:38:50] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:38:50] {2637} INFO - Time taken to find the best model: 45.50761580467224
[flaml.automl: 09-17 08:38:50] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 12, 'min_child_weight': 0.22715509478483517, 'learning_rate': 0.6172511938767024, 'subsample': 0.8750838741026672, 'colsample_bylevel': 0.4133599162347451, 'colsample_bytree': 0.7592639642076335, 'reg_alpha': 0.007128224391292669, 'reg_lambda': 18.560362963807897, 'FLAML_sample_size': 44217}
PM2.5(0)最佳损失：-3.144412856419364
PM2.5(0)最好结果：{'pred_time': 1.6901708052014226e-05, 'wall_clock_time': 45.50761580467224, 'metric_for_logging': {'pred_time': 1.6901708052014226e-05}, 'val_loss': 4.144412856419364, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 12, 'min_child_weight': 0.22715509478483517, 'learning_rate': 0.6172511938767024, 'subsample': 0.8750838741026672, 'colsample_bylevel': 0.4133599162347451, 'colsample_bytree': 0.7592639642076335, 'reg_alpha': 0.007128224391292669, 'reg_lambda': 18.560362963807897, 'FLAML_sample_size': 44217}, 'config/n_estimators': 7, 'config/max_leaves': 12, 'config/min_child_weight': 0.22715509478483517, 'config/learning_rate': 0.6172511938767024, 'config/subsample': 0.8750838741026672, 'config/colsample_bylevel': 0.4133599162347451, 'config/colsample_bytree': 0.7592639642076335, 'config/reg_alpha': 0.007128224391292669, 'config/reg_lambda': 18.560362963807897, 'config/FLAML_sample_size': 44217, 'experiment_tag': 'exp', 'time_total_s': 9.085519075393677}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.4133599162347451, colsample_bynode=1,
             colsample_bytree=0.7592639642076335, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6172511938767024,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.22715509478483517, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007128224391292669, reg_lambda=18.560362963807897,
             scale_pos_weight=1, subsample=0.8750838741026672,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8318985356780465
PM2.5(0)的mse=33.625409420709225
PM2.5(0)的mae=4.139879087974658
PM2.5(0)的mar=0.24043287283762374
总共花费的时间为：61.02
云浮市
1712A
[flaml.automl: 09-17 08:42:20] {2390} INFO - task = regression
[flaml.automl: 09-17 08:42:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:42:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:42:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:42:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:42:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:42:23] {3025} INFO - Estimated sufficient time budget=21570s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 08:42:23] {3072} INFO -  at 2.2s,	estimator xgboost's best error=12.7247,	best estimator xgboost's best error=12.7247
[flaml.automl: 09-17 08:42:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:42:26] {3072} INFO -  at 5.5s,	estimator xgboost's best error=6.8610,	best estimator xgboost's best error=6.8610
[flaml.automl: 09-17 08:42:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:42:28] {3072} INFO -  at 7.4s,	estimator xgboost's best error=6.8610,	best estimator xgboost's best error=6.8610
[flaml.automl: 09-17 08:42:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:42:40] {3072} INFO -  at 19.6s,	estimator xgboost's best error=6.8610,	best estimator xgboost's best error=6.8610
[flaml.automl: 09-17 08:42:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:42:41] {3072} INFO -  at 20.7s,	estimator xgboost's best error=3.3800,	best estimator xgboost's best error=3.3800
[flaml.automl: 09-17 08:42:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:42:43] {3072} INFO -  at 22.3s,	estimator xgboost's best error=2.7674,	best estimator xgboost's best error=2.7674
[flaml.automl: 09-17 08:42:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:42:44] {3072} INFO -  at 23.9s,	estimator xgboost's best error=2.7108,	best estimator xgboost's best error=2.7108
[flaml.automl: 09-17 08:42:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:42:47] {3072} INFO -  at 26.3s,	estimator xgboost's best error=2.7108,	best estimator xgboost's best error=2.7108
[flaml.automl: 09-17 08:42:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:42:48] {3072} INFO -  at 27.9s,	estimator xgboost's best error=2.7108,	best estimator xgboost's best error=2.7108
[flaml.automl: 09-17 08:42:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:42:51] {3072} INFO -  at 30.5s,	estimator xgboost's best error=2.7108,	best estimator xgboost's best error=2.7108
[flaml.automl: 09-17 08:42:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:42:52] {3072} INFO -  at 32.2s,	estimator xgboost's best error=2.3427,	best estimator xgboost's best error=2.3427
[flaml.automl: 09-17 08:42:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:42:54] {3072} INFO -  at 33.3s,	estimator xgboost's best error=2.3427,	best estimator xgboost's best error=2.3427
[flaml.automl: 09-17 08:42:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:43:00] {3072} INFO -  at 40.1s,	estimator xgboost's best error=2.0019,	best estimator xgboost's best error=2.0019
[flaml.automl: 09-17 08:43:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:43:16] {3072} INFO -  at 55.5s,	estimator xgboost's best error=1.9222,	best estimator xgboost's best error=1.9222
[flaml.automl: 09-17 08:43:31] {3335} INFO - retrain xgboost for 15.1s
[flaml.automl: 09-17 08:43:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:43:31] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:43:31] {2637} INFO - Time taken to find the best model: 55.45005750656128
[flaml.automl: 09-17 08:43:31] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 23, 'max_leaves': 6, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463}
PM2.5(0)最佳损失：-0.9221530557870865
PM2.5(0)最好结果：{'pred_time': 6.95958137512207e-05, 'wall_clock_time': 55.45005750656128, 'metric_for_logging': {'pred_time': 6.95958137512207e-05}, 'val_loss': 1.9221530557870865, 'training_iteration': 1, 'config': {'n_estimators': 23, 'max_leaves': 6, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463}, 'config/n_estimators': 23, 'config/max_leaves': 6, 'config/min_child_weight': 6.532042959250319, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8835265175628242, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.10065328594609463, 'experiment_tag': 'exp', 'time_total_s': 15.393611669540405}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9341117684235187
PM2.5(0)的mse=8.768766916294023
PM2.5(0)的mae=2.056049722685243
PM2.5(0)的mar=0.13093977698401119
总共花费的时间为：70.94
玉溪市
2882A
2883A
[flaml.automl: 09-17 08:49:22] {2390} INFO - task = regression
[flaml.automl: 09-17 08:49:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:49:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:49:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:49:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:49:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:49:23] {3025} INFO - Estimated sufficient time budget=12060s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 08:49:23] {3072} INFO -  at 1.3s,	estimator xgboost's best error=12.2997,	best estimator xgboost's best error=12.2997
[flaml.automl: 09-17 08:49:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:49:25] {3072} INFO -  at 3.4s,	estimator xgboost's best error=6.3398,	best estimator xgboost's best error=6.3398
[flaml.automl: 09-17 08:49:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:49:26] {3072} INFO -  at 4.6s,	estimator xgboost's best error=6.3398,	best estimator xgboost's best error=6.3398
[flaml.automl: 09-17 08:49:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:49:36] {3072} INFO -  at 14.0s,	estimator xgboost's best error=6.3398,	best estimator xgboost's best error=6.3398
[flaml.automl: 09-17 08:49:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:49:37] {3072} INFO -  at 15.2s,	estimator xgboost's best error=4.7929,	best estimator xgboost's best error=4.7929
[flaml.automl: 09-17 08:49:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:49:38] {3072} INFO -  at 16.8s,	estimator xgboost's best error=4.3244,	best estimator xgboost's best error=4.3244
[flaml.automl: 09-17 08:49:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:49:40] {3072} INFO -  at 18.4s,	estimator xgboost's best error=4.0790,	best estimator xgboost's best error=4.0790
[flaml.automl: 09-17 08:49:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:49:43] {3072} INFO -  at 21.1s,	estimator xgboost's best error=4.0790,	best estimator xgboost's best error=4.0790
[flaml.automl: 09-17 08:49:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:49:44] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.0790,	best estimator xgboost's best error=4.0790
[flaml.automl: 09-17 08:49:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:49:47] {3072} INFO -  at 25.8s,	estimator xgboost's best error=3.7833,	best estimator xgboost's best error=3.7833
[flaml.automl: 09-17 08:49:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:49:49] {3072} INFO -  at 27.4s,	estimator xgboost's best error=3.7833,	best estimator xgboost's best error=3.7833
[flaml.automl: 09-17 08:49:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:49:50] {3072} INFO -  at 28.6s,	estimator xgboost's best error=3.7833,	best estimator xgboost's best error=3.7833
[flaml.automl: 09-17 08:49:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:50:02] {3072} INFO -  at 40.6s,	estimator xgboost's best error=3.6032,	best estimator xgboost's best error=3.6032
[flaml.automl: 09-17 08:50:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:50:21] {3072} INFO -  at 59.8s,	estimator xgboost's best error=3.4841,	best estimator xgboost's best error=3.4841
[flaml.automl: 09-17 08:50:43] {3335} INFO - retrain xgboost for 21.9s
[flaml.automl: 09-17 08:50:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:50:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:50:43] {2637} INFO - Time taken to find the best model: 59.82604670524597
[flaml.automl: 09-17 08:50:43] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-2.4840600678921705
PM2.5(0)最好结果：{'pred_time': 1.6200931645938822e-05, 'wall_clock_time': 59.82604670524597, 'metric_for_logging': {'pred_time': 1.6200931645938822e-05}, 'val_loss': 3.4840600678921705, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 19.23751401901245}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8738029728280867
PM2.5(0)的mse=25.41454871071142
PM2.5(0)的mae=3.4721974714169
PM2.5(0)的mar=0.3039438094306126
总共花费的时间为：82.04
菏泽市
1719A
[flaml.automl: 09-17 08:53:58] {2390} INFO - task = regression
[flaml.automl: 09-17 08:53:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:53:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:53:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:53:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:53:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:53:59] {3025} INFO - Estimated sufficient time budget=11987s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 08:53:59] {3072} INFO -  at 1.2s,	estimator xgboost's best error=27.3783,	best estimator xgboost's best error=27.3783
[flaml.automl: 09-17 08:53:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:54:01] {3072} INFO -  at 3.1s,	estimator xgboost's best error=15.3043,	best estimator xgboost's best error=15.3043
[flaml.automl: 09-17 08:54:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:54:02] {3072} INFO -  at 4.3s,	estimator xgboost's best error=15.3043,	best estimator xgboost's best error=15.3043
[flaml.automl: 09-17 08:54:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:54:09] {3072} INFO -  at 11.4s,	estimator xgboost's best error=15.3043,	best estimator xgboost's best error=15.3043
[flaml.automl: 09-17 08:54:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:54:11] {3072} INFO -  at 12.5s,	estimator xgboost's best error=8.6257,	best estimator xgboost's best error=8.6257
[flaml.automl: 09-17 08:54:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:54:12] {3072} INFO -  at 14.1s,	estimator xgboost's best error=6.8346,	best estimator xgboost's best error=6.8346
[flaml.automl: 09-17 08:54:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:54:15] {3072} INFO -  at 16.7s,	estimator xgboost's best error=6.8346,	best estimator xgboost's best error=6.8346
[flaml.automl: 09-17 08:54:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:54:19] {3072} INFO -  at 20.6s,	estimator xgboost's best error=6.8346,	best estimator xgboost's best error=6.8346
[flaml.automl: 09-17 08:54:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:54:21] {3072} INFO -  at 22.7s,	estimator xgboost's best error=6.8346,	best estimator xgboost's best error=6.8346
[flaml.automl: 09-17 08:54:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:54:25] {3072} INFO -  at 26.7s,	estimator xgboost's best error=6.8346,	best estimator xgboost's best error=6.8346
[flaml.automl: 09-17 08:54:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:54:27] {3072} INFO -  at 28.7s,	estimator xgboost's best error=6.8346,	best estimator xgboost's best error=6.8346
[flaml.automl: 09-17 08:54:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:54:29] {3072} INFO -  at 30.6s,	estimator xgboost's best error=6.8346,	best estimator xgboost's best error=6.8346
[flaml.automl: 09-17 08:54:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:54:38] {3072} INFO -  at 39.8s,	estimator xgboost's best error=6.0020,	best estimator xgboost's best error=6.0020
[flaml.automl: 09-17 08:54:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:54:54] {3072} INFO -  at 56.4s,	estimator xgboost's best error=5.5663,	best estimator xgboost's best error=5.5663
[flaml.automl: 09-17 08:55:10] {3335} INFO - retrain xgboost for 15.3s
[flaml.automl: 09-17 08:55:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:55:10] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:55:10] {2637} INFO - Time taken to find the best model: 56.40969967842102
[flaml.automl: 09-17 08:55:10] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
PM2.5(0)最佳损失：-4.566320516755633
PM2.5(0)最好结果：{'pred_time': 3.726777918574399e-05, 'wall_clock_time': 56.40969967842102, 'metric_for_logging': {'pred_time': 3.726777918574399e-05}, 'val_loss': 5.566320516755633, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 17, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 16.597605228424072}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9289672875575117
PM2.5(0)的mse=64.17616737478166
PM2.5(0)的mae=5.540807234935271
PM2.5(0)的mar=0.23414350549295276
总共花费的时间为：72.00
大同市
1721A
1725A
1726A
3565A
3566A
3567A
[flaml.automl: 09-17 09:14:38] {2390} INFO - task = regression
[flaml.automl: 09-17 09:14:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:14:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:14:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:14:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:14:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:14:39] {3025} INFO - Estimated sufficient time budget=76179s. Estimated necessary time budget=76s.
[flaml.automl: 09-17 09:14:39] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.4249,	best estimator xgboost's best error=18.4249
[flaml.automl: 09-17 09:14:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:14:41] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.6774,	best estimator xgboost's best error=9.6774
[flaml.automl: 09-17 09:14:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:14:42] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.6774,	best estimator xgboost's best error=9.6774
[flaml.automl: 09-17 09:14:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:14:46] {3072} INFO -  at 8.5s,	estimator xgboost's best error=9.6774,	best estimator xgboost's best error=9.6774
[flaml.automl: 09-17 09:14:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:14:47] {3072} INFO -  at 9.7s,	estimator xgboost's best error=7.4471,	best estimator xgboost's best error=7.4471
[flaml.automl: 09-17 09:14:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:14:49] {3072} INFO -  at 11.2s,	estimator xgboost's best error=7.4471,	best estimator xgboost's best error=7.4471
[flaml.automl: 09-17 09:14:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:14:50] {3072} INFO -  at 12.9s,	estimator xgboost's best error=6.3895,	best estimator xgboost's best error=6.3895
[flaml.automl: 09-17 09:14:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:14:53] {3072} INFO -  at 15.6s,	estimator xgboost's best error=6.3895,	best estimator xgboost's best error=6.3895
[flaml.automl: 09-17 09:14:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:14:55] {3072} INFO -  at 17.2s,	estimator xgboost's best error=6.3895,	best estimator xgboost's best error=6.3895
[flaml.automl: 09-17 09:14:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:14:58] {3072} INFO -  at 20.3s,	estimator xgboost's best error=6.3895,	best estimator xgboost's best error=6.3895
[flaml.automl: 09-17 09:14:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:15:00] {3072} INFO -  at 22.8s,	estimator xgboost's best error=6.3895,	best estimator xgboost's best error=6.3895
[flaml.automl: 09-17 09:15:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:15:03] {3072} INFO -  at 25.7s,	estimator xgboost's best error=6.3895,	best estimator xgboost's best error=6.3895
[flaml.automl: 09-17 09:15:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:15:05] {3072} INFO -  at 27.6s,	estimator xgboost's best error=6.3895,	best estimator xgboost's best error=6.3895
[flaml.automl: 09-17 09:15:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:15:18] {3072} INFO -  at 40.4s,	estimator xgboost's best error=6.0359,	best estimator xgboost's best error=6.0359
[flaml.automl: 09-17 09:15:18] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 09:15:36] {3072} INFO -  at 58.7s,	estimator xgboost's best error=5.9369,	best estimator xgboost's best error=5.9369
[flaml.automl: 09-17 09:16:08] {3335} INFO - retrain xgboost for 31.6s
[flaml.automl: 09-17 09:16:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:16:08] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:16:08] {2637} INFO - Time taken to find the best model: 58.74507975578308
[flaml.automl: 09-17 09:16:08] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64160}
PM2.5(0)最佳损失：-4.936948647186722
PM2.5(0)最好结果：{'pred_time': 1.5888596704755662e-05, 'wall_clock_time': 58.74507975578308, 'metric_for_logging': {'pred_time': 1.5888596704755662e-05}, 'val_loss': 5.936948647186722, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64160}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 64160, 'experiment_tag': 'exp', 'time_total_s': 18.384369373321533}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.843073832941001
PM2.5(0)的mse=76.57843488514304
PM2.5(0)的mae=5.915189991783415
PM2.5(0)的mar=0.39974698792684915
总共花费的时间为：91.72
长治市
1728A
1731A
2845A
3568A
3569A
3570A
[flaml.automl: 09-17 09:35:40] {2390} INFO - task = regression
[flaml.automl: 09-17 09:35:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:35:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:35:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:35:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:35:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:35:41] {3025} INFO - Estimated sufficient time budget=64093s. Estimated necessary time budget=64s.
[flaml.automl: 09-17 09:35:41] {3072} INFO -  at 1.4s,	estimator xgboost's best error=24.2702,	best estimator xgboost's best error=24.2702
[flaml.automl: 09-17 09:35:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:35:43] {3072} INFO -  at 3.5s,	estimator xgboost's best error=11.9965,	best estimator xgboost's best error=11.9965
[flaml.automl: 09-17 09:35:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:35:44] {3072} INFO -  at 4.7s,	estimator xgboost's best error=11.9965,	best estimator xgboost's best error=11.9965
[flaml.automl: 09-17 09:35:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:35:49] {3072} INFO -  at 9.6s,	estimator xgboost's best error=11.9965,	best estimator xgboost's best error=11.9965
[flaml.automl: 09-17 09:35:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:35:51] {3072} INFO -  at 11.6s,	estimator xgboost's best error=8.3280,	best estimator xgboost's best error=8.3280
[flaml.automl: 09-17 09:35:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:35:54] {3072} INFO -  at 14.6s,	estimator xgboost's best error=8.3280,	best estimator xgboost's best error=8.3280
[flaml.automl: 09-17 09:35:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:35:57] {3072} INFO -  at 17.6s,	estimator xgboost's best error=6.8515,	best estimator xgboost's best error=6.8515
[flaml.automl: 09-17 09:35:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:36:01] {3072} INFO -  at 21.0s,	estimator xgboost's best error=6.8515,	best estimator xgboost's best error=6.8515
[flaml.automl: 09-17 09:36:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:36:04] {3072} INFO -  at 24.1s,	estimator xgboost's best error=6.8515,	best estimator xgboost's best error=6.8515
[flaml.automl: 09-17 09:36:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:36:06] {3072} INFO -  at 26.6s,	estimator xgboost's best error=6.8515,	best estimator xgboost's best error=6.8515
[flaml.automl: 09-17 09:36:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:36:09] {3072} INFO -  at 29.3s,	estimator xgboost's best error=6.7678,	best estimator xgboost's best error=6.7678
[flaml.automl: 09-17 09:36:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:36:11] {3072} INFO -  at 31.4s,	estimator xgboost's best error=6.7678,	best estimator xgboost's best error=6.7678
[flaml.automl: 09-17 09:36:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:36:13] {3072} INFO -  at 33.7s,	estimator xgboost's best error=6.5817,	best estimator xgboost's best error=6.5817
[flaml.automl: 09-17 09:36:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:36:15] {3072} INFO -  at 35.5s,	estimator xgboost's best error=6.5817,	best estimator xgboost's best error=6.5817
[flaml.automl: 09-17 09:36:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 09:36:17] {3072} INFO -  at 37.1s,	estimator xgboost's best error=6.5817,	best estimator xgboost's best error=6.5817
[flaml.automl: 09-17 09:36:17] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 09:36:19] {3072} INFO -  at 39.1s,	estimator xgboost's best error=6.5817,	best estimator xgboost's best error=6.5817
[flaml.automl: 09-17 09:36:19] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 09:36:21] {3072} INFO -  at 41.0s,	estimator xgboost's best error=6.5817,	best estimator xgboost's best error=6.5817
[flaml.automl: 09-17 09:36:21] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 09:36:32] {3072} INFO -  at 52.0s,	estimator xgboost's best error=6.0717,	best estimator xgboost's best error=6.0717
[flaml.automl: 09-17 09:36:43] {3335} INFO - retrain xgboost for 10.9s
[flaml.automl: 09-17 09:36:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:36:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:36:43] {2637} INFO - Time taken to find the best model: 51.97994375228882
[flaml.automl: 09-17 09:36:43] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 53371}
PM2.5(0)最佳损失：-5.0716658219823225
PM2.5(0)最好结果：{'pred_time': 1.2839831382945705e-05, 'wall_clock_time': 51.97994375228882, 'metric_for_logging': {'pred_time': 1.2839831382945705e-05}, 'val_loss': 6.0716658219823225, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 53371}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'config/FLAML_sample_size': 53371, 'experiment_tag': 'exp', 'time_total_s': 10.965064764022827}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8780324993191745
PM2.5(0)的mse=71.903007050185
PM2.5(0)的mae=6.113204927907547
PM2.5(0)的mar=0.2737686219698639
总共花费的时间为：64.32
临汾市
3668A
[flaml.automl: 09-17 09:40:20] {2390} INFO - task = regression
[flaml.automl: 09-17 09:40:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:40:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:40:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:40:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:40:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:40:21] {3025} INFO - Estimated sufficient time budget=11866s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 09:40:21] {3072} INFO -  at 1.2s,	estimator xgboost's best error=30.5426,	best estimator xgboost's best error=30.5426
[flaml.automl: 09-17 09:40:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:40:23] {3072} INFO -  at 3.4s,	estimator xgboost's best error=16.7422,	best estimator xgboost's best error=16.7422
[flaml.automl: 09-17 09:40:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:40:25] {3072} INFO -  at 5.6s,	estimator xgboost's best error=16.7422,	best estimator xgboost's best error=16.7422
[flaml.automl: 09-17 09:40:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:40:38] {3072} INFO -  at 18.5s,	estimator xgboost's best error=16.7422,	best estimator xgboost's best error=16.7422
[flaml.automl: 09-17 09:40:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:40:40] {3072} INFO -  at 20.6s,	estimator xgboost's best error=9.8842,	best estimator xgboost's best error=9.8842
[flaml.automl: 09-17 09:40:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:40:43] {3072} INFO -  at 23.6s,	estimator xgboost's best error=8.5298,	best estimator xgboost's best error=8.5298
[flaml.automl: 09-17 09:40:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:40:46] {3072} INFO -  at 26.4s,	estimator xgboost's best error=8.5298,	best estimator xgboost's best error=8.5298
[flaml.automl: 09-17 09:40:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:40:50] {3072} INFO -  at 30.6s,	estimator xgboost's best error=8.5298,	best estimator xgboost's best error=8.5298
[flaml.automl: 09-17 09:40:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:40:52] {3072} INFO -  at 32.7s,	estimator xgboost's best error=8.5298,	best estimator xgboost's best error=8.5298
[flaml.automl: 09-17 09:40:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:40:57] {3072} INFO -  at 37.2s,	estimator xgboost's best error=8.5298,	best estimator xgboost's best error=8.5298
[flaml.automl: 09-17 09:40:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:40:59] {3072} INFO -  at 39.4s,	estimator xgboost's best error=8.5298,	best estimator xgboost's best error=8.5298
[flaml.automl: 09-17 09:40:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:41:02] {3072} INFO -  at 42.2s,	estimator xgboost's best error=8.5298,	best estimator xgboost's best error=8.5298
[flaml.automl: 09-17 09:41:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:41:17] {3072} INFO -  at 57.2s,	estimator xgboost's best error=7.7158,	best estimator xgboost's best error=7.7158
[flaml.automl: 09-17 09:41:32] {3335} INFO - retrain xgboost for 15.5s
[flaml.automl: 09-17 09:41:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 09:41:32] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:41:32] {2637} INFO - Time taken to find the best model: 57.23935866355896
[flaml.automl: 09-17 09:41:32] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
PM2.5(0)最佳损失：-6.715779944874839
PM2.5(0)最好结果：{'pred_time': 0.00010156631469726562, 'wall_clock_time': 57.23935866355896, 'metric_for_logging': {'pred_time': 0.00010156631469726562}, 'val_loss': 7.715779944874839, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 12, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 15.019415140151978}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8849217096377954
PM2.5(0)的mse=106.23369386565713
PM2.5(0)的mae=7.459915570943204
PM2.5(0)的mar=0.2885872486117032
总共花费的时间为：73.01
阳泉市
1738A
1739A
1743A
3619A
[flaml.automl: 09-17 09:54:10] {2390} INFO - task = regression
[flaml.automl: 09-17 09:54:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:54:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:54:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:54:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:54:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:54:11] {3025} INFO - Estimated sufficient time budget=51801s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 09:54:11] {3072} INFO -  at 1.4s,	estimator xgboost's best error=25.7176,	best estimator xgboost's best error=25.7176
[flaml.automl: 09-17 09:54:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:54:13] {3072} INFO -  at 3.5s,	estimator xgboost's best error=12.3776,	best estimator xgboost's best error=12.3776
[flaml.automl: 09-17 09:54:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:54:14] {3072} INFO -  at 4.7s,	estimator xgboost's best error=12.3776,	best estimator xgboost's best error=12.3776
[flaml.automl: 09-17 09:54:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:54:20] {3072} INFO -  at 10.5s,	estimator xgboost's best error=12.3776,	best estimator xgboost's best error=12.3776
[flaml.automl: 09-17 09:54:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:54:21] {3072} INFO -  at 11.6s,	estimator xgboost's best error=9.0060,	best estimator xgboost's best error=9.0060
[flaml.automl: 09-17 09:54:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:54:23] {3072} INFO -  at 13.2s,	estimator xgboost's best error=9.0060,	best estimator xgboost's best error=9.0060
[flaml.automl: 09-17 09:54:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:54:25] {3072} INFO -  at 14.9s,	estimator xgboost's best error=6.2853,	best estimator xgboost's best error=6.2853
[flaml.automl: 09-17 09:54:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:54:27] {3072} INFO -  at 17.6s,	estimator xgboost's best error=6.2853,	best estimator xgboost's best error=6.2853
[flaml.automl: 09-17 09:54:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:54:29] {3072} INFO -  at 19.2s,	estimator xgboost's best error=6.2853,	best estimator xgboost's best error=6.2853
[flaml.automl: 09-17 09:54:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:54:32] {3072} INFO -  at 22.2s,	estimator xgboost's best error=6.2853,	best estimator xgboost's best error=6.2853
[flaml.automl: 09-17 09:54:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:54:34] {3072} INFO -  at 23.7s,	estimator xgboost's best error=6.2853,	best estimator xgboost's best error=6.2853
[flaml.automl: 09-17 09:54:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:54:35] {3072} INFO -  at 25.4s,	estimator xgboost's best error=6.2725,	best estimator xgboost's best error=6.2725
[flaml.automl: 09-17 09:54:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:54:36] {3072} INFO -  at 26.6s,	estimator xgboost's best error=6.2725,	best estimator xgboost's best error=6.2725
[flaml.automl: 09-17 09:54:36] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:54:43] {3072} INFO -  at 33.6s,	estimator xgboost's best error=5.3864,	best estimator xgboost's best error=5.3864
[flaml.automl: 09-17 09:54:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 09:54:56] {3072} INFO -  at 46.4s,	estimator xgboost's best error=5.3109,	best estimator xgboost's best error=5.3109
[flaml.automl: 09-17 09:54:56] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 09:55:03] {3072} INFO -  at 53.5s,	estimator xgboost's best error=5.3109,	best estimator xgboost's best error=5.3109
[flaml.automl: 09-17 09:55:16] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 09:55:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:55:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:55:16] {2637} INFO - Time taken to find the best model: 46.44136023521423
[flaml.automl: 09-17 09:55:16] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43129}
PM2.5(0)最佳损失：-4.310934039214795
PM2.5(0)最好结果：{'pred_time': 8.22218280730953e-06, 'wall_clock_time': 46.44136023521423, 'metric_for_logging': {'pred_time': 8.22218280730953e-06}, 'val_loss': 5.310934039214795, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43129}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 43129, 'experiment_tag': 'exp', 'time_total_s': 12.814327478408813}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9144194271043056
PM2.5(0)的mse=63.11442935884014
PM2.5(0)的mae=5.354841703182095
PM2.5(0)的mar=0.18975847110853344
总共花费的时间为：66.96
赤峰市
1744A
1745A
3286A
[flaml.automl: 09-17 10:04:57] {2390} INFO - task = regression
[flaml.automl: 09-17 10:04:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:04:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:04:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:04:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:04:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:04:58] {3025} INFO - Estimated sufficient time budget=12089s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 10:04:58] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.2642,	best estimator xgboost's best error=11.2642
[flaml.automl: 09-17 10:04:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:05:00] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.8403,	best estimator xgboost's best error=5.8403
[flaml.automl: 09-17 10:05:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:05:02] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.8403,	best estimator xgboost's best error=5.8403
[flaml.automl: 09-17 10:05:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:05:11] {3072} INFO -  at 14.6s,	estimator xgboost's best error=5.8403,	best estimator xgboost's best error=5.8403
[flaml.automl: 09-17 10:05:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:05:13] {3072} INFO -  at 15.8s,	estimator xgboost's best error=4.4608,	best estimator xgboost's best error=4.4608
[flaml.automl: 09-17 10:05:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:05:14] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.4608,	best estimator xgboost's best error=4.4608
[flaml.automl: 09-17 10:05:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:05:16] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.6330,	best estimator xgboost's best error=3.6330
[flaml.automl: 09-17 10:05:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:05:19] {3072} INFO -  at 21.8s,	estimator xgboost's best error=3.6330,	best estimator xgboost's best error=3.6330
[flaml.automl: 09-17 10:05:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:05:20] {3072} INFO -  at 23.4s,	estimator xgboost's best error=3.6330,	best estimator xgboost's best error=3.6330
[flaml.automl: 09-17 10:05:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:05:23] {3072} INFO -  at 26.4s,	estimator xgboost's best error=3.6330,	best estimator xgboost's best error=3.6330
[flaml.automl: 09-17 10:05:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:05:25] {3072} INFO -  at 27.9s,	estimator xgboost's best error=3.6330,	best estimator xgboost's best error=3.6330
[flaml.automl: 09-17 10:05:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:05:26] {3072} INFO -  at 29.0s,	estimator xgboost's best error=3.6330,	best estimator xgboost's best error=3.6330
[flaml.automl: 09-17 10:05:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:05:33] {3072} INFO -  at 36.1s,	estimator xgboost's best error=3.3373,	best estimator xgboost's best error=3.3373
[flaml.automl: 09-17 10:05:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:05:46] {3072} INFO -  at 48.9s,	estimator xgboost's best error=3.2622,	best estimator xgboost's best error=3.2622
[flaml.automl: 09-17 10:05:59] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 10:05:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 10:05:59] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:05:59] {2637} INFO - Time taken to find the best model: 48.878883600234985
[flaml.automl: 09-17 10:05:59] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-2.2621964918477473
PM2.5(0)最好结果：{'pred_time': 1.1102499447935762e-05, 'wall_clock_time': 48.878883600234985, 'metric_for_logging': {'pred_time': 1.1102499447935762e-05}, 'val_loss': 3.2621964918477473, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 12.798057556152344}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8935879323583662
PM2.5(0)的mse=23.314643313278356
PM2.5(0)的mae=3.142281364141914
PM2.5(0)的mar=0.2810954091786859
总共花费的时间为：62.18
鞍山市
1749A
1750A
1751A
1752A
1753A
1754A
[flaml.automl: 09-17 10:24:43] {2390} INFO - task = regression
[flaml.automl: 09-17 10:24:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:24:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:24:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:24:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:24:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:24:46] {3025} INFO - Estimated sufficient time budget=143554s. Estimated necessary time budget=144s.
[flaml.automl: 09-17 10:24:46] {3072} INFO -  at 2.5s,	estimator xgboost's best error=20.6647,	best estimator xgboost's best error=20.6647
[flaml.automl: 09-17 10:24:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:24:49] {3072} INFO -  at 6.1s,	estimator xgboost's best error=10.4955,	best estimator xgboost's best error=10.4955
[flaml.automl: 09-17 10:24:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:24:51] {3072} INFO -  at 7.9s,	estimator xgboost's best error=10.4955,	best estimator xgboost's best error=10.4955
[flaml.automl: 09-17 10:24:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:24:54] {3072} INFO -  at 11.1s,	estimator xgboost's best error=10.4955,	best estimator xgboost's best error=10.4955
[flaml.automl: 09-17 10:24:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:24:56] {3072} INFO -  at 12.9s,	estimator xgboost's best error=8.1307,	best estimator xgboost's best error=8.1307
[flaml.automl: 09-17 10:24:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:24:59] {3072} INFO -  at 15.7s,	estimator xgboost's best error=8.1307,	best estimator xgboost's best error=8.1307
[flaml.automl: 09-17 10:24:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:25:01] {3072} INFO -  at 18.1s,	estimator xgboost's best error=6.6736,	best estimator xgboost's best error=6.6736
[flaml.automl: 09-17 10:25:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:25:04] {3072} INFO -  at 21.0s,	estimator xgboost's best error=6.6736,	best estimator xgboost's best error=6.6736
[flaml.automl: 09-17 10:25:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:25:07] {3072} INFO -  at 23.5s,	estimator xgboost's best error=6.6736,	best estimator xgboost's best error=6.6736
[flaml.automl: 09-17 10:25:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:25:09] {3072} INFO -  at 26.2s,	estimator xgboost's best error=6.6736,	best estimator xgboost's best error=6.6736
[flaml.automl: 09-17 10:25:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:25:12] {3072} INFO -  at 28.4s,	estimator xgboost's best error=6.6736,	best estimator xgboost's best error=6.6736
[flaml.automl: 09-17 10:25:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:25:15] {3072} INFO -  at 31.4s,	estimator xgboost's best error=6.6539,	best estimator xgboost's best error=6.6539
[flaml.automl: 09-17 10:25:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:25:17] {3072} INFO -  at 33.4s,	estimator xgboost's best error=6.6539,	best estimator xgboost's best error=6.6539
[flaml.automl: 09-17 10:25:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:25:29] {3072} INFO -  at 45.4s,	estimator xgboost's best error=6.2015,	best estimator xgboost's best error=6.2015
[flaml.automl: 09-17 10:25:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 10:25:41] {3072} INFO -  at 58.2s,	estimator xgboost's best error=6.0660,	best estimator xgboost's best error=6.0660
[flaml.automl: 09-17 10:25:54] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 10:25:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 10:25:54] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:25:54] {2637} INFO - Time taken to find the best model: 58.20234441757202
[flaml.automl: 09-17 10:25:54] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64573}
PM2.5(0)最佳损失：-5.06598683064823
PM2.5(0)最好结果：{'pred_time': 5.510011201120835e-06, 'wall_clock_time': 58.20234441757202, 'metric_for_logging': {'pred_time': 5.510011201120835e-06}, 'val_loss': 6.06598683064823, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64573}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 64573, 'experiment_tag': 'exp', 'time_total_s': 12.848355293273926}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8186503200935533
PM2.5(0)的mse=102.00747256549914
PM2.5(0)的mae=5.998638696665747
PM2.5(0)的mar=0.2324773918810575
总共花费的时间为：71.96
抚顺市
1755A
1756A
1757A
1758A
1760A
[flaml.automl: 09-17 10:42:33] {2390} INFO - task = regression
[flaml.automl: 09-17 10:42:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:42:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:42:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:42:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:42:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:42:34] {3025} INFO - Estimated sufficient time budget=64952s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 10:42:34] {3072} INFO -  at 1.4s,	estimator xgboost's best error=22.9975,	best estimator xgboost's best error=22.9975
[flaml.automl: 09-17 10:42:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:42:36] {3072} INFO -  at 3.5s,	estimator xgboost's best error=11.6588,	best estimator xgboost's best error=11.6588
[flaml.automl: 09-17 10:42:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:42:38] {3072} INFO -  at 4.7s,	estimator xgboost's best error=11.6588,	best estimator xgboost's best error=11.6588
[flaml.automl: 09-17 10:42:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:42:42] {3072} INFO -  at 9.4s,	estimator xgboost's best error=11.6588,	best estimator xgboost's best error=11.6588
[flaml.automl: 09-17 10:42:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:42:43] {3072} INFO -  at 10.6s,	estimator xgboost's best error=9.5635,	best estimator xgboost's best error=9.5635
[flaml.automl: 09-17 10:42:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:42:45] {3072} INFO -  at 12.1s,	estimator xgboost's best error=8.6156,	best estimator xgboost's best error=8.6156
[flaml.automl: 09-17 10:42:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:42:47] {3072} INFO -  at 13.7s,	estimator xgboost's best error=7.7322,	best estimator xgboost's best error=7.7322
[flaml.automl: 09-17 10:42:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:42:49] {3072} INFO -  at 16.4s,	estimator xgboost's best error=7.7322,	best estimator xgboost's best error=7.7322
[flaml.automl: 09-17 10:42:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:42:51] {3072} INFO -  at 18.0s,	estimator xgboost's best error=7.7322,	best estimator xgboost's best error=7.7322
[flaml.automl: 09-17 10:42:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:42:54] {3072} INFO -  at 21.0s,	estimator xgboost's best error=6.8687,	best estimator xgboost's best error=6.8687
[flaml.automl: 09-17 10:42:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:42:55] {3072} INFO -  at 22.6s,	estimator xgboost's best error=6.8687,	best estimator xgboost's best error=6.8687
[flaml.automl: 09-17 10:42:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:42:57] {3072} INFO -  at 23.8s,	estimator xgboost's best error=6.8687,	best estimator xgboost's best error=6.8687
[flaml.automl: 09-17 10:42:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:42:59] {3072} INFO -  at 26.7s,	estimator xgboost's best error=6.8687,	best estimator xgboost's best error=6.8687
[flaml.automl: 09-17 10:42:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:43:02] {3072} INFO -  at 29.5s,	estimator xgboost's best error=6.8687,	best estimator xgboost's best error=6.8687
[flaml.automl: 09-17 10:43:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 10:43:07] {3072} INFO -  at 33.8s,	estimator xgboost's best error=6.8635,	best estimator xgboost's best error=6.8635
[flaml.automl: 09-17 10:43:07] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 10:43:20] {3072} INFO -  at 47.6s,	estimator xgboost's best error=6.8635,	best estimator xgboost's best error=6.8635
[flaml.automl: 09-17 10:43:20] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 10:43:26] {3072} INFO -  at 53.1s,	estimator xgboost's best error=6.8352,	best estimator xgboost's best error=6.8352
[flaml.automl: 09-17 10:43:26] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 10:43:31] {3072} INFO -  at 57.8s,	estimator xgboost's best error=6.8352,	best estimator xgboost's best error=6.8352
[flaml.automl: 09-17 10:43:36] {3335} INFO - retrain xgboost for 5.4s
[flaml.automl: 09-17 10:43:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 10:43:36] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:43:36] {2637} INFO - Time taken to find the best model: 53.0859100818634
[flaml.automl: 09-17 10:43:36] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 53370}
PM2.5(0)最佳损失：-5.835196243412196
PM2.5(0)最好结果：{'pred_time': 1.4735482189671786e-05, 'wall_clock_time': 53.0859100818634, 'metric_for_logging': {'pred_time': 1.4735482189671786e-05}, 'val_loss': 6.835196243412196, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 53370}, 'config/n_estimators': 7, 'config/max_leaves': 4, 'config/min_child_weight': 0.031257867156220746, 'config/learning_rate': 0.3417584818146937, 'config/subsample': 0.8188658116452241, 'config/colsample_bylevel': 0.9718368347363348, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.001562580531679857, 'config/reg_lambda': 0.36612402315272025, 'config/FLAML_sample_size': 53370, 'experiment_tag': 'exp', 'time_total_s': 5.450582504272461}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8073424083751441
PM2.5(0)的mse=109.16227407272926
PM2.5(0)的mae=6.689872486382556
PM2.5(0)的mar=0.25701686429912834
总共花费的时间为：64.20
本溪市
1761A
1762A
1763A
1764A
1765A
[flaml.automl: 09-17 10:58:20] {2390} INFO - task = regression
[flaml.automl: 09-17 10:58:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:58:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:58:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:58:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:58:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:58:21] {3025} INFO - Estimated sufficient time budget=65390s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 10:58:21] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.7440,	best estimator xgboost's best error=17.7440
[flaml.automl: 09-17 10:58:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:58:23] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.9081,	best estimator xgboost's best error=8.9081
[flaml.automl: 09-17 10:58:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:58:24] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.9081,	best estimator xgboost's best error=8.9081
[flaml.automl: 09-17 10:58:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:58:29] {3072} INFO -  at 9.5s,	estimator xgboost's best error=8.9081,	best estimator xgboost's best error=8.9081
[flaml.automl: 09-17 10:58:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:58:30] {3072} INFO -  at 10.7s,	estimator xgboost's best error=7.4315,	best estimator xgboost's best error=7.4315
[flaml.automl: 09-17 10:58:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:58:32] {3072} INFO -  at 12.3s,	estimator xgboost's best error=7.4315,	best estimator xgboost's best error=7.4315
[flaml.automl: 09-17 10:58:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:58:33] {3072} INFO -  at 14.0s,	estimator xgboost's best error=5.2759,	best estimator xgboost's best error=5.2759
[flaml.automl: 09-17 10:58:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:58:36] {3072} INFO -  at 16.7s,	estimator xgboost's best error=5.2759,	best estimator xgboost's best error=5.2759
[flaml.automl: 09-17 10:58:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:58:38] {3072} INFO -  at 18.3s,	estimator xgboost's best error=5.2759,	best estimator xgboost's best error=5.2759
[flaml.automl: 09-17 10:58:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:58:41] {3072} INFO -  at 21.3s,	estimator xgboost's best error=5.2759,	best estimator xgboost's best error=5.2759
[flaml.automl: 09-17 10:58:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:58:42] {3072} INFO -  at 22.7s,	estimator xgboost's best error=5.2759,	best estimator xgboost's best error=5.2759
[flaml.automl: 09-17 10:58:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:58:44] {3072} INFO -  at 24.5s,	estimator xgboost's best error=5.2759,	best estimator xgboost's best error=5.2759
[flaml.automl: 09-17 10:58:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:58:45] {3072} INFO -  at 25.6s,	estimator xgboost's best error=5.2759,	best estimator xgboost's best error=5.2759
[flaml.automl: 09-17 10:58:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:58:52] {3072} INFO -  at 32.7s,	estimator xgboost's best error=4.9403,	best estimator xgboost's best error=4.9403
[flaml.automl: 09-17 10:58:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 10:59:05] {3072} INFO -  at 45.5s,	estimator xgboost's best error=4.8227,	best estimator xgboost's best error=4.8227
[flaml.automl: 09-17 10:59:05] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 10:59:14] {3072} INFO -  at 54.6s,	estimator xgboost's best error=4.8227,	best estimator xgboost's best error=4.8227
[flaml.automl: 09-17 10:59:38] {3335} INFO - retrain xgboost for 23.6s
[flaml.automl: 09-17 10:59:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 10:59:38] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:59:38] {2637} INFO - Time taken to find the best model: 45.50683546066284
[flaml.automl: 09-17 10:59:38] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 55196}
PM2.5(0)最佳损失：-3.822722340648644
PM2.5(0)最好结果：{'pred_time': 6.690026107083987e-06, 'wall_clock_time': 45.50683546066284, 'metric_for_logging': {'pred_time': 6.690026107083987e-06}, 'val_loss': 4.822722340648644, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 55196}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 55196, 'experiment_tag': 'exp', 'time_total_s': 12.809509754180908}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8440697584759322
PM2.5(0)的mse=64.94604630631063
PM2.5(0)的mae=4.810698698761703
PM2.5(0)的mar=0.19961152033032303
总共花费的时间为：79.20
锦州市
1767A
1768A
1770A
1771A
[flaml.automl: 09-17 11:12:18] {2390} INFO - task = regression
[flaml.automl: 09-17 11:12:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:12:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:12:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:12:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:12:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:12:19] {3025} INFO - Estimated sufficient time budget=53116s. Estimated necessary time budget=53s.
[flaml.automl: 09-17 11:12:19] {3072} INFO -  at 1.4s,	estimator xgboost's best error=23.2747,	best estimator xgboost's best error=23.2747
[flaml.automl: 09-17 11:12:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:12:22] {3072} INFO -  at 3.6s,	estimator xgboost's best error=12.0023,	best estimator xgboost's best error=12.0023
[flaml.automl: 09-17 11:12:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:12:23] {3072} INFO -  at 4.8s,	estimator xgboost's best error=12.0023,	best estimator xgboost's best error=12.0023
[flaml.automl: 09-17 11:12:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:12:29] {3072} INFO -  at 10.6s,	estimator xgboost's best error=12.0023,	best estimator xgboost's best error=12.0023
[flaml.automl: 09-17 11:12:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:12:30] {3072} INFO -  at 11.8s,	estimator xgboost's best error=9.2212,	best estimator xgboost's best error=9.2212
[flaml.automl: 09-17 11:12:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:12:31] {3072} INFO -  at 13.4s,	estimator xgboost's best error=9.1091,	best estimator xgboost's best error=9.1091
[flaml.automl: 09-17 11:12:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:12:33] {3072} INFO -  at 15.0s,	estimator xgboost's best error=7.6928,	best estimator xgboost's best error=7.6928
[flaml.automl: 09-17 11:12:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:12:36] {3072} INFO -  at 17.7s,	estimator xgboost's best error=7.6928,	best estimator xgboost's best error=7.6928
[flaml.automl: 09-17 11:12:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:12:37] {3072} INFO -  at 19.3s,	estimator xgboost's best error=7.6928,	best estimator xgboost's best error=7.6928
[flaml.automl: 09-17 11:12:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:12:40] {3072} INFO -  at 22.3s,	estimator xgboost's best error=7.3082,	best estimator xgboost's best error=7.3082
[flaml.automl: 09-17 11:12:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:12:42] {3072} INFO -  at 23.9s,	estimator xgboost's best error=7.3082,	best estimator xgboost's best error=7.3082
[flaml.automl: 09-17 11:12:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:12:43] {3072} INFO -  at 25.1s,	estimator xgboost's best error=7.3082,	best estimator xgboost's best error=7.3082
[flaml.automl: 09-17 11:12:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:12:47] {3072} INFO -  at 28.9s,	estimator xgboost's best error=7.1380,	best estimator xgboost's best error=7.1380
[flaml.automl: 09-17 11:12:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:12:50] {3072} INFO -  at 31.8s,	estimator xgboost's best error=7.1380,	best estimator xgboost's best error=7.1380
[flaml.automl: 09-17 11:12:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 11:12:52] {3072} INFO -  at 34.3s,	estimator xgboost's best error=6.9637,	best estimator xgboost's best error=6.9637
[flaml.automl: 09-17 11:12:52] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 11:12:55] {3072} INFO -  at 36.5s,	estimator xgboost's best error=6.9637,	best estimator xgboost's best error=6.9637
[flaml.automl: 09-17 11:12:55] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 11:12:57] {3072} INFO -  at 38.7s,	estimator xgboost's best error=6.9438,	best estimator xgboost's best error=6.9438
[flaml.automl: 09-17 11:12:57] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 11:12:58] {3072} INFO -  at 40.4s,	estimator xgboost's best error=6.9438,	best estimator xgboost's best error=6.9438
[flaml.automl: 09-17 11:12:58] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 11:13:00] {3072} INFO -  at 42.1s,	estimator xgboost's best error=6.9438,	best estimator xgboost's best error=6.9438
[flaml.automl: 09-17 11:13:00] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 11:13:02] {3072} INFO -  at 43.7s,	estimator xgboost's best error=6.9438,	best estimator xgboost's best error=6.9438
[flaml.automl: 09-17 11:13:02] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 11:13:03] {3072} INFO -  at 45.2s,	estimator xgboost's best error=6.7807,	best estimator xgboost's best error=6.7807
[flaml.automl: 09-17 11:13:03] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 11:13:06] {3072} INFO -  at 47.8s,	estimator xgboost's best error=6.7807,	best estimator xgboost's best error=6.7807
[flaml.automl: 09-17 11:13:06] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-17 11:13:07] {3072} INFO -  at 48.8s,	estimator xgboost's best error=6.7807,	best estimator xgboost's best error=6.7807
[flaml.automl: 09-17 11:13:07] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-17 11:13:10] {3072} INFO -  at 52.1s,	estimator xgboost's best error=6.7807,	best estimator xgboost's best error=6.7807
[flaml.automl: 09-17 11:13:10] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-17 11:13:11] {3072} INFO -  at 52.8s,	estimator xgboost's best error=6.7807,	best estimator xgboost's best error=6.7807
[flaml.automl: 09-17 11:13:11] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-17 11:13:18] {3072} INFO -  at 59.8s,	estimator xgboost's best error=6.6305,	best estimator xgboost's best error=6.6305
[flaml.automl: 09-17 11:13:39] {3335} INFO - retrain xgboost for 21.4s
[flaml.automl: 09-17 11:13:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8732721209051209, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=24, min_child_weight=0.0023588925015011544,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009451087049979049, reg_lambda=0.08827632666156185,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:13:39] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:13:39] {2637} INFO - Time taken to find the best model: 59.764599323272705
[flaml.automl: 09-17 11:13:39] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 10, 'max_leaves': 24, 'min_child_weight': 0.0023588925015011544, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8732721209051209, 'reg_alpha': 0.009451087049979049, 'reg_lambda': 0.08827632666156185, 'FLAML_sample_size': 43761}
PM2.5(0)最佳损失：-5.630497771698875
PM2.5(0)最好结果：{'pred_time': 8.240271663803046e-06, 'wall_clock_time': 59.764599323272705, 'metric_for_logging': {'pred_time': 8.240271663803046e-06}, 'val_loss': 6.630497771698875, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 24, 'min_child_weight': 0.0023588925015011544, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8732721209051209, 'reg_alpha': 0.009451087049979049, 'reg_lambda': 0.08827632666156185, 'FLAML_sample_size': 43761}, 'config/n_estimators': 10, 'config/max_leaves': 24, 'config/min_child_weight': 0.0023588925015011544, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8732721209051209, 'config/reg_alpha': 0.009451087049979049, 'config/reg_lambda': 0.08827632666156185, 'config/FLAML_sample_size': 43761, 'experiment_tag': 'exp', 'time_total_s': 6.927166223526001}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8732721209051209, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=24, min_child_weight=0.0023588925015011544,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009451087049979049, reg_lambda=0.08827632666156185,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8495632138806916
PM2.5(0)的mse=121.38890838511038
PM2.5(0)的mae=6.6112909803738225
PM2.5(0)的mar=0.2662501051066907
总共花费的时间为：81.84
吉林市
1772A
1774A
1775A
1776A
2868A
[flaml.automl: 09-17 11:28:53] {2390} INFO - task = regression
[flaml.automl: 09-17 11:28:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:28:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:28:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:28:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:28:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:28:54] {3025} INFO - Estimated sufficient time budget=64681s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 11:28:54] {3072} INFO -  at 1.5s,	estimator xgboost's best error=17.7763,	best estimator xgboost's best error=17.7763
[flaml.automl: 09-17 11:28:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:28:56] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.0231,	best estimator xgboost's best error=9.0231
[flaml.automl: 09-17 11:28:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:28:57] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.0231,	best estimator xgboost's best error=9.0231
[flaml.automl: 09-17 11:28:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:29:02] {3072} INFO -  at 9.6s,	estimator xgboost's best error=9.0231,	best estimator xgboost's best error=9.0231
[flaml.automl: 09-17 11:29:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:29:03] {3072} INFO -  at 10.7s,	estimator xgboost's best error=7.1923,	best estimator xgboost's best error=7.1923
[flaml.automl: 09-17 11:29:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:29:05] {3072} INFO -  at 12.3s,	estimator xgboost's best error=7.1923,	best estimator xgboost's best error=7.1923
[flaml.automl: 09-17 11:29:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:29:07] {3072} INFO -  at 14.0s,	estimator xgboost's best error=5.8292,	best estimator xgboost's best error=5.8292
[flaml.automl: 09-17 11:29:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:29:09] {3072} INFO -  at 16.7s,	estimator xgboost's best error=5.8292,	best estimator xgboost's best error=5.8292
[flaml.automl: 09-17 11:29:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:29:11] {3072} INFO -  at 18.3s,	estimator xgboost's best error=5.8292,	best estimator xgboost's best error=5.8292
[flaml.automl: 09-17 11:29:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:29:14] {3072} INFO -  at 21.3s,	estimator xgboost's best error=5.8292,	best estimator xgboost's best error=5.8292
[flaml.automl: 09-17 11:29:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:29:15] {3072} INFO -  at 22.7s,	estimator xgboost's best error=5.8292,	best estimator xgboost's best error=5.8292
[flaml.automl: 09-17 11:29:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:29:17] {3072} INFO -  at 24.5s,	estimator xgboost's best error=5.8261,	best estimator xgboost's best error=5.8261
[flaml.automl: 09-17 11:29:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:29:18] {3072} INFO -  at 25.7s,	estimator xgboost's best error=5.8261,	best estimator xgboost's best error=5.8261
[flaml.automl: 09-17 11:29:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:29:25] {3072} INFO -  at 32.7s,	estimator xgboost's best error=5.5338,	best estimator xgboost's best error=5.5338
[flaml.automl: 09-17 11:29:25] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 11:29:38] {3072} INFO -  at 45.5s,	estimator xgboost's best error=5.3763,	best estimator xgboost's best error=5.3763
[flaml.automl: 09-17 11:29:38] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 11:29:45] {3072} INFO -  at 52.6s,	estimator xgboost's best error=5.3763,	best estimator xgboost's best error=5.3763
[flaml.automl: 09-17 11:29:58] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 11:29:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:29:58] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:29:58] {2637} INFO - Time taken to find the best model: 45.54740595817566
[flaml.automl: 09-17 11:29:58] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54045}
PM2.5(0)最佳损失：-4.376268756419395
PM2.5(0)最好结果：{'pred_time': 7.718965274705974e-06, 'wall_clock_time': 45.54740595817566, 'metric_for_logging': {'pred_time': 7.718965274705974e-06}, 'val_loss': 5.376268756419395, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54045}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 54045, 'experiment_tag': 'exp', 'time_total_s': 12.825764417648315}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8247980656464843
PM2.5(0)的mse=97.49827815364307
PM2.5(0)的mae=5.4169719906630736
PM2.5(0)的mar=0.23610882492753998
总共花费的时间为：66.38
齐齐哈尔市
1779A
1781A
3662A
[flaml.automl: 09-17 11:39:21] {2390} INFO - task = regression
[flaml.automl: 09-17 11:39:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:39:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:39:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:39:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:39:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:39:22] {3025} INFO - Estimated sufficient time budget=12044s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 11:39:22] {3072} INFO -  at 1.4s,	estimator xgboost's best error=11.4888,	best estimator xgboost's best error=11.4888
[flaml.automl: 09-17 11:39:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:39:24] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.9543,	best estimator xgboost's best error=5.9543
[flaml.automl: 09-17 11:39:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:39:25] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.9543,	best estimator xgboost's best error=5.9543
[flaml.automl: 09-17 11:39:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:39:35] {3072} INFO -  at 14.6s,	estimator xgboost's best error=5.9543,	best estimator xgboost's best error=5.9543
[flaml.automl: 09-17 11:39:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:39:36] {3072} INFO -  at 15.7s,	estimator xgboost's best error=5.1181,	best estimator xgboost's best error=5.1181
[flaml.automl: 09-17 11:39:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:39:38] {3072} INFO -  at 17.3s,	estimator xgboost's best error=4.5490,	best estimator xgboost's best error=4.5490
[flaml.automl: 09-17 11:39:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:39:40] {3072} INFO -  at 18.9s,	estimator xgboost's best error=4.2284,	best estimator xgboost's best error=4.2284
[flaml.automl: 09-17 11:39:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:39:42] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.2284,	best estimator xgboost's best error=4.2284
[flaml.automl: 09-17 11:39:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:39:44] {3072} INFO -  at 23.2s,	estimator xgboost's best error=4.2284,	best estimator xgboost's best error=4.2284
[flaml.automl: 09-17 11:39:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:39:47] {3072} INFO -  at 26.2s,	estimator xgboost's best error=3.8045,	best estimator xgboost's best error=3.8045
[flaml.automl: 09-17 11:39:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:39:49] {3072} INFO -  at 27.9s,	estimator xgboost's best error=3.8045,	best estimator xgboost's best error=3.8045
[flaml.automl: 09-17 11:39:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:39:50] {3072} INFO -  at 29.0s,	estimator xgboost's best error=3.8045,	best estimator xgboost's best error=3.8045
[flaml.automl: 09-17 11:39:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:40:03] {3072} INFO -  at 42.7s,	estimator xgboost's best error=3.7016,	best estimator xgboost's best error=3.7016
[flaml.automl: 09-17 11:40:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:40:21] {3072} INFO -  at 59.9s,	estimator xgboost's best error=3.5176,	best estimator xgboost's best error=3.5176
[flaml.automl: 09-17 11:40:45] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-17 11:40:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:40:45] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:40:45] {2637} INFO - Time taken to find the best model: 59.899712562561035
[flaml.automl: 09-17 11:40:45] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-2.5175677813633435
PM2.5(0)最好结果：{'pred_time': 1.1177132501968456e-05, 'wall_clock_time': 59.899712562561035, 'metric_for_logging': {'pred_time': 1.1177132501968456e-05}, 'val_loss': 3.5175677813633435, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 17.191638231277466}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8807457232407145
PM2.5(0)的mse=26.025150716094124
PM2.5(0)的mae=3.447756834455985
PM2.5(0)的mar=0.3115237240173911
总共花费的时间为：84.34
牡丹江市
1784A
1785A
1786A
1787A
[flaml.automl: 09-17 11:52:49] {2390} INFO - task = regression
[flaml.automl: 09-17 11:52:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:52:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:52:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:52:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:52:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:52:50] {3025} INFO - Estimated sufficient time budget=12118s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 11:52:50] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.9371,	best estimator xgboost's best error=15.9371
[flaml.automl: 09-17 11:52:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:52:52] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.9777,	best estimator xgboost's best error=7.9777
[flaml.automl: 09-17 11:52:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:52:53] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.9777,	best estimator xgboost's best error=7.9777
[flaml.automl: 09-17 11:52:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:53:03] {3072} INFO -  at 14.7s,	estimator xgboost's best error=7.9777,	best estimator xgboost's best error=7.9777
[flaml.automl: 09-17 11:53:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:53:05] {3072} INFO -  at 15.9s,	estimator xgboost's best error=6.0331,	best estimator xgboost's best error=6.0331
[flaml.automl: 09-17 11:53:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:53:06] {3072} INFO -  at 17.5s,	estimator xgboost's best error=6.0331,	best estimator xgboost's best error=6.0331
[flaml.automl: 09-17 11:53:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:53:08] {3072} INFO -  at 19.2s,	estimator xgboost's best error=5.4205,	best estimator xgboost's best error=5.4205
[flaml.automl: 09-17 11:53:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:53:11] {3072} INFO -  at 21.9s,	estimator xgboost's best error=5.4205,	best estimator xgboost's best error=5.4205
[flaml.automl: 09-17 11:53:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:53:12] {3072} INFO -  at 23.6s,	estimator xgboost's best error=5.4205,	best estimator xgboost's best error=5.4205
[flaml.automl: 09-17 11:53:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:53:15] {3072} INFO -  at 26.6s,	estimator xgboost's best error=5.3365,	best estimator xgboost's best error=5.3365
[flaml.automl: 09-17 11:53:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:53:17] {3072} INFO -  at 28.3s,	estimator xgboost's best error=5.3365,	best estimator xgboost's best error=5.3365
[flaml.automl: 09-17 11:53:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:53:18] {3072} INFO -  at 29.5s,	estimator xgboost's best error=5.3365,	best estimator xgboost's best error=5.3365
[flaml.automl: 09-17 11:53:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:53:32] {3072} INFO -  at 43.1s,	estimator xgboost's best error=5.0638,	best estimator xgboost's best error=5.0638
[flaml.automl: 09-17 11:53:32] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:53:48] {3072} INFO -  at 59.6s,	estimator xgboost's best error=4.8989,	best estimator xgboost's best error=4.8989
[flaml.automl: 09-17 11:54:12] {3335} INFO - retrain xgboost for 24.0s
[flaml.automl: 09-17 11:54:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.3638133431214387,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.029920148019616434, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.007290129763188211, scale_pos_weight=1,
             subsample=0.840665579419843, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:54:12] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:54:12] {2637} INFO - Time taken to find the best model: 59.61466598510742
[flaml.automl: 09-17 11:54:12] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.029920148019616434, 'learning_rate': 0.3638133431214387, 'subsample': 0.840665579419843, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.007290129763188211}
PM2.5(0)最佳损失：-3.898921944246454
PM2.5(0)最好结果：{'pred_time': 9.132536117639918e-06, 'wall_clock_time': 59.61466598510742, 'metric_for_logging': {'pred_time': 9.132536117639918e-06}, 'val_loss': 4.898921944246454, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.029920148019616434, 'learning_rate': 0.3638133431214387, 'subsample': 0.840665579419843, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.007290129763188211}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.029920148019616434, 'config/learning_rate': 0.3638133431214387, 'config/subsample': 0.840665579419843, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.007290129763188211, 'experiment_tag': 'exp', 'time_total_s': 16.501887798309326}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.3638133431214387,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.029920148019616434, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.007290129763188211, scale_pos_weight=1,
             subsample=0.840665579419843, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8338054242572841
PM2.5(0)的mse=67.49849554169626
PM2.5(0)的mae=4.986623320179901
PM2.5(0)的mar=0.24247032485020914
总共花费的时间为：84.20
大庆市
1789A
1790A
1792A
1793A
3481A
[flaml.automl: 09-17 12:09:22] {2390} INFO - task = regression
[flaml.automl: 09-17 12:09:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:09:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:09:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:09:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:09:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:09:24] {3025} INFO - Estimated sufficient time budget=116745s. Estimated necessary time budget=117s.
[flaml.automl: 09-17 12:09:24] {3072} INFO -  at 2.5s,	estimator xgboost's best error=15.6892,	best estimator xgboost's best error=15.6892
[flaml.automl: 09-17 12:09:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:09:27] {3072} INFO -  at 5.6s,	estimator xgboost's best error=8.0821,	best estimator xgboost's best error=8.0821
[flaml.automl: 09-17 12:09:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:09:30] {3072} INFO -  at 7.7s,	estimator xgboost's best error=8.0821,	best estimator xgboost's best error=8.0821
[flaml.automl: 09-17 12:09:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:09:34] {3072} INFO -  at 11.7s,	estimator xgboost's best error=8.0821,	best estimator xgboost's best error=8.0821
[flaml.automl: 09-17 12:09:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:09:36] {3072} INFO -  at 14.0s,	estimator xgboost's best error=6.5403,	best estimator xgboost's best error=6.5403
[flaml.automl: 09-17 12:09:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:09:39] {3072} INFO -  at 16.9s,	estimator xgboost's best error=5.9557,	best estimator xgboost's best error=5.9557
[flaml.automl: 09-17 12:09:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:09:42] {3072} INFO -  at 19.7s,	estimator xgboost's best error=5.5490,	best estimator xgboost's best error=5.5490
[flaml.automl: 09-17 12:09:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:09:45] {3072} INFO -  at 22.9s,	estimator xgboost's best error=5.5490,	best estimator xgboost's best error=5.5490
[flaml.automl: 09-17 12:09:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:09:47] {3072} INFO -  at 25.6s,	estimator xgboost's best error=5.5490,	best estimator xgboost's best error=5.5490
[flaml.automl: 09-17 12:09:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:09:50] {3072} INFO -  at 28.3s,	estimator xgboost's best error=5.5490,	best estimator xgboost's best error=5.5490
[flaml.automl: 09-17 12:09:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:09:53] {3072} INFO -  at 30.8s,	estimator xgboost's best error=5.5490,	best estimator xgboost's best error=5.5490
[flaml.automl: 09-17 12:09:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:09:54] {3072} INFO -  at 32.4s,	estimator xgboost's best error=5.5490,	best estimator xgboost's best error=5.5490
[flaml.automl: 09-17 12:09:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:09:55] {3072} INFO -  at 33.6s,	estimator xgboost's best error=5.5490,	best estimator xgboost's best error=5.5490
[flaml.automl: 09-17 12:09:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:10:02] {3072} INFO -  at 40.5s,	estimator xgboost's best error=4.7085,	best estimator xgboost's best error=4.7085
[flaml.automl: 09-17 12:10:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 12:10:15] {3072} INFO -  at 53.3s,	estimator xgboost's best error=4.6336,	best estimator xgboost's best error=4.6336
[flaml.automl: 09-17 12:10:29] {3335} INFO - retrain xgboost for 14.0s
[flaml.automl: 09-17 12:10:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:10:29] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:10:29] {2637} INFO - Time taken to find the best model: 53.33305287361145
[flaml.automl: 09-17 12:10:29] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 52371}
PM2.5(0)最佳损失：-3.6336426767696626
PM2.5(0)最好结果：{'pred_time': 6.80295872114778e-06, 'wall_clock_time': 53.33305287361145, 'metric_for_logging': {'pred_time': 6.80295872114778e-06}, 'val_loss': 4.633642676769663, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 52371}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 52371, 'experiment_tag': 'exp', 'time_total_s': 12.816584587097168}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8667315407046289
PM2.5(0)的mse=49.1450401409118
PM2.5(0)的mae=4.617092693217059
PM2.5(0)的mar=0.34871905637163797
总共花费的时间为：68.34
芜湖市
1795A
1796A
3328A
3465A
3466A
[flaml.automl: 09-17 12:26:36] {2390} INFO - task = regression
[flaml.automl: 09-17 12:26:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:26:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:26:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:26:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:26:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:26:37] {3025} INFO - Estimated sufficient time budget=65562s. Estimated necessary time budget=66s.
[flaml.automl: 09-17 12:26:37] {3072} INFO -  at 1.4s,	estimator xgboost's best error=20.3517,	best estimator xgboost's best error=20.3517
[flaml.automl: 09-17 12:26:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:26:40] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.5314,	best estimator xgboost's best error=9.5314
[flaml.automl: 09-17 12:26:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:26:41] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.5314,	best estimator xgboost's best error=9.5314
[flaml.automl: 09-17 12:26:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:26:46] {3072} INFO -  at 9.6s,	estimator xgboost's best error=9.5314,	best estimator xgboost's best error=9.5314
[flaml.automl: 09-17 12:26:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:26:47] {3072} INFO -  at 10.7s,	estimator xgboost's best error=6.4267,	best estimator xgboost's best error=6.4267
[flaml.automl: 09-17 12:26:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:26:48] {3072} INFO -  at 12.2s,	estimator xgboost's best error=6.4267,	best estimator xgboost's best error=6.4267
[flaml.automl: 09-17 12:26:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:26:50] {3072} INFO -  at 13.9s,	estimator xgboost's best error=4.1866,	best estimator xgboost's best error=4.1866
[flaml.automl: 09-17 12:26:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:26:53] {3072} INFO -  at 16.6s,	estimator xgboost's best error=4.1866,	best estimator xgboost's best error=4.1866
[flaml.automl: 09-17 12:26:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:26:54] {3072} INFO -  at 18.3s,	estimator xgboost's best error=4.1866,	best estimator xgboost's best error=4.1866
[flaml.automl: 09-17 12:26:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:26:57] {3072} INFO -  at 21.3s,	estimator xgboost's best error=4.1866,	best estimator xgboost's best error=4.1866
[flaml.automl: 09-17 12:26:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:26:59] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.1866,	best estimator xgboost's best error=4.1866
[flaml.automl: 09-17 12:26:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:27:00] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.1866,	best estimator xgboost's best error=4.1866
[flaml.automl: 09-17 12:27:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:27:02] {3072} INFO -  at 25.6s,	estimator xgboost's best error=4.1866,	best estimator xgboost's best error=4.1866
[flaml.automl: 09-17 12:27:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:27:11] {3072} INFO -  at 35.4s,	estimator xgboost's best error=3.3925,	best estimator xgboost's best error=3.3925
[flaml.automl: 09-17 12:27:11] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 12:27:35] {3072} INFO -  at 59.1s,	estimator xgboost's best error=3.2811,	best estimator xgboost's best error=3.2811
[flaml.automl: 09-17 12:27:59] {3335} INFO - retrain xgboost for 23.6s
[flaml.automl: 09-17 12:27:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:27:59] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:27:59] {2637} INFO - Time taken to find the best model: 59.06111764907837
[flaml.automl: 09-17 12:27:59] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54850}
PM2.5(0)最佳损失：-2.2810846376067015
PM2.5(0)最好结果：{'pred_time': 1.0591456888150347e-05, 'wall_clock_time': 59.06111764907837, 'metric_for_logging': {'pred_time': 1.0591456888150347e-05}, 'val_loss': 3.2810846376067015, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54850}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 54850, 'experiment_tag': 'exp', 'time_total_s': 23.675622940063477}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.936087917491636
PM2.5(0)的mse=24.75845400208076
PM2.5(0)的mae=3.278710997657387
PM2.5(0)的mar=0.13909818061913665
总共花费的时间为：83.70
马鞍山市
1798A
1800A
3633A
[flaml.automl: 09-17 12:37:42] {2390} INFO - task = regression
[flaml.automl: 09-17 12:37:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:37:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:37:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:37:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:37:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:37:44] {3025} INFO - Estimated sufficient time budget=12079s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 12:37:44] {3072} INFO -  at 1.3s,	estimator xgboost's best error=21.8427,	best estimator xgboost's best error=21.8427
[flaml.automl: 09-17 12:37:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:37:46] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.1804,	best estimator xgboost's best error=10.1804
[flaml.automl: 09-17 12:37:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:37:47] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.1804,	best estimator xgboost's best error=10.1804
[flaml.automl: 09-17 12:37:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:38:04] {3072} INFO -  at 22.1s,	estimator xgboost's best error=10.1804,	best estimator xgboost's best error=10.1804
[flaml.automl: 09-17 12:38:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:38:07] {3072} INFO -  at 24.2s,	estimator xgboost's best error=6.2210,	best estimator xgboost's best error=6.2210
[flaml.automl: 09-17 12:38:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:38:09] {3072} INFO -  at 27.1s,	estimator xgboost's best error=6.2210,	best estimator xgboost's best error=6.2210
[flaml.automl: 09-17 12:38:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:38:13] {3072} INFO -  at 30.4s,	estimator xgboost's best error=4.6002,	best estimator xgboost's best error=4.6002
[flaml.automl: 09-17 12:38:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:38:18] {3072} INFO -  at 35.4s,	estimator xgboost's best error=4.6002,	best estimator xgboost's best error=4.6002
[flaml.automl: 09-17 12:38:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:38:21] {3072} INFO -  at 38.3s,	estimator xgboost's best error=4.6002,	best estimator xgboost's best error=4.6002
[flaml.automl: 09-17 12:38:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:38:26] {3072} INFO -  at 43.9s,	estimator xgboost's best error=4.6002,	best estimator xgboost's best error=4.6002
[flaml.automl: 09-17 12:38:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:38:29] {3072} INFO -  at 46.7s,	estimator xgboost's best error=4.5799,	best estimator xgboost's best error=4.5799
[flaml.automl: 09-17 12:38:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:38:31] {3072} INFO -  at 48.8s,	estimator xgboost's best error=4.5799,	best estimator xgboost's best error=4.5799
[flaml.automl: 09-17 12:38:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:38:41] {3072} INFO -  at 58.4s,	estimator xgboost's best error=3.9453,	best estimator xgboost's best error=3.9453
[flaml.automl: 09-17 12:38:55] {3335} INFO - retrain xgboost for 14.4s
[flaml.automl: 09-17 12:38:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:38:55] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:38:55] {2637} INFO - Time taken to find the best model: 58.44229054450989
[flaml.automl: 09-17 12:38:55] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
PM2.5(0)最佳损失：-2.9453419018268057
PM2.5(0)最好结果：{'pred_time': 3.444235544490497e-05, 'wall_clock_time': 58.44229054450989, 'metric_for_logging': {'pred_time': 3.444235544490497e-05}, 'val_loss': 3.9453419018268057, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 9.659080505371094}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9295827058496061
PM2.5(0)的mse=30.658634806751767
PM2.5(0)的mae=3.669446338306774
PM2.5(0)的mar=0.13316211244409373
总共花费的时间为：73.42
九江市
1803A
1804A
1805A
1806A
1810A
[flaml.automl: 09-17 12:54:55] {2390} INFO - task = regression
[flaml.automl: 09-17 12:54:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:54:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:54:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:54:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:54:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:54:57] {3025} INFO - Estimated sufficient time budget=111575s. Estimated necessary time budget=112s.
[flaml.automl: 09-17 12:54:57] {3072} INFO -  at 2.4s,	estimator xgboost's best error=17.7051,	best estimator xgboost's best error=17.7051
[flaml.automl: 09-17 12:54:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:55:01] {3072} INFO -  at 6.4s,	estimator xgboost's best error=8.6425,	best estimator xgboost's best error=8.6425
[flaml.automl: 09-17 12:55:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:55:04] {3072} INFO -  at 8.6s,	estimator xgboost's best error=8.6425,	best estimator xgboost's best error=8.6425
[flaml.automl: 09-17 12:55:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:55:08] {3072} INFO -  at 12.7s,	estimator xgboost's best error=8.6425,	best estimator xgboost's best error=8.6425
[flaml.automl: 09-17 12:55:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:55:10] {3072} INFO -  at 14.8s,	estimator xgboost's best error=6.5384,	best estimator xgboost's best error=6.5384
[flaml.automl: 09-17 12:55:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:55:13] {3072} INFO -  at 17.8s,	estimator xgboost's best error=6.5384,	best estimator xgboost's best error=6.5384
[flaml.automl: 09-17 12:55:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:55:16] {3072} INFO -  at 20.8s,	estimator xgboost's best error=4.6376,	best estimator xgboost's best error=4.6376
[flaml.automl: 09-17 12:55:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:55:19] {3072} INFO -  at 24.3s,	estimator xgboost's best error=4.6376,	best estimator xgboost's best error=4.6376
[flaml.automl: 09-17 12:55:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:55:22] {3072} INFO -  at 27.4s,	estimator xgboost's best error=4.6376,	best estimator xgboost's best error=4.6376
[flaml.automl: 09-17 12:55:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:55:25] {3072} INFO -  at 30.2s,	estimator xgboost's best error=4.6376,	best estimator xgboost's best error=4.6376
[flaml.automl: 09-17 12:55:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:55:27] {3072} INFO -  at 32.1s,	estimator xgboost's best error=4.6376,	best estimator xgboost's best error=4.6376
[flaml.automl: 09-17 12:55:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:55:32] {3072} INFO -  at 37.0s,	estimator xgboost's best error=4.5821,	best estimator xgboost's best error=4.5821
[flaml.automl: 09-17 12:55:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:55:35] {3072} INFO -  at 40.4s,	estimator xgboost's best error=4.5821,	best estimator xgboost's best error=4.5821
[flaml.automl: 09-17 12:55:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:55:47] {3072} INFO -  at 52.2s,	estimator xgboost's best error=4.1117,	best estimator xgboost's best error=4.1117
[flaml.automl: 09-17 12:55:58] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-17 12:55:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:55:58] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:55:58] {2637} INFO - Time taken to find the best model: 52.24220085144043
[flaml.automl: 09-17 12:55:58] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 51651}
PM2.5(0)最佳损失：-3.111708702183617
PM2.5(0)最好结果：{'pred_time': 9.583645860665682e-06, 'wall_clock_time': 52.24220085144043, 'metric_for_logging': {'pred_time': 9.583645860665682e-06}, 'val_loss': 4.111708702183617, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 51651}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 51651, 'experiment_tag': 'exp', 'time_total_s': 11.86349368095398}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9097464902800886
PM2.5(0)的mse=31.10904013635748
PM2.5(0)的mae=4.052479258334355
PM2.5(0)的mar=0.24710902444217142
总共花费的时间为：63.82
洛阳市
1815A
1817A
3341A
3593A
3635A
3636A
[flaml.automl: 09-17 13:14:38] {2390} INFO - task = regression
[flaml.automl: 09-17 13:14:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:14:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:14:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:14:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:14:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:14:42] {3025} INFO - Estimated sufficient time budget=225799s. Estimated necessary time budget=226s.
[flaml.automl: 09-17 13:14:42] {3072} INFO -  at 3.9s,	estimator xgboost's best error=27.3520,	best estimator xgboost's best error=27.3520
[flaml.automl: 09-17 13:14:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:14:45] {3072} INFO -  at 7.5s,	estimator xgboost's best error=14.6596,	best estimator xgboost's best error=14.6596
[flaml.automl: 09-17 13:14:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:14:47] {3072} INFO -  at 9.7s,	estimator xgboost's best error=14.6596,	best estimator xgboost's best error=14.6596
[flaml.automl: 09-17 13:14:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:14:51] {3072} INFO -  at 12.8s,	estimator xgboost's best error=14.6596,	best estimator xgboost's best error=14.6596
[flaml.automl: 09-17 13:14:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:14:53] {3072} INFO -  at 14.8s,	estimator xgboost's best error=9.2622,	best estimator xgboost's best error=9.2622
[flaml.automl: 09-17 13:14:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:14:55] {3072} INFO -  at 17.6s,	estimator xgboost's best error=9.2010,	best estimator xgboost's best error=9.2010
[flaml.automl: 09-17 13:14:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:14:58] {3072} INFO -  at 20.5s,	estimator xgboost's best error=6.3862,	best estimator xgboost's best error=6.3862
[flaml.automl: 09-17 13:14:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:15:01] {3072} INFO -  at 23.1s,	estimator xgboost's best error=6.3862,	best estimator xgboost's best error=6.3862
[flaml.automl: 09-17 13:15:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:15:03] {3072} INFO -  at 24.7s,	estimator xgboost's best error=6.3862,	best estimator xgboost's best error=6.3862
[flaml.automl: 09-17 13:15:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:15:05] {3072} INFO -  at 27.3s,	estimator xgboost's best error=5.6399,	best estimator xgboost's best error=5.6399
[flaml.automl: 09-17 13:15:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:15:07] {3072} INFO -  at 29.0s,	estimator xgboost's best error=5.6399,	best estimator xgboost's best error=5.6399
[flaml.automl: 09-17 13:15:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:15:08] {3072} INFO -  at 30.1s,	estimator xgboost's best error=5.6399,	best estimator xgboost's best error=5.6399
[flaml.automl: 09-17 13:15:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:15:10] {3072} INFO -  at 32.1s,	estimator xgboost's best error=5.6399,	best estimator xgboost's best error=5.6399
[flaml.automl: 09-17 13:15:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:15:12] {3072} INFO -  at 34.0s,	estimator xgboost's best error=5.6399,	best estimator xgboost's best error=5.6399
[flaml.automl: 09-17 13:15:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 13:15:15] {3072} INFO -  at 37.1s,	estimator xgboost's best error=4.9630,	best estimator xgboost's best error=4.9630
[flaml.automl: 09-17 13:15:15] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 13:15:20] {3072} INFO -  at 42.0s,	estimator xgboost's best error=4.9630,	best estimator xgboost's best error=4.9630
[flaml.automl: 09-17 13:15:20] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 13:15:22] {3072} INFO -  at 43.9s,	estimator xgboost's best error=4.7583,	best estimator xgboost's best error=4.7583
[flaml.automl: 09-17 13:15:22] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 13:15:23] {3072} INFO -  at 45.6s,	estimator xgboost's best error=4.7583,	best estimator xgboost's best error=4.7583
[flaml.automl: 09-17 13:15:23] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 13:15:28] {3072} INFO -  at 50.0s,	estimator xgboost's best error=4.2393,	best estimator xgboost's best error=4.2393
[flaml.automl: 09-17 13:15:28] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 13:15:29] {3072} INFO -  at 51.7s,	estimator xgboost's best error=4.2393,	best estimator xgboost's best error=4.2393
[flaml.automl: 09-17 13:15:29] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 13:15:37] {3072} INFO -  at 59.0s,	estimator xgboost's best error=3.7733,	best estimator xgboost's best error=3.7733
[flaml.automl: 09-17 13:15:53] {3335} INFO - retrain xgboost for 16.5s
[flaml.automl: 09-17 13:15:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:15:53] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:15:53] {2637} INFO - Time taken to find the best model: 58.99916195869446
[flaml.automl: 09-17 13:15:53] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 64661}
PM2.5(0)最佳损失：-2.7733343714048404
PM2.5(0)最好结果：{'pred_time': 5.836725732727688e-06, 'wall_clock_time': 58.99916195869446, 'metric_for_logging': {'pred_time': 5.836725732727688e-06}, 'val_loss': 3.7733343714048404, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 64661}, 'config/n_estimators': 13, 'config/max_leaves': 18, 'config/min_child_weight': 0.002212987346630184, 'config/learning_rate': 1.0, 'config/subsample': 0.9736629142498529, 'config/colsample_bylevel': 0.8717925041332704, 'config/colsample_bytree': 0.9857450191529009, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.04619111740498511, 'config/FLAML_sample_size': 64661, 'experiment_tag': 'exp', 'time_total_s': 7.296499252319336}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9604526376770981
PM2.5(0)的mse=37.157460795862775
PM2.5(0)的mae=3.715545799369936
PM2.5(0)的mar=0.12532092004407483
总共花费的时间为：76.66
安阳市
1818A
1819A
3141A
3669A
[flaml.automl: 09-17 13:27:59] {2390} INFO - task = regression
[flaml.automl: 09-17 13:27:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:27:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:27:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:27:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:27:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:28:02] {3025} INFO - Estimated sufficient time budget=89759s. Estimated necessary time budget=90s.
[flaml.automl: 09-17 13:28:02] {3072} INFO -  at 2.4s,	estimator xgboost's best error=29.0204,	best estimator xgboost's best error=29.0204
[flaml.automl: 09-17 13:28:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:28:06] {3072} INFO -  at 6.3s,	estimator xgboost's best error=13.7543,	best estimator xgboost's best error=13.7543
[flaml.automl: 09-17 13:28:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:28:08] {3072} INFO -  at 8.5s,	estimator xgboost's best error=13.7543,	best estimator xgboost's best error=13.7543
[flaml.automl: 09-17 13:28:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:28:13] {3072} INFO -  at 13.6s,	estimator xgboost's best error=13.7543,	best estimator xgboost's best error=13.7543
[flaml.automl: 09-17 13:28:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:28:15] {3072} INFO -  at 15.7s,	estimator xgboost's best error=10.4045,	best estimator xgboost's best error=10.4045
[flaml.automl: 09-17 13:28:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:28:18] {3072} INFO -  at 18.7s,	estimator xgboost's best error=10.4045,	best estimator xgboost's best error=10.4045
[flaml.automl: 09-17 13:28:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:28:21] {3072} INFO -  at 21.8s,	estimator xgboost's best error=5.9739,	best estimator xgboost's best error=5.9739
[flaml.automl: 09-17 13:28:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:28:25] {3072} INFO -  at 26.0s,	estimator xgboost's best error=5.9739,	best estimator xgboost's best error=5.9739
[flaml.automl: 09-17 13:28:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:28:28] {3072} INFO -  at 28.9s,	estimator xgboost's best error=5.9739,	best estimator xgboost's best error=5.9739
[flaml.automl: 09-17 13:28:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:28:31] {3072} INFO -  at 32.2s,	estimator xgboost's best error=5.9739,	best estimator xgboost's best error=5.9739
[flaml.automl: 09-17 13:28:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:28:34] {3072} INFO -  at 34.9s,	estimator xgboost's best error=5.9739,	best estimator xgboost's best error=5.9739
[flaml.automl: 09-17 13:28:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:28:37] {3072} INFO -  at 38.1s,	estimator xgboost's best error=5.9739,	best estimator xgboost's best error=5.9739
[flaml.automl: 09-17 13:28:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:28:39] {3072} INFO -  at 40.2s,	estimator xgboost's best error=5.9739,	best estimator xgboost's best error=5.9739
[flaml.automl: 09-17 13:28:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:28:52] {3072} INFO -  at 53.2s,	estimator xgboost's best error=4.6381,	best estimator xgboost's best error=4.6381
[flaml.automl: 09-17 13:29:05] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 13:29:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:29:05] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:29:05] {2637} INFO - Time taken to find the best model: 53.23889183998108
[flaml.automl: 09-17 13:29:05] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41245}
PM2.5(0)最佳损失：-3.6380660167960084
PM2.5(0)最好结果：{'pred_time': 1.4107325647742422e-05, 'wall_clock_time': 53.23889183998108, 'metric_for_logging': {'pred_time': 1.4107325647742422e-05}, 'val_loss': 4.638066016796008, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41245}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 41245, 'experiment_tag': 'exp', 'time_total_s': 12.989388942718506}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.947509152151496
PM2.5(0)的mse=55.98856971611196
PM2.5(0)的mae=4.784333283519145
PM2.5(0)的mar=0.1502590560449581
总共花费的时间为：66.80
开封市
3210A
3473A
3592A
[flaml.automl: 09-17 13:40:00] {2390} INFO - task = regression
[flaml.automl: 09-17 13:40:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:40:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:40:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:40:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:40:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:40:02] {3025} INFO - Estimated sufficient time budget=22612s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 13:40:02] {3072} INFO -  at 2.5s,	estimator xgboost's best error=28.7277,	best estimator xgboost's best error=28.7277
[flaml.automl: 09-17 13:40:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:40:06] {3072} INFO -  at 6.4s,	estimator xgboost's best error=13.5351,	best estimator xgboost's best error=13.5351
[flaml.automl: 09-17 13:40:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:40:08] {3072} INFO -  at 8.7s,	estimator xgboost's best error=13.5351,	best estimator xgboost's best error=13.5351
[flaml.automl: 09-17 13:40:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:40:26] {3072} INFO -  at 27.1s,	estimator xgboost's best error=13.5351,	best estimator xgboost's best error=13.5351
[flaml.automl: 09-17 13:40:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:40:29] {3072} INFO -  at 29.2s,	estimator xgboost's best error=9.7671,	best estimator xgboost's best error=9.7671
[flaml.automl: 09-17 13:40:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:40:31] {3072} INFO -  at 32.1s,	estimator xgboost's best error=9.6472,	best estimator xgboost's best error=9.6472
[flaml.automl: 09-17 13:40:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:40:34] {3072} INFO -  at 35.0s,	estimator xgboost's best error=6.9563,	best estimator xgboost's best error=6.9563
[flaml.automl: 09-17 13:40:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:40:39] {3072} INFO -  at 40.0s,	estimator xgboost's best error=6.9563,	best estimator xgboost's best error=6.9563
[flaml.automl: 09-17 13:40:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:40:42] {3072} INFO -  at 42.9s,	estimator xgboost's best error=6.5035,	best estimator xgboost's best error=6.5035
[flaml.automl: 09-17 13:40:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:40:48] {3072} INFO -  at 48.5s,	estimator xgboost's best error=5.9786,	best estimator xgboost's best error=5.9786
[flaml.automl: 09-17 13:40:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:40:50] {3072} INFO -  at 50.6s,	estimator xgboost's best error=5.9786,	best estimator xgboost's best error=5.9786
[flaml.automl: 09-17 13:40:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:40:59] {3072} INFO -  at 59.3s,	estimator xgboost's best error=4.6706,	best estimator xgboost's best error=4.6706
[flaml.automl: 09-17 13:41:12] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-17 13:41:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:41:12] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:41:12] {2637} INFO - Time taken to find the best model: 59.34689140319824
[flaml.automl: 09-17 13:41:12] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
PM2.5(0)最佳损失：-3.670630244670593
PM2.5(0)最好结果：{'pred_time': 2.043998737707924e-05, 'wall_clock_time': 59.34689140319824, 'metric_for_logging': {'pred_time': 2.043998737707924e-05}, 'val_loss': 4.670630244670593, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 8.726721048355103}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9557488149310078
PM2.5(0)的mse=49.36320872650331
PM2.5(0)的mae=4.4387168920428985
PM2.5(0)的mar=0.14360186677709363
总共花费的时间为：73.51
焦作市
1830A
3169A
3335A
3336A
3477A
[flaml.automl: 09-17 13:58:48] {2390} INFO - task = regression
[flaml.automl: 09-17 13:58:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:58:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:58:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:58:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:58:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:58:49] {3025} INFO - Estimated sufficient time budget=61239s. Estimated necessary time budget=61s.
[flaml.automl: 09-17 13:58:49] {3072} INFO -  at 1.4s,	estimator xgboost's best error=28.1380,	best estimator xgboost's best error=28.1380
[flaml.automl: 09-17 13:58:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:58:51] {3072} INFO -  at 3.5s,	estimator xgboost's best error=13.3729,	best estimator xgboost's best error=13.3729
[flaml.automl: 09-17 13:58:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:58:52] {3072} INFO -  at 4.8s,	estimator xgboost's best error=13.3729,	best estimator xgboost's best error=13.3729
[flaml.automl: 09-17 13:58:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:58:57] {3072} INFO -  at 9.4s,	estimator xgboost's best error=13.3729,	best estimator xgboost's best error=13.3729
[flaml.automl: 09-17 13:58:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:59:00] {3072} INFO -  at 12.5s,	estimator xgboost's best error=9.3318,	best estimator xgboost's best error=9.3318
[flaml.automl: 09-17 13:59:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:59:04] {3072} INFO -  at 16.9s,	estimator xgboost's best error=9.3318,	best estimator xgboost's best error=9.3318
[flaml.automl: 09-17 13:59:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:59:08] {3072} INFO -  at 20.3s,	estimator xgboost's best error=8.9100,	best estimator xgboost's best error=8.9100
[flaml.automl: 09-17 13:59:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:59:10] {3072} INFO -  at 23.1s,	estimator xgboost's best error=8.9100,	best estimator xgboost's best error=8.9100
[flaml.automl: 09-17 13:59:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:59:14] {3072} INFO -  at 26.4s,	estimator xgboost's best error=8.9100,	best estimator xgboost's best error=8.9100
[flaml.automl: 09-17 13:59:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:59:17] {3072} INFO -  at 29.2s,	estimator xgboost's best error=8.9100,	best estimator xgboost's best error=8.9100
[flaml.automl: 09-17 13:59:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:59:19] {3072} INFO -  at 32.0s,	estimator xgboost's best error=8.9100,	best estimator xgboost's best error=8.9100
[flaml.automl: 09-17 13:59:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:59:24] {3072} INFO -  at 36.8s,	estimator xgboost's best error=6.5521,	best estimator xgboost's best error=6.5521
[flaml.automl: 09-17 13:59:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:59:28] {3072} INFO -  at 40.1s,	estimator xgboost's best error=6.5521,	best estimator xgboost's best error=6.5521
[flaml.automl: 09-17 13:59:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:59:47] {3072} INFO -  at 59.7s,	estimator xgboost's best error=5.5728,	best estimator xgboost's best error=5.5728
[flaml.automl: 09-17 14:00:00] {3335} INFO - retrain xgboost for 13.4s
[flaml.automl: 09-17 14:00:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:00:00] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:00:00] {2637} INFO - Time taken to find the best model: 59.65400433540344
[flaml.automl: 09-17 14:00:00] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 51018}
PM2.5(0)最佳损失：-4.572761615299636
PM2.5(0)最好结果：{'pred_time': 2.0675167959189367e-05, 'wall_clock_time': 59.65400433540344, 'metric_for_logging': {'pred_time': 2.0675167959189367e-05}, 'val_loss': 5.572761615299636, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 51018}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 51018, 'experiment_tag': 'exp', 'time_total_s': 19.55704164505005}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9144622847304146
PM2.5(0)的mse=77.30104669325628
PM2.5(0)的mae=5.765590234945115
PM2.5(0)的mar=0.1717751009895401
总共花费的时间为：73.96
平顶山市
1833A
3204A
3594A
[flaml.automl: 09-17 14:09:58] {2390} INFO - task = regression
[flaml.automl: 09-17 14:09:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:09:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:09:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:09:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:09:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:10:00] {3025} INFO - Estimated sufficient time budget=21619s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 14:10:00] {3072} INFO -  at 2.3s,	estimator xgboost's best error=25.8757,	best estimator xgboost's best error=25.8757
[flaml.automl: 09-17 14:10:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:10:04] {3072} INFO -  at 6.1s,	estimator xgboost's best error=12.4344,	best estimator xgboost's best error=12.4344
[flaml.automl: 09-17 14:10:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:10:06] {3072} INFO -  at 8.3s,	estimator xgboost's best error=12.4344,	best estimator xgboost's best error=12.4344
[flaml.automl: 09-17 14:10:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:10:25] {3072} INFO -  at 26.8s,	estimator xgboost's best error=12.4344,	best estimator xgboost's best error=12.4344
[flaml.automl: 09-17 14:10:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:10:27] {3072} INFO -  at 28.9s,	estimator xgboost's best error=8.7823,	best estimator xgboost's best error=8.7823
[flaml.automl: 09-17 14:10:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:10:30] {3072} INFO -  at 31.8s,	estimator xgboost's best error=8.7823,	best estimator xgboost's best error=8.7823
[flaml.automl: 09-17 14:10:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:10:33] {3072} INFO -  at 35.1s,	estimator xgboost's best error=6.3458,	best estimator xgboost's best error=6.3458
[flaml.automl: 09-17 14:10:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:10:38] {3072} INFO -  at 40.1s,	estimator xgboost's best error=6.3458,	best estimator xgboost's best error=6.3458
[flaml.automl: 09-17 14:10:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:10:41] {3072} INFO -  at 43.2s,	estimator xgboost's best error=6.3458,	best estimator xgboost's best error=6.3458
[flaml.automl: 09-17 14:10:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:10:47] {3072} INFO -  at 48.8s,	estimator xgboost's best error=6.3458,	best estimator xgboost's best error=6.3458
[flaml.automl: 09-17 14:10:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:10:50] {3072} INFO -  at 51.9s,	estimator xgboost's best error=6.1954,	best estimator xgboost's best error=6.1954
[flaml.automl: 09-17 14:10:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:10:52] {3072} INFO -  at 54.1s,	estimator xgboost's best error=6.1954,	best estimator xgboost's best error=6.1954
[flaml.automl: 09-17 14:10:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:10:57] {3072} INFO -  at 59.3s,	estimator xgboost's best error=5.6833,	best estimator xgboost's best error=5.6833
[flaml.automl: 09-17 14:11:08] {3335} INFO - retrain xgboost for 10.9s
[flaml.automl: 09-17 14:11:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:11:08] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:11:08] {2637} INFO - Time taken to find the best model: 59.336353063583374
[flaml.automl: 09-17 14:11:08] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
PM2.5(0)最佳损失：-4.6832920014469686
PM2.5(0)最好结果：{'pred_time': 2.6167915573020647e-05, 'wall_clock_time': 59.336353063583374, 'metric_for_logging': {'pred_time': 2.6167915573020647e-05}, 'val_loss': 5.6832920014469686, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 5.255505800247192}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9208193533668987
PM2.5(0)的mse=66.8092344525708
PM2.5(0)的mae=5.434496706131442
PM2.5(0)的mar=0.19344294935858575
总共花费的时间为：70.72
三门峡市
1835A
1836A
1838A
3598A
[flaml.automl: 09-17 14:24:18] {2390} INFO - task = regression
[flaml.automl: 09-17 14:24:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:24:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:24:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:24:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:24:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:24:20] {3025} INFO - Estimated sufficient time budget=90620s. Estimated necessary time budget=91s.
[flaml.automl: 09-17 14:24:20] {3072} INFO -  at 2.4s,	estimator xgboost's best error=27.6016,	best estimator xgboost's best error=27.6016
[flaml.automl: 09-17 14:24:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:24:25] {3072} INFO -  at 7.3s,	estimator xgboost's best error=13.1492,	best estimator xgboost's best error=13.1492
[flaml.automl: 09-17 14:24:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:24:28] {3072} INFO -  at 10.6s,	estimator xgboost's best error=13.1492,	best estimator xgboost's best error=13.1492
[flaml.automl: 09-17 14:24:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:24:33] {3072} INFO -  at 15.2s,	estimator xgboost's best error=13.1492,	best estimator xgboost's best error=13.1492
[flaml.automl: 09-17 14:24:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:24:36] {3072} INFO -  at 18.3s,	estimator xgboost's best error=9.3750,	best estimator xgboost's best error=9.3750
[flaml.automl: 09-17 14:24:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:24:40] {3072} INFO -  at 22.5s,	estimator xgboost's best error=9.3750,	best estimator xgboost's best error=9.3750
[flaml.automl: 09-17 14:24:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:24:44] {3072} INFO -  at 26.4s,	estimator xgboost's best error=6.8467,	best estimator xgboost's best error=6.8467
[flaml.automl: 09-17 14:24:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:24:47] {3072} INFO -  at 29.2s,	estimator xgboost's best error=6.8467,	best estimator xgboost's best error=6.8467
[flaml.automl: 09-17 14:24:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:24:50] {3072} INFO -  at 32.2s,	estimator xgboost's best error=6.8467,	best estimator xgboost's best error=6.8467
[flaml.automl: 09-17 14:24:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:24:52] {3072} INFO -  at 34.9s,	estimator xgboost's best error=6.8467,	best estimator xgboost's best error=6.8467
[flaml.automl: 09-17 14:24:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:24:55] {3072} INFO -  at 37.4s,	estimator xgboost's best error=6.8467,	best estimator xgboost's best error=6.8467
[flaml.automl: 09-17 14:24:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:24:59] {3072} INFO -  at 42.0s,	estimator xgboost's best error=5.8606,	best estimator xgboost's best error=5.8606
[flaml.automl: 09-17 14:24:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:25:02] {3072} INFO -  at 45.0s,	estimator xgboost's best error=5.8606,	best estimator xgboost's best error=5.8606
[flaml.automl: 09-17 14:25:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:25:17] {3072} INFO -  at 59.6s,	estimator xgboost's best error=4.6958,	best estimator xgboost's best error=4.6958
[flaml.automl: 09-17 14:25:32] {3335} INFO - retrain xgboost for 14.6s
[flaml.automl: 09-17 14:25:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:25:32] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:25:32] {2637} INFO - Time taken to find the best model: 59.6190299987793
[flaml.automl: 09-17 14:25:32] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41445}
PM2.5(0)最佳损失：-3.6958388795552377
PM2.5(0)最好结果：{'pred_time': 2.735642216751811e-05, 'wall_clock_time': 59.6190299987793, 'metric_for_logging': {'pred_time': 2.735642216751811e-05}, 'val_loss': 4.695838879555238, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41445}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 41445, 'experiment_tag': 'exp', 'time_total_s': 14.594398498535156}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9542823649212544
PM2.5(0)的mse=42.3785668987799
PM2.5(0)的mae=4.343708674237205
PM2.5(0)的mar=0.1603564509249475
总共花费的时间为：74.98
宜昌市
1840A
1841A
1842A
1843A
3546A
3653A
[flaml.automl: 09-17 14:45:35] {2390} INFO - task = regression
[flaml.automl: 09-17 14:45:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:45:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:45:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:45:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:45:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:45:37] {3025} INFO - Estimated sufficient time budget=77756s. Estimated necessary time budget=78s.
[flaml.automl: 09-17 14:45:37] {3072} INFO -  at 1.5s,	estimator xgboost's best error=22.5122,	best estimator xgboost's best error=22.5122
[flaml.automl: 09-17 14:45:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:45:39] {3072} INFO -  at 3.6s,	estimator xgboost's best error=10.9033,	best estimator xgboost's best error=10.9033
[flaml.automl: 09-17 14:45:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:45:40] {3072} INFO -  at 4.8s,	estimator xgboost's best error=10.9033,	best estimator xgboost's best error=10.9033
[flaml.automl: 09-17 14:45:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:45:44] {3072} INFO -  at 8.6s,	estimator xgboost's best error=10.9033,	best estimator xgboost's best error=10.9033
[flaml.automl: 09-17 14:45:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:45:46] {3072} INFO -  at 10.5s,	estimator xgboost's best error=7.6856,	best estimator xgboost's best error=7.6856
[flaml.automl: 09-17 14:45:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:45:49] {3072} INFO -  at 13.4s,	estimator xgboost's best error=7.6856,	best estimator xgboost's best error=7.6856
[flaml.automl: 09-17 14:45:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:45:52] {3072} INFO -  at 16.5s,	estimator xgboost's best error=5.4537,	best estimator xgboost's best error=5.4537
[flaml.automl: 09-17 14:45:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:45:54] {3072} INFO -  at 19.2s,	estimator xgboost's best error=5.4537,	best estimator xgboost's best error=5.4537
[flaml.automl: 09-17 14:45:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:45:57] {3072} INFO -  at 21.7s,	estimator xgboost's best error=5.4537,	best estimator xgboost's best error=5.4537
[flaml.automl: 09-17 14:45:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:46:00] {3072} INFO -  at 24.4s,	estimator xgboost's best error=5.4537,	best estimator xgboost's best error=5.4537
[flaml.automl: 09-17 14:46:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:46:02] {3072} INFO -  at 26.6s,	estimator xgboost's best error=5.4537,	best estimator xgboost's best error=5.4537
[flaml.automl: 09-17 14:46:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:46:05] {3072} INFO -  at 29.8s,	estimator xgboost's best error=5.4537,	best estimator xgboost's best error=5.4537
[flaml.automl: 09-17 14:46:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:46:07] {3072} INFO -  at 31.9s,	estimator xgboost's best error=5.4537,	best estimator xgboost's best error=5.4537
[flaml.automl: 09-17 14:46:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:46:20] {3072} INFO -  at 44.9s,	estimator xgboost's best error=4.6656,	best estimator xgboost's best error=4.6656
[flaml.automl: 09-17 14:46:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 14:46:35] {3072} INFO -  at 59.6s,	estimator xgboost's best error=4.6388,	best estimator xgboost's best error=4.6388
[flaml.automl: 09-17 14:46:52] {3335} INFO - retrain xgboost for 17.1s
[flaml.automl: 09-17 14:46:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:46:52] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:46:52] {2637} INFO - Time taken to find the best model: 59.62843322753906
[flaml.automl: 09-17 14:46:52] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64854}
PM2.5(0)最佳损失：-3.6388414756292704
PM2.5(0)最好结果：{'pred_time': 1.3445292727302477e-05, 'wall_clock_time': 59.62843322753906, 'metric_for_logging': {'pred_time': 1.3445292727302477e-05}, 'val_loss': 4.63884147562927, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64854}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 64854, 'experiment_tag': 'exp', 'time_total_s': 14.747386455535889}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9277438134115643
PM2.5(0)的mse=40.94597072432546
PM2.5(0)的mae=4.634756031503269
PM2.5(0)的mar=0.23303839053606082
总共花费的时间为：77.84
荆州市
1845A
3548A
[flaml.automl: 09-17 14:53:48] {2390} INFO - task = regression
[flaml.automl: 09-17 14:53:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:53:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:53:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:53:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:53:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:53:50] {3025} INFO - Estimated sufficient time budget=21686s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 14:53:50] {3072} INFO -  at 2.2s,	estimator xgboost's best error=25.8221,	best estimator xgboost's best error=25.8221
[flaml.automl: 09-17 14:53:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:53:54] {3072} INFO -  at 5.6s,	estimator xgboost's best error=13.9442,	best estimator xgboost's best error=13.9442
[flaml.automl: 09-17 14:53:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:53:56] {3072} INFO -  at 7.9s,	estimator xgboost's best error=13.9442,	best estimator xgboost's best error=13.9442
[flaml.automl: 09-17 14:53:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:54:11] {3072} INFO -  at 23.3s,	estimator xgboost's best error=13.9442,	best estimator xgboost's best error=13.9442
[flaml.automl: 09-17 14:54:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:54:14] {3072} INFO -  at 25.5s,	estimator xgboost's best error=7.7555,	best estimator xgboost's best error=7.7555
[flaml.automl: 09-17 14:54:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:54:17] {3072} INFO -  at 28.5s,	estimator xgboost's best error=7.7390,	best estimator xgboost's best error=7.7390
[flaml.automl: 09-17 14:54:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:54:20] {3072} INFO -  at 31.6s,	estimator xgboost's best error=6.0011,	best estimator xgboost's best error=6.0011
[flaml.automl: 09-17 14:54:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:54:24] {3072} INFO -  at 36.0s,	estimator xgboost's best error=6.0011,	best estimator xgboost's best error=6.0011
[flaml.automl: 09-17 14:54:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:54:27] {3072} INFO -  at 38.9s,	estimator xgboost's best error=6.0011,	best estimator xgboost's best error=6.0011
[flaml.automl: 09-17 14:54:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:54:33] {3072} INFO -  at 44.4s,	estimator xgboost's best error=5.1802,	best estimator xgboost's best error=5.1802
[flaml.automl: 09-17 14:54:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:54:36] {3072} INFO -  at 47.5s,	estimator xgboost's best error=5.1802,	best estimator xgboost's best error=5.1802
[flaml.automl: 09-17 14:54:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:54:38] {3072} INFO -  at 49.6s,	estimator xgboost's best error=5.1802,	best estimator xgboost's best error=5.1802
[flaml.automl: 09-17 14:54:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:54:47] {3072} INFO -  at 59.2s,	estimator xgboost's best error=4.8209,	best estimator xgboost's best error=4.8209
[flaml.automl: 09-17 14:55:02] {3335} INFO - retrain xgboost for 14.3s
[flaml.automl: 09-17 14:55:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 14:55:02] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:55:02] {2637} INFO - Time taken to find the best model: 59.15501832962036
[flaml.automl: 09-17 14:55:02] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-3.8208960913741326
PM2.5(0)最好结果：{'pred_time': 4.3081584373303894e-05, 'wall_clock_time': 59.15501832962036, 'metric_for_logging': {'pred_time': 4.3081584373303894e-05}, 'val_loss': 4.8208960913741326, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 9.52118182182312}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9260461548890264
PM2.5(0)的mse=49.188014700047525
PM2.5(0)的mae=4.8888810254405595
PM2.5(0)的mar=0.19932689448462712
总共花费的时间为：73.90
岳阳市
1847A
1848A
1850A
1851A
1852A
[flaml.automl: 09-17 15:10:42] {2390} INFO - task = regression
[flaml.automl: 09-17 15:10:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:10:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:10:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:10:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:10:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:10:45] {3025} INFO - Estimated sufficient time budget=175657s. Estimated necessary time budget=176s.
[flaml.automl: 09-17 15:10:45] {3072} INFO -  at 3.6s,	estimator xgboost's best error=20.8627,	best estimator xgboost's best error=20.8627
[flaml.automl: 09-17 15:10:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:10:50] {3072} INFO -  at 8.9s,	estimator xgboost's best error=11.1078,	best estimator xgboost's best error=11.1078
[flaml.automl: 09-17 15:10:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:10:54] {3072} INFO -  at 12.1s,	estimator xgboost's best error=11.1078,	best estimator xgboost's best error=11.1078
[flaml.automl: 09-17 15:10:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:10:57] {3072} INFO -  at 15.6s,	estimator xgboost's best error=11.1078,	best estimator xgboost's best error=11.1078
[flaml.automl: 09-17 15:10:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:11:00] {3072} INFO -  at 18.4s,	estimator xgboost's best error=6.1451,	best estimator xgboost's best error=6.1451
[flaml.automl: 09-17 15:11:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:11:03] {3072} INFO -  at 21.3s,	estimator xgboost's best error=5.9501,	best estimator xgboost's best error=5.9501
[flaml.automl: 09-17 15:11:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:11:06] {3072} INFO -  at 24.2s,	estimator xgboost's best error=4.6611,	best estimator xgboost's best error=4.6611
[flaml.automl: 09-17 15:11:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:11:08] {3072} INFO -  at 26.9s,	estimator xgboost's best error=4.6611,	best estimator xgboost's best error=4.6611
[flaml.automl: 09-17 15:11:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:11:10] {3072} INFO -  at 28.8s,	estimator xgboost's best error=4.6611,	best estimator xgboost's best error=4.6611
[flaml.automl: 09-17 15:11:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:11:13] {3072} INFO -  at 31.4s,	estimator xgboost's best error=4.4814,	best estimator xgboost's best error=4.4814
[flaml.automl: 09-17 15:11:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:11:15] {3072} INFO -  at 33.0s,	estimator xgboost's best error=4.4814,	best estimator xgboost's best error=4.4814
[flaml.automl: 09-17 15:11:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:11:16] {3072} INFO -  at 34.1s,	estimator xgboost's best error=4.4814,	best estimator xgboost's best error=4.4814
[flaml.automl: 09-17 15:11:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:11:18] {3072} INFO -  at 36.0s,	estimator xgboost's best error=4.2240,	best estimator xgboost's best error=4.2240
[flaml.automl: 09-17 15:11:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:11:20] {3072} INFO -  at 38.2s,	estimator xgboost's best error=4.2240,	best estimator xgboost's best error=4.2240
[flaml.automl: 09-17 15:11:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 15:11:21] {3072} INFO -  at 39.6s,	estimator xgboost's best error=3.8117,	best estimator xgboost's best error=3.8117
[flaml.automl: 09-17 15:11:21] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 15:11:23] {3072} INFO -  at 41.8s,	estimator xgboost's best error=3.8117,	best estimator xgboost's best error=3.8117
[flaml.automl: 09-17 15:11:23] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 15:11:25] {3072} INFO -  at 43.4s,	estimator xgboost's best error=3.8035,	best estimator xgboost's best error=3.8035
[flaml.automl: 09-17 15:11:25] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 15:11:26] {3072} INFO -  at 44.7s,	estimator xgboost's best error=3.8035,	best estimator xgboost's best error=3.8035
[flaml.automl: 09-17 15:11:26] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 15:11:28] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.8035,	best estimator xgboost's best error=3.8035
[flaml.automl: 09-17 15:11:28] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 15:11:29] {3072} INFO -  at 47.7s,	estimator xgboost's best error=3.8035,	best estimator xgboost's best error=3.8035
[flaml.automl: 09-17 15:11:29] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 15:11:31] {3072} INFO -  at 49.2s,	estimator xgboost's best error=3.5585,	best estimator xgboost's best error=3.5585
[flaml.automl: 09-17 15:11:31] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 15:11:33] {3072} INFO -  at 51.8s,	estimator xgboost's best error=3.5585,	best estimator xgboost's best error=3.5585
[flaml.automl: 09-17 15:11:33] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-17 15:11:34] {3072} INFO -  at 52.7s,	estimator xgboost's best error=3.5585,	best estimator xgboost's best error=3.5585
[flaml.automl: 09-17 15:11:34] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-17 15:11:38] {3072} INFO -  at 56.1s,	estimator xgboost's best error=3.5268,	best estimator xgboost's best error=3.5268
[flaml.automl: 09-17 15:11:38] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-17 15:11:39] {3072} INFO -  at 57.6s,	estimator xgboost's best error=3.5268,	best estimator xgboost's best error=3.5268
[flaml.automl: 09-17 15:12:27] {3335} INFO - retrain xgboost for 47.5s
[flaml.automl: 09-17 15:12:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 15:12:27] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:12:27] {2637} INFO - Time taken to find the best model: 56.09465026855469
[flaml.automl: 09-17 15:12:27] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}
PM2.5(0)最佳损失：-2.5267793240355108
PM2.5(0)最好结果：{'pred_time': 7.060029251980489e-06, 'wall_clock_time': 56.09465026855469, 'metric_for_logging': {'pred_time': 7.060029251980489e-06}, 'val_loss': 3.5267793240355108, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}, 'config/n_estimators': 15, 'config/max_leaves': 57, 'config/min_child_weight': 0.0011006053677836264, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532848210950454, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.0763912614998318, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.3584229946136475}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9306985387814665
PM2.5(0)的mse=22.544961499417212
PM2.5(0)的mae=3.4346795676635464
PM2.5(0)的mar=0.13824000568311318
总共花费的时间为：106.05
常德市
1854A
1857A
3138A
3139A
3140A
[flaml.automl: 09-17 15:28:17] {2390} INFO - task = regression
[flaml.automl: 09-17 15:28:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:28:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:28:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:28:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:28:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:28:18] {3025} INFO - Estimated sufficient time budget=60971s. Estimated necessary time budget=61s.
[flaml.automl: 09-17 15:28:18] {3072} INFO -  at 1.4s,	estimator xgboost's best error=22.6343,	best estimator xgboost's best error=22.6343
[flaml.automl: 09-17 15:28:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:28:20] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.5166,	best estimator xgboost's best error=10.5166
[flaml.automl: 09-17 15:28:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:28:21] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.5166,	best estimator xgboost's best error=10.5166
[flaml.automl: 09-17 15:28:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:28:26] {3072} INFO -  at 10.1s,	estimator xgboost's best error=10.5166,	best estimator xgboost's best error=10.5166
[flaml.automl: 09-17 15:28:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:28:28] {3072} INFO -  at 11.2s,	estimator xgboost's best error=7.1045,	best estimator xgboost's best error=7.1045
[flaml.automl: 09-17 15:28:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:28:29] {3072} INFO -  at 12.8s,	estimator xgboost's best error=5.9248,	best estimator xgboost's best error=5.9248
[flaml.automl: 09-17 15:28:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:28:31] {3072} INFO -  at 14.4s,	estimator xgboost's best error=5.3554,	best estimator xgboost's best error=5.3554
[flaml.automl: 09-17 15:28:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:28:33] {3072} INFO -  at 17.1s,	estimator xgboost's best error=5.3554,	best estimator xgboost's best error=5.3554
[flaml.automl: 09-17 15:28:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:28:35] {3072} INFO -  at 18.7s,	estimator xgboost's best error=5.3554,	best estimator xgboost's best error=5.3554
[flaml.automl: 09-17 15:28:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:28:38] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.2019,	best estimator xgboost's best error=4.2019
[flaml.automl: 09-17 15:28:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:28:40] {3072} INFO -  at 23.2s,	estimator xgboost's best error=4.2019,	best estimator xgboost's best error=4.2019
[flaml.automl: 09-17 15:28:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:28:41] {3072} INFO -  at 24.4s,	estimator xgboost's best error=4.2019,	best estimator xgboost's best error=4.2019
[flaml.automl: 09-17 15:28:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:28:44] {3072} INFO -  at 27.2s,	estimator xgboost's best error=4.0919,	best estimator xgboost's best error=4.0919
[flaml.automl: 09-17 15:28:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:28:46] {3072} INFO -  at 30.0s,	estimator xgboost's best error=4.0919,	best estimator xgboost's best error=4.0919
[flaml.automl: 09-17 15:28:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 15:28:49] {3072} INFO -  at 32.6s,	estimator xgboost's best error=4.0165,	best estimator xgboost's best error=4.0165
[flaml.automl: 09-17 15:28:49] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 15:28:51] {3072} INFO -  at 34.8s,	estimator xgboost's best error=4.0165,	best estimator xgboost's best error=4.0165
[flaml.automl: 09-17 15:28:51] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 15:28:53] {3072} INFO -  at 37.0s,	estimator xgboost's best error=3.7780,	best estimator xgboost's best error=3.7780
[flaml.automl: 09-17 15:28:53] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 15:28:55] {3072} INFO -  at 38.6s,	estimator xgboost's best error=3.7780,	best estimator xgboost's best error=3.7780
[flaml.automl: 09-17 15:28:55] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 15:28:57] {3072} INFO -  at 40.3s,	estimator xgboost's best error=3.7780,	best estimator xgboost's best error=3.7780
[flaml.automl: 09-17 15:28:57] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 15:28:58] {3072} INFO -  at 41.9s,	estimator xgboost's best error=3.7780,	best estimator xgboost's best error=3.7780
[flaml.automl: 09-17 15:28:58] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 15:29:00] {3072} INFO -  at 43.4s,	estimator xgboost's best error=3.6343,	best estimator xgboost's best error=3.6343
[flaml.automl: 09-17 15:29:00] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 15:29:02] {3072} INFO -  at 46.0s,	estimator xgboost's best error=3.6343,	best estimator xgboost's best error=3.6343
[flaml.automl: 09-17 15:29:02] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-17 15:29:03] {3072} INFO -  at 46.9s,	estimator xgboost's best error=3.6343,	best estimator xgboost's best error=3.6343
[flaml.automl: 09-17 15:29:03] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-17 15:29:07] {3072} INFO -  at 50.2s,	estimator xgboost's best error=3.5359,	best estimator xgboost's best error=3.5359
[flaml.automl: 09-17 15:29:07] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-17 15:29:08] {3072} INFO -  at 51.7s,	estimator xgboost's best error=3.5359,	best estimator xgboost's best error=3.5359
[flaml.automl: 09-17 15:29:08] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-17 15:29:09] {3072} INFO -  at 52.8s,	estimator xgboost's best error=3.5359,	best estimator xgboost's best error=3.5359
[flaml.automl: 09-17 15:29:09] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-17 15:29:20] {3072} INFO -  at 63.5s,	estimator xgboost's best error=3.5359,	best estimator xgboost's best error=3.5359
[flaml.automl: 09-17 15:30:07] {3335} INFO - retrain xgboost for 47.5s
[flaml.automl: 09-17 15:30:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 15:30:07] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:30:07] {2637} INFO - Time taken to find the best model: 50.201199769973755
[flaml.automl: 09-17 15:30:07] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}
PM2.5(0)最佳损失：-2.5359031880584584
PM2.5(0)最好结果：{'pred_time': 7.034587439986578e-06, 'wall_clock_time': 50.201199769973755, 'metric_for_logging': {'pred_time': 7.034587439986578e-06}, 'val_loss': 3.5359031880584584, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}, 'config/n_estimators': 15, 'config/max_leaves': 57, 'config/min_child_weight': 0.0011006053677836264, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532848210950454, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.0763912614998318, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.32661509513855}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9536006623135431
PM2.5(0)的mse=27.11043620875515
PM2.5(0)的mae=3.377987137960948
PM2.5(0)的mar=0.13216495697476338
总共花费的时间为：111.88
张家界市
1858A
1859A
1861A
[flaml.automl: 09-17 15:39:26] {2390} INFO - task = regression
[flaml.automl: 09-17 15:39:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:39:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:39:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:39:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:39:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:39:27] {3025} INFO - Estimated sufficient time budget=11972s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 15:39:27] {3072} INFO -  at 1.3s,	estimator xgboost's best error=15.1653,	best estimator xgboost's best error=15.1653
[flaml.automl: 09-17 15:39:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:39:29] {3072} INFO -  at 3.4s,	estimator xgboost's best error=7.4486,	best estimator xgboost's best error=7.4486
[flaml.automl: 09-17 15:39:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:39:30] {3072} INFO -  at 4.6s,	estimator xgboost's best error=7.4486,	best estimator xgboost's best error=7.4486
[flaml.automl: 09-17 15:39:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:39:40] {3072} INFO -  at 14.5s,	estimator xgboost's best error=7.4486,	best estimator xgboost's best error=7.4486
[flaml.automl: 09-17 15:39:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:39:41] {3072} INFO -  at 15.6s,	estimator xgboost's best error=5.8161,	best estimator xgboost's best error=5.8161
[flaml.automl: 09-17 15:39:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:39:43] {3072} INFO -  at 17.2s,	estimator xgboost's best error=5.8161,	best estimator xgboost's best error=5.8161
[flaml.automl: 09-17 15:39:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:39:45] {3072} INFO -  at 18.9s,	estimator xgboost's best error=4.1459,	best estimator xgboost's best error=4.1459
[flaml.automl: 09-17 15:39:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:39:47] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.1459,	best estimator xgboost's best error=4.1459
[flaml.automl: 09-17 15:39:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:39:49] {3072} INFO -  at 23.2s,	estimator xgboost's best error=4.1459,	best estimator xgboost's best error=4.1459
[flaml.automl: 09-17 15:39:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:39:52] {3072} INFO -  at 26.2s,	estimator xgboost's best error=4.1459,	best estimator xgboost's best error=4.1459
[flaml.automl: 09-17 15:39:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:39:54] {3072} INFO -  at 27.9s,	estimator xgboost's best error=4.1459,	best estimator xgboost's best error=4.1459
[flaml.automl: 09-17 15:39:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:39:55] {3072} INFO -  at 29.1s,	estimator xgboost's best error=4.1459,	best estimator xgboost's best error=4.1459
[flaml.automl: 09-17 15:39:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:40:01] {3072} INFO -  at 35.6s,	estimator xgboost's best error=3.6742,	best estimator xgboost's best error=3.6742
[flaml.automl: 09-17 15:40:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:40:13] {3072} INFO -  at 47.2s,	estimator xgboost's best error=3.6226,	best estimator xgboost's best error=3.6226
[flaml.automl: 09-17 15:40:13] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 15:40:20] {3072} INFO -  at 53.8s,	estimator xgboost's best error=3.6226,	best estimator xgboost's best error=3.6226
[flaml.automl: 09-17 15:40:31] {3335} INFO - retrain xgboost for 11.6s
[flaml.automl: 09-17 15:40:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:40:31] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:40:31] {2637} INFO - Time taken to find the best model: 47.22868371009827
[flaml.automl: 09-17 15:40:31] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 28, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-2.6225772847855775
PM2.5(0)最好结果：{'pred_time': 1.2575762527541923e-05, 'wall_clock_time': 47.22868371009827, 'metric_for_logging': {'pred_time': 1.2575762527541923e-05}, 'val_loss': 3.6225772847855775, 'training_iteration': 1, 'config': {'n_estimators': 28, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 28, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 11.607823848724365}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9207587948151198
PM2.5(0)的mse=27.819068136200624
PM2.5(0)的mae=3.6246054654703235
PM2.5(0)的mar=0.2758624636437339
总共花费的时间为：65.86
桂林市
1862A
1864A
1865A
3403A
3531A
[flaml.automl: 09-17 15:55:53] {2390} INFO - task = regression
[flaml.automl: 09-17 15:55:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:55:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:55:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:55:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:55:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:55:55] {3025} INFO - Estimated sufficient time budget=61393s. Estimated necessary time budget=61s.
[flaml.automl: 09-17 15:55:55] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.5757,	best estimator xgboost's best error=15.5757
[flaml.automl: 09-17 15:55:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:55:57] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.5948,	best estimator xgboost's best error=7.5948
[flaml.automl: 09-17 15:55:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:55:58] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.5948,	best estimator xgboost's best error=7.5948
[flaml.automl: 09-17 15:55:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:56:03] {3072} INFO -  at 10.0s,	estimator xgboost's best error=7.5948,	best estimator xgboost's best error=7.5948
[flaml.automl: 09-17 15:56:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:56:04] {3072} INFO -  at 11.2s,	estimator xgboost's best error=5.5747,	best estimator xgboost's best error=5.5747
[flaml.automl: 09-17 15:56:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:56:06] {3072} INFO -  at 12.7s,	estimator xgboost's best error=5.5747,	best estimator xgboost's best error=5.5747
[flaml.automl: 09-17 15:56:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:56:08] {3072} INFO -  at 14.4s,	estimator xgboost's best error=3.7173,	best estimator xgboost's best error=3.7173
[flaml.automl: 09-17 15:56:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:56:10] {3072} INFO -  at 17.1s,	estimator xgboost's best error=3.7173,	best estimator xgboost's best error=3.7173
[flaml.automl: 09-17 15:56:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:56:12] {3072} INFO -  at 18.7s,	estimator xgboost's best error=3.7173,	best estimator xgboost's best error=3.7173
[flaml.automl: 09-17 15:56:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:56:15] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.7173,	best estimator xgboost's best error=3.7173
[flaml.automl: 09-17 15:56:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:56:16] {3072} INFO -  at 23.1s,	estimator xgboost's best error=3.7173,	best estimator xgboost's best error=3.7173
[flaml.automl: 09-17 15:56:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:56:18] {3072} INFO -  at 24.9s,	estimator xgboost's best error=3.6688,	best estimator xgboost's best error=3.6688
[flaml.automl: 09-17 15:56:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:56:19] {3072} INFO -  at 26.0s,	estimator xgboost's best error=3.6688,	best estimator xgboost's best error=3.6688
[flaml.automl: 09-17 15:56:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:56:26] {3072} INFO -  at 33.1s,	estimator xgboost's best error=3.1824,	best estimator xgboost's best error=3.1824
[flaml.automl: 09-17 15:56:26] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 15:56:39] {3072} INFO -  at 45.9s,	estimator xgboost's best error=3.1086,	best estimator xgboost's best error=3.1086
[flaml.automl: 09-17 15:56:39] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 15:56:46] {3072} INFO -  at 53.0s,	estimator xgboost's best error=3.1086,	best estimator xgboost's best error=3.1086
[flaml.automl: 09-17 15:56:59] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 15:56:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:56:59] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:56:59] {2637} INFO - Time taken to find the best model: 45.9357213973999
[flaml.automl: 09-17 15:56:59] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 51383}
PM2.5(0)最佳损失：-2.1086117600392544
PM2.5(0)最好结果：{'pred_time': 8.061387284207052e-06, 'wall_clock_time': 45.9357213973999, 'metric_for_logging': {'pred_time': 8.061387284207052e-06}, 'val_loss': 3.1086117600392544, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 51383}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 51383, 'experiment_tag': 'exp', 'time_total_s': 12.855960845947266}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9281731970765089
PM2.5(0)的mse=22.61330441136745
PM2.5(0)的mae=3.0293780025024284
PM2.5(0)的mar=0.1919749369456898
总共花费的时间为：66.69
北海市
1866A
1868A
1869A
3400A
[flaml.automl: 09-17 16:08:52] {2390} INFO - task = regression
[flaml.automl: 09-17 16:08:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:08:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:08:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:08:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:08:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:08:54] {3025} INFO - Estimated sufficient time budget=48116s. Estimated necessary time budget=48s.
[flaml.automl: 09-17 16:08:54] {3072} INFO -  at 1.4s,	estimator xgboost's best error=14.0621,	best estimator xgboost's best error=14.0621
[flaml.automl: 09-17 16:08:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:08:56] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.0739,	best estimator xgboost's best error=7.0739
[flaml.automl: 09-17 16:08:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:08:57] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.0739,	best estimator xgboost's best error=7.0739
[flaml.automl: 09-17 16:08:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:09:03] {3072} INFO -  at 11.0s,	estimator xgboost's best error=7.0739,	best estimator xgboost's best error=7.0739
[flaml.automl: 09-17 16:09:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:09:04] {3072} INFO -  at 12.1s,	estimator xgboost's best error=5.4999,	best estimator xgboost's best error=5.4999
[flaml.automl: 09-17 16:09:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:09:06] {3072} INFO -  at 13.7s,	estimator xgboost's best error=5.0266,	best estimator xgboost's best error=5.0266
[flaml.automl: 09-17 16:09:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:09:08] {3072} INFO -  at 15.3s,	estimator xgboost's best error=4.7232,	best estimator xgboost's best error=4.7232
[flaml.automl: 09-17 16:09:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:09:10] {3072} INFO -  at 18.0s,	estimator xgboost's best error=4.7232,	best estimator xgboost's best error=4.7232
[flaml.automl: 09-17 16:09:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:09:12] {3072} INFO -  at 19.6s,	estimator xgboost's best error=4.7232,	best estimator xgboost's best error=4.7232
[flaml.automl: 09-17 16:09:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:09:15] {3072} INFO -  at 22.6s,	estimator xgboost's best error=4.2336,	best estimator xgboost's best error=4.2336
[flaml.automl: 09-17 16:09:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:09:16] {3072} INFO -  at 24.2s,	estimator xgboost's best error=4.2336,	best estimator xgboost's best error=4.2336
[flaml.automl: 09-17 16:09:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:09:18] {3072} INFO -  at 25.3s,	estimator xgboost's best error=4.2336,	best estimator xgboost's best error=4.2336
[flaml.automl: 09-17 16:09:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:09:21] {3072} INFO -  at 29.0s,	estimator xgboost's best error=4.2336,	best estimator xgboost's best error=4.2336
[flaml.automl: 09-17 16:09:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:09:25] {3072} INFO -  at 32.8s,	estimator xgboost's best error=4.2336,	best estimator xgboost's best error=4.2336
[flaml.automl: 09-17 16:09:25] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 16:09:28] {3072} INFO -  at 35.8s,	estimator xgboost's best error=4.2336,	best estimator xgboost's best error=4.2336
[flaml.automl: 09-17 16:09:28] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 16:09:33] {3072} INFO -  at 40.7s,	estimator xgboost's best error=4.2336,	best estimator xgboost's best error=4.2336
[flaml.automl: 09-17 16:09:33] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 16:09:35] {3072} INFO -  at 42.6s,	estimator xgboost's best error=4.2336,	best estimator xgboost's best error=4.2336
[flaml.automl: 09-17 16:09:35] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 16:09:36] {3072} INFO -  at 44.2s,	estimator xgboost's best error=4.2336,	best estimator xgboost's best error=4.2336
[flaml.automl: 09-17 16:09:36] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 16:09:44] {3072} INFO -  at 51.6s,	estimator xgboost's best error=4.1893,	best estimator xgboost's best error=4.1893
[flaml.automl: 09-17 16:09:44] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 16:09:46] {3072} INFO -  at 54.0s,	estimator xgboost's best error=4.1893,	best estimator xgboost's best error=4.1893
[flaml.automl: 09-17 16:09:54] {3335} INFO - retrain xgboost for 7.3s
[flaml.automl: 09-17 16:09:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5492977310397356, colsample_bynode=1,
             colsample_bytree=0.9510761842589558, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.04321599195729943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5415707634193446, scale_pos_weight=1,
             subsample=0.9351529901519405, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:09:54] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:09:54] {2637} INFO - Time taken to find the best model: 51.559266090393066
[flaml.automl: 09-17 16:09:54] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.04321599195729943, 'learning_rate': 1.0, 'subsample': 0.9351529901519405, 'colsample_bylevel': 0.5492977310397356, 'colsample_bytree': 0.9510761842589558, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5415707634193446, 'FLAML_sample_size': 40542}
PM2.5(0)最佳损失：-3.1893203867659317
PM2.5(0)最好结果：{'pred_time': 8.819158810224967e-06, 'wall_clock_time': 51.559266090393066, 'metric_for_logging': {'pred_time': 8.819158810224967e-06}, 'val_loss': 4.189320386765932, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.04321599195729943, 'learning_rate': 1.0, 'subsample': 0.9351529901519405, 'colsample_bylevel': 0.5492977310397356, 'colsample_bytree': 0.9510761842589558, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5415707634193446, 'FLAML_sample_size': 40542}, 'config/n_estimators': 8, 'config/max_leaves': 16, 'config/min_child_weight': 0.04321599195729943, 'config/learning_rate': 1.0, 'config/subsample': 0.9351529901519405, 'config/colsample_bylevel': 0.5492977310397356, 'config/colsample_bytree': 0.9510761842589558, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.5415707634193446, 'config/FLAML_sample_size': 40542, 'experiment_tag': 'exp', 'time_total_s': 7.333238363265991}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5492977310397356, colsample_bynode=1,
             colsample_bytree=0.9510761842589558, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.04321599195729943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5415707634193446, scale_pos_weight=1,
             subsample=0.9351529901519405, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8582513125138063
PM2.5(0)的mse=42.39382356985833
PM2.5(0)的mae=4.38470146368372
PM2.5(0)的mar=0.32734997271642713
总共花费的时间为：61.94
柳州市
1870A
1872A
1873A
1874A
1875A
3402A
[flaml.automl: 09-17 16:28:45] {2390} INFO - task = regression
[flaml.automl: 09-17 16:28:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:28:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:28:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:28:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:28:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:28:46] {3025} INFO - Estimated sufficient time budget=76542s. Estimated necessary time budget=77s.
[flaml.automl: 09-17 16:28:46] {3072} INFO -  at 1.5s,	estimator xgboost's best error=16.9147,	best estimator xgboost's best error=16.9147
[flaml.automl: 09-17 16:28:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:28:48] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.1232,	best estimator xgboost's best error=8.1232
[flaml.automl: 09-17 16:28:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:28:49] {3072} INFO -  at 4.8s,	estimator xgboost's best error=8.1232,	best estimator xgboost's best error=8.1232
[flaml.automl: 09-17 16:28:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:28:53] {3072} INFO -  at 8.5s,	estimator xgboost's best error=8.1232,	best estimator xgboost's best error=8.1232
[flaml.automl: 09-17 16:28:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:28:54] {3072} INFO -  at 9.7s,	estimator xgboost's best error=5.3415,	best estimator xgboost's best error=5.3415
[flaml.automl: 09-17 16:28:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:28:56] {3072} INFO -  at 11.2s,	estimator xgboost's best error=5.3415,	best estimator xgboost's best error=5.3415
[flaml.automl: 09-17 16:28:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:28:57] {3072} INFO -  at 12.9s,	estimator xgboost's best error=4.0853,	best estimator xgboost's best error=4.0853
[flaml.automl: 09-17 16:28:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:29:00] {3072} INFO -  at 15.5s,	estimator xgboost's best error=4.0853,	best estimator xgboost's best error=4.0853
[flaml.automl: 09-17 16:29:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:29:02] {3072} INFO -  at 17.1s,	estimator xgboost's best error=4.0853,	best estimator xgboost's best error=4.0853
[flaml.automl: 09-17 16:29:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:29:05] {3072} INFO -  at 20.2s,	estimator xgboost's best error=4.0853,	best estimator xgboost's best error=4.0853
[flaml.automl: 09-17 16:29:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:29:06] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.0853,	best estimator xgboost's best error=4.0853
[flaml.automl: 09-17 16:29:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:29:08] {3072} INFO -  at 23.3s,	estimator xgboost's best error=4.0768,	best estimator xgboost's best error=4.0768
[flaml.automl: 09-17 16:29:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:29:09] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.0768,	best estimator xgboost's best error=4.0768
[flaml.automl: 09-17 16:29:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:29:16] {3072} INFO -  at 31.5s,	estimator xgboost's best error=3.5562,	best estimator xgboost's best error=3.5562
[flaml.automl: 09-17 16:29:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 16:29:29] {3072} INFO -  at 44.2s,	estimator xgboost's best error=3.4739,	best estimator xgboost's best error=3.4739
[flaml.automl: 09-17 16:29:29] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 16:29:36] {3072} INFO -  at 51.3s,	estimator xgboost's best error=3.4739,	best estimator xgboost's best error=3.4739
[flaml.automl: 09-17 16:29:49] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 16:29:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:29:49] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:29:49] {2637} INFO - Time taken to find the best model: 44.20244216918945
[flaml.automl: 09-17 16:29:49] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64273}
PM2.5(0)最佳损失：-2.4739386977617963
PM2.5(0)最好结果：{'pred_time': 5.74734991453369e-06, 'wall_clock_time': 44.20244216918945, 'metric_for_logging': {'pred_time': 5.74734991453369e-06}, 'val_loss': 3.4739386977617963, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64273}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 64273, 'experiment_tag': 'exp', 'time_total_s': 12.675559043884277}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9172071583427249
PM2.5(0)的mse=26.189730087797084
PM2.5(0)的mae=3.368410874134201
PM2.5(0)的mar=0.18807413344585788
总共花费的时间为：65.10
三亚市
1876A
3540A
[flaml.automl: 09-17 16:36:29] {2390} INFO - task = regression
[flaml.automl: 09-17 16:36:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:36:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:36:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:36:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:36:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:36:30] {3025} INFO - Estimated sufficient time budget=12278s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 16:36:30] {3072} INFO -  at 1.3s,	estimator xgboost's best error=6.6584,	best estimator xgboost's best error=6.6584
[flaml.automl: 09-17 16:36:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:36:32] {3072} INFO -  at 3.4s,	estimator xgboost's best error=3.1415,	best estimator xgboost's best error=3.1415
[flaml.automl: 09-17 16:36:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:36:33] {3072} INFO -  at 4.6s,	estimator xgboost's best error=3.1415,	best estimator xgboost's best error=3.1415
[flaml.automl: 09-17 16:36:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:36:42] {3072} INFO -  at 14.0s,	estimator xgboost's best error=3.1415,	best estimator xgboost's best error=3.1415
[flaml.automl: 09-17 16:36:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:36:44] {3072} INFO -  at 15.2s,	estimator xgboost's best error=1.9977,	best estimator xgboost's best error=1.9977
[flaml.automl: 09-17 16:36:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:36:45] {3072} INFO -  at 16.7s,	estimator xgboost's best error=1.7190,	best estimator xgboost's best error=1.7190
[flaml.automl: 09-17 16:36:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:36:47] {3072} INFO -  at 18.4s,	estimator xgboost's best error=1.5811,	best estimator xgboost's best error=1.5811
[flaml.automl: 09-17 16:36:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:36:50] {3072} INFO -  at 21.1s,	estimator xgboost's best error=1.5811,	best estimator xgboost's best error=1.5811
[flaml.automl: 09-17 16:36:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:36:51] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.5694,	best estimator xgboost's best error=1.5694
[flaml.automl: 09-17 16:36:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:36:54] {3072} INFO -  at 25.7s,	estimator xgboost's best error=1.4780,	best estimator xgboost's best error=1.4780
[flaml.automl: 09-17 16:36:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:36:55] {3072} INFO -  at 26.9s,	estimator xgboost's best error=1.4780,	best estimator xgboost's best error=1.4780
[flaml.automl: 09-17 16:36:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:37:07] {3072} INFO -  at 38.9s,	estimator xgboost's best error=1.3555,	best estimator xgboost's best error=1.3555
[flaml.automl: 09-17 16:37:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:37:28] {3072} INFO -  at 59.5s,	estimator xgboost's best error=1.3152,	best estimator xgboost's best error=1.3152
[flaml.automl: 09-17 16:37:50] {3335} INFO - retrain xgboost for 21.9s
[flaml.automl: 09-17 16:37:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 16:37:50] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:37:50] {2637} INFO - Time taken to find the best model: 59.49212408065796
[flaml.automl: 09-17 16:37:50] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}
PM2.5(0)最佳损失：-0.31518093204273745
PM2.5(0)最好结果：{'pred_time': 1.7031307204837784e-05, 'wall_clock_time': 59.49212408065796, 'metric_for_logging': {'pred_time': 1.7031307204837784e-05}, 'val_loss': 1.3151809320427374, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.006967902231194494, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7520708370056695, 'config/colsample_bylevel': 0.948506134666318, 'config/colsample_bytree': 0.8223167301060249, 'config/reg_alpha': 0.0038027739156058313, 'config/reg_lambda': 0.5403518701157697, 'experiment_tag': 'exp', 'time_total_s': 20.60713267326355}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8670011541807113
PM2.5(0)的mse=3.4970494338467297
PM2.5(0)的mae=1.3051858194806767
PM2.5(0)的mar=0.16317460927925548
总共花费的时间为：81.76
德阳市
1902A
3639A
[flaml.automl: 09-17 16:44:09] {2390} INFO - task = regression
[flaml.automl: 09-17 16:44:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:44:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:44:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:44:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:44:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:44:10] {3025} INFO - Estimated sufficient time budget=12100s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 16:44:10] {3072} INFO -  at 1.3s,	estimator xgboost's best error=21.6592,	best estimator xgboost's best error=21.6592
[flaml.automl: 09-17 16:44:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:44:12] {3072} INFO -  at 3.4s,	estimator xgboost's best error=9.9564,	best estimator xgboost's best error=9.9564
[flaml.automl: 09-17 16:44:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:44:13] {3072} INFO -  at 4.6s,	estimator xgboost's best error=9.9564,	best estimator xgboost's best error=9.9564
[flaml.automl: 09-17 16:44:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:44:23] {3072} INFO -  at 14.1s,	estimator xgboost's best error=9.9564,	best estimator xgboost's best error=9.9564
[flaml.automl: 09-17 16:44:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:44:24] {3072} INFO -  at 15.2s,	estimator xgboost's best error=6.7282,	best estimator xgboost's best error=6.7282
[flaml.automl: 09-17 16:44:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:44:26] {3072} INFO -  at 16.8s,	estimator xgboost's best error=6.6416,	best estimator xgboost's best error=6.6416
[flaml.automl: 09-17 16:44:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:44:27] {3072} INFO -  at 18.5s,	estimator xgboost's best error=4.8779,	best estimator xgboost's best error=4.8779
[flaml.automl: 09-17 16:44:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:44:30] {3072} INFO -  at 21.1s,	estimator xgboost's best error=4.8779,	best estimator xgboost's best error=4.8779
[flaml.automl: 09-17 16:44:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:44:32] {3072} INFO -  at 22.8s,	estimator xgboost's best error=4.7081,	best estimator xgboost's best error=4.7081
[flaml.automl: 09-17 16:44:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:44:35] {3072} INFO -  at 25.8s,	estimator xgboost's best error=4.1959,	best estimator xgboost's best error=4.1959
[flaml.automl: 09-17 16:44:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:44:36] {3072} INFO -  at 27.0s,	estimator xgboost's best error=4.1959,	best estimator xgboost's best error=4.1959
[flaml.automl: 09-17 16:44:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:44:48] {3072} INFO -  at 38.9s,	estimator xgboost's best error=3.5124,	best estimator xgboost's best error=3.5124
[flaml.automl: 09-17 16:44:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:45:09] {3072} INFO -  at 60.1s,	estimator xgboost's best error=3.2697,	best estimator xgboost's best error=3.2697
[flaml.automl: 09-17 16:45:43] {3335} INFO - retrain xgboost for 34.2s
[flaml.automl: 09-17 16:45:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 16:45:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:45:43] {2637} INFO - Time taken to find the best model: 60.07019400596619
[flaml.automl: 09-17 16:45:43] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}
PM2.5(0)最佳损失：-2.2697317267144044
PM2.5(0)最好结果：{'pred_time': 3.148562553700993e-05, 'wall_clock_time': 60.07019400596619, 'metric_for_logging': {'pred_time': 3.148562553700993e-05}, 'val_loss': 3.2697317267144044, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.006967902231194494, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7520708370056695, 'config/colsample_bylevel': 0.948506134666318, 'config/colsample_bytree': 0.8223167301060249, 'config/reg_alpha': 0.0038027739156058313, 'config/reg_lambda': 0.5403518701157697, 'experiment_tag': 'exp', 'time_total_s': 21.146613359451294}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9595991245064645
PM2.5(0)的mse=21.469346398171865
PM2.5(0)的mae=3.249735815696353
PM2.5(0)的mar=0.13818937392869443
总共花费的时间为：94.86
南充市
1905A
1906A
1907A
1908A
1909A
[flaml.automl: 09-17 17:01:24] {2390} INFO - task = regression
[flaml.automl: 09-17 17:01:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:01:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:01:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:01:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:01:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:01:26] {3025} INFO - Estimated sufficient time budget=103187s. Estimated necessary time budget=103s.
[flaml.automl: 09-17 17:01:26] {3072} INFO -  at 2.3s,	estimator xgboost's best error=21.2825,	best estimator xgboost's best error=21.2825
[flaml.automl: 09-17 17:01:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:01:30] {3072} INFO -  at 6.1s,	estimator xgboost's best error=10.2729,	best estimator xgboost's best error=10.2729
[flaml.automl: 09-17 17:01:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:01:32] {3072} INFO -  at 8.0s,	estimator xgboost's best error=10.2729,	best estimator xgboost's best error=10.2729
[flaml.automl: 09-17 17:01:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:01:36] {3072} INFO -  at 12.6s,	estimator xgboost's best error=10.2729,	best estimator xgboost's best error=10.2729
[flaml.automl: 09-17 17:01:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:01:39] {3072} INFO -  at 15.3s,	estimator xgboost's best error=7.1910,	best estimator xgboost's best error=7.1910
[flaml.automl: 09-17 17:01:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:01:42] {3072} INFO -  at 18.9s,	estimator xgboost's best error=7.1197,	best estimator xgboost's best error=7.1197
[flaml.automl: 09-17 17:01:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:01:46] {3072} INFO -  at 22.5s,	estimator xgboost's best error=5.8310,	best estimator xgboost's best error=5.8310
[flaml.automl: 09-17 17:01:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:01:48] {3072} INFO -  at 24.9s,	estimator xgboost's best error=5.8310,	best estimator xgboost's best error=5.8310
[flaml.automl: 09-17 17:01:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:01:51] {3072} INFO -  at 27.6s,	estimator xgboost's best error=5.8310,	best estimator xgboost's best error=5.8310
[flaml.automl: 09-17 17:01:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:01:53] {3072} INFO -  at 29.7s,	estimator xgboost's best error=5.8310,	best estimator xgboost's best error=5.8310
[flaml.automl: 09-17 17:01:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:01:55] {3072} INFO -  at 31.9s,	estimator xgboost's best error=5.8310,	best estimator xgboost's best error=5.8310
[flaml.automl: 09-17 17:01:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:01:59] {3072} INFO -  at 35.8s,	estimator xgboost's best error=5.7975,	best estimator xgboost's best error=5.7975
[flaml.automl: 09-17 17:01:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:02:02] {3072} INFO -  at 38.3s,	estimator xgboost's best error=5.7975,	best estimator xgboost's best error=5.7975
[flaml.automl: 09-17 17:02:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:02:13] {3072} INFO -  at 49.7s,	estimator xgboost's best error=4.8890,	best estimator xgboost's best error=4.8890
[flaml.automl: 09-17 17:02:25] {3335} INFO - retrain xgboost for 11.6s
[flaml.automl: 09-17 17:02:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 17:02:25] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:02:25] {2637} INFO - Time taken to find the best model: 49.67959761619568
[flaml.automl: 09-17 17:02:25] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 53937}
PM2.5(0)最佳损失：-3.8890453330826906
PM2.5(0)最好结果：{'pred_time': 8.908080227342414e-06, 'wall_clock_time': 49.67959761619568, 'metric_for_logging': {'pred_time': 8.908080227342414e-06}, 'val_loss': 4.889045333082691, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 53937}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 53937, 'experiment_tag': 'exp', 'time_total_s': 11.356480121612549}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9044185985144504
PM2.5(0)的mse=42.40200132384274
PM2.5(0)的mae=4.873042923889641
PM2.5(0)的mar=0.27918738581550234
总共花费的时间为：62.20
遵义市
1911A
1912A
1913A
1914A
3536A
[flaml.automl: 09-17 17:17:47] {2390} INFO - task = regression
[flaml.automl: 09-17 17:17:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:17:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:17:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:17:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:17:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:17:48] {3025} INFO - Estimated sufficient time budget=65592s. Estimated necessary time budget=66s.
[flaml.automl: 09-17 17:17:48] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.0588,	best estimator xgboost's best error=13.0588
[flaml.automl: 09-17 17:17:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:17:50] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.2949,	best estimator xgboost's best error=6.2949
[flaml.automl: 09-17 17:17:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:17:52] {3072} INFO -  at 4.8s,	estimator xgboost's best error=6.2949,	best estimator xgboost's best error=6.2949
[flaml.automl: 09-17 17:17:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:17:56] {3072} INFO -  at 9.2s,	estimator xgboost's best error=6.2949,	best estimator xgboost's best error=6.2949
[flaml.automl: 09-17 17:17:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:17:57] {3072} INFO -  at 10.4s,	estimator xgboost's best error=4.6423,	best estimator xgboost's best error=4.6423
[flaml.automl: 09-17 17:17:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:17:59] {3072} INFO -  at 11.9s,	estimator xgboost's best error=4.6423,	best estimator xgboost's best error=4.6423
[flaml.automl: 09-17 17:17:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:18:00] {3072} INFO -  at 13.6s,	estimator xgboost's best error=3.6812,	best estimator xgboost's best error=3.6812
[flaml.automl: 09-17 17:18:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:18:03] {3072} INFO -  at 16.3s,	estimator xgboost's best error=3.6812,	best estimator xgboost's best error=3.6812
[flaml.automl: 09-17 17:18:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:18:05] {3072} INFO -  at 18.0s,	estimator xgboost's best error=3.6812,	best estimator xgboost's best error=3.6812
[flaml.automl: 09-17 17:18:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:18:08] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.6812,	best estimator xgboost's best error=3.6812
[flaml.automl: 09-17 17:18:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:18:09] {3072} INFO -  at 22.4s,	estimator xgboost's best error=3.6812,	best estimator xgboost's best error=3.6812
[flaml.automl: 09-17 17:18:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:18:11] {3072} INFO -  at 24.1s,	estimator xgboost's best error=3.6600,	best estimator xgboost's best error=3.6600
[flaml.automl: 09-17 17:18:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:18:12] {3072} INFO -  at 25.3s,	estimator xgboost's best error=3.6600,	best estimator xgboost's best error=3.6600
[flaml.automl: 09-17 17:18:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:18:19] {3072} INFO -  at 32.3s,	estimator xgboost's best error=3.2892,	best estimator xgboost's best error=3.2892
[flaml.automl: 09-17 17:18:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 17:18:38] {3072} INFO -  at 51.2s,	estimator xgboost's best error=3.2514,	best estimator xgboost's best error=3.2514
[flaml.automl: 09-17 17:18:58] {3335} INFO - retrain xgboost for 20.2s
[flaml.automl: 09-17 17:18:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:18:58] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:18:58] {2637} INFO - Time taken to find the best model: 51.185542583465576
[flaml.automl: 09-17 17:18:58] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54561}
PM2.5(0)最佳损失：-2.2514311951928736
PM2.5(0)最好结果：{'pred_time': 9.314171968661734e-06, 'wall_clock_time': 51.185542583465576, 'metric_for_logging': {'pred_time': 9.314171968661734e-06}, 'val_loss': 3.2514311951928736, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54561}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 54561, 'experiment_tag': 'exp', 'time_total_s': 18.85672092437744}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.878371991809257
PM2.5(0)的mse=19.259276301708947
PM2.5(0)的mae=3.237347272696261
PM2.5(0)的mar=0.2074579763398405
总共花费的时间为：72.23
曲靖市
1917A
3376A
3377A
[flaml.automl: 09-17 17:28:20] {2390} INFO - task = regression
[flaml.automl: 09-17 17:28:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:28:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:28:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:28:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:28:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:28:21] {3025} INFO - Estimated sufficient time budget=12156s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 17:28:21] {3072} INFO -  at 1.3s,	estimator xgboost's best error=13.8521,	best estimator xgboost's best error=13.8521
[flaml.automl: 09-17 17:28:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:28:23] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.9529,	best estimator xgboost's best error=6.9529
[flaml.automl: 09-17 17:28:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:28:25] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.9529,	best estimator xgboost's best error=6.9529
[flaml.automl: 09-17 17:28:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:28:34] {3072} INFO -  at 14.6s,	estimator xgboost's best error=6.9529,	best estimator xgboost's best error=6.9529
[flaml.automl: 09-17 17:28:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:28:36] {3072} INFO -  at 15.8s,	estimator xgboost's best error=5.1327,	best estimator xgboost's best error=5.1327
[flaml.automl: 09-17 17:28:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:28:37] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.7086,	best estimator xgboost's best error=4.7086
[flaml.automl: 09-17 17:28:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:28:39] {3072} INFO -  at 19.0s,	estimator xgboost's best error=4.4156,	best estimator xgboost's best error=4.4156
[flaml.automl: 09-17 17:28:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:28:42] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.4156,	best estimator xgboost's best error=4.4156
[flaml.automl: 09-17 17:28:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:28:43] {3072} INFO -  at 23.4s,	estimator xgboost's best error=4.4156,	best estimator xgboost's best error=4.4156
[flaml.automl: 09-17 17:28:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:28:46] {3072} INFO -  at 26.4s,	estimator xgboost's best error=4.1803,	best estimator xgboost's best error=4.1803
[flaml.automl: 09-17 17:28:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:28:48] {3072} INFO -  at 28.1s,	estimator xgboost's best error=4.1803,	best estimator xgboost's best error=4.1803
[flaml.automl: 09-17 17:28:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:28:49] {3072} INFO -  at 29.2s,	estimator xgboost's best error=4.1803,	best estimator xgboost's best error=4.1803
[flaml.automl: 09-17 17:28:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:29:03] {3072} INFO -  at 42.9s,	estimator xgboost's best error=3.9316,	best estimator xgboost's best error=3.9316
[flaml.automl: 09-17 17:29:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:29:19] {3072} INFO -  at 59.3s,	estimator xgboost's best error=3.8478,	best estimator xgboost's best error=3.8478
[flaml.automl: 09-17 17:29:43] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-17 17:29:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:29:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:29:43] {2637} INFO - Time taken to find the best model: 59.31125330924988
[flaml.automl: 09-17 17:29:43] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-2.847797643825314
PM2.5(0)最好结果：{'pred_time': 1.0887629615399718e-05, 'wall_clock_time': 59.31125330924988, 'metric_for_logging': {'pred_time': 1.0887629615399718e-05}, 'val_loss': 3.847797643825314, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.45777940750122}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8416782680540583
PM2.5(0)的mse=25.940588239237574
PM2.5(0)的mae=3.7661617483076117
PM2.5(0)的mar=0.2630418608615522
总共花费的时间为：83.74
咸阳市
1918A
1919A
1920A
3525A
[flaml.automl: 09-17 17:41:55] {2390} INFO - task = regression
[flaml.automl: 09-17 17:41:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:41:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:41:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:41:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:41:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:41:57] {3025} INFO - Estimated sufficient time budget=49599s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 17:41:57] {3072} INFO -  at 1.4s,	estimator xgboost's best error=29.9221,	best estimator xgboost's best error=29.9221
[flaml.automl: 09-17 17:41:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:41:59] {3072} INFO -  at 3.5s,	estimator xgboost's best error=14.2097,	best estimator xgboost's best error=14.2097
[flaml.automl: 09-17 17:41:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:42:00] {3072} INFO -  at 4.7s,	estimator xgboost's best error=14.2097,	best estimator xgboost's best error=14.2097
[flaml.automl: 09-17 17:42:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:42:06] {3072} INFO -  at 11.1s,	estimator xgboost's best error=14.2097,	best estimator xgboost's best error=14.2097
[flaml.automl: 09-17 17:42:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:42:07] {3072} INFO -  at 12.2s,	estimator xgboost's best error=10.2542,	best estimator xgboost's best error=10.2542
[flaml.automl: 09-17 17:42:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:42:09] {3072} INFO -  at 13.8s,	estimator xgboost's best error=10.2542,	best estimator xgboost's best error=10.2542
[flaml.automl: 09-17 17:42:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:42:11] {3072} INFO -  at 15.5s,	estimator xgboost's best error=6.3852,	best estimator xgboost's best error=6.3852
[flaml.automl: 09-17 17:42:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:42:13] {3072} INFO -  at 18.1s,	estimator xgboost's best error=6.3852,	best estimator xgboost's best error=6.3852
[flaml.automl: 09-17 17:42:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:42:15] {3072} INFO -  at 19.8s,	estimator xgboost's best error=6.3852,	best estimator xgboost's best error=6.3852
[flaml.automl: 09-17 17:42:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:42:18] {3072} INFO -  at 22.8s,	estimator xgboost's best error=6.3852,	best estimator xgboost's best error=6.3852
[flaml.automl: 09-17 17:42:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:42:20] {3072} INFO -  at 24.2s,	estimator xgboost's best error=6.3852,	best estimator xgboost's best error=6.3852
[flaml.automl: 09-17 17:42:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:42:21] {3072} INFO -  at 25.9s,	estimator xgboost's best error=6.3830,	best estimator xgboost's best error=6.3830
[flaml.automl: 09-17 17:42:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:42:22] {3072} INFO -  at 27.1s,	estimator xgboost's best error=6.3830,	best estimator xgboost's best error=6.3830
[flaml.automl: 09-17 17:42:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:42:29] {3072} INFO -  at 34.1s,	estimator xgboost's best error=5.0637,	best estimator xgboost's best error=5.0637
[flaml.automl: 09-17 17:42:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 17:42:42] {3072} INFO -  at 46.8s,	estimator xgboost's best error=4.8710,	best estimator xgboost's best error=4.8710
[flaml.automl: 09-17 17:42:42] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 17:42:49] {3072} INFO -  at 53.8s,	estimator xgboost's best error=4.8710,	best estimator xgboost's best error=4.8710
[flaml.automl: 09-17 17:43:02] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-17 17:43:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:43:02] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:43:02] {2637} INFO - Time taken to find the best model: 46.815608739852905
[flaml.automl: 09-17 17:43:02] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 41112}
PM2.5(0)最佳损失：-3.8710498661585735
PM2.5(0)最好结果：{'pred_time': 8.471427573422834e-06, 'wall_clock_time': 46.815608739852905, 'metric_for_logging': {'pred_time': 8.471427573422834e-06}, 'val_loss': 4.8710498661585735, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 41112}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 41112, 'experiment_tag': 'exp', 'time_total_s': 12.687967300415039}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9473709767515065
PM2.5(0)的mse=65.39078130456488
PM2.5(0)的mae=4.882034446283811
PM2.5(0)的mar=0.1297068305446995
总共花费的时间为：67.20
铜川市
1922A
1923A
[flaml.automl: 09-17 17:49:32] {2390} INFO - task = regression
[flaml.automl: 09-17 17:49:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:49:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:49:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:49:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:49:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:49:33] {3025} INFO - Estimated sufficient time budget=12058s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 17:49:33] {3072} INFO -  at 1.3s,	estimator xgboost's best error=22.1630,	best estimator xgboost's best error=22.1630
[flaml.automl: 09-17 17:49:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:49:35] {3072} INFO -  at 3.2s,	estimator xgboost's best error=11.8640,	best estimator xgboost's best error=11.8640
[flaml.automl: 09-17 17:49:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:49:36] {3072} INFO -  at 4.3s,	estimator xgboost's best error=11.8640,	best estimator xgboost's best error=11.8640
[flaml.automl: 09-17 17:49:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:49:45] {3072} INFO -  at 12.8s,	estimator xgboost's best error=11.8640,	best estimator xgboost's best error=11.8640
[flaml.automl: 09-17 17:49:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:49:46] {3072} INFO -  at 13.9s,	estimator xgboost's best error=7.3906,	best estimator xgboost's best error=7.3906
[flaml.automl: 09-17 17:49:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:49:47] {3072} INFO -  at 15.5s,	estimator xgboost's best error=6.2028,	best estimator xgboost's best error=6.2028
[flaml.automl: 09-17 17:49:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:49:49] {3072} INFO -  at 17.1s,	estimator xgboost's best error=5.7770,	best estimator xgboost's best error=5.7770
[flaml.automl: 09-17 17:49:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:49:51] {3072} INFO -  at 19.5s,	estimator xgboost's best error=5.7770,	best estimator xgboost's best error=5.7770
[flaml.automl: 09-17 17:49:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:49:53] {3072} INFO -  at 21.1s,	estimator xgboost's best error=5.7770,	best estimator xgboost's best error=5.7770
[flaml.automl: 09-17 17:49:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:49:56] {3072} INFO -  at 24.1s,	estimator xgboost's best error=4.4674,	best estimator xgboost's best error=4.4674
[flaml.automl: 09-17 17:49:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:49:58] {3072} INFO -  at 25.8s,	estimator xgboost's best error=4.4674,	best estimator xgboost's best error=4.4674
[flaml.automl: 09-17 17:49:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:49:59] {3072} INFO -  at 26.9s,	estimator xgboost's best error=4.4674,	best estimator xgboost's best error=4.4674
[flaml.automl: 09-17 17:49:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:50:11] {3072} INFO -  at 38.9s,	estimator xgboost's best error=4.3697,	best estimator xgboost's best error=4.3697
[flaml.automl: 09-17 17:50:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:50:31] {3072} INFO -  at 59.3s,	estimator xgboost's best error=3.9828,	best estimator xgboost's best error=3.9828
[flaml.automl: 09-17 17:50:52] {3335} INFO - retrain xgboost for 21.2s
[flaml.automl: 09-17 17:50:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:50:52] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:50:52] {2637} INFO - Time taken to find the best model: 59.33335018157959
[flaml.automl: 09-17 17:50:52] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-2.9827660451635127
PM2.5(0)最好结果：{'pred_time': 1.7625534875671303e-05, 'wall_clock_time': 59.33335018157959, 'metric_for_logging': {'pred_time': 1.7625534875671303e-05}, 'val_loss': 3.9827660451635127, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 31, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 20.447694778442383}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.925360535073059
PM2.5(0)的mse=41.98906419423861
PM2.5(0)的mae=3.9471243840207455
PM2.5(0)的mar=0.14878805552400562
总共花费的时间为：80.95
延安市
1926A
1927A
1929A
3652A
[flaml.automl: 09-17 18:03:38] {2390} INFO - task = regression
[flaml.automl: 09-17 18:03:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:03:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:03:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:03:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:03:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:03:39] {3025} INFO - Estimated sufficient time budget=49890s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 18:03:39] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.2969,	best estimator xgboost's best error=18.2969
[flaml.automl: 09-17 18:03:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:03:41] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.9041,	best estimator xgboost's best error=8.9041
[flaml.automl: 09-17 18:03:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:03:42] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.9041,	best estimator xgboost's best error=8.9041
[flaml.automl: 09-17 18:03:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:03:48] {3072} INFO -  at 11.0s,	estimator xgboost's best error=8.9041,	best estimator xgboost's best error=8.9041
[flaml.automl: 09-17 18:03:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:03:50] {3072} INFO -  at 12.1s,	estimator xgboost's best error=6.5387,	best estimator xgboost's best error=6.5387
[flaml.automl: 09-17 18:03:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:03:51] {3072} INFO -  at 13.7s,	estimator xgboost's best error=6.5387,	best estimator xgboost's best error=6.5387
[flaml.automl: 09-17 18:03:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:03:53] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.6969,	best estimator xgboost's best error=4.6969
[flaml.automl: 09-17 18:03:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:03:56] {3072} INFO -  at 18.1s,	estimator xgboost's best error=4.6969,	best estimator xgboost's best error=4.6969
[flaml.automl: 09-17 18:03:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:03:57] {3072} INFO -  at 19.7s,	estimator xgboost's best error=4.6969,	best estimator xgboost's best error=4.6969
[flaml.automl: 09-17 18:03:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:04:00] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.6969,	best estimator xgboost's best error=4.6969
[flaml.automl: 09-17 18:04:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:04:02] {3072} INFO -  at 24.2s,	estimator xgboost's best error=4.6969,	best estimator xgboost's best error=4.6969
[flaml.automl: 09-17 18:04:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:04:03] {3072} INFO -  at 25.9s,	estimator xgboost's best error=4.6412,	best estimator xgboost's best error=4.6412
[flaml.automl: 09-17 18:04:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:04:04] {3072} INFO -  at 27.1s,	estimator xgboost's best error=4.6412,	best estimator xgboost's best error=4.6412
[flaml.automl: 09-17 18:04:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:04:12] {3072} INFO -  at 34.1s,	estimator xgboost's best error=4.1676,	best estimator xgboost's best error=4.1676
[flaml.automl: 09-17 18:04:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 18:04:24] {3072} INFO -  at 47.0s,	estimator xgboost's best error=4.0277,	best estimator xgboost's best error=4.0277
[flaml.automl: 09-17 18:04:24] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 18:04:31] {3072} INFO -  at 54.0s,	estimator xgboost's best error=4.0277,	best estimator xgboost's best error=4.0277
[flaml.automl: 09-17 18:04:44] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 18:04:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:04:44] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:04:44] {2637} INFO - Time taken to find the best model: 46.95480704307556
[flaml.automl: 09-17 18:04:44] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 41868}
PM2.5(0)最佳损失：-3.0277084688102205
PM2.5(0)最好结果：{'pred_time': 8.800376647778693e-06, 'wall_clock_time': 46.95480704307556, 'metric_for_logging': {'pred_time': 8.800376647778693e-06}, 'val_loss': 4.0277084688102205, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 41868}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 41868, 'experiment_tag': 'exp', 'time_total_s': 12.845561265945435}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8821346917220645
PM2.5(0)的mse=45.556795120126175
PM2.5(0)的mae=4.030673419207162
PM2.5(0)的mar=0.17623253088276375
总共花费的时间为：67.44
宝鸡市
1930A
1931A
1932A
1933A
1934A
1935A
1937A
[flaml.automl: 09-17 18:26:29] {2390} INFO - task = regression
[flaml.automl: 09-17 18:26:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:26:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:26:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:26:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:26:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:26:30] {3025} INFO - Estimated sufficient time budget=86685s. Estimated necessary time budget=87s.
[flaml.automl: 09-17 18:26:30] {3072} INFO -  at 1.5s,	estimator xgboost's best error=24.9161,	best estimator xgboost's best error=24.9161
[flaml.automl: 09-17 18:26:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:26:32] {3072} INFO -  at 3.6s,	estimator xgboost's best error=11.8389,	best estimator xgboost's best error=11.8389
[flaml.automl: 09-17 18:26:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:26:33] {3072} INFO -  at 4.8s,	estimator xgboost's best error=11.8389,	best estimator xgboost's best error=11.8389
[flaml.automl: 09-17 18:26:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:26:36] {3072} INFO -  at 8.0s,	estimator xgboost's best error=11.8389,	best estimator xgboost's best error=11.8389
[flaml.automl: 09-17 18:26:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:26:37] {3072} INFO -  at 9.1s,	estimator xgboost's best error=8.5616,	best estimator xgboost's best error=8.5616
[flaml.automl: 09-17 18:26:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:26:39] {3072} INFO -  at 10.7s,	estimator xgboost's best error=7.2085,	best estimator xgboost's best error=7.2085
[flaml.automl: 09-17 18:26:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:26:41] {3072} INFO -  at 12.3s,	estimator xgboost's best error=6.6113,	best estimator xgboost's best error=6.6113
[flaml.automl: 09-17 18:26:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:26:43] {3072} INFO -  at 14.9s,	estimator xgboost's best error=6.6113,	best estimator xgboost's best error=6.6113
[flaml.automl: 09-17 18:26:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:26:45] {3072} INFO -  at 16.5s,	estimator xgboost's best error=6.1608,	best estimator xgboost's best error=6.1608
[flaml.automl: 09-17 18:26:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:26:47] {3072} INFO -  at 19.1s,	estimator xgboost's best error=5.7788,	best estimator xgboost's best error=5.7788
[flaml.automl: 09-17 18:26:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:26:49] {3072} INFO -  at 20.2s,	estimator xgboost's best error=5.7788,	best estimator xgboost's best error=5.7788
[flaml.automl: 09-17 18:26:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:26:51] {3072} INFO -  at 22.1s,	estimator xgboost's best error=5.5723,	best estimator xgboost's best error=5.5723
[flaml.automl: 09-17 18:26:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:26:53] {3072} INFO -  at 24.3s,	estimator xgboost's best error=5.5723,	best estimator xgboost's best error=5.5723
[flaml.automl: 09-17 18:26:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:26:56] {3072} INFO -  at 27.3s,	estimator xgboost's best error=5.2941,	best estimator xgboost's best error=5.2941
[flaml.automl: 09-17 18:26:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 18:26:59] {3072} INFO -  at 30.4s,	estimator xgboost's best error=5.2941,	best estimator xgboost's best error=5.2941
[flaml.automl: 09-17 18:26:59] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 18:27:00] {3072} INFO -  at 31.9s,	estimator xgboost's best error=5.2941,	best estimator xgboost's best error=5.2941
[flaml.automl: 09-17 18:27:00] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 18:27:02] {3072} INFO -  at 33.6s,	estimator xgboost's best error=5.2941,	best estimator xgboost's best error=5.2941
[flaml.automl: 09-17 18:27:02] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 18:27:05] {3072} INFO -  at 36.4s,	estimator xgboost's best error=5.2941,	best estimator xgboost's best error=5.2941
[flaml.automl: 09-17 18:27:05] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 18:27:12] {3072} INFO -  at 43.6s,	estimator xgboost's best error=4.4764,	best estimator xgboost's best error=4.4764
[flaml.automl: 09-17 18:27:12] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 18:27:15] {3072} INFO -  at 46.4s,	estimator xgboost's best error=4.4764,	best estimator xgboost's best error=4.4764
[flaml.automl: 09-17 18:27:15] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 18:27:26] {3072} INFO -  at 58.0s,	estimator xgboost's best error=4.3253,	best estimator xgboost's best error=4.3253
[flaml.automl: 09-17 18:27:50] {3335} INFO - retrain xgboost for 23.1s
[flaml.automl: 09-17 18:27:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7122437241538152, colsample_bynode=1,
             colsample_bytree=0.6630580580188818, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=41, min_child_weight=0.00429421248387549,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.044438635238518605, reg_lambda=2.3018534279172544,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:27:50] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:27:50] {2637} INFO - Time taken to find the best model: 58.04062104225159
[flaml.automl: 09-17 18:27:50] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 10, 'max_leaves': 41, 'min_child_weight': 0.00429421248387549, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7122437241538152, 'colsample_bytree': 0.6630580580188818, 'reg_alpha': 0.044438635238518605, 'reg_lambda': 2.3018534279172544, 'FLAML_sample_size': 74343}
PM2.5(0)最佳损失：-3.325291154257914
PM2.5(0)最好结果：{'pred_time': 5.590671806672543e-06, 'wall_clock_time': 58.04062104225159, 'metric_for_logging': {'pred_time': 5.590671806672543e-06}, 'val_loss': 4.325291154257914, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 41, 'min_child_weight': 0.00429421248387549, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7122437241538152, 'colsample_bytree': 0.6630580580188818, 'reg_alpha': 0.044438635238518605, 'reg_lambda': 2.3018534279172544, 'FLAML_sample_size': 74343}, 'config/n_estimators': 10, 'config/max_leaves': 41, 'config/min_child_weight': 0.00429421248387549, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7122437241538152, 'config/colsample_bytree': 0.6630580580188818, 'config/reg_alpha': 0.044438635238518605, 'config/reg_lambda': 2.3018534279172544, 'config/FLAML_sample_size': 74343, 'experiment_tag': 'exp', 'time_total_s': 11.661630392074585}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7122437241538152, colsample_bynode=1,
             colsample_bytree=0.6630580580188818, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=41, min_child_weight=0.00429421248387549,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.044438635238518605, reg_lambda=2.3018534279172544,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9361473886110594
PM2.5(0)的mse=55.37877409834467
PM2.5(0)的mae=4.396288350687163
PM2.5(0)的mar=0.1433483772590306
总共花费的时间为：82.14
渭南市
1938A
1939A
1941A
[flaml.automl: 09-17 18:36:55] {2390} INFO - task = regression
[flaml.automl: 09-17 18:36:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:36:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:36:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:36:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:36:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:36:56] {3025} INFO - Estimated sufficient time budget=12053s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 18:36:56] {3072} INFO -  at 1.4s,	estimator xgboost's best error=28.9811,	best estimator xgboost's best error=28.9811
[flaml.automl: 09-17 18:36:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:36:58] {3072} INFO -  at 3.5s,	estimator xgboost's best error=14.0121,	best estimator xgboost's best error=14.0121
[flaml.automl: 09-17 18:36:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:36:59] {3072} INFO -  at 4.7s,	estimator xgboost's best error=14.0121,	best estimator xgboost's best error=14.0121
[flaml.automl: 09-17 18:36:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:37:09] {3072} INFO -  at 14.7s,	estimator xgboost's best error=14.0121,	best estimator xgboost's best error=14.0121
[flaml.automl: 09-17 18:37:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:37:10] {3072} INFO -  at 15.8s,	estimator xgboost's best error=10.0445,	best estimator xgboost's best error=10.0445
[flaml.automl: 09-17 18:37:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:37:12] {3072} INFO -  at 17.4s,	estimator xgboost's best error=10.0445,	best estimator xgboost's best error=10.0445
[flaml.automl: 09-17 18:37:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:37:14] {3072} INFO -  at 19.1s,	estimator xgboost's best error=6.8996,	best estimator xgboost's best error=6.8996
[flaml.automl: 09-17 18:37:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:37:16] {3072} INFO -  at 21.8s,	estimator xgboost's best error=6.8996,	best estimator xgboost's best error=6.8996
[flaml.automl: 09-17 18:37:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:37:18] {3072} INFO -  at 23.4s,	estimator xgboost's best error=6.8996,	best estimator xgboost's best error=6.8996
[flaml.automl: 09-17 18:37:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:37:21] {3072} INFO -  at 26.5s,	estimator xgboost's best error=6.8996,	best estimator xgboost's best error=6.8996
[flaml.automl: 09-17 18:37:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:37:22] {3072} INFO -  at 27.9s,	estimator xgboost's best error=6.8996,	best estimator xgboost's best error=6.8996
[flaml.automl: 09-17 18:37:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:37:24] {3072} INFO -  at 29.1s,	estimator xgboost's best error=6.8996,	best estimator xgboost's best error=6.8996
[flaml.automl: 09-17 18:37:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:37:31] {3072} INFO -  at 36.1s,	estimator xgboost's best error=5.8520,	best estimator xgboost's best error=5.8520
[flaml.automl: 09-17 18:37:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:37:52] {3072} INFO -  at 57.4s,	estimator xgboost's best error=5.7273,	best estimator xgboost's best error=5.7273
[flaml.automl: 09-17 18:38:15] {3335} INFO - retrain xgboost for 23.0s
[flaml.automl: 09-17 18:38:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:38:15] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:38:15] {2637} INFO - Time taken to find the best model: 57.44391632080078
[flaml.automl: 09-17 18:38:15] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-4.727268667972607
PM2.5(0)最好结果：{'pred_time': 2.0249598268149555e-05, 'wall_clock_time': 57.44391632080078, 'metric_for_logging': {'pred_time': 2.0249598268149555e-05}, 'val_loss': 5.727268667972607, 'training_iteration': 1, 'config': {'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 30, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 21.318872690200806}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9467775258521961
PM2.5(0)的mse=60.324133964632175
PM2.5(0)的mae=5.387061391317326
PM2.5(0)的mar=0.18549914567700526
总共花费的时间为：81.04
金昌市
金昌市没有数据
嘉峪关市
3248A
[flaml.automl: 09-17 18:42:08] {2390} INFO - task = regression
[flaml.automl: 09-17 18:42:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:42:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:42:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:42:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:42:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:42:09] {3025} INFO - Estimated sufficient time budget=12033s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 18:42:09] {3072} INFO -  at 1.3s,	estimator xgboost's best error=15.0013,	best estimator xgboost's best error=15.0013
[flaml.automl: 09-17 18:42:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:42:11] {3072} INFO -  at 3.1s,	estimator xgboost's best error=8.1798,	best estimator xgboost's best error=8.1798
[flaml.automl: 09-17 18:42:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:42:12] {3072} INFO -  at 4.3s,	estimator xgboost's best error=8.1798,	best estimator xgboost's best error=8.1798
[flaml.automl: 09-17 18:42:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:42:19] {3072} INFO -  at 11.4s,	estimator xgboost's best error=8.1798,	best estimator xgboost's best error=8.1798
[flaml.automl: 09-17 18:42:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:42:21] {3072} INFO -  at 12.5s,	estimator xgboost's best error=5.0141,	best estimator xgboost's best error=5.0141
[flaml.automl: 09-17 18:42:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:42:22] {3072} INFO -  at 14.1s,	estimator xgboost's best error=4.7311,	best estimator xgboost's best error=4.7311
[flaml.automl: 09-17 18:42:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:42:24] {3072} INFO -  at 15.7s,	estimator xgboost's best error=4.5457,	best estimator xgboost's best error=4.5457
[flaml.automl: 09-17 18:42:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:42:26] {3072} INFO -  at 18.0s,	estimator xgboost's best error=4.5457,	best estimator xgboost's best error=4.5457
[flaml.automl: 09-17 18:42:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:42:28] {3072} INFO -  at 19.6s,	estimator xgboost's best error=4.5457,	best estimator xgboost's best error=4.5457
[flaml.automl: 09-17 18:42:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:42:30] {3072} INFO -  at 22.3s,	estimator xgboost's best error=4.5432,	best estimator xgboost's best error=4.5432
[flaml.automl: 09-17 18:42:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:42:32] {3072} INFO -  at 23.9s,	estimator xgboost's best error=4.5432,	best estimator xgboost's best error=4.5432
[flaml.automl: 09-17 18:42:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:42:33] {3072} INFO -  at 25.0s,	estimator xgboost's best error=4.5432,	best estimator xgboost's best error=4.5432
[flaml.automl: 09-17 18:42:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:42:43] {3072} INFO -  at 34.6s,	estimator xgboost's best error=4.0802,	best estimator xgboost's best error=4.0802
[flaml.automl: 09-17 18:42:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:42:59] {3072} INFO -  at 50.9s,	estimator xgboost's best error=3.8614,	best estimator xgboost's best error=3.8614
[flaml.automl: 09-17 18:43:15] {3335} INFO - retrain xgboost for 16.4s
[flaml.automl: 09-17 18:43:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:43:15] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:43:15] {2637} INFO - Time taken to find the best model: 50.92189335823059
[flaml.automl: 09-17 18:43:15] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 11, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-2.8613960371033116
PM2.5(0)最好结果：{'pred_time': 3.203226036234638e-05, 'wall_clock_time': 50.92189335823059, 'metric_for_logging': {'pred_time': 3.203226036234638e-05}, 'val_loss': 3.8613960371033116, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 11, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 26, 'config/max_leaves': 11, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.335705518722534}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8464874213125886
PM2.5(0)的mse=52.52284092008125
PM2.5(0)的mae=3.569479887518985
PM2.5(0)的mar=0.17395008943049925
总共花费的时间为：67.57
石嘴山市
1947A
1949A
1950A
3520A
3521A
[flaml.automl: 09-17 18:58:47] {2390} INFO - task = regression
[flaml.automl: 09-17 18:58:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:58:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:58:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:58:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:58:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:58:48] {3025} INFO - Estimated sufficient time budget=59065s. Estimated necessary time budget=59s.
[flaml.automl: 09-17 18:58:48] {3072} INFO -  at 1.4s,	estimator xgboost's best error=21.9289,	best estimator xgboost's best error=21.9289
[flaml.automl: 09-17 18:58:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:58:50] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.6006,	best estimator xgboost's best error=10.6006
[flaml.automl: 09-17 18:58:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:58:51] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.6006,	best estimator xgboost's best error=10.6006
[flaml.automl: 09-17 18:58:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:58:57] {3072} INFO -  at 10.0s,	estimator xgboost's best error=10.6006,	best estimator xgboost's best error=10.6006
[flaml.automl: 09-17 18:58:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:58:58] {3072} INFO -  at 11.2s,	estimator xgboost's best error=8.8689,	best estimator xgboost's best error=8.8689
[flaml.automl: 09-17 18:58:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:58:59] {3072} INFO -  at 12.8s,	estimator xgboost's best error=8.8689,	best estimator xgboost's best error=8.8689
[flaml.automl: 09-17 18:58:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:59:01] {3072} INFO -  at 14.4s,	estimator xgboost's best error=6.0033,	best estimator xgboost's best error=6.0033
[flaml.automl: 09-17 18:59:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:59:04] {3072} INFO -  at 17.1s,	estimator xgboost's best error=6.0033,	best estimator xgboost's best error=6.0033
[flaml.automl: 09-17 18:59:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:59:05] {3072} INFO -  at 18.7s,	estimator xgboost's best error=6.0033,	best estimator xgboost's best error=6.0033
[flaml.automl: 09-17 18:59:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:59:08] {3072} INFO -  at 21.7s,	estimator xgboost's best error=6.0033,	best estimator xgboost's best error=6.0033
[flaml.automl: 09-17 18:59:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:59:10] {3072} INFO -  at 23.2s,	estimator xgboost's best error=6.0033,	best estimator xgboost's best error=6.0033
[flaml.automl: 09-17 18:59:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:59:11] {3072} INFO -  at 24.9s,	estimator xgboost's best error=5.9195,	best estimator xgboost's best error=5.9195
[flaml.automl: 09-17 18:59:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:59:13] {3072} INFO -  at 26.1s,	estimator xgboost's best error=5.9195,	best estimator xgboost's best error=5.9195
[flaml.automl: 09-17 18:59:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:59:20] {3072} INFO -  at 33.1s,	estimator xgboost's best error=5.5479,	best estimator xgboost's best error=5.5479
[flaml.automl: 09-17 18:59:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 18:59:32] {3072} INFO -  at 45.8s,	estimator xgboost's best error=5.3311,	best estimator xgboost's best error=5.3311
[flaml.automl: 09-17 18:59:32] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 18:59:39] {3072} INFO -  at 52.8s,	estimator xgboost's best error=5.3311,	best estimator xgboost's best error=5.3311
[flaml.automl: 09-17 18:59:52] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-17 18:59:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:59:52] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:59:52] {2637} INFO - Time taken to find the best model: 45.7664897441864
[flaml.automl: 09-17 18:59:52] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 48664}
PM2.5(0)最佳损失：-4.331131508540825
PM2.5(0)最好结果：{'pred_time': 7.856835627696923e-06, 'wall_clock_time': 45.7664897441864, 'metric_for_logging': {'pred_time': 7.856835627696923e-06}, 'val_loss': 5.331131508540825, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 48664}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 48664, 'experiment_tag': 'exp', 'time_total_s': 12.671732187271118}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8803389193444303
PM2.5(0)的mse=85.36726489782465
PM2.5(0)的mae=5.174935529774896
PM2.5(0)的mar=0.1785345033089221
总共花费的时间为：66.35
克拉玛依市
1951A
1955A
3612A
[flaml.automl: 09-17 19:09:40] {2390} INFO - task = regression
[flaml.automl: 09-17 19:09:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:09:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:09:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:09:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:09:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:09:42] {3025} INFO - Estimated sufficient time budget=18970s. Estimated necessary time budget=19s.
[flaml.automl: 09-17 19:09:42] {3072} INFO -  at 2.1s,	estimator xgboost's best error=13.0217,	best estimator xgboost's best error=13.0217
[flaml.automl: 09-17 19:09:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:09:45] {3072} INFO -  at 5.4s,	estimator xgboost's best error=7.9902,	best estimator xgboost's best error=7.9902
[flaml.automl: 09-17 19:09:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:09:48] {3072} INFO -  at 7.5s,	estimator xgboost's best error=7.9902,	best estimator xgboost's best error=7.9902
[flaml.automl: 09-17 19:09:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:10:04] {3072} INFO -  at 23.8s,	estimator xgboost's best error=7.9902,	best estimator xgboost's best error=7.9902
[flaml.automl: 09-17 19:10:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:10:06] {3072} INFO -  at 25.7s,	estimator xgboost's best error=6.7498,	best estimator xgboost's best error=6.7498
[flaml.automl: 09-17 19:10:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:10:08] {3072} INFO -  at 28.1s,	estimator xgboost's best error=6.3321,	best estimator xgboost's best error=6.3321
[flaml.automl: 09-17 19:10:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:10:11] {3072} INFO -  at 31.2s,	estimator xgboost's best error=6.1495,	best estimator xgboost's best error=6.1495
[flaml.automl: 09-17 19:10:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:10:16] {3072} INFO -  at 35.8s,	estimator xgboost's best error=6.1495,	best estimator xgboost's best error=6.1495
[flaml.automl: 09-17 19:10:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:10:19] {3072} INFO -  at 38.7s,	estimator xgboost's best error=6.0988,	best estimator xgboost's best error=6.0988
[flaml.automl: 09-17 19:10:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:10:23] {3072} INFO -  at 42.5s,	estimator xgboost's best error=6.0169,	best estimator xgboost's best error=6.0169
[flaml.automl: 09-17 19:10:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:10:24] {3072} INFO -  at 43.7s,	estimator xgboost's best error=6.0169,	best estimator xgboost's best error=6.0169
[flaml.automl: 09-17 19:10:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:10:37] {3072} INFO -  at 57.4s,	estimator xgboost's best error=5.8404,	best estimator xgboost's best error=5.8404
[flaml.automl: 09-17 19:10:51] {3335} INFO - retrain xgboost for 13.7s
[flaml.automl: 09-17 19:10:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:10:51] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:10:51] {2637} INFO - Time taken to find the best model: 57.36869263648987
[flaml.automl: 09-17 19:10:51] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
PM2.5(0)最佳损失：-4.840429540728752
PM2.5(0)最好结果：{'pred_time': 1.1370021940174459e-05, 'wall_clock_time': 57.36869263648987, 'metric_for_logging': {'pred_time': 1.1370021940174459e-05}, 'val_loss': 5.840429540728752, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 13.685245513916016}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8459635099103892
PM2.5(0)的mse=69.9788005321724
PM2.5(0)的mae=5.954093437760522
PM2.5(0)的mar=0.910564206561239
总共花费的时间为：71.71
巴音郭楞州
1957A
1958A
[flaml.automl: 09-17 19:17:17] {2390} INFO - task = regression
[flaml.automl: 09-17 19:17:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:17:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:17:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:17:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:17:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:17:19] {3025} INFO - Estimated sufficient time budget=12217s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 19:17:19] {3072} INFO -  at 1.3s,	estimator xgboost's best error=23.8599,	best estimator xgboost's best error=23.8599
[flaml.automl: 09-17 19:17:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:17:21] {3072} INFO -  at 3.4s,	estimator xgboost's best error=13.3193,	best estimator xgboost's best error=13.3193
[flaml.automl: 09-17 19:17:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:17:22] {3072} INFO -  at 4.6s,	estimator xgboost's best error=13.3193,	best estimator xgboost's best error=13.3193
[flaml.automl: 09-17 19:17:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:17:32] {3072} INFO -  at 14.2s,	estimator xgboost's best error=13.3193,	best estimator xgboost's best error=13.3193
[flaml.automl: 09-17 19:17:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:17:33] {3072} INFO -  at 16.1s,	estimator xgboost's best error=10.6519,	best estimator xgboost's best error=10.6519
[flaml.automl: 09-17 19:17:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:17:36] {3072} INFO -  at 18.8s,	estimator xgboost's best error=10.1111,	best estimator xgboost's best error=10.1111
[flaml.automl: 09-17 19:17:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:17:39] {3072} INFO -  at 21.3s,	estimator xgboost's best error=9.5981,	best estimator xgboost's best error=9.5981
[flaml.automl: 09-17 19:17:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:17:43] {3072} INFO -  at 25.4s,	estimator xgboost's best error=9.5981,	best estimator xgboost's best error=9.5981
[flaml.automl: 09-17 19:17:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:17:46] {3072} INFO -  at 28.2s,	estimator xgboost's best error=9.5676,	best estimator xgboost's best error=9.5676
[flaml.automl: 09-17 19:17:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:17:52] {3072} INFO -  at 34.4s,	estimator xgboost's best error=9.5427,	best estimator xgboost's best error=9.5427
[flaml.automl: 09-17 19:17:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:17:54] {3072} INFO -  at 36.9s,	estimator xgboost's best error=9.5427,	best estimator xgboost's best error=9.5427
[flaml.automl: 09-17 19:17:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:18:16] {3072} INFO -  at 58.4s,	estimator xgboost's best error=9.1663,	best estimator xgboost's best error=9.1663
[flaml.automl: 09-17 19:18:40] {3335} INFO - retrain xgboost for 23.8s
[flaml.automl: 09-17 19:18:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=15, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:18:40] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:18:40] {2637} INFO - Time taken to find the best model: 58.40211057662964
[flaml.automl: 09-17 19:18:40] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
PM2.5(0)最佳损失：-8.166282814206475
PM2.5(0)最好结果：{'pred_time': 4.48324949638392e-05, 'wall_clock_time': 58.40211057662964, 'metric_for_logging': {'pred_time': 4.48324949638392e-05}, 'val_loss': 9.166282814206475, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 21.483691930770874}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=15, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.7078608917647464
PM2.5(0)的mse=171.6698354137853
PM2.5(0)的mae=9.205563979282557
PM2.5(0)的mar=0.5764475091081827
总共花费的时间为：82.61
信阳市
2054A
2064A
2065A
2066A
[flaml.automl: 09-17 19:31:01] {2390} INFO - task = regression
[flaml.automl: 09-17 19:31:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:31:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:31:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:31:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:31:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:31:02] {3025} INFO - Estimated sufficient time budget=50145s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 19:31:02] {3072} INFO -  at 1.4s,	estimator xgboost's best error=22.0624,	best estimator xgboost's best error=22.0624
[flaml.automl: 09-17 19:31:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:31:04] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.4704,	best estimator xgboost's best error=10.4704
[flaml.automl: 09-17 19:31:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:31:05] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.4704,	best estimator xgboost's best error=10.4704
[flaml.automl: 09-17 19:31:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:31:12] {3072} INFO -  at 11.1s,	estimator xgboost's best error=10.4704,	best estimator xgboost's best error=10.4704
[flaml.automl: 09-17 19:31:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:31:13] {3072} INFO -  at 12.2s,	estimator xgboost's best error=7.1846,	best estimator xgboost's best error=7.1846
[flaml.automl: 09-17 19:31:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:31:14] {3072} INFO -  at 13.8s,	estimator xgboost's best error=7.1846,	best estimator xgboost's best error=7.1846
[flaml.automl: 09-17 19:31:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:31:16] {3072} INFO -  at 15.5s,	estimator xgboost's best error=4.9326,	best estimator xgboost's best error=4.9326
[flaml.automl: 09-17 19:31:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:31:19] {3072} INFO -  at 18.1s,	estimator xgboost's best error=4.9326,	best estimator xgboost's best error=4.9326
[flaml.automl: 09-17 19:31:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:31:20] {3072} INFO -  at 19.8s,	estimator xgboost's best error=4.9326,	best estimator xgboost's best error=4.9326
[flaml.automl: 09-17 19:31:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:31:23] {3072} INFO -  at 22.8s,	estimator xgboost's best error=4.9326,	best estimator xgboost's best error=4.9326
[flaml.automl: 09-17 19:31:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:31:25] {3072} INFO -  at 24.2s,	estimator xgboost's best error=4.9326,	best estimator xgboost's best error=4.9326
[flaml.automl: 09-17 19:31:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:31:26] {3072} INFO -  at 25.9s,	estimator xgboost's best error=4.9326,	best estimator xgboost's best error=4.9326
[flaml.automl: 09-17 19:31:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:31:28] {3072} INFO -  at 27.1s,	estimator xgboost's best error=4.9326,	best estimator xgboost's best error=4.9326
[flaml.automl: 09-17 19:31:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:31:35] {3072} INFO -  at 34.1s,	estimator xgboost's best error=4.1928,	best estimator xgboost's best error=4.1928
[flaml.automl: 09-17 19:31:35] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 19:31:50] {3072} INFO -  at 49.8s,	estimator xgboost's best error=4.0539,	best estimator xgboost's best error=4.0539
[flaml.automl: 09-17 19:32:12] {3335} INFO - retrain xgboost for 21.4s
[flaml.automl: 09-17 19:32:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:32:12] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:32:12] {2637} INFO - Time taken to find the best model: 49.77188801765442
[flaml.automl: 09-17 19:32:12] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 41811}
PM2.5(0)最佳损失：-3.053854179033369
PM2.5(0)最好结果：{'pred_time': 1.5549037767493207e-05, 'wall_clock_time': 49.77188801765442, 'metric_for_logging': {'pred_time': 1.5549037767493207e-05}, 'val_loss': 4.053854179033369, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 41811}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 41811, 'experiment_tag': 'exp', 'time_total_s': 15.6831374168396}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9383850282442258
PM2.5(0)的mse=33.75634766377867
PM2.5(0)的mae=4.01452193741464
PM2.5(0)的mar=0.15713435851889884
总共花费的时间为：71.88
周口市
2067A
2068A
2069A
2070A
[flaml.automl: 09-17 19:44:16] {2390} INFO - task = regression
[flaml.automl: 09-17 19:44:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:44:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:44:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:44:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:44:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:44:17] {3025} INFO - Estimated sufficient time budget=58140s. Estimated necessary time budget=58s.
[flaml.automl: 09-17 19:44:17] {3072} INFO -  at 1.7s,	estimator xgboost's best error=24.2134,	best estimator xgboost's best error=24.2134
[flaml.automl: 09-17 19:44:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:44:20] {3072} INFO -  at 4.8s,	estimator xgboost's best error=11.3077,	best estimator xgboost's best error=11.3077
[flaml.automl: 09-17 19:44:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:44:22] {3072} INFO -  at 6.6s,	estimator xgboost's best error=11.3077,	best estimator xgboost's best error=11.3077
[flaml.automl: 09-17 19:44:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:44:29] {3072} INFO -  at 13.1s,	estimator xgboost's best error=11.3077,	best estimator xgboost's best error=11.3077
[flaml.automl: 09-17 19:44:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:44:30] {3072} INFO -  at 14.7s,	estimator xgboost's best error=8.0200,	best estimator xgboost's best error=8.0200
[flaml.automl: 09-17 19:44:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:44:33] {3072} INFO -  at 17.1s,	estimator xgboost's best error=7.9304,	best estimator xgboost's best error=7.9304
[flaml.automl: 09-17 19:44:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:44:35] {3072} INFO -  at 19.6s,	estimator xgboost's best error=5.9274,	best estimator xgboost's best error=5.9274
[flaml.automl: 09-17 19:44:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:44:40] {3072} INFO -  at 24.3s,	estimator xgboost's best error=5.9274,	best estimator xgboost's best error=5.9274
[flaml.automl: 09-17 19:44:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:44:43] {3072} INFO -  at 27.4s,	estimator xgboost's best error=5.9274,	best estimator xgboost's best error=5.9274
[flaml.automl: 09-17 19:44:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:44:46] {3072} INFO -  at 30.6s,	estimator xgboost's best error=5.9274,	best estimator xgboost's best error=5.9274
[flaml.automl: 09-17 19:44:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:44:48] {3072} INFO -  at 32.8s,	estimator xgboost's best error=5.9274,	best estimator xgboost's best error=5.9274
[flaml.automl: 09-17 19:44:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:44:51] {3072} INFO -  at 35.7s,	estimator xgboost's best error=5.7502,	best estimator xgboost's best error=5.7502
[flaml.automl: 09-17 19:44:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:44:53] {3072} INFO -  at 37.9s,	estimator xgboost's best error=5.7502,	best estimator xgboost's best error=5.7502
[flaml.automl: 09-17 19:44:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:45:05] {3072} INFO -  at 49.3s,	estimator xgboost's best error=3.4381,	best estimator xgboost's best error=3.4381
[flaml.automl: 09-17 19:45:15] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-17 19:45:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 19:45:15] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:45:15] {2637} INFO - Time taken to find the best model: 49.25691890716553
[flaml.automl: 09-17 19:45:15] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 40923}
PM2.5(0)最佳损失：-2.438141139327379
PM2.5(0)最好结果：{'pred_time': 1.432000280160493e-05, 'wall_clock_time': 49.25691890716553, 'metric_for_logging': {'pred_time': 1.432000280160493e-05}, 'val_loss': 3.438141139327379, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 40923}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 40923, 'experiment_tag': 'exp', 'time_total_s': 11.40008544921875}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9634964514144104
PM2.5(0)的mse=32.43401943461393
PM2.5(0)的mae=3.437299248748974
PM2.5(0)的mar=0.13466491794269944
总共花费的时间为：60.59
漳州市
2075A
2920A
3216A
3530A
[flaml.automl: 09-17 19:57:58] {2390} INFO - task = regression
[flaml.automl: 09-17 19:57:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:57:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:57:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:57:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:57:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:57:59] {3025} INFO - Estimated sufficient time budget=53961s. Estimated necessary time budget=54s.
[flaml.automl: 09-17 19:57:59] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.0480,	best estimator xgboost's best error=15.0480
[flaml.automl: 09-17 19:57:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:58:01] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.1025,	best estimator xgboost's best error=7.1025
[flaml.automl: 09-17 19:58:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:58:02] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.1025,	best estimator xgboost's best error=7.1025
[flaml.automl: 09-17 19:58:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:58:09] {3072} INFO -  at 11.1s,	estimator xgboost's best error=7.1025,	best estimator xgboost's best error=7.1025
[flaml.automl: 09-17 19:58:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:58:10] {3072} INFO -  at 12.2s,	estimator xgboost's best error=4.8516,	best estimator xgboost's best error=4.8516
[flaml.automl: 09-17 19:58:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:58:11] {3072} INFO -  at 13.8s,	estimator xgboost's best error=4.8516,	best estimator xgboost's best error=4.8516
[flaml.automl: 09-17 19:58:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:58:13] {3072} INFO -  at 15.5s,	estimator xgboost's best error=3.4464,	best estimator xgboost's best error=3.4464
[flaml.automl: 09-17 19:58:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:58:16] {3072} INFO -  at 18.2s,	estimator xgboost's best error=3.4464,	best estimator xgboost's best error=3.4464
[flaml.automl: 09-17 19:58:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:58:17] {3072} INFO -  at 19.8s,	estimator xgboost's best error=3.4464,	best estimator xgboost's best error=3.4464
[flaml.automl: 09-17 19:58:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:58:20] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.4464,	best estimator xgboost's best error=3.4464
[flaml.automl: 09-17 19:58:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:58:22] {3072} INFO -  at 24.2s,	estimator xgboost's best error=3.4153,	best estimator xgboost's best error=3.4153
[flaml.automl: 09-17 19:58:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:58:23] {3072} INFO -  at 25.3s,	estimator xgboost's best error=3.4153,	best estimator xgboost's best error=3.4153
[flaml.automl: 09-17 19:58:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:58:27] {3072} INFO -  at 29.4s,	estimator xgboost's best error=3.1785,	best estimator xgboost's best error=3.1785
[flaml.automl: 09-17 19:58:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:58:30] {3072} INFO -  at 32.6s,	estimator xgboost's best error=3.1785,	best estimator xgboost's best error=3.1785
[flaml.automl: 09-17 19:58:30] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 19:58:32] {3072} INFO -  at 34.9s,	estimator xgboost's best error=3.1785,	best estimator xgboost's best error=3.1785
[flaml.automl: 09-17 19:58:32] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 19:58:34] {3072} INFO -  at 36.8s,	estimator xgboost's best error=3.1785,	best estimator xgboost's best error=3.1785
[flaml.automl: 09-17 19:58:34] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 19:58:37] {3072} INFO -  at 39.3s,	estimator xgboost's best error=3.1785,	best estimator xgboost's best error=3.1785
[flaml.automl: 09-17 19:58:37] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 19:58:47] {3072} INFO -  at 49.8s,	estimator xgboost's best error=3.0508,	best estimator xgboost's best error=3.0508
[flaml.automl: 09-17 19:58:58] {3335} INFO - retrain xgboost for 10.3s
[flaml.automl: 09-17 19:58:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:58:58] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:58:58] {2637} INFO - Time taken to find the best model: 49.78750491142273
[flaml.automl: 09-17 19:58:58] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 42105}
PM2.5(0)最佳损失：-2.05081518561597
PM2.5(0)最好结果：{'pred_time': 1.763573923944586e-05, 'wall_clock_time': 49.78750491142273, 'metric_for_logging': {'pred_time': 1.763573923944586e-05}, 'val_loss': 3.05081518561597, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 42105}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'config/FLAML_sample_size': 42105, 'experiment_tag': 'exp', 'time_total_s': 10.492262125015259}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8963830289661044
PM2.5(0)的mse=18.42098611961347
PM2.5(0)的mae=3.0001130295524367
PM2.5(0)的mar=0.16705045316828493
总共花费的时间为：60.84
晋城市
2160A
2161A
2162A
2163A
3620A
[flaml.automl: 09-17 20:13:50] {2390} INFO - task = regression
[flaml.automl: 09-17 20:13:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:13:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:13:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:13:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:13:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:13:51] {3025} INFO - Estimated sufficient time budget=62855s. Estimated necessary time budget=63s.
[flaml.automl: 09-17 20:13:51] {3072} INFO -  at 1.4s,	estimator xgboost's best error=21.8414,	best estimator xgboost's best error=21.8414
[flaml.automl: 09-17 20:13:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:13:53] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.8733,	best estimator xgboost's best error=10.8733
[flaml.automl: 09-17 20:13:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:13:55] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.8733,	best estimator xgboost's best error=10.8733
[flaml.automl: 09-17 20:13:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:13:59] {3072} INFO -  at 9.5s,	estimator xgboost's best error=10.8733,	best estimator xgboost's best error=10.8733
[flaml.automl: 09-17 20:13:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:14:01] {3072} INFO -  at 10.6s,	estimator xgboost's best error=7.6431,	best estimator xgboost's best error=7.6431
[flaml.automl: 09-17 20:14:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:14:02] {3072} INFO -  at 12.2s,	estimator xgboost's best error=7.6431,	best estimator xgboost's best error=7.6431
[flaml.automl: 09-17 20:14:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:14:04] {3072} INFO -  at 13.9s,	estimator xgboost's best error=6.1912,	best estimator xgboost's best error=6.1912
[flaml.automl: 09-17 20:14:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:14:07] {3072} INFO -  at 16.6s,	estimator xgboost's best error=6.1912,	best estimator xgboost's best error=6.1912
[flaml.automl: 09-17 20:14:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:14:08] {3072} INFO -  at 18.2s,	estimator xgboost's best error=6.1912,	best estimator xgboost's best error=6.1912
[flaml.automl: 09-17 20:14:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:14:11] {3072} INFO -  at 21.2s,	estimator xgboost's best error=6.1912,	best estimator xgboost's best error=6.1912
[flaml.automl: 09-17 20:14:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:14:13] {3072} INFO -  at 22.6s,	estimator xgboost's best error=6.0866,	best estimator xgboost's best error=6.0866
[flaml.automl: 09-17 20:14:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:14:14] {3072} INFO -  at 23.7s,	estimator xgboost's best error=6.0866,	best estimator xgboost's best error=6.0866
[flaml.automl: 09-17 20:14:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:14:17] {3072} INFO -  at 27.0s,	estimator xgboost's best error=5.5419,	best estimator xgboost's best error=5.5419
[flaml.automl: 09-17 20:14:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:14:20] {3072} INFO -  at 30.0s,	estimator xgboost's best error=5.4842,	best estimator xgboost's best error=5.4842
[flaml.automl: 09-17 20:14:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:14:22] {3072} INFO -  at 32.3s,	estimator xgboost's best error=5.4842,	best estimator xgboost's best error=5.4842
[flaml.automl: 09-17 20:14:22] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 20:14:24] {3072} INFO -  at 34.5s,	estimator xgboost's best error=5.4842,	best estimator xgboost's best error=5.4842
[flaml.automl: 09-17 20:14:24] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 20:14:27] {3072} INFO -  at 36.8s,	estimator xgboost's best error=5.4842,	best estimator xgboost's best error=5.4842
[flaml.automl: 09-17 20:14:27] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 20:14:29] {3072} INFO -  at 38.9s,	estimator xgboost's best error=5.4842,	best estimator xgboost's best error=5.4842
[flaml.automl: 09-17 20:14:29] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 20:14:39] {3072} INFO -  at 49.5s,	estimator xgboost's best error=5.2734,	best estimator xgboost's best error=5.2734
[flaml.automl: 09-17 20:14:50] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-17 20:14:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:14:50] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:14:50] {2637} INFO - Time taken to find the best model: 49.52469873428345
[flaml.automl: 09-17 20:14:50] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52322}
PM2.5(0)最佳损失：-4.27337144361889
PM2.5(0)最好结果：{'pred_time': 7.19065349619537e-06, 'wall_clock_time': 49.52469873428345, 'metric_for_logging': {'pred_time': 7.19065349619537e-06}, 'val_loss': 5.27337144361889, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52322}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 52322, 'experiment_tag': 'exp', 'time_total_s': 10.630660057067871}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8999050848515114
PM2.5(0)的mse=54.29675961518129
PM2.5(0)的mae=5.280134718078578
PM2.5(0)的mar=0.287955219752373
总共花费的时间为：61.17
朔州市
2166A
2167A
2168A
2169A
2170A
3571A
[flaml.automl: 09-17 20:33:28] {2390} INFO - task = regression
[flaml.automl: 09-17 20:33:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:33:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:33:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:33:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:33:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:33:29] {3025} INFO - Estimated sufficient time budget=77793s. Estimated necessary time budget=78s.
[flaml.automl: 09-17 20:33:29] {3072} INFO -  at 1.5s,	estimator xgboost's best error=19.6612,	best estimator xgboost's best error=19.6612
[flaml.automl: 09-17 20:33:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:33:32] {3072} INFO -  at 3.6s,	estimator xgboost's best error=10.3829,	best estimator xgboost's best error=10.3829
[flaml.automl: 09-17 20:33:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:33:33] {3072} INFO -  at 4.8s,	estimator xgboost's best error=10.3829,	best estimator xgboost's best error=10.3829
[flaml.automl: 09-17 20:33:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:33:37] {3072} INFO -  at 8.6s,	estimator xgboost's best error=10.3829,	best estimator xgboost's best error=10.3829
[flaml.automl: 09-17 20:33:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:33:38] {3072} INFO -  at 9.7s,	estimator xgboost's best error=8.5180,	best estimator xgboost's best error=8.5180
[flaml.automl: 09-17 20:33:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:33:39] {3072} INFO -  at 11.3s,	estimator xgboost's best error=8.5180,	best estimator xgboost's best error=8.5180
[flaml.automl: 09-17 20:33:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:33:41] {3072} INFO -  at 13.0s,	estimator xgboost's best error=6.8781,	best estimator xgboost's best error=6.8781
[flaml.automl: 09-17 20:33:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:33:44] {3072} INFO -  at 15.7s,	estimator xgboost's best error=6.8781,	best estimator xgboost's best error=6.8781
[flaml.automl: 09-17 20:33:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:33:45] {3072} INFO -  at 17.3s,	estimator xgboost's best error=6.8781,	best estimator xgboost's best error=6.8781
[flaml.automl: 09-17 20:33:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:33:48] {3072} INFO -  at 20.3s,	estimator xgboost's best error=6.8781,	best estimator xgboost's best error=6.8781
[flaml.automl: 09-17 20:33:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:33:50] {3072} INFO -  at 21.8s,	estimator xgboost's best error=6.8527,	best estimator xgboost's best error=6.8527
[flaml.automl: 09-17 20:33:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:33:51] {3072} INFO -  at 22.9s,	estimator xgboost's best error=6.8527,	best estimator xgboost's best error=6.8527
[flaml.automl: 09-17 20:33:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:33:53] {3072} INFO -  at 25.2s,	estimator xgboost's best error=6.6098,	best estimator xgboost's best error=6.6098
[flaml.automl: 09-17 20:33:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:33:55] {3072} INFO -  at 27.4s,	estimator xgboost's best error=6.4169,	best estimator xgboost's best error=6.4169
[flaml.automl: 09-17 20:33:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:33:58] {3072} INFO -  at 29.7s,	estimator xgboost's best error=6.4169,	best estimator xgboost's best error=6.4169
[flaml.automl: 09-17 20:33:58] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 20:34:00] {3072} INFO -  at 31.9s,	estimator xgboost's best error=6.4169,	best estimator xgboost's best error=6.4169
[flaml.automl: 09-17 20:34:00] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 20:34:02] {3072} INFO -  at 33.8s,	estimator xgboost's best error=6.4169,	best estimator xgboost's best error=6.4169
[flaml.automl: 09-17 20:34:02] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 20:34:04] {3072} INFO -  at 35.7s,	estimator xgboost's best error=6.4169,	best estimator xgboost's best error=6.4169
[flaml.automl: 09-17 20:34:04] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 20:34:14] {3072} INFO -  at 46.5s,	estimator xgboost's best error=6.1933,	best estimator xgboost's best error=6.1933
[flaml.automl: 09-17 20:34:14] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 20:34:18] {3072} INFO -  at 50.5s,	estimator xgboost's best error=6.1933,	best estimator xgboost's best error=6.1933
[flaml.automl: 09-17 20:34:29] {3335} INFO - retrain xgboost for 10.8s
[flaml.automl: 09-17 20:34:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:34:29] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:34:29] {2637} INFO - Time taken to find the best model: 46.45898151397705
[flaml.automl: 09-17 20:34:29] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 64745}
PM2.5(0)最佳损失：-5.193256600916568
PM2.5(0)最好结果：{'pred_time': 5.523761444897794e-06, 'wall_clock_time': 46.45898151397705, 'metric_for_logging': {'pred_time': 5.523761444897794e-06}, 'val_loss': 6.193256600916568, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 64745}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 64745, 'experiment_tag': 'exp', 'time_total_s': 10.803843975067139}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8338076899650254
PM2.5(0)的mse=74.48656772476646
PM2.5(0)的mae=6.052375147245454
PM2.5(0)的mar=0.3286161969221443
总共花费的时间为：62.35
晋中市
2171A
2174A
2865A
[flaml.automl: 09-17 20:43:56] {2390} INFO - task = regression
[flaml.automl: 09-17 20:43:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:43:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:43:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:43:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:43:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:43:58] {3025} INFO - Estimated sufficient time budget=12120s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 20:43:58] {3072} INFO -  at 1.4s,	estimator xgboost's best error=24.5283,	best estimator xgboost's best error=24.5283
[flaml.automl: 09-17 20:43:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:44:00] {3072} INFO -  at 3.5s,	estimator xgboost's best error=11.5576,	best estimator xgboost's best error=11.5576
[flaml.automl: 09-17 20:44:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:44:01] {3072} INFO -  at 4.7s,	estimator xgboost's best error=11.5576,	best estimator xgboost's best error=11.5576
[flaml.automl: 09-17 20:44:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:44:11] {3072} INFO -  at 14.7s,	estimator xgboost's best error=11.5576,	best estimator xgboost's best error=11.5576
[flaml.automl: 09-17 20:44:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:44:12] {3072} INFO -  at 15.9s,	estimator xgboost's best error=7.3939,	best estimator xgboost's best error=7.3939
[flaml.automl: 09-17 20:44:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:44:14] {3072} INFO -  at 17.5s,	estimator xgboost's best error=6.9227,	best estimator xgboost's best error=6.9227
[flaml.automl: 09-17 20:44:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:44:15] {3072} INFO -  at 19.1s,	estimator xgboost's best error=6.0867,	best estimator xgboost's best error=6.0867
[flaml.automl: 09-17 20:44:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:44:18] {3072} INFO -  at 21.8s,	estimator xgboost's best error=6.0867,	best estimator xgboost's best error=6.0867
[flaml.automl: 09-17 20:44:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:44:20] {3072} INFO -  at 23.5s,	estimator xgboost's best error=6.0867,	best estimator xgboost's best error=6.0867
[flaml.automl: 09-17 20:44:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:44:23] {3072} INFO -  at 26.5s,	estimator xgboost's best error=5.6891,	best estimator xgboost's best error=5.6891
[flaml.automl: 09-17 20:44:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:44:24] {3072} INFO -  at 28.1s,	estimator xgboost's best error=5.6891,	best estimator xgboost's best error=5.6891
[flaml.automl: 09-17 20:44:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:44:26] {3072} INFO -  at 29.3s,	estimator xgboost's best error=5.6891,	best estimator xgboost's best error=5.6891
[flaml.automl: 09-17 20:44:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:44:39] {3072} INFO -  at 43.0s,	estimator xgboost's best error=5.2568,	best estimator xgboost's best error=5.2568
[flaml.automl: 09-17 20:44:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:44:56] {3072} INFO -  at 59.4s,	estimator xgboost's best error=5.0600,	best estimator xgboost's best error=5.0600
[flaml.automl: 09-17 20:45:20] {3335} INFO - retrain xgboost for 23.8s
[flaml.automl: 09-17 20:45:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:45:20] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:45:20] {2637} INFO - Time taken to find the best model: 59.42295002937317
[flaml.automl: 09-17 20:45:20] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-4.060047309136404
PM2.5(0)最好结果：{'pred_time': 1.1188700370852471e-05, 'wall_clock_time': 59.42295002937317, 'metric_for_logging': {'pred_time': 1.1188700370852471e-05}, 'val_loss': 5.060047309136404, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.460800647735596}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8975927659477079
PM2.5(0)的mse=61.00228501437267
PM2.5(0)的mae=5.195113455770004
PM2.5(0)的mar=0.18203630934255566
总共花费的时间为：83.77
运城市
2175A
2178A
2179A
3670A
[flaml.automl: 09-17 20:57:42] {2390} INFO - task = regression
[flaml.automl: 09-17 20:57:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:57:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:57:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:57:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:57:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:57:43] {3025} INFO - Estimated sufficient time budget=50136s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 20:57:43] {3072} INFO -  at 1.4s,	estimator xgboost's best error=29.0097,	best estimator xgboost's best error=29.0097
[flaml.automl: 09-17 20:57:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:57:45] {3072} INFO -  at 3.5s,	estimator xgboost's best error=13.5819,	best estimator xgboost's best error=13.5819
[flaml.automl: 09-17 20:57:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:57:46] {3072} INFO -  at 4.7s,	estimator xgboost's best error=13.5819,	best estimator xgboost's best error=13.5819
[flaml.automl: 09-17 20:57:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:57:53] {3072} INFO -  at 11.1s,	estimator xgboost's best error=13.5819,	best estimator xgboost's best error=13.5819
[flaml.automl: 09-17 20:57:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:57:54] {3072} INFO -  at 12.2s,	estimator xgboost's best error=9.3453,	best estimator xgboost's best error=9.3453
[flaml.automl: 09-17 20:57:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:57:56] {3072} INFO -  at 13.8s,	estimator xgboost's best error=9.3453,	best estimator xgboost's best error=9.3453
[flaml.automl: 09-17 20:57:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:57:57] {3072} INFO -  at 15.5s,	estimator xgboost's best error=6.2845,	best estimator xgboost's best error=6.2845
[flaml.automl: 09-17 20:57:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:58:00] {3072} INFO -  at 18.2s,	estimator xgboost's best error=6.2845,	best estimator xgboost's best error=6.2845
[flaml.automl: 09-17 20:58:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:58:02] {3072} INFO -  at 19.8s,	estimator xgboost's best error=6.2845,	best estimator xgboost's best error=6.2845
[flaml.automl: 09-17 20:58:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:58:05] {3072} INFO -  at 22.8s,	estimator xgboost's best error=6.2845,	best estimator xgboost's best error=6.2845
[flaml.automl: 09-17 20:58:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:58:06] {3072} INFO -  at 24.3s,	estimator xgboost's best error=6.1043,	best estimator xgboost's best error=6.1043
[flaml.automl: 09-17 20:58:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:58:07] {3072} INFO -  at 25.4s,	estimator xgboost's best error=6.1043,	best estimator xgboost's best error=6.1043
[flaml.automl: 09-17 20:58:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:58:11] {3072} INFO -  at 29.2s,	estimator xgboost's best error=5.4499,	best estimator xgboost's best error=5.4499
[flaml.automl: 09-17 20:58:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:58:14] {3072} INFO -  at 32.7s,	estimator xgboost's best error=5.3252,	best estimator xgboost's best error=5.3252
[flaml.automl: 09-17 20:58:14] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:58:17] {3072} INFO -  at 35.5s,	estimator xgboost's best error=5.3252,	best estimator xgboost's best error=5.3252
[flaml.automl: 09-17 20:58:17] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 20:58:19] {3072} INFO -  at 37.7s,	estimator xgboost's best error=5.3252,	best estimator xgboost's best error=5.3252
[flaml.automl: 09-17 20:58:19] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 20:58:22] {3072} INFO -  at 40.0s,	estimator xgboost's best error=5.3252,	best estimator xgboost's best error=5.3252
[flaml.automl: 09-17 20:58:22] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 20:58:24] {3072} INFO -  at 42.1s,	estimator xgboost's best error=5.3252,	best estimator xgboost's best error=5.3252
[flaml.automl: 09-17 20:58:24] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 20:58:35] {3072} INFO -  at 52.9s,	estimator xgboost's best error=5.0250,	best estimator xgboost's best error=5.0250
[flaml.automl: 09-17 20:58:45] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-17 20:58:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:58:45] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:58:45] {2637} INFO - Time taken to find the best model: 52.90130138397217
[flaml.automl: 09-17 20:58:45] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 42002}
PM2.5(0)最佳损失：-4.0249648767491335
PM2.5(0)最好结果：{'pred_time': 9.2405973523951e-06, 'wall_clock_time': 52.90130138397217, 'metric_for_logging': {'pred_time': 9.2405973523951e-06}, 'val_loss': 5.0249648767491335, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 42002}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 42002, 'experiment_tag': 'exp', 'time_total_s': 10.776715755462646}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9502346373089635
PM2.5(0)的mse=46.749794132446716
PM2.5(0)的mae=4.975836860227897
PM2.5(0)的mar=0.1685801790443106
总共花费的时间为：64.31
忻州市
2182A
3208A
[flaml.automl: 09-17 21:05:47] {2390} INFO - task = regression
[flaml.automl: 09-17 21:05:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:05:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:05:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:05:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:05:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:05:48] {3025} INFO - Estimated sufficient time budget=12034s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:05:48] {3072} INFO -  at 1.3s,	estimator xgboost's best error=23.7098,	best estimator xgboost's best error=23.7098
[flaml.automl: 09-17 21:05:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:05:50] {3072} INFO -  at 3.4s,	estimator xgboost's best error=12.0659,	best estimator xgboost's best error=12.0659
[flaml.automl: 09-17 21:05:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:05:51] {3072} INFO -  at 4.6s,	estimator xgboost's best error=12.0659,	best estimator xgboost's best error=12.0659
[flaml.automl: 09-17 21:05:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:06:01] {3072} INFO -  at 14.0s,	estimator xgboost's best error=12.0659,	best estimator xgboost's best error=12.0659
[flaml.automl: 09-17 21:06:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:06:02] {3072} INFO -  at 15.2s,	estimator xgboost's best error=9.0615,	best estimator xgboost's best error=9.0615
[flaml.automl: 09-17 21:06:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:06:03] {3072} INFO -  at 16.7s,	estimator xgboost's best error=9.0615,	best estimator xgboost's best error=9.0615
[flaml.automl: 09-17 21:06:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:06:05] {3072} INFO -  at 18.4s,	estimator xgboost's best error=7.1640,	best estimator xgboost's best error=7.1640
[flaml.automl: 09-17 21:06:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:06:08] {3072} INFO -  at 21.1s,	estimator xgboost's best error=7.1640,	best estimator xgboost's best error=7.1640
[flaml.automl: 09-17 21:06:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:06:09] {3072} INFO -  at 22.8s,	estimator xgboost's best error=7.1640,	best estimator xgboost's best error=7.1640
[flaml.automl: 09-17 21:06:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:06:12] {3072} INFO -  at 25.7s,	estimator xgboost's best error=7.1640,	best estimator xgboost's best error=7.1640
[flaml.automl: 09-17 21:06:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:06:14] {3072} INFO -  at 27.4s,	estimator xgboost's best error=6.8929,	best estimator xgboost's best error=6.8929
[flaml.automl: 09-17 21:06:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:06:15] {3072} INFO -  at 28.6s,	estimator xgboost's best error=6.8929,	best estimator xgboost's best error=6.8929
[flaml.automl: 09-17 21:06:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:06:22] {3072} INFO -  at 35.0s,	estimator xgboost's best error=6.3274,	best estimator xgboost's best error=6.3274
[flaml.automl: 09-17 21:06:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:06:33] {3072} INFO -  at 46.1s,	estimator xgboost's best error=6.2403,	best estimator xgboost's best error=6.2403
[flaml.automl: 09-17 21:06:33] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 21:06:39] {3072} INFO -  at 52.5s,	estimator xgboost's best error=6.2403,	best estimator xgboost's best error=6.2403
[flaml.automl: 09-17 21:06:50] {3335} INFO - retrain xgboost for 11.0s
[flaml.automl: 09-17 21:06:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:06:50] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:06:50] {2637} INFO - Time taken to find the best model: 46.083855867385864
[flaml.automl: 09-17 21:06:50] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-5.240270872438357
PM2.5(0)最好结果：{'pred_time': 1.6726500599096852e-05, 'wall_clock_time': 46.083855867385864, 'metric_for_logging': {'pred_time': 1.6726500599096852e-05}, 'val_loss': 6.240270872438357, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 11.078644752502441}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9005140861145857
PM2.5(0)的mse=78.68545531104536
PM2.5(0)的mae=6.370097099034278
PM2.5(0)的mar=0.40931780247316896
总共花费的时间为：63.95
吕梁市
2183A
2867A
[flaml.automl: 09-17 21:13:46] {2390} INFO - task = regression
[flaml.automl: 09-17 21:13:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:13:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:13:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:13:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:13:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:13:47] {3025} INFO - Estimated sufficient time budget=12018s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:13:47] {3072} INFO -  at 1.3s,	estimator xgboost's best error=17.4778,	best estimator xgboost's best error=17.4778
[flaml.automl: 09-17 21:13:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:13:49] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.7250,	best estimator xgboost's best error=8.7250
[flaml.automl: 09-17 21:13:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:13:50] {3072} INFO -  at 4.6s,	estimator xgboost's best error=8.7250,	best estimator xgboost's best error=8.7250
[flaml.automl: 09-17 21:13:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:14:00] {3072} INFO -  at 14.0s,	estimator xgboost's best error=8.7250,	best estimator xgboost's best error=8.7250
[flaml.automl: 09-17 21:14:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:14:01] {3072} INFO -  at 15.2s,	estimator xgboost's best error=6.0578,	best estimator xgboost's best error=6.0578
[flaml.automl: 09-17 21:14:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:14:02] {3072} INFO -  at 16.7s,	estimator xgboost's best error=5.7400,	best estimator xgboost's best error=5.7400
[flaml.automl: 09-17 21:14:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:14:04] {3072} INFO -  at 18.4s,	estimator xgboost's best error=5.4081,	best estimator xgboost's best error=5.4081
[flaml.automl: 09-17 21:14:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:14:06] {3072} INFO -  at 20.7s,	estimator xgboost's best error=5.4081,	best estimator xgboost's best error=5.4081
[flaml.automl: 09-17 21:14:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:14:08] {3072} INFO -  at 22.3s,	estimator xgboost's best error=5.4081,	best estimator xgboost's best error=5.4081
[flaml.automl: 09-17 21:14:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:14:11] {3072} INFO -  at 25.3s,	estimator xgboost's best error=5.2122,	best estimator xgboost's best error=5.2122
[flaml.automl: 09-17 21:14:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:14:13] {3072} INFO -  at 26.9s,	estimator xgboost's best error=5.2122,	best estimator xgboost's best error=5.2122
[flaml.automl: 09-17 21:14:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:14:14] {3072} INFO -  at 28.1s,	estimator xgboost's best error=5.2122,	best estimator xgboost's best error=5.2122
[flaml.automl: 09-17 21:14:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:14:26] {3072} INFO -  at 40.0s,	estimator xgboost's best error=5.0762,	best estimator xgboost's best error=5.0762
[flaml.automl: 09-17 21:14:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:14:45] {3072} INFO -  at 59.9s,	estimator xgboost's best error=4.9148,	best estimator xgboost's best error=4.9148
[flaml.automl: 09-17 21:15:07] {3335} INFO - retrain xgboost for 21.8s
[flaml.automl: 09-17 21:15:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:15:07] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:15:07] {2637} INFO - Time taken to find the best model: 59.867210149765015
[flaml.automl: 09-17 21:15:07] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-3.91484879105031
PM2.5(0)最好结果：{'pred_time': 1.896299233957499e-05, 'wall_clock_time': 59.867210149765015, 'metric_for_logging': {'pred_time': 1.896299233957499e-05}, 'val_loss': 4.91484879105031, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 19.87235736846924}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8355389026154147
PM2.5(0)的mse=41.203194972719984
PM2.5(0)的mae=4.864220047718185
PM2.5(0)的mar=0.2853442387204108
总共花费的时间为：82.12
乌海市
2188A
3284A
3621A
[flaml.automl: 09-17 21:24:53] {2390} INFO - task = regression
[flaml.automl: 09-17 21:24:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:24:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:24:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:24:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:24:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:24:55] {3025} INFO - Estimated sufficient time budget=12232s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:24:55] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.3970,	best estimator xgboost's best error=18.3970
[flaml.automl: 09-17 21:24:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:24:57] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.1536,	best estimator xgboost's best error=10.1536
[flaml.automl: 09-17 21:24:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:24:58] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.1536,	best estimator xgboost's best error=10.1536
[flaml.automl: 09-17 21:24:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:25:08] {3072} INFO -  at 14.8s,	estimator xgboost's best error=10.1536,	best estimator xgboost's best error=10.1536
[flaml.automl: 09-17 21:25:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:25:09] {3072} INFO -  at 15.9s,	estimator xgboost's best error=8.3390,	best estimator xgboost's best error=8.3390
[flaml.automl: 09-17 21:25:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:25:11] {3072} INFO -  at 17.5s,	estimator xgboost's best error=8.3390,	best estimator xgboost's best error=8.3390
[flaml.automl: 09-17 21:25:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:25:13] {3072} INFO -  at 19.2s,	estimator xgboost's best error=7.4857,	best estimator xgboost's best error=7.4857
[flaml.automl: 09-17 21:25:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:25:15] {3072} INFO -  at 21.9s,	estimator xgboost's best error=7.4857,	best estimator xgboost's best error=7.4857
[flaml.automl: 09-17 21:25:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:25:17] {3072} INFO -  at 23.5s,	estimator xgboost's best error=7.4857,	best estimator xgboost's best error=7.4857
[flaml.automl: 09-17 21:25:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:25:20] {3072} INFO -  at 26.5s,	estimator xgboost's best error=7.4857,	best estimator xgboost's best error=7.4857
[flaml.automl: 09-17 21:25:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:25:21] {3072} INFO -  at 27.9s,	estimator xgboost's best error=7.4857,	best estimator xgboost's best error=7.4857
[flaml.automl: 09-17 21:25:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:25:22] {3072} INFO -  at 29.0s,	estimator xgboost's best error=7.4857,	best estimator xgboost's best error=7.4857
[flaml.automl: 09-17 21:25:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:25:29] {3072} INFO -  at 35.9s,	estimator xgboost's best error=7.1077,	best estimator xgboost's best error=7.1077
[flaml.automl: 09-17 21:25:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:25:42] {3072} INFO -  at 48.5s,	estimator xgboost's best error=6.9851,	best estimator xgboost's best error=6.9851
[flaml.automl: 09-17 21:25:55] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-17 21:25:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:25:55] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:25:55] {2637} INFO - Time taken to find the best model: 48.49899077415466
[flaml.automl: 09-17 21:25:55] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-5.985104231520014
PM2.5(0)最好结果：{'pred_time': 1.0802850618467227e-05, 'wall_clock_time': 48.49899077415466, 'metric_for_logging': {'pred_time': 1.0802850618467227e-05}, 'val_loss': 6.985104231520014, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 12.589653015136719}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.7058809384402347
PM2.5(0)的mse=125.51897988998664
PM2.5(0)的mae=6.973533025630767
PM2.5(0)的mar=0.3629408876474385
总共花费的时间为：61.71
通辽市
2191A
3706A
3708A
[flaml.automl: 09-17 21:35:05] {2390} INFO - task = regression
[flaml.automl: 09-17 21:35:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:35:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:35:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:35:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:35:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:35:06] {3025} INFO - Estimated sufficient time budget=12146s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:35:06] {3072} INFO -  at 1.3s,	estimator xgboost's best error=17.0685,	best estimator xgboost's best error=17.0685
[flaml.automl: 09-17 21:35:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:35:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.3733,	best estimator xgboost's best error=8.3733
[flaml.automl: 09-17 21:35:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:35:09] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.3733,	best estimator xgboost's best error=8.3733
[flaml.automl: 09-17 21:35:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:35:19] {3072} INFO -  at 14.7s,	estimator xgboost's best error=8.3733,	best estimator xgboost's best error=8.3733
[flaml.automl: 09-17 21:35:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:35:20] {3072} INFO -  at 15.9s,	estimator xgboost's best error=6.7158,	best estimator xgboost's best error=6.7158
[flaml.automl: 09-17 21:35:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:35:22] {3072} INFO -  at 17.4s,	estimator xgboost's best error=6.7158,	best estimator xgboost's best error=6.7158
[flaml.automl: 09-17 21:35:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:35:24] {3072} INFO -  at 19.2s,	estimator xgboost's best error=5.0133,	best estimator xgboost's best error=5.0133
[flaml.automl: 09-17 21:35:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:35:26] {3072} INFO -  at 21.9s,	estimator xgboost's best error=5.0133,	best estimator xgboost's best error=5.0133
[flaml.automl: 09-17 21:35:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:35:28] {3072} INFO -  at 23.5s,	estimator xgboost's best error=5.0133,	best estimator xgboost's best error=5.0133
[flaml.automl: 09-17 21:35:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:35:31] {3072} INFO -  at 26.5s,	estimator xgboost's best error=5.0133,	best estimator xgboost's best error=5.0133
[flaml.automl: 09-17 21:35:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:35:33] {3072} INFO -  at 28.0s,	estimator xgboost's best error=5.0133,	best estimator xgboost's best error=5.0133
[flaml.automl: 09-17 21:35:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:35:34] {3072} INFO -  at 29.2s,	estimator xgboost's best error=5.0133,	best estimator xgboost's best error=5.0133
[flaml.automl: 09-17 21:35:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:35:41] {3072} INFO -  at 36.2s,	estimator xgboost's best error=4.6831,	best estimator xgboost's best error=4.6831
[flaml.automl: 09-17 21:35:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:35:54] {3072} INFO -  at 49.0s,	estimator xgboost's best error=4.5826,	best estimator xgboost's best error=4.5826
[flaml.automl: 09-17 21:36:06] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-17 21:36:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:36:06] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:36:06] {2637} INFO - Time taken to find the best model: 48.99563455581665
[flaml.automl: 09-17 21:36:06] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-3.5826278548184334
PM2.5(0)最好结果：{'pred_time': 1.1670695914551766e-05, 'wall_clock_time': 48.99563455581665, 'metric_for_logging': {'pred_time': 1.1670695914551766e-05}, 'val_loss': 4.582627854818433, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 12.76859974861145}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8536678320072735
PM2.5(0)的mse=47.99303919569656
PM2.5(0)的mae=4.527180949682022
PM2.5(0)的mar=0.24750641771184767
总共花费的时间为：62.30
呼伦贝尔市
2192A
[flaml.automl: 09-17 21:39:45] {2390} INFO - task = regression
[flaml.automl: 09-17 21:39:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:39:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:39:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:39:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:39:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:39:46] {3025} INFO - Estimated sufficient time budget=12061s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:39:46] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.6964,	best estimator xgboost's best error=11.6964
[flaml.automl: 09-17 21:39:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:39:48] {3072} INFO -  at 3.1s,	estimator xgboost's best error=6.3005,	best estimator xgboost's best error=6.3005
[flaml.automl: 09-17 21:39:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:39:49] {3072} INFO -  at 4.3s,	estimator xgboost's best error=6.3005,	best estimator xgboost's best error=6.3005
[flaml.automl: 09-17 21:39:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:39:56] {3072} INFO -  at 11.1s,	estimator xgboost's best error=6.3005,	best estimator xgboost's best error=6.3005
[flaml.automl: 09-17 21:39:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:39:57] {3072} INFO -  at 12.2s,	estimator xgboost's best error=3.7221,	best estimator xgboost's best error=3.7221
[flaml.automl: 09-17 21:39:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:39:58] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.4600,	best estimator xgboost's best error=3.4600
[flaml.automl: 09-17 21:39:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:40:00] {3072} INFO -  at 15.5s,	estimator xgboost's best error=3.4024,	best estimator xgboost's best error=3.4024
[flaml.automl: 09-17 21:40:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:40:02] {3072} INFO -  at 17.8s,	estimator xgboost's best error=3.4024,	best estimator xgboost's best error=3.4024
[flaml.automl: 09-17 21:40:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:40:04] {3072} INFO -  at 19.5s,	estimator xgboost's best error=3.3120,	best estimator xgboost's best error=3.3120
[flaml.automl: 09-17 21:40:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:40:07] {3072} INFO -  at 22.1s,	estimator xgboost's best error=3.2878,	best estimator xgboost's best error=3.2878
[flaml.automl: 09-17 21:40:07] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:40:08] {3072} INFO -  at 23.2s,	estimator xgboost's best error=3.2878,	best estimator xgboost's best error=3.2878
[flaml.automl: 09-17 21:40:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:40:17] {3072} INFO -  at 32.8s,	estimator xgboost's best error=3.2878,	best estimator xgboost's best error=3.2878
[flaml.automl: 09-17 21:40:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:40:22] {3072} INFO -  at 37.5s,	estimator xgboost's best error=3.2545,	best estimator xgboost's best error=3.2545
[flaml.automl: 09-17 21:40:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:40:25] {3072} INFO -  at 40.1s,	estimator xgboost's best error=3.2545,	best estimator xgboost's best error=3.2545
[flaml.automl: 09-17 21:40:25] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 21:40:32] {3072} INFO -  at 47.7s,	estimator xgboost's best error=3.2545,	best estimator xgboost's best error=3.2545
[flaml.automl: 09-17 21:40:32] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 21:40:35] {3072} INFO -  at 51.0s,	estimator xgboost's best error=3.1991,	best estimator xgboost's best error=3.1991
[flaml.automl: 09-17 21:40:35] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 21:40:38] {3072} INFO -  at 53.8s,	estimator xgboost's best error=3.1991,	best estimator xgboost's best error=3.1991
[flaml.automl: 09-17 21:40:38] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 21:40:44] {3072} INFO -  at 59.5s,	estimator xgboost's best error=3.1991,	best estimator xgboost's best error=3.1991
[flaml.automl: 09-17 21:40:47] {3335} INFO - retrain xgboost for 3.3s
[flaml.automl: 09-17 21:40:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.34626633530280915,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=0.008209511633703386, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001948985968793587, reg_lambda=0.9688361855260479,
             scale_pos_weight=1, subsample=0.6609434038352751,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 21:40:47] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:40:47] {2637} INFO - Time taken to find the best model: 50.98919463157654
[flaml.automl: 09-17 21:40:47] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 4, 'min_child_weight': 0.008209511633703386, 'learning_rate': 0.34626633530280915, 'subsample': 0.6609434038352751, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.001948985968793587, 'reg_lambda': 0.9688361855260479}
PM2.5(0)最佳损失：-2.1990717767209422
PM2.5(0)最好结果：{'pred_time': 3.7758301715461575e-05, 'wall_clock_time': 50.98919463157654, 'metric_for_logging': {'pred_time': 3.7758301715461575e-05}, 'val_loss': 3.1990717767209422, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 4, 'min_child_weight': 0.008209511633703386, 'learning_rate': 0.34626633530280915, 'subsample': 0.6609434038352751, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.001948985968793587, 'reg_lambda': 0.9688361855260479}, 'config/n_estimators': 13, 'config/max_leaves': 4, 'config/min_child_weight': 0.008209511633703386, 'config/learning_rate': 0.34626633530280915, 'config/subsample': 0.6609434038352751, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.001948985968793587, 'config/reg_lambda': 0.9688361855260479, 'experiment_tag': 'exp', 'time_total_s': 3.301389455795288}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.34626633530280915,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=0.008209511633703386, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001948985968793587, reg_lambda=0.9688361855260479,
             scale_pos_weight=1, subsample=0.6609434038352751,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.631595152583107
PM2.5(0)的mse=15.504919414728493
PM2.5(0)的mae=3.1467472733204938
PM2.5(0)的mar=0.20259950214115444
总共花费的时间为：63.02
巴彦淖尔市
2196A
[flaml.automl: 09-17 21:44:09] {2390} INFO - task = regression
[flaml.automl: 09-17 21:44:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:44:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:44:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:44:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:44:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:44:10] {3025} INFO - Estimated sufficient time budget=11970s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:44:10] {3072} INFO -  at 1.2s,	estimator xgboost's best error=18.8317,	best estimator xgboost's best error=18.8317
[flaml.automl: 09-17 21:44:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:44:12] {3072} INFO -  at 3.1s,	estimator xgboost's best error=10.1692,	best estimator xgboost's best error=10.1692
[flaml.automl: 09-17 21:44:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:44:13] {3072} INFO -  at 4.3s,	estimator xgboost's best error=10.1692,	best estimator xgboost's best error=10.1692
[flaml.automl: 09-17 21:44:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:44:20] {3072} INFO -  at 11.4s,	estimator xgboost's best error=10.1692,	best estimator xgboost's best error=10.1692
[flaml.automl: 09-17 21:44:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:44:21] {3072} INFO -  at 12.5s,	estimator xgboost's best error=6.5268,	best estimator xgboost's best error=6.5268
[flaml.automl: 09-17 21:44:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:44:23] {3072} INFO -  at 14.1s,	estimator xgboost's best error=5.8458,	best estimator xgboost's best error=5.8458
[flaml.automl: 09-17 21:44:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:44:24] {3072} INFO -  at 15.7s,	estimator xgboost's best error=5.8088,	best estimator xgboost's best error=5.8088
[flaml.automl: 09-17 21:44:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:44:27] {3072} INFO -  at 18.1s,	estimator xgboost's best error=5.8088,	best estimator xgboost's best error=5.8088
[flaml.automl: 09-17 21:44:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:44:28] {3072} INFO -  at 19.7s,	estimator xgboost's best error=5.8088,	best estimator xgboost's best error=5.8088
[flaml.automl: 09-17 21:44:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:44:31] {3072} INFO -  at 22.3s,	estimator xgboost's best error=5.7179,	best estimator xgboost's best error=5.7179
[flaml.automl: 09-17 21:44:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:44:33] {3072} INFO -  at 23.9s,	estimator xgboost's best error=5.7179,	best estimator xgboost's best error=5.7179
[flaml.automl: 09-17 21:44:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:44:34] {3072} INFO -  at 25.1s,	estimator xgboost's best error=5.7179,	best estimator xgboost's best error=5.7179
[flaml.automl: 09-17 21:44:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:44:43] {3072} INFO -  at 34.7s,	estimator xgboost's best error=5.5933,	best estimator xgboost's best error=5.5933
[flaml.automl: 09-17 21:44:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:45:00] {3072} INFO -  at 51.1s,	estimator xgboost's best error=5.3311,	best estimator xgboost's best error=5.3311
[flaml.automl: 09-17 21:45:24] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-17 21:45:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:45:24] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:45:24] {2637} INFO - Time taken to find the best model: 51.08809804916382
[flaml.automl: 09-17 21:45:24] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 11, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-4.331083689960221
PM2.5(0)最好结果：{'pred_time': 3.654627359410533e-05, 'wall_clock_time': 51.08809804916382, 'metric_for_logging': {'pred_time': 3.654627359410533e-05}, 'val_loss': 5.331083689960221, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 11, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 26, 'config/max_leaves': 11, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.388649940490723}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.763415089488559
PM2.5(0)的mse=93.68466164681159
PM2.5(0)的mae=5.675181366749946
PM2.5(0)的mar=0.22352426551185864
总共花费的时间为：75.22
乌兰察布市
2197A
3285A
3421A
[flaml.automl: 09-17 21:54:47] {2390} INFO - task = regression
[flaml.automl: 09-17 21:54:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:54:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:54:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:54:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:54:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:54:48] {3025} INFO - Estimated sufficient time budget=12065s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:54:48] {3072} INFO -  at 1.3s,	estimator xgboost's best error=13.0463,	best estimator xgboost's best error=13.0463
[flaml.automl: 09-17 21:54:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:54:51] {3072} INFO -  at 3.4s,	estimator xgboost's best error=6.7065,	best estimator xgboost's best error=6.7065
[flaml.automl: 09-17 21:54:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:54:52] {3072} INFO -  at 4.6s,	estimator xgboost's best error=6.7065,	best estimator xgboost's best error=6.7065
[flaml.automl: 09-17 21:54:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:55:02] {3072} INFO -  at 14.5s,	estimator xgboost's best error=6.7065,	best estimator xgboost's best error=6.7065
[flaml.automl: 09-17 21:55:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:55:03] {3072} INFO -  at 15.7s,	estimator xgboost's best error=5.1719,	best estimator xgboost's best error=5.1719
[flaml.automl: 09-17 21:55:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:55:04] {3072} INFO -  at 17.3s,	estimator xgboost's best error=4.7874,	best estimator xgboost's best error=4.7874
[flaml.automl: 09-17 21:55:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:55:06] {3072} INFO -  at 18.9s,	estimator xgboost's best error=4.5364,	best estimator xgboost's best error=4.5364
[flaml.automl: 09-17 21:55:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:55:09] {3072} INFO -  at 21.5s,	estimator xgboost's best error=4.5364,	best estimator xgboost's best error=4.5364
[flaml.automl: 09-17 21:55:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:55:10] {3072} INFO -  at 23.2s,	estimator xgboost's best error=4.5364,	best estimator xgboost's best error=4.5364
[flaml.automl: 09-17 21:55:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:55:13] {3072} INFO -  at 26.2s,	estimator xgboost's best error=4.2848,	best estimator xgboost's best error=4.2848
[flaml.automl: 09-17 21:55:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:55:15] {3072} INFO -  at 27.8s,	estimator xgboost's best error=4.2848,	best estimator xgboost's best error=4.2848
[flaml.automl: 09-17 21:55:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:55:16] {3072} INFO -  at 29.0s,	estimator xgboost's best error=4.2848,	best estimator xgboost's best error=4.2848
[flaml.automl: 09-17 21:55:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:55:30] {3072} INFO -  at 42.6s,	estimator xgboost's best error=4.1891,	best estimator xgboost's best error=4.1891
[flaml.automl: 09-17 21:55:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:55:47] {3072} INFO -  at 59.7s,	estimator xgboost's best error=4.0360,	best estimator xgboost's best error=4.0360
[flaml.automl: 09-17 21:56:11] {3335} INFO - retrain xgboost for 24.0s
[flaml.automl: 09-17 21:56:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:56:11] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:56:11] {2637} INFO - Time taken to find the best model: 59.68885898590088
[flaml.automl: 09-17 21:56:11] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-3.0360036397757737
PM2.5(0)最好结果：{'pred_time': 1.0616559049357539e-05, 'wall_clock_time': 59.68885898590088, 'metric_for_logging': {'pred_time': 1.0616559049357539e-05}, 'val_loss': 4.036003639775774, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 17.12935757637024}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8016518574791707
PM2.5(0)的mse=41.088599073784046
PM2.5(0)的mae=4.2040573032306385
PM2.5(0)的mar=0.3351938826571317
总共花费的时间为：84.20
阜新市
2207A
2208A
2209A
2210A
2211A
[flaml.automl: 09-17 22:11:16] {2390} INFO - task = regression
[flaml.automl: 09-17 22:11:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:11:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:11:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:11:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:11:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:11:17] {3025} INFO - Estimated sufficient time budget=64790s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 22:11:17] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.6890,	best estimator xgboost's best error=18.6890
[flaml.automl: 09-17 22:11:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:11:19] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.7995,	best estimator xgboost's best error=9.7995
[flaml.automl: 09-17 22:11:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:11:20] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.7995,	best estimator xgboost's best error=9.7995
[flaml.automl: 09-17 22:11:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:11:25] {3072} INFO -  at 9.6s,	estimator xgboost's best error=9.7995,	best estimator xgboost's best error=9.7995
[flaml.automl: 09-17 22:11:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:11:26] {3072} INFO -  at 10.7s,	estimator xgboost's best error=7.7610,	best estimator xgboost's best error=7.7610
[flaml.automl: 09-17 22:11:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:11:28] {3072} INFO -  at 12.3s,	estimator xgboost's best error=7.7610,	best estimator xgboost's best error=7.7610
[flaml.automl: 09-17 22:11:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:11:29] {3072} INFO -  at 14.0s,	estimator xgboost's best error=6.3601,	best estimator xgboost's best error=6.3601
[flaml.automl: 09-17 22:11:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:11:32] {3072} INFO -  at 16.6s,	estimator xgboost's best error=6.3601,	best estimator xgboost's best error=6.3601
[flaml.automl: 09-17 22:11:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:11:34] {3072} INFO -  at 18.3s,	estimator xgboost's best error=6.3601,	best estimator xgboost's best error=6.3601
[flaml.automl: 09-17 22:11:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:11:37] {3072} INFO -  at 21.3s,	estimator xgboost's best error=6.3601,	best estimator xgboost's best error=6.3601
[flaml.automl: 09-17 22:11:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:11:38] {3072} INFO -  at 22.8s,	estimator xgboost's best error=6.3601,	best estimator xgboost's best error=6.3601
[flaml.automl: 09-17 22:11:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:11:40] {3072} INFO -  at 24.5s,	estimator xgboost's best error=6.3489,	best estimator xgboost's best error=6.3489
[flaml.automl: 09-17 22:11:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:11:41] {3072} INFO -  at 25.7s,	estimator xgboost's best error=6.3489,	best estimator xgboost's best error=6.3489
[flaml.automl: 09-17 22:11:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:11:48] {3072} INFO -  at 32.7s,	estimator xgboost's best error=5.8810,	best estimator xgboost's best error=5.8810
[flaml.automl: 09-17 22:11:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:12:01] {3072} INFO -  at 45.5s,	estimator xgboost's best error=5.7627,	best estimator xgboost's best error=5.7627
[flaml.automl: 09-17 22:12:01] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 22:12:08] {3072} INFO -  at 52.6s,	estimator xgboost's best error=5.7627,	best estimator xgboost's best error=5.7627
[flaml.automl: 09-17 22:12:26] {3335} INFO - retrain xgboost for 17.6s
[flaml.automl: 09-17 22:12:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:12:26] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:12:26] {2637} INFO - Time taken to find the best model: 45.54878878593445
[flaml.automl: 09-17 22:12:26] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54204}
PM2.5(0)最佳损失：-4.762698179361691
PM2.5(0)最好结果：{'pred_time': 6.642864923352086e-06, 'wall_clock_time': 45.54878878593445, 'metric_for_logging': {'pred_time': 6.642864923352086e-06}, 'val_loss': 5.762698179361691, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54204}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 54204, 'experiment_tag': 'exp', 'time_total_s': 12.827215194702148}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8366700775064371
PM2.5(0)的mse=93.59081648213292
PM2.5(0)的mae=5.747242132819704
PM2.5(0)的mar=0.2816209334747868
总共花费的时间为：71.10
辽阳市
2212A
2213A
2214A
2215A
[flaml.automl: 09-17 22:25:02] {2390} INFO - task = regression
[flaml.automl: 09-17 22:25:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:25:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:25:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:25:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:25:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:25:05] {3025} INFO - Estimated sufficient time budget=112453s. Estimated necessary time budget=112s.
[flaml.automl: 09-17 22:25:05] {3072} INFO -  at 2.9s,	estimator xgboost's best error=20.6452,	best estimator xgboost's best error=20.6452
[flaml.automl: 09-17 22:25:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:25:10] {3072} INFO -  at 7.7s,	estimator xgboost's best error=10.6841,	best estimator xgboost's best error=10.6841
[flaml.automl: 09-17 22:25:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:25:12] {3072} INFO -  at 10.3s,	estimator xgboost's best error=10.6841,	best estimator xgboost's best error=10.6841
[flaml.automl: 09-17 22:25:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:25:17] {3072} INFO -  at 15.3s,	estimator xgboost's best error=10.6841,	best estimator xgboost's best error=10.6841
[flaml.automl: 09-17 22:25:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:25:19] {3072} INFO -  at 17.6s,	estimator xgboost's best error=9.1012,	best estimator xgboost's best error=9.1012
[flaml.automl: 09-17 22:25:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:25:22] {3072} INFO -  at 20.4s,	estimator xgboost's best error=9.1012,	best estimator xgboost's best error=9.1012
[flaml.automl: 09-17 22:25:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:25:25] {3072} INFO -  at 23.5s,	estimator xgboost's best error=6.5796,	best estimator xgboost's best error=6.5796
[flaml.automl: 09-17 22:25:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:25:29] {3072} INFO -  at 26.8s,	estimator xgboost's best error=6.5796,	best estimator xgboost's best error=6.5796
[flaml.automl: 09-17 22:25:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:25:32] {3072} INFO -  at 29.7s,	estimator xgboost's best error=6.5796,	best estimator xgboost's best error=6.5796
[flaml.automl: 09-17 22:25:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:25:34] {3072} INFO -  at 32.6s,	estimator xgboost's best error=6.5796,	best estimator xgboost's best error=6.5796
[flaml.automl: 09-17 22:25:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:25:36] {3072} INFO -  at 34.0s,	estimator xgboost's best error=6.5796,	best estimator xgboost's best error=6.5796
[flaml.automl: 09-17 22:25:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:25:38] {3072} INFO -  at 35.7s,	estimator xgboost's best error=6.4572,	best estimator xgboost's best error=6.4572
[flaml.automl: 09-17 22:25:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:25:39] {3072} INFO -  at 36.9s,	estimator xgboost's best error=6.4572,	best estimator xgboost's best error=6.4572
[flaml.automl: 09-17 22:25:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:25:46] {3072} INFO -  at 44.0s,	estimator xgboost's best error=6.0703,	best estimator xgboost's best error=6.0703
[flaml.automl: 09-17 22:25:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:25:59] {3072} INFO -  at 56.7s,	estimator xgboost's best error=5.8871,	best estimator xgboost's best error=5.8871
[flaml.automl: 09-17 22:26:11] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-17 22:26:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:26:11] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:26:11] {2637} INFO - Time taken to find the best model: 56.70685386657715
[flaml.automl: 09-17 22:26:11] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 44020}
PM2.5(0)最佳损失：-4.887089950085076
PM2.5(0)最好结果：{'pred_time': 8.277442741861912e-06, 'wall_clock_time': 56.70685386657715, 'metric_for_logging': {'pred_time': 8.277442741861912e-06}, 'val_loss': 5.887089950085076, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 44020}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 44020, 'experiment_tag': 'exp', 'time_total_s': 12.753775119781494}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8391433850263245
PM2.5(0)的mse=93.00800219496733
PM2.5(0)的mae=5.513400796234005
PM2.5(0)的mar=0.21491620438995757
总共花费的时间为：70.20
铁岭市
2216A
2217A
2218A
2219A
[flaml.automl: 09-17 22:38:14] {2390} INFO - task = regression
[flaml.automl: 09-17 22:38:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:38:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:38:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:38:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:38:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:38:15] {3025} INFO - Estimated sufficient time budget=51042s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 22:38:15] {3072} INFO -  at 1.4s,	estimator xgboost's best error=19.7269,	best estimator xgboost's best error=19.7269
[flaml.automl: 09-17 22:38:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:38:17] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.2398,	best estimator xgboost's best error=10.2398
[flaml.automl: 09-17 22:38:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:38:18] {3072} INFO -  at 4.6s,	estimator xgboost's best error=10.2398,	best estimator xgboost's best error=10.2398
[flaml.automl: 09-17 22:38:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:38:25] {3072} INFO -  at 10.9s,	estimator xgboost's best error=10.2398,	best estimator xgboost's best error=10.2398
[flaml.automl: 09-17 22:38:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:38:26] {3072} INFO -  at 12.0s,	estimator xgboost's best error=8.5263,	best estimator xgboost's best error=8.5263
[flaml.automl: 09-17 22:38:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:38:27] {3072} INFO -  at 13.6s,	estimator xgboost's best error=8.5263,	best estimator xgboost's best error=8.5263
[flaml.automl: 09-17 22:38:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:38:29] {3072} INFO -  at 15.3s,	estimator xgboost's best error=6.3537,	best estimator xgboost's best error=6.3537
[flaml.automl: 09-17 22:38:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:38:32] {3072} INFO -  at 18.0s,	estimator xgboost's best error=6.3537,	best estimator xgboost's best error=6.3537
[flaml.automl: 09-17 22:38:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:38:33] {3072} INFO -  at 19.6s,	estimator xgboost's best error=6.3537,	best estimator xgboost's best error=6.3537
[flaml.automl: 09-17 22:38:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:38:36] {3072} INFO -  at 22.6s,	estimator xgboost's best error=6.3537,	best estimator xgboost's best error=6.3537
[flaml.automl: 09-17 22:38:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:38:38] {3072} INFO -  at 24.0s,	estimator xgboost's best error=6.3537,	best estimator xgboost's best error=6.3537
[flaml.automl: 09-17 22:38:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:38:39] {3072} INFO -  at 25.8s,	estimator xgboost's best error=6.3491,	best estimator xgboost's best error=6.3491
[flaml.automl: 09-17 22:38:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:38:41] {3072} INFO -  at 26.9s,	estimator xgboost's best error=6.3491,	best estimator xgboost's best error=6.3491
[flaml.automl: 09-17 22:38:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:38:48] {3072} INFO -  at 34.0s,	estimator xgboost's best error=5.8372,	best estimator xgboost's best error=5.8372
[flaml.automl: 09-17 22:38:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:39:00] {3072} INFO -  at 46.8s,	estimator xgboost's best error=5.7170,	best estimator xgboost's best error=5.7170
[flaml.automl: 09-17 22:39:00] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 22:39:08] {3072} INFO -  at 53.8s,	estimator xgboost's best error=5.7170,	best estimator xgboost's best error=5.7170
[flaml.automl: 09-17 22:39:20] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-17 22:39:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:39:20] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:39:20] {2637} INFO - Time taken to find the best model: 46.762943267822266
[flaml.automl: 09-17 22:39:20] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43310}
PM2.5(0)最佳损失：-4.717020954878692
PM2.5(0)最好结果：{'pred_time': 8.109302944783334e-06, 'wall_clock_time': 46.762943267822266, 'metric_for_logging': {'pred_time': 8.109302944783334e-06}, 'val_loss': 5.717020954878692, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43310}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 43310, 'experiment_tag': 'exp', 'time_total_s': 12.781731367111206}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.834709136637806
PM2.5(0)的mse=89.71592614792779
PM2.5(0)的mae=5.479371407046279
PM2.5(0)的mar=0.2351226148309093
总共花费的时间为：67.17
朝阳市
2220A
2221A
2222A
2223A
[flaml.automl: 09-17 22:51:27] {2390} INFO - task = regression
[flaml.automl: 09-17 22:51:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:51:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:51:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:51:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:51:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:51:28] {3025} INFO - Estimated sufficient time budget=51625s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 22:51:28] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.1536,	best estimator xgboost's best error=17.1536
[flaml.automl: 09-17 22:51:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:51:30] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.9015,	best estimator xgboost's best error=8.9015
[flaml.automl: 09-17 22:51:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:51:31] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.9015,	best estimator xgboost's best error=8.9015
[flaml.automl: 09-17 22:51:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:51:37] {3072} INFO -  at 10.6s,	estimator xgboost's best error=8.9015,	best estimator xgboost's best error=8.9015
[flaml.automl: 09-17 22:51:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:51:38] {3072} INFO -  at 11.7s,	estimator xgboost's best error=6.8985,	best estimator xgboost's best error=6.8985
[flaml.automl: 09-17 22:51:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:51:40] {3072} INFO -  at 13.3s,	estimator xgboost's best error=6.8985,	best estimator xgboost's best error=6.8985
[flaml.automl: 09-17 22:51:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:51:42] {3072} INFO -  at 14.9s,	estimator xgboost's best error=5.4790,	best estimator xgboost's best error=5.4790
[flaml.automl: 09-17 22:51:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:51:44] {3072} INFO -  at 17.6s,	estimator xgboost's best error=5.4790,	best estimator xgboost's best error=5.4790
[flaml.automl: 09-17 22:51:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:51:46] {3072} INFO -  at 19.2s,	estimator xgboost's best error=5.4790,	best estimator xgboost's best error=5.4790
[flaml.automl: 09-17 22:51:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:51:49] {3072} INFO -  at 22.2s,	estimator xgboost's best error=5.4790,	best estimator xgboost's best error=5.4790
[flaml.automl: 09-17 22:51:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:51:50] {3072} INFO -  at 23.7s,	estimator xgboost's best error=5.4790,	best estimator xgboost's best error=5.4790
[flaml.automl: 09-17 22:51:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:51:52] {3072} INFO -  at 25.4s,	estimator xgboost's best error=5.4745,	best estimator xgboost's best error=5.4745
[flaml.automl: 09-17 22:51:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:51:53] {3072} INFO -  at 26.5s,	estimator xgboost's best error=5.4745,	best estimator xgboost's best error=5.4745
[flaml.automl: 09-17 22:51:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:52:00] {3072} INFO -  at 33.6s,	estimator xgboost's best error=5.0502,	best estimator xgboost's best error=5.0502
[flaml.automl: 09-17 22:52:00] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:52:15] {3072} INFO -  at 48.8s,	estimator xgboost's best error=4.9155,	best estimator xgboost's best error=4.9155
[flaml.automl: 09-17 22:52:37] {3335} INFO - retrain xgboost for 21.8s
[flaml.automl: 09-17 22:52:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:52:37] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:52:37] {2637} INFO - Time taken to find the best model: 48.83928036689758
[flaml.automl: 09-17 22:52:37] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43268}
PM2.5(0)最佳损失：-3.91553780849335
PM2.5(0)最好结果：{'pred_time': 1.7362456155100994e-05, 'wall_clock_time': 48.83928036689758, 'metric_for_logging': {'pred_time': 1.7362456155100994e-05}, 'val_loss': 4.91553780849335, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43268}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 43268, 'experiment_tag': 'exp', 'time_total_s': 15.268823146820068}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8535429963301057
PM2.5(0)的mse=66.19205544479678
PM2.5(0)的mae=4.755759176601859
PM2.5(0)的mar=0.24705934423200576
总共花费的时间为：71.29
四平市
2226A
3486A
3713A
[flaml.automl: 09-17 23:01:52] {2390} INFO - task = regression
[flaml.automl: 09-17 23:01:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:01:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:01:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:01:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:01:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:01:54] {3025} INFO - Estimated sufficient time budget=12121s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 23:01:54] {3072} INFO -  at 1.3s,	estimator xgboost's best error=16.9500,	best estimator xgboost's best error=16.9500
[flaml.automl: 09-17 23:01:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:01:56] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.7164,	best estimator xgboost's best error=8.7164
[flaml.automl: 09-17 23:01:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:01:57] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.7164,	best estimator xgboost's best error=8.7164
[flaml.automl: 09-17 23:01:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:02:07] {3072} INFO -  at 14.7s,	estimator xgboost's best error=8.7164,	best estimator xgboost's best error=8.7164
[flaml.automl: 09-17 23:02:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:02:08] {3072} INFO -  at 15.8s,	estimator xgboost's best error=7.5795,	best estimator xgboost's best error=7.5795
[flaml.automl: 09-17 23:02:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:02:10] {3072} INFO -  at 17.4s,	estimator xgboost's best error=7.5795,	best estimator xgboost's best error=7.5795
[flaml.automl: 09-17 23:02:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:02:11] {3072} INFO -  at 19.1s,	estimator xgboost's best error=5.8544,	best estimator xgboost's best error=5.8544
[flaml.automl: 09-17 23:02:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:02:14] {3072} INFO -  at 21.8s,	estimator xgboost's best error=5.8544,	best estimator xgboost's best error=5.8544
[flaml.automl: 09-17 23:02:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:02:16] {3072} INFO -  at 23.4s,	estimator xgboost's best error=5.8544,	best estimator xgboost's best error=5.8544
[flaml.automl: 09-17 23:02:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:02:19] {3072} INFO -  at 26.5s,	estimator xgboost's best error=5.8544,	best estimator xgboost's best error=5.8544
[flaml.automl: 09-17 23:02:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:02:20] {3072} INFO -  at 27.9s,	estimator xgboost's best error=5.8544,	best estimator xgboost's best error=5.8544
[flaml.automl: 09-17 23:02:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:02:21] {3072} INFO -  at 29.1s,	estimator xgboost's best error=5.8544,	best estimator xgboost's best error=5.8544
[flaml.automl: 09-17 23:02:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:02:28] {3072} INFO -  at 36.1s,	estimator xgboost's best error=5.4993,	best estimator xgboost's best error=5.4993
[flaml.automl: 09-17 23:02:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:02:41] {3072} INFO -  at 48.8s,	estimator xgboost's best error=5.4269,	best estimator xgboost's best error=5.4269
[flaml.automl: 09-17 23:02:54] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 23:02:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:02:54] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:02:54] {2637} INFO - Time taken to find the best model: 48.8404335975647
[flaml.automl: 09-17 23:02:54] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-4.426881458857398
PM2.5(0)最好结果：{'pred_time': 1.090993542248149e-05, 'wall_clock_time': 48.8404335975647, 'metric_for_logging': {'pred_time': 1.090993542248149e-05}, 'val_loss': 5.426881458857398, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 12.76325511932373}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.834886829917495
PM2.5(0)的mse=60.285742682868126
PM2.5(0)的mae=5.4417163597246265
PM2.5(0)的mar=0.34610894714642904
总共花费的时间为：62.10
辽源市
2227A
[flaml.automl: 09-17 23:05:57] {2390} INFO - task = regression
[flaml.automl: 09-17 23:05:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:05:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:05:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:05:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:05:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:06:00] {3025} INFO - Estimated sufficient time budget=34766s. Estimated necessary time budget=35s.
[flaml.automl: 09-17 23:06:00] {3072} INFO -  at 3.5s,	estimator xgboost's best error=19.8988,	best estimator xgboost's best error=19.8988
[flaml.automl: 09-17 23:06:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:06:06] {3072} INFO -  at 8.9s,	estimator xgboost's best error=11.0359,	best estimator xgboost's best error=11.0359
[flaml.automl: 09-17 23:06:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:06:09] {3072} INFO -  at 12.3s,	estimator xgboost's best error=11.0359,	best estimator xgboost's best error=11.0359
[flaml.automl: 09-17 23:06:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:06:21] {3072} INFO -  at 23.8s,	estimator xgboost's best error=11.0359,	best estimator xgboost's best error=11.0359
[flaml.automl: 09-17 23:06:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:06:22] {3072} INFO -  at 25.6s,	estimator xgboost's best error=6.3479,	best estimator xgboost's best error=6.3479
[flaml.automl: 09-17 23:06:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:06:25] {3072} INFO -  at 27.8s,	estimator xgboost's best error=5.7389,	best estimator xgboost's best error=5.7389
[flaml.automl: 09-17 23:06:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:06:27] {3072} INFO -  at 30.2s,	estimator xgboost's best error=5.4793,	best estimator xgboost's best error=5.4793
[flaml.automl: 09-17 23:06:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:06:31] {3072} INFO -  at 34.0s,	estimator xgboost's best error=5.4793,	best estimator xgboost's best error=5.4793
[flaml.automl: 09-17 23:06:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:06:34] {3072} INFO -  at 36.6s,	estimator xgboost's best error=5.2263,	best estimator xgboost's best error=5.2263
[flaml.automl: 09-17 23:06:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:06:38] {3072} INFO -  at 40.9s,	estimator xgboost's best error=5.1885,	best estimator xgboost's best error=5.1885
[flaml.automl: 09-17 23:06:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:06:40] {3072} INFO -  at 42.6s,	estimator xgboost's best error=5.1885,	best estimator xgboost's best error=5.1885
[flaml.automl: 09-17 23:06:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:06:52] {3072} INFO -  at 55.6s,	estimator xgboost's best error=4.6726,	best estimator xgboost's best error=4.6726
[flaml.automl: 09-17 23:07:02] {3335} INFO - retrain xgboost for 9.6s
[flaml.automl: 09-17 23:07:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:07:02] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:07:02] {2637} INFO - Time taken to find the best model: 55.573472023010254
[flaml.automl: 09-17 23:07:02] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
PM2.5(0)最佳损失：-3.672564082765539
PM2.5(0)最好结果：{'pred_time': 3.4048684897216174e-05, 'wall_clock_time': 55.573472023010254, 'metric_for_logging': {'pred_time': 3.4048684897216174e-05}, 'val_loss': 4.672564082765539, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 12, 'config/max_leaves': 14, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 12.92494010925293}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8947147759957182
PM2.5(0)的mse=48.037659053293176
PM2.5(0)的mae=4.478733887632293
PM2.5(0)的mar=0.18509955766389025
总共花费的时间为：65.40
通化市
2229A
2230A
[flaml.automl: 09-17 23:13:34] {2390} INFO - task = regression
[flaml.automl: 09-17 23:13:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:13:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:13:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:13:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:13:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:13:36] {3025} INFO - Estimated sufficient time budget=19501s. Estimated necessary time budget=20s.
[flaml.automl: 09-17 23:13:36] {3072} INFO -  at 2.1s,	estimator xgboost's best error=13.8120,	best estimator xgboost's best error=13.8120
[flaml.automl: 09-17 23:13:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:13:39] {3072} INFO -  at 5.5s,	estimator xgboost's best error=7.5836,	best estimator xgboost's best error=7.5836
[flaml.automl: 09-17 23:13:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:13:41] {3072} INFO -  at 7.4s,	estimator xgboost's best error=7.5836,	best estimator xgboost's best error=7.5836
[flaml.automl: 09-17 23:13:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:13:52] {3072} INFO -  at 18.0s,	estimator xgboost's best error=7.5836,	best estimator xgboost's best error=7.5836
[flaml.automl: 09-17 23:13:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:13:53] {3072} INFO -  at 19.1s,	estimator xgboost's best error=6.4060,	best estimator xgboost's best error=6.4060
[flaml.automl: 09-17 23:13:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:13:54] {3072} INFO -  at 20.6s,	estimator xgboost's best error=6.4060,	best estimator xgboost's best error=6.4060
[flaml.automl: 09-17 23:13:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:13:56] {3072} INFO -  at 22.2s,	estimator xgboost's best error=5.4219,	best estimator xgboost's best error=5.4219
[flaml.automl: 09-17 23:13:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:13:58] {3072} INFO -  at 24.5s,	estimator xgboost's best error=5.4219,	best estimator xgboost's best error=5.4219
[flaml.automl: 09-17 23:13:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:14:00] {3072} INFO -  at 26.1s,	estimator xgboost's best error=5.4219,	best estimator xgboost's best error=5.4219
[flaml.automl: 09-17 23:14:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:14:03] {3072} INFO -  at 29.0s,	estimator xgboost's best error=5.3988,	best estimator xgboost's best error=5.3988
[flaml.automl: 09-17 23:14:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:14:04] {3072} INFO -  at 30.7s,	estimator xgboost's best error=5.3988,	best estimator xgboost's best error=5.3988
[flaml.automl: 09-17 23:14:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:14:05] {3072} INFO -  at 31.8s,	estimator xgboost's best error=5.3988,	best estimator xgboost's best error=5.3988
[flaml.automl: 09-17 23:14:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:14:17] {3072} INFO -  at 43.6s,	estimator xgboost's best error=5.1053,	best estimator xgboost's best error=5.1053
[flaml.automl: 09-17 23:14:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:14:33] {3072} INFO -  at 59.3s,	estimator xgboost's best error=4.9773,	best estimator xgboost's best error=4.9773
[flaml.automl: 09-17 23:14:55] {3335} INFO - retrain xgboost for 21.8s
[flaml.automl: 09-17 23:14:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.3638133431214387,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.029920148019616434, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.007290129763188211, scale_pos_weight=1,
             subsample=0.840665579419843, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:14:55] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:14:55] {2637} INFO - Time taken to find the best model: 59.27198600769043
[flaml.automl: 09-17 23:14:55] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.029920148019616434, 'learning_rate': 0.3638133431214387, 'subsample': 0.840665579419843, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.007290129763188211}
PM2.5(0)最佳损失：-3.9772500602030645
PM2.5(0)最好结果：{'pred_time': 1.6496323872703154e-05, 'wall_clock_time': 59.27198600769043, 'metric_for_logging': {'pred_time': 1.6496323872703154e-05}, 'val_loss': 4.9772500602030645, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.029920148019616434, 'learning_rate': 0.3638133431214387, 'subsample': 0.840665579419843, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.007290129763188211}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.029920148019616434, 'config/learning_rate': 0.3638133431214387, 'config/subsample': 0.840665579419843, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.007290129763188211, 'experiment_tag': 'exp', 'time_total_s': 15.648805141448975}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.3638133431214387,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.029920148019616434, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.007290129763188211, scale_pos_weight=1,
             subsample=0.840665579419843, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8190144852105073
PM2.5(0)的mse=44.38407810866048
PM2.5(0)的mae=4.754796233573338
PM2.5(0)的mar=0.45485752809672747
总共花费的时间为：81.43
白山市
2231A
2232A
[flaml.automl: 09-17 23:21:33] {2390} INFO - task = regression
[flaml.automl: 09-17 23:21:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:21:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:21:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:21:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:21:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:21:36] {3025} INFO - Estimated sufficient time budget=31489s. Estimated necessary time budget=31s.
[flaml.automl: 09-17 23:21:36] {3072} INFO -  at 3.3s,	estimator xgboost's best error=14.2343,	best estimator xgboost's best error=14.2343
[flaml.automl: 09-17 23:21:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:21:42] {3072} INFO -  at 8.9s,	estimator xgboost's best error=7.2037,	best estimator xgboost's best error=7.2037
[flaml.automl: 09-17 23:21:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:21:45] {3072} INFO -  at 12.0s,	estimator xgboost's best error=7.2037,	best estimator xgboost's best error=7.2037
[flaml.automl: 09-17 23:21:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:22:07] {3072} INFO -  at 34.3s,	estimator xgboost's best error=7.2037,	best estimator xgboost's best error=7.2037
[flaml.automl: 09-17 23:22:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:22:08] {3072} INFO -  at 35.4s,	estimator xgboost's best error=5.5078,	best estimator xgboost's best error=5.5078
[flaml.automl: 09-17 23:22:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:22:10] {3072} INFO -  at 37.0s,	estimator xgboost's best error=5.1825,	best estimator xgboost's best error=5.1825
[flaml.automl: 09-17 23:22:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:22:11] {3072} INFO -  at 38.6s,	estimator xgboost's best error=4.7044,	best estimator xgboost's best error=4.7044
[flaml.automl: 09-17 23:22:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:22:14] {3072} INFO -  at 41.2s,	estimator xgboost's best error=4.7044,	best estimator xgboost's best error=4.7044
[flaml.automl: 09-17 23:22:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:22:15] {3072} INFO -  at 42.8s,	estimator xgboost's best error=4.7044,	best estimator xgboost's best error=4.7044
[flaml.automl: 09-17 23:22:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:22:18] {3072} INFO -  at 45.8s,	estimator xgboost's best error=4.3921,	best estimator xgboost's best error=4.3921
[flaml.automl: 09-17 23:22:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:22:20] {3072} INFO -  at 47.4s,	estimator xgboost's best error=4.3921,	best estimator xgboost's best error=4.3921
[flaml.automl: 09-17 23:22:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:22:21] {3072} INFO -  at 48.5s,	estimator xgboost's best error=4.3921,	best estimator xgboost's best error=4.3921
[flaml.automl: 09-17 23:22:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:22:32] {3072} INFO -  at 59.5s,	estimator xgboost's best error=4.2691,	best estimator xgboost's best error=4.2691
[flaml.automl: 09-17 23:22:44] {3335} INFO - retrain xgboost for 11.8s
[flaml.automl: 09-17 23:22:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 23:22:44] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:22:44] {2637} INFO - Time taken to find the best model: 59.45943808555603
[flaml.automl: 09-17 23:22:44] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-3.2690538393631394
PM2.5(0)最好结果：{'pred_time': 1.7173742596137197e-05, 'wall_clock_time': 59.45943808555603, 'metric_for_logging': {'pred_time': 1.7173742596137197e-05}, 'val_loss': 4.269053839363139, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 10.950307846069336}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8347881343956576
PM2.5(0)的mse=35.93859658361584
PM2.5(0)的mae=4.363736878390459
PM2.5(0)的mar=0.29063870309875905
总共花费的时间为：71.91
松原市
2233A
2234A
[flaml.automl: 09-17 23:29:20] {2390} INFO - task = regression
[flaml.automl: 09-17 23:29:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:29:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:29:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:29:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:29:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:29:25] {3025} INFO - Estimated sufficient time budget=44349s. Estimated necessary time budget=44s.
[flaml.automl: 09-17 23:29:25] {3072} INFO -  at 4.6s,	estimator xgboost's best error=14.1869,	best estimator xgboost's best error=14.1869
[flaml.automl: 09-17 23:29:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:29:32] {3072} INFO -  at 12.2s,	estimator xgboost's best error=8.5345,	best estimator xgboost's best error=8.5345
[flaml.automl: 09-17 23:29:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:29:37] {3072} INFO -  at 16.9s,	estimator xgboost's best error=8.5345,	best estimator xgboost's best error=8.5345
[flaml.automl: 09-17 23:29:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:30:03] {3072} INFO -  at 42.9s,	estimator xgboost's best error=8.5345,	best estimator xgboost's best error=8.5345
[flaml.automl: 09-17 23:30:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:30:06] {3072} INFO -  at 45.9s,	estimator xgboost's best error=7.1422,	best estimator xgboost's best error=7.1422
[flaml.automl: 09-17 23:30:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:30:09] {3072} INFO -  at 48.4s,	estimator xgboost's best error=6.5670,	best estimator xgboost's best error=6.5670
[flaml.automl: 09-17 23:30:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:30:10] {3072} INFO -  at 50.0s,	estimator xgboost's best error=6.1349,	best estimator xgboost's best error=6.1349
[flaml.automl: 09-17 23:30:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:30:13] {3072} INFO -  at 52.3s,	estimator xgboost's best error=6.1349,	best estimator xgboost's best error=6.1349
[flaml.automl: 09-17 23:30:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:30:14] {3072} INFO -  at 53.9s,	estimator xgboost's best error=6.1349,	best estimator xgboost's best error=6.1349
[flaml.automl: 09-17 23:30:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:30:17] {3072} INFO -  at 56.8s,	estimator xgboost's best error=5.5743,	best estimator xgboost's best error=5.5743
[flaml.automl: 09-17 23:30:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:30:19] {3072} INFO -  at 58.4s,	estimator xgboost's best error=5.5743,	best estimator xgboost's best error=5.5743
[flaml.automl: 09-17 23:30:22] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-17 23:30:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 23:30:22] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:30:22] {2637} INFO - Time taken to find the best model: 56.845643758773804
[flaml.automl: 09-17 23:30:22] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-4.57430142468015
PM2.5(0)最好结果：{'pred_time': 1.7148853395860935e-05, 'wall_clock_time': 56.845643758773804, 'metric_for_logging': {'pred_time': 1.7148853395860935e-05}, 'val_loss': 5.57430142468015, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 2.95335054397583}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7592948779227535
PM2.5(0)的mse=66.27776978301792
PM2.5(0)的mae=5.5753352047191305
PM2.5(0)的mar=0.5313417013527939
总共花费的时间为：61.76
白城市
2235A
2236A
[flaml.automl: 09-17 23:36:53] {2390} INFO - task = regression
[flaml.automl: 09-17 23:36:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:36:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:36:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:36:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:36:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:36:57] {3025} INFO - Estimated sufficient time budget=32673s. Estimated necessary time budget=33s.
[flaml.automl: 09-17 23:36:57] {3072} INFO -  at 3.4s,	estimator xgboost's best error=13.3446,	best estimator xgboost's best error=13.3446
[flaml.automl: 09-17 23:36:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:37:02] {3072} INFO -  at 8.5s,	estimator xgboost's best error=7.5712,	best estimator xgboost's best error=7.5712
[flaml.automl: 09-17 23:37:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:37:05] {3072} INFO -  at 11.7s,	estimator xgboost's best error=7.5712,	best estimator xgboost's best error=7.5712
[flaml.automl: 09-17 23:37:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:37:28] {3072} INFO -  at 34.4s,	estimator xgboost's best error=7.5712,	best estimator xgboost's best error=7.5712
[flaml.automl: 09-17 23:37:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:37:31] {3072} INFO -  at 37.5s,	estimator xgboost's best error=5.3686,	best estimator xgboost's best error=5.3686
[flaml.automl: 09-17 23:37:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:37:35] {3072} INFO -  at 41.7s,	estimator xgboost's best error=4.9910,	best estimator xgboost's best error=4.9910
[flaml.automl: 09-17 23:37:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:37:39] {3072} INFO -  at 46.2s,	estimator xgboost's best error=4.6666,	best estimator xgboost's best error=4.6666
[flaml.automl: 09-17 23:37:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:37:46] {3072} INFO -  at 52.3s,	estimator xgboost's best error=4.6666,	best estimator xgboost's best error=4.6666
[flaml.automl: 09-17 23:37:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:37:48] {3072} INFO -  at 54.2s,	estimator xgboost's best error=4.6666,	best estimator xgboost's best error=4.6666
[flaml.automl: 09-17 23:37:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:37:50] {3072} INFO -  at 57.2s,	estimator xgboost's best error=4.2871,	best estimator xgboost's best error=4.2871
[flaml.automl: 09-17 23:37:53] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-17 23:37:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 23:37:53] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:37:53] {2637} INFO - Time taken to find the best model: 57.16405534744263
[flaml.automl: 09-17 23:37:53] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-3.287106409874319
PM2.5(0)最好结果：{'pred_time': 1.7498906001137226e-05, 'wall_clock_time': 57.16405534744263, 'metric_for_logging': {'pred_time': 1.7498906001137226e-05}, 'val_loss': 4.287106409874319, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 2.9456512928009033}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7163613796584085
PM2.5(0)的mse=53.07306039445544
PM2.5(0)的mae=4.531531388495339
PM2.5(0)的mar=0.2879903520782192
总共花费的时间为：60.44
延边朝鲜族自治州
2237A
2238A
2239A
[flaml.automl: 09-17 23:48:34] {2390} INFO - task = regression
[flaml.automl: 09-17 23:48:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:48:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:48:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:48:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:48:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:48:37] {3025} INFO - Estimated sufficient time budget=28057s. Estimated necessary time budget=28s.
[flaml.automl: 09-17 23:48:37] {3072} INFO -  at 2.9s,	estimator xgboost's best error=10.3651,	best estimator xgboost's best error=10.3651
[flaml.automl: 09-17 23:48:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:48:41] {3072} INFO -  at 6.7s,	estimator xgboost's best error=5.4440,	best estimator xgboost's best error=5.4440
[flaml.automl: 09-17 23:48:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:48:43] {3072} INFO -  at 8.9s,	estimator xgboost's best error=5.4440,	best estimator xgboost's best error=5.4440
[flaml.automl: 09-17 23:48:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:49:01] {3072} INFO -  at 27.2s,	estimator xgboost's best error=5.4440,	best estimator xgboost's best error=5.4440
[flaml.automl: 09-17 23:49:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:49:03] {3072} INFO -  at 29.3s,	estimator xgboost's best error=4.6395,	best estimator xgboost's best error=4.6395
[flaml.automl: 09-17 23:49:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:49:06] {3072} INFO -  at 32.2s,	estimator xgboost's best error=4.2488,	best estimator xgboost's best error=4.2488
[flaml.automl: 09-17 23:49:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:49:09] {3072} INFO -  at 35.2s,	estimator xgboost's best error=4.2166,	best estimator xgboost's best error=4.2166
[flaml.automl: 09-17 23:49:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:49:14] {3072} INFO -  at 40.1s,	estimator xgboost's best error=4.2166,	best estimator xgboost's best error=4.2166
[flaml.automl: 09-17 23:49:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:49:17] {3072} INFO -  at 43.2s,	estimator xgboost's best error=4.2166,	best estimator xgboost's best error=4.2166
[flaml.automl: 09-17 23:49:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:49:23] {3072} INFO -  at 48.8s,	estimator xgboost's best error=3.7980,	best estimator xgboost's best error=3.7980
[flaml.automl: 09-17 23:49:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:49:25] {3072} INFO -  at 50.5s,	estimator xgboost's best error=3.7980,	best estimator xgboost's best error=3.7980
[flaml.automl: 09-17 23:49:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:49:26] {3072} INFO -  at 51.7s,	estimator xgboost's best error=3.7980,	best estimator xgboost's best error=3.7980
[flaml.automl: 09-17 23:49:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:49:33] {3072} INFO -  at 59.0s,	estimator xgboost's best error=3.7980,	best estimator xgboost's best error=3.7980
[flaml.automl: 09-17 23:49:36] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-17 23:49:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 23:49:36] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:49:36] {2637} INFO - Time taken to find the best model: 48.84304428100586
[flaml.automl: 09-17 23:49:36] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-2.7979831074717376
PM2.5(0)最好结果：{'pred_time': 2.0515816843407787e-05, 'wall_clock_time': 48.84304428100586, 'metric_for_logging': {'pred_time': 2.0515816843407787e-05}, 'val_loss': 3.7979831074717376, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 5.651309251785278}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.5911250701598243
PM2.5(0)的mse=51.44070762360034
PM2.5(0)的mae=3.9433487093754302
PM2.5(0)的mar=0.29048134892027994
总共花费的时间为：62.61
鸡西市
2240A
2243A
3707A
3709A
[flaml.automl: 09-18 00:02:51] {2390} INFO - task = regression
[flaml.automl: 09-18 00:02:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:02:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:02:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:02:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:02:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:02:55] {3025} INFO - Estimated sufficient time budget=166925s. Estimated necessary time budget=167s.
[flaml.automl: 09-18 00:02:55] {3072} INFO -  at 4.2s,	estimator xgboost's best error=14.8202,	best estimator xgboost's best error=14.8202
[flaml.automl: 09-18 00:02:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:03:01] {3072} INFO -  at 10.2s,	estimator xgboost's best error=9.5144,	best estimator xgboost's best error=9.5144
[flaml.automl: 09-18 00:03:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:03:05] {3072} INFO -  at 14.1s,	estimator xgboost's best error=9.5144,	best estimator xgboost's best error=9.5144
[flaml.automl: 09-18 00:03:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:03:10] {3072} INFO -  at 18.8s,	estimator xgboost's best error=9.5144,	best estimator xgboost's best error=9.5144
[flaml.automl: 09-18 00:03:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:03:13] {3072} INFO -  at 22.0s,	estimator xgboost's best error=6.4918,	best estimator xgboost's best error=6.4918
[flaml.automl: 09-18 00:03:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:03:18] {3072} INFO -  at 26.6s,	estimator xgboost's best error=6.4918,	best estimator xgboost's best error=6.4918
[flaml.automl: 09-18 00:03:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:03:21] {3072} INFO -  at 30.2s,	estimator xgboost's best error=6.4918,	best estimator xgboost's best error=6.4918
[flaml.automl: 09-18 00:03:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:03:24] {3072} INFO -  at 32.8s,	estimator xgboost's best error=6.4918,	best estimator xgboost's best error=6.4918
[flaml.automl: 09-18 00:03:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:03:26] {3072} INFO -  at 34.7s,	estimator xgboost's best error=6.4918,	best estimator xgboost's best error=6.4918
[flaml.automl: 09-18 00:03:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:03:31] {3072} INFO -  at 39.7s,	estimator xgboost's best error=6.4918,	best estimator xgboost's best error=6.4918
[flaml.automl: 09-18 00:03:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:03:37] {3072} INFO -  at 45.7s,	estimator xgboost's best error=6.2320,	best estimator xgboost's best error=6.2320
[flaml.automl: 09-18 00:03:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:03:40] {3072} INFO -  at 49.0s,	estimator xgboost's best error=6.2320,	best estimator xgboost's best error=6.2320
[flaml.automl: 09-18 00:03:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:03:43] {3072} INFO -  at 52.4s,	estimator xgboost's best error=6.2320,	best estimator xgboost's best error=6.2320
[flaml.automl: 09-18 00:03:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:03:49] {3072} INFO -  at 58.2s,	estimator xgboost's best error=5.0252,	best estimator xgboost's best error=5.0252
[flaml.automl: 09-18 00:04:02] {3335} INFO - retrain xgboost for 12.4s
[flaml.automl: 09-18 00:04:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972436, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.2041994338779402,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 00:04:02] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:04:02] {2637} INFO - Time taken to find the best model: 58.192089557647705
[flaml.automl: 09-18 00:04:02] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972436, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.2041994338779402, 'FLAML_sample_size': 42788}
PM2.5(0)最佳损失：-4.025152062918486
PM2.5(0)最好结果：{'pred_time': 1.776506722788204e-05, 'wall_clock_time': 58.192089557647705, 'metric_for_logging': {'pred_time': 1.776506722788204e-05}, 'val_loss': 5.025152062918486, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972436, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.2041994338779402, 'FLAML_sample_size': 42788}, 'config/n_estimators': 8, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972436, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.2041994338779402, 'config/FLAML_sample_size': 42788, 'experiment_tag': 'exp', 'time_total_s': 5.83290958404541}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972436, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.2041994338779402,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.851105712100463
PM2.5(0)的mse=69.27340521448566
PM2.5(0)的mae=5.114914254259481
PM2.5(0)的mar=0.35061303619308815
总共花费的时间为：71.54
鹤岗市
2244A
2245A
2246A
2247A
[flaml.automl: 09-18 00:16:39] {2390} INFO - task = regression
[flaml.automl: 09-18 00:16:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:16:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:16:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:16:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:16:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:16:42] {3025} INFO - Estimated sufficient time budget=87660s. Estimated necessary time budget=88s.
[flaml.automl: 09-18 00:16:42] {3072} INFO -  at 2.4s,	estimator xgboost's best error=11.5936,	best estimator xgboost's best error=11.5936
[flaml.automl: 09-18 00:16:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:16:46] {3072} INFO -  at 6.2s,	estimator xgboost's best error=5.4965,	best estimator xgboost's best error=5.4965
[flaml.automl: 09-18 00:16:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:16:48] {3072} INFO -  at 8.3s,	estimator xgboost's best error=5.4965,	best estimator xgboost's best error=5.4965
[flaml.automl: 09-18 00:16:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:16:54] {3072} INFO -  at 14.3s,	estimator xgboost's best error=5.4965,	best estimator xgboost's best error=5.4965
[flaml.automl: 09-18 00:16:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:16:56] {3072} INFO -  at 16.4s,	estimator xgboost's best error=4.0520,	best estimator xgboost's best error=4.0520
[flaml.automl: 09-18 00:16:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:16:59] {3072} INFO -  at 19.3s,	estimator xgboost's best error=4.0520,	best estimator xgboost's best error=4.0520
[flaml.automl: 09-18 00:16:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:17:02] {3072} INFO -  at 22.4s,	estimator xgboost's best error=3.2897,	best estimator xgboost's best error=3.2897
[flaml.automl: 09-18 00:17:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:17:06] {3072} INFO -  at 26.5s,	estimator xgboost's best error=3.2897,	best estimator xgboost's best error=3.2897
[flaml.automl: 09-18 00:17:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:17:09] {3072} INFO -  at 29.4s,	estimator xgboost's best error=3.2897,	best estimator xgboost's best error=3.2897
[flaml.automl: 09-18 00:17:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:17:12] {3072} INFO -  at 32.7s,	estimator xgboost's best error=3.2897,	best estimator xgboost's best error=3.2897
[flaml.automl: 09-18 00:17:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:17:15] {3072} INFO -  at 35.2s,	estimator xgboost's best error=3.2324,	best estimator xgboost's best error=3.2324
[flaml.automl: 09-18 00:17:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:17:17] {3072} INFO -  at 37.3s,	estimator xgboost's best error=3.2324,	best estimator xgboost's best error=3.2324
[flaml.automl: 09-18 00:17:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:17:19] {3072} INFO -  at 39.6s,	estimator xgboost's best error=3.2324,	best estimator xgboost's best error=3.2324
[flaml.automl: 09-18 00:17:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:17:21] {3072} INFO -  at 41.7s,	estimator xgboost's best error=3.2324,	best estimator xgboost's best error=3.2324
[flaml.automl: 09-18 00:17:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 00:17:23] {3072} INFO -  at 43.8s,	estimator xgboost's best error=3.2324,	best estimator xgboost's best error=3.2324
[flaml.automl: 09-18 00:17:23] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 00:17:26] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.2324,	best estimator xgboost's best error=3.2324
[flaml.automl: 09-18 00:17:26] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 00:17:30] {3072} INFO -  at 50.6s,	estimator xgboost's best error=3.1287,	best estimator xgboost's best error=3.1287
[flaml.automl: 09-18 00:17:30] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 00:17:33] {3072} INFO -  at 53.2s,	estimator xgboost's best error=3.1287,	best estimator xgboost's best error=3.1287
[flaml.automl: 09-18 00:17:33] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 00:17:35] {3072} INFO -  at 55.4s,	estimator xgboost's best error=3.1287,	best estimator xgboost's best error=3.1287
[flaml.automl: 09-18 00:17:35] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 00:17:38] {3072} INFO -  at 59.1s,	estimator xgboost's best error=3.1287,	best estimator xgboost's best error=3.1287
[flaml.automl: 09-18 00:17:43] {3335} INFO - retrain xgboost for 4.2s
[flaml.automl: 09-18 00:17:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6820933558464604, colsample_bynode=1,
             colsample_bytree=0.8855316740065101, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4759259244306817,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=128.0, missing=nan, monotone_constraints='()',
             n_estimators=5, n_jobs=-1, num_parallel_tree=1, random_state=0,
             reg_alpha=0.0009765625, reg_lambda=0.6256118275208931,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:17:43] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:17:43] {2637} INFO - Time taken to find the best model: 50.62476897239685
[flaml.automl: 09-18 00:17:43] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 5, 'max_leaves': 7, 'min_child_weight': 128.0, 'learning_rate': 0.4759259244306817, 'subsample': 1.0, 'colsample_bylevel': 0.6820933558464604, 'colsample_bytree': 0.8855316740065101, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6256118275208931, 'FLAML_sample_size': 40413}
PM2.5(0)最佳损失：-2.1287450425159267
PM2.5(0)最好结果：{'pred_time': 1.897798086746953e-05, 'wall_clock_time': 50.62476897239685, 'metric_for_logging': {'pred_time': 1.897798086746953e-05}, 'val_loss': 3.1287450425159267, 'training_iteration': 1, 'config': {'n_estimators': 5, 'max_leaves': 7, 'min_child_weight': 128.0, 'learning_rate': 0.4759259244306817, 'subsample': 1.0, 'colsample_bylevel': 0.6820933558464604, 'colsample_bytree': 0.8855316740065101, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6256118275208931, 'FLAML_sample_size': 40413}, 'config/n_estimators': 5, 'config/max_leaves': 7, 'config/min_child_weight': 128.0, 'config/learning_rate': 0.4759259244306817, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.6820933558464604, 'config/colsample_bytree': 0.8855316740065101, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6256118275208931, 'config/FLAML_sample_size': 40413, 'experiment_tag': 'exp', 'time_total_s': 4.182403326034546}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6820933558464604, colsample_bynode=1,
             colsample_bytree=0.8855316740065101, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4759259244306817,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=128.0, missing=nan, monotone_constraints='()',
             n_estimators=5, n_jobs=-1, num_parallel_tree=1, random_state=0,
             reg_alpha=0.0009765625, reg_lambda=0.6256118275208931,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8282966588193995
PM2.5(0)的mse=21.438290115675148
PM2.5(0)的mae=3.179215958696568
PM2.5(0)的mar=0.20405927000621557
总共花费的时间为：64.06
双鸭山市
2248A
2249A
2250A
2251A
[flaml.automl: 09-18 00:29:38] {2390} INFO - task = regression
[flaml.automl: 09-18 00:29:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:29:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:29:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:29:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:29:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:29:39] {3025} INFO - Estimated sufficient time budget=49317s. Estimated necessary time budget=49s.
[flaml.automl: 09-18 00:29:39] {3072} INFO -  at 1.4s,	estimator xgboost's best error=14.7939,	best estimator xgboost's best error=14.7939
[flaml.automl: 09-18 00:29:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:29:41] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.4967,	best estimator xgboost's best error=7.4967
[flaml.automl: 09-18 00:29:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:29:43] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.4967,	best estimator xgboost's best error=7.4967
[flaml.automl: 09-18 00:29:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:29:49] {3072} INFO -  at 11.0s,	estimator xgboost's best error=7.4967,	best estimator xgboost's best error=7.4967
[flaml.automl: 09-18 00:29:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:29:50] {3072} INFO -  at 12.1s,	estimator xgboost's best error=6.0448,	best estimator xgboost's best error=6.0448
[flaml.automl: 09-18 00:29:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:29:52] {3072} INFO -  at 13.7s,	estimator xgboost's best error=5.5536,	best estimator xgboost's best error=5.5536
[flaml.automl: 09-18 00:29:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:29:53] {3072} INFO -  at 15.3s,	estimator xgboost's best error=5.1631,	best estimator xgboost's best error=5.1631
[flaml.automl: 09-18 00:29:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:29:56] {3072} INFO -  at 18.0s,	estimator xgboost's best error=5.1631,	best estimator xgboost's best error=5.1631
[flaml.automl: 09-18 00:29:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:29:57] {3072} INFO -  at 19.6s,	estimator xgboost's best error=5.1631,	best estimator xgboost's best error=5.1631
[flaml.automl: 09-18 00:29:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:30:00] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.6062,	best estimator xgboost's best error=4.6062
[flaml.automl: 09-18 00:30:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:30:02] {3072} INFO -  at 24.3s,	estimator xgboost's best error=4.6062,	best estimator xgboost's best error=4.6062
[flaml.automl: 09-18 00:30:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:30:03] {3072} INFO -  at 25.4s,	estimator xgboost's best error=4.6062,	best estimator xgboost's best error=4.6062
[flaml.automl: 09-18 00:30:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:30:07] {3072} INFO -  at 29.2s,	estimator xgboost's best error=4.6062,	best estimator xgboost's best error=4.6062
[flaml.automl: 09-18 00:30:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:30:10] {3072} INFO -  at 32.6s,	estimator xgboost's best error=4.6062,	best estimator xgboost's best error=4.6062
[flaml.automl: 09-18 00:30:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 00:30:13] {3072} INFO -  at 35.6s,	estimator xgboost's best error=4.4849,	best estimator xgboost's best error=4.4849
[flaml.automl: 09-18 00:30:13] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 00:30:18] {3072} INFO -  at 40.5s,	estimator xgboost's best error=4.4849,	best estimator xgboost's best error=4.4849
[flaml.automl: 09-18 00:30:18] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 00:30:20] {3072} INFO -  at 42.4s,	estimator xgboost's best error=4.4849,	best estimator xgboost's best error=4.4849
[flaml.automl: 09-18 00:30:20] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 00:30:22] {3072} INFO -  at 44.1s,	estimator xgboost's best error=4.4849,	best estimator xgboost's best error=4.4849
[flaml.automl: 09-18 00:30:22] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 00:30:29] {3072} INFO -  at 51.5s,	estimator xgboost's best error=4.3747,	best estimator xgboost's best error=4.3747
[flaml.automl: 09-18 00:30:29] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 00:30:32] {3072} INFO -  at 54.0s,	estimator xgboost's best error=4.3747,	best estimator xgboost's best error=4.3747
[flaml.automl: 09-18 00:30:45] {3335} INFO - retrain xgboost for 12.9s
[flaml.automl: 09-18 00:30:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5492977310397356, colsample_bynode=1,
             colsample_bytree=0.9510761842589558, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.04321599195729943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5415707634193446, scale_pos_weight=1,
             subsample=0.9351529901519405, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:30:45] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:30:45] {2637} INFO - Time taken to find the best model: 51.457003116607666
[flaml.automl: 09-18 00:30:45] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.04321599195729943, 'learning_rate': 1.0, 'subsample': 0.9351529901519405, 'colsample_bylevel': 0.5492977310397356, 'colsample_bytree': 0.9510761842589558, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5415707634193446, 'FLAML_sample_size': 41272}
PM2.5(0)最佳损失：-3.374673765245297
PM2.5(0)最好结果：{'pred_time': 8.586292125437388e-06, 'wall_clock_time': 51.457003116607666, 'metric_for_logging': {'pred_time': 8.586292125437388e-06}, 'val_loss': 4.374673765245297, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.04321599195729943, 'learning_rate': 1.0, 'subsample': 0.9351529901519405, 'colsample_bylevel': 0.5492977310397356, 'colsample_bytree': 0.9510761842589558, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5415707634193446, 'FLAML_sample_size': 41272}, 'config/n_estimators': 8, 'config/max_leaves': 16, 'config/min_child_weight': 0.04321599195729943, 'config/learning_rate': 1.0, 'config/subsample': 0.9351529901519405, 'config/colsample_bylevel': 0.5492977310397356, 'config/colsample_bytree': 0.9510761842589558, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.5415707634193446, 'config/FLAML_sample_size': 41272, 'experiment_tag': 'exp', 'time_total_s': 7.391903638839722}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5492977310397356, colsample_bynode=1,
             colsample_bytree=0.9510761842589558, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.04321599195729943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5415707634193446, scale_pos_weight=1,
             subsample=0.9351529901519405, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8572101919281805
PM2.5(0)的mse=43.165426665461105
PM2.5(0)的mae=4.4107015869306805
PM2.5(0)的mar=0.30496452731067925
总共花费的时间为：67.53
伊春市
2252A
2253A
2254A
3342A
3343A
3344A
3480A
[flaml.automl: 09-18 00:52:19] {2390} INFO - task = regression
[flaml.automl: 09-18 00:52:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:52:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:52:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:52:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:52:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:52:21] {3025} INFO - Estimated sufficient time budget=87219s. Estimated necessary time budget=87s.
[flaml.automl: 09-18 00:52:21] {3072} INFO -  at 1.6s,	estimator xgboost's best error=11.4406,	best estimator xgboost's best error=11.4406
[flaml.automl: 09-18 00:52:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:52:23] {3072} INFO -  at 3.7s,	estimator xgboost's best error=6.2071,	best estimator xgboost's best error=6.2071
[flaml.automl: 09-18 00:52:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:52:24] {3072} INFO -  at 4.9s,	estimator xgboost's best error=6.2071,	best estimator xgboost's best error=6.2071
[flaml.automl: 09-18 00:52:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:52:27] {3072} INFO -  at 8.2s,	estimator xgboost's best error=6.2071,	best estimator xgboost's best error=6.2071
[flaml.automl: 09-18 00:52:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:52:28] {3072} INFO -  at 9.3s,	estimator xgboost's best error=4.9949,	best estimator xgboost's best error=4.9949
[flaml.automl: 09-18 00:52:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:52:30] {3072} INFO -  at 10.9s,	estimator xgboost's best error=4.9949,	best estimator xgboost's best error=4.9949
[flaml.automl: 09-18 00:52:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:52:32] {3072} INFO -  at 12.6s,	estimator xgboost's best error=4.3897,	best estimator xgboost's best error=4.3897
[flaml.automl: 09-18 00:52:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:52:34] {3072} INFO -  at 15.3s,	estimator xgboost's best error=4.3897,	best estimator xgboost's best error=4.3897
[flaml.automl: 09-18 00:52:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:52:36] {3072} INFO -  at 16.9s,	estimator xgboost's best error=4.3897,	best estimator xgboost's best error=4.3897
[flaml.automl: 09-18 00:52:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:52:38] {3072} INFO -  at 19.5s,	estimator xgboost's best error=4.3897,	best estimator xgboost's best error=4.3897
[flaml.automl: 09-18 00:52:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:52:40] {3072} INFO -  at 20.9s,	estimator xgboost's best error=4.3897,	best estimator xgboost's best error=4.3897
[flaml.automl: 09-18 00:52:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:52:42] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.3437,	best estimator xgboost's best error=4.3437
[flaml.automl: 09-18 00:52:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:52:43] {3072} INFO -  at 23.8s,	estimator xgboost's best error=4.3437,	best estimator xgboost's best error=4.3437
[flaml.automl: 09-18 00:52:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:52:50] {3072} INFO -  at 30.9s,	estimator xgboost's best error=4.0899,	best estimator xgboost's best error=4.0899
[flaml.automl: 09-18 00:52:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 00:53:03] {3072} INFO -  at 43.7s,	estimator xgboost's best error=4.0441,	best estimator xgboost's best error=4.0441
[flaml.automl: 09-18 00:53:03] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 00:53:10] {3072} INFO -  at 50.8s,	estimator xgboost's best error=4.0441,	best estimator xgboost's best error=4.0441
[flaml.automl: 09-18 00:53:26] {3335} INFO - retrain xgboost for 16.6s
[flaml.automl: 09-18 00:53:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:53:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:53:26] {2637} INFO - Time taken to find the best model: 43.69508337974548
[flaml.automl: 09-18 00:53:26] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 73211}
PM2.5(0)最佳损失：-3.0440585799208435
PM2.5(0)最好结果：{'pred_time': 5.5361761223266075e-06, 'wall_clock_time': 43.69508337974548, 'metric_for_logging': {'pred_time': 5.5361761223266075e-06}, 'val_loss': 4.0440585799208435, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 73211}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 73211, 'experiment_tag': 'exp', 'time_total_s': 12.820941925048828}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8345143154801847
PM2.5(0)的mse=37.48604369022898
PM2.5(0)的mae=4.042910690420509
PM2.5(0)的mar=0.39988395974121355
总共花费的时间为：68.52
佳木斯市
2255A
2256A
2257A
2258A
2259A
[flaml.automl: 09-18 01:08:43] {2390} INFO - task = regression
[flaml.automl: 09-18 01:08:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:08:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:08:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:08:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:08:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:08:46] {3025} INFO - Estimated sufficient time budget=122325s. Estimated necessary time budget=122s.
[flaml.automl: 09-18 01:08:46] {3072} INFO -  at 2.5s,	estimator xgboost's best error=15.7635,	best estimator xgboost's best error=15.7635
[flaml.automl: 09-18 01:08:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:08:50] {3072} INFO -  at 6.6s,	estimator xgboost's best error=7.5437,	best estimator xgboost's best error=7.5437
[flaml.automl: 09-18 01:08:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:08:51] {3072} INFO -  at 8.2s,	estimator xgboost's best error=7.5437,	best estimator xgboost's best error=7.5437
[flaml.automl: 09-18 01:08:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:08:56] {3072} INFO -  at 12.5s,	estimator xgboost's best error=7.5437,	best estimator xgboost's best error=7.5437
[flaml.automl: 09-18 01:08:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:08:57] {3072} INFO -  at 13.7s,	estimator xgboost's best error=6.3004,	best estimator xgboost's best error=6.3004
[flaml.automl: 09-18 01:08:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:08:58] {3072} INFO -  at 15.3s,	estimator xgboost's best error=5.2721,	best estimator xgboost's best error=5.2721
[flaml.automl: 09-18 01:08:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:09:00] {3072} INFO -  at 16.9s,	estimator xgboost's best error=4.6697,	best estimator xgboost's best error=4.6697
[flaml.automl: 09-18 01:09:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:09:03] {3072} INFO -  at 19.6s,	estimator xgboost's best error=4.6697,	best estimator xgboost's best error=4.6697
[flaml.automl: 09-18 01:09:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:09:06] {3072} INFO -  at 22.5s,	estimator xgboost's best error=4.6697,	best estimator xgboost's best error=4.6697
[flaml.automl: 09-18 01:09:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:09:09] {3072} INFO -  at 25.7s,	estimator xgboost's best error=4.6697,	best estimator xgboost's best error=4.6697
[flaml.automl: 09-18 01:09:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:09:12] {3072} INFO -  at 28.5s,	estimator xgboost's best error=4.6697,	best estimator xgboost's best error=4.6697
[flaml.automl: 09-18 01:09:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:09:15] {3072} INFO -  at 31.5s,	estimator xgboost's best error=4.5786,	best estimator xgboost's best error=4.5786
[flaml.automl: 09-18 01:09:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:09:17] {3072} INFO -  at 33.7s,	estimator xgboost's best error=4.5786,	best estimator xgboost's best error=4.5786
[flaml.automl: 09-18 01:09:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:09:30] {3072} INFO -  at 46.6s,	estimator xgboost's best error=3.1962,	best estimator xgboost's best error=3.1962
[flaml.automl: 09-18 01:09:30] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:09:42] {3072} INFO -  at 59.1s,	estimator xgboost's best error=3.1623,	best estimator xgboost's best error=3.1623
[flaml.automl: 09-18 01:10:05] {3335} INFO - retrain xgboost for 23.3s
[flaml.automl: 09-18 01:10:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:10:05] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:10:05] {2637} INFO - Time taken to find the best model: 59.13690447807312
[flaml.automl: 09-18 01:10:05] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 53703}
PM2.5(0)最佳损失：-2.16230519617291
PM2.5(0)最好结果：{'pred_time': 1.448057292293927e-05, 'wall_clock_time': 59.13690447807312, 'metric_for_logging': {'pred_time': 1.448057292293927e-05}, 'val_loss': 3.16230519617291, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 53703}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 53703, 'experiment_tag': 'exp', 'time_total_s': 12.540881633758545}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9411802176591764
PM2.5(0)的mse=26.785032945883607
PM2.5(0)的mae=3.2086343146830263
PM2.5(0)的mar=0.17769419190348038
总共花费的时间为：83.26
七台河市
2262A
3345A
3637A
3684A
[flaml.automl: 09-18 01:22:49] {2390} INFO - task = regression
[flaml.automl: 09-18 01:22:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:22:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:22:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:22:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:22:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:22:51] {3025} INFO - Estimated sufficient time budget=93498s. Estimated necessary time budget=93s.
[flaml.automl: 09-18 01:22:51] {3072} INFO -  at 2.4s,	estimator xgboost's best error=15.3350,	best estimator xgboost's best error=15.3350
[flaml.automl: 09-18 01:22:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:22:55] {3072} INFO -  at 6.4s,	estimator xgboost's best error=7.8695,	best estimator xgboost's best error=7.8695
[flaml.automl: 09-18 01:22:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:22:58] {3072} INFO -  at 8.6s,	estimator xgboost's best error=7.8695,	best estimator xgboost's best error=7.8695
[flaml.automl: 09-18 01:22:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:23:03] {3072} INFO -  at 13.8s,	estimator xgboost's best error=7.8695,	best estimator xgboost's best error=7.8695
[flaml.automl: 09-18 01:23:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:23:05] {3072} INFO -  at 15.9s,	estimator xgboost's best error=6.4462,	best estimator xgboost's best error=6.4462
[flaml.automl: 09-18 01:23:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:23:08] {3072} INFO -  at 18.8s,	estimator xgboost's best error=5.7746,	best estimator xgboost's best error=5.7746
[flaml.automl: 09-18 01:23:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:23:11] {3072} INFO -  at 21.8s,	estimator xgboost's best error=5.4289,	best estimator xgboost's best error=5.4289
[flaml.automl: 09-18 01:23:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:23:14] {3072} INFO -  at 25.4s,	estimator xgboost's best error=5.4289,	best estimator xgboost's best error=5.4289
[flaml.automl: 09-18 01:23:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:23:17] {3072} INFO -  at 28.4s,	estimator xgboost's best error=5.4289,	best estimator xgboost's best error=5.4289
[flaml.automl: 09-18 01:23:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:23:21] {3072} INFO -  at 31.6s,	estimator xgboost's best error=5.4289,	best estimator xgboost's best error=5.4289
[flaml.automl: 09-18 01:23:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:23:23] {3072} INFO -  at 34.3s,	estimator xgboost's best error=5.4289,	best estimator xgboost's best error=5.4289
[flaml.automl: 09-18 01:23:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:23:26] {3072} INFO -  at 37.4s,	estimator xgboost's best error=5.2905,	best estimator xgboost's best error=5.2905
[flaml.automl: 09-18 01:23:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:23:29] {3072} INFO -  at 39.5s,	estimator xgboost's best error=5.2905,	best estimator xgboost's best error=5.2905
[flaml.automl: 09-18 01:23:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:23:41] {3072} INFO -  at 52.2s,	estimator xgboost's best error=4.5725,	best estimator xgboost's best error=4.5725
[flaml.automl: 09-18 01:23:54] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-18 01:23:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 01:23:54] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:23:54] {2637} INFO - Time taken to find the best model: 52.217273235321045
[flaml.automl: 09-18 01:23:54] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 42095}
PM2.5(0)最佳损失：-3.572469086206688
PM2.5(0)最好结果：{'pred_time': 1.567533353599232e-05, 'wall_clock_time': 52.217273235321045, 'metric_for_logging': {'pred_time': 1.567533353599232e-05}, 'val_loss': 4.572469086206688, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 42095}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 42095, 'experiment_tag': 'exp', 'time_total_s': 12.756873369216919}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8503274218570636
PM2.5(0)的mse=60.39477313731942
PM2.5(0)的mae=4.752081321619418
PM2.5(0)的mar=0.3369103388933091
总共花费的时间为：65.62
黑河市
2263A
2264A
2265A
[flaml.automl: 09-18 01:33:37] {2390} INFO - task = regression
[flaml.automl: 09-18 01:33:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:33:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:33:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:33:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:33:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:33:38] {3025} INFO - Estimated sufficient time budget=12086s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 01:33:38] {3072} INFO -  at 1.3s,	estimator xgboost's best error=7.9939,	best estimator xgboost's best error=7.9939
[flaml.automl: 09-18 01:33:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:33:41] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.1039,	best estimator xgboost's best error=4.1039
[flaml.automl: 09-18 01:33:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:33:42] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.1039,	best estimator xgboost's best error=4.1039
[flaml.automl: 09-18 01:33:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:33:52] {3072} INFO -  at 14.7s,	estimator xgboost's best error=4.1039,	best estimator xgboost's best error=4.1039
[flaml.automl: 09-18 01:33:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:33:53] {3072} INFO -  at 15.8s,	estimator xgboost's best error=3.3418,	best estimator xgboost's best error=3.3418
[flaml.automl: 09-18 01:33:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:33:54] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.0776,	best estimator xgboost's best error=3.0776
[flaml.automl: 09-18 01:33:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:33:56] {3072} INFO -  at 19.0s,	estimator xgboost's best error=2.9203,	best estimator xgboost's best error=2.9203
[flaml.automl: 09-18 01:33:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:33:59] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.9203,	best estimator xgboost's best error=2.9203
[flaml.automl: 09-18 01:33:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:34:00] {3072} INFO -  at 23.3s,	estimator xgboost's best error=2.9203,	best estimator xgboost's best error=2.9203
[flaml.automl: 09-18 01:34:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:34:05] {3072} INFO -  at 28.3s,	estimator xgboost's best error=2.7912,	best estimator xgboost's best error=2.7912
[flaml.automl: 09-18 01:34:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:34:08] {3072} INFO -  at 31.3s,	estimator xgboost's best error=2.7912,	best estimator xgboost's best error=2.7912
[flaml.automl: 09-18 01:34:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:34:11] {3072} INFO -  at 33.5s,	estimator xgboost's best error=2.7912,	best estimator xgboost's best error=2.7912
[flaml.automl: 09-18 01:34:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:34:36] {3072} INFO -  at 58.7s,	estimator xgboost's best error=2.7257,	best estimator xgboost's best error=2.7257
[flaml.automl: 09-18 01:35:01] {3335} INFO - retrain xgboost for 24.9s
[flaml.automl: 09-18 01:35:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 01:35:01] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:35:01] {2637} INFO - Time taken to find the best model: 58.71794891357422
[flaml.automl: 09-18 01:35:01] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-1.7257060284482462
PM2.5(0)最好结果：{'pred_time': 2.2603032984547507e-05, 'wall_clock_time': 58.71794891357422, 'metric_for_logging': {'pred_time': 2.2603032984547507e-05}, 'val_loss': 2.725706028448246, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 25.207422018051147}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7776453076349764
PM2.5(0)的mse=14.646124795001315
PM2.5(0)的mae=2.7516539560885596
PM2.5(0)的mar=0.3154629161477478
总共花费的时间为：84.16
绥化市
2266A
2267A
[flaml.automl: 09-18 01:41:59] {2390} INFO - task = regression
[flaml.automl: 09-18 01:41:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:41:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:41:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:41:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:41:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:42:01] {3025} INFO - Estimated sufficient time budget=12132s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 01:42:01] {3072} INFO -  at 1.3s,	estimator xgboost's best error=19.2623,	best estimator xgboost's best error=19.2623
[flaml.automl: 09-18 01:42:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:42:03] {3072} INFO -  at 3.4s,	estimator xgboost's best error=9.8696,	best estimator xgboost's best error=9.8696
[flaml.automl: 09-18 01:42:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:42:04] {3072} INFO -  at 4.6s,	estimator xgboost's best error=9.8696,	best estimator xgboost's best error=9.8696
[flaml.automl: 09-18 01:42:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:42:13] {3072} INFO -  at 14.1s,	estimator xgboost's best error=9.8696,	best estimator xgboost's best error=9.8696
[flaml.automl: 09-18 01:42:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:42:15] {3072} INFO -  at 15.2s,	estimator xgboost's best error=7.3542,	best estimator xgboost's best error=7.3542
[flaml.automl: 09-18 01:42:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:42:16] {3072} INFO -  at 16.8s,	estimator xgboost's best error=7.3394,	best estimator xgboost's best error=7.3394
[flaml.automl: 09-18 01:42:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:42:19] {3072} INFO -  at 19.5s,	estimator xgboost's best error=6.0383,	best estimator xgboost's best error=6.0383
[flaml.automl: 09-18 01:42:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:42:23] {3072} INFO -  at 23.7s,	estimator xgboost's best error=6.0383,	best estimator xgboost's best error=6.0383
[flaml.automl: 09-18 01:42:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:42:26] {3072} INFO -  at 26.7s,	estimator xgboost's best error=6.0383,	best estimator xgboost's best error=6.0383
[flaml.automl: 09-18 01:42:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:42:32] {3072} INFO -  at 32.3s,	estimator xgboost's best error=5.5501,	best estimator xgboost's best error=5.5501
[flaml.automl: 09-18 01:42:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:42:35] {3072} INFO -  at 35.3s,	estimator xgboost's best error=5.5501,	best estimator xgboost's best error=5.5501
[flaml.automl: 09-18 01:42:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:42:37] {3072} INFO -  at 37.4s,	estimator xgboost's best error=5.5501,	best estimator xgboost's best error=5.5501
[flaml.automl: 09-18 01:42:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:42:59] {3072} INFO -  at 59.2s,	estimator xgboost's best error=5.2062,	best estimator xgboost's best error=5.2062
[flaml.automl: 09-18 01:43:21] {3335} INFO - retrain xgboost for 22.1s
[flaml.automl: 09-18 01:43:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 01:43:21] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:43:21] {2637} INFO - Time taken to find the best model: 59.19614338874817
[flaml.automl: 09-18 01:43:21] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-4.206216966873689
PM2.5(0)最好结果：{'pred_time': 3.468193567372627e-05, 'wall_clock_time': 59.19614338874817, 'metric_for_logging': {'pred_time': 3.468193567372627e-05}, 'val_loss': 5.206216966873689, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 21.79094099998474}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9128257380724334
PM2.5(0)的mse=73.49748551717337
PM2.5(0)的mae=5.165902212844498
PM2.5(0)的mar=0.25270518698916816
总共花费的时间为：81.82
大兴安岭地区
3663A
[flaml.automl: 09-18 01:46:47] {2390} INFO - task = regression
[flaml.automl: 09-18 01:46:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:46:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:46:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:46:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:46:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:46:48] {3025} INFO - Estimated sufficient time budget=11944s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 01:46:48] {3072} INFO -  at 1.3s,	estimator xgboost's best error=9.5317,	best estimator xgboost's best error=9.5317
[flaml.automl: 09-18 01:46:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:46:50] {3072} INFO -  at 3.1s,	estimator xgboost's best error=5.3019,	best estimator xgboost's best error=5.3019
[flaml.automl: 09-18 01:46:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:46:51] {3072} INFO -  at 4.3s,	estimator xgboost's best error=5.3019,	best estimator xgboost's best error=5.3019
[flaml.automl: 09-18 01:46:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:46:58] {3072} INFO -  at 11.4s,	estimator xgboost's best error=5.3019,	best estimator xgboost's best error=5.3019
[flaml.automl: 09-18 01:46:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:46:59] {3072} INFO -  at 12.5s,	estimator xgboost's best error=3.6309,	best estimator xgboost's best error=3.6309
[flaml.automl: 09-18 01:46:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:47:01] {3072} INFO -  at 14.9s,	estimator xgboost's best error=3.2661,	best estimator xgboost's best error=3.2661
[flaml.automl: 09-18 01:47:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:47:04] {3072} INFO -  at 17.9s,	estimator xgboost's best error=3.2661,	best estimator xgboost's best error=3.2661
[flaml.automl: 09-18 01:47:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:47:09] {3072} INFO -  at 22.0s,	estimator xgboost's best error=3.2661,	best estimator xgboost's best error=3.2661
[flaml.automl: 09-18 01:47:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:47:11] {3072} INFO -  at 24.2s,	estimator xgboost's best error=3.2661,	best estimator xgboost's best error=3.2661
[flaml.automl: 09-18 01:47:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:47:15] {3072} INFO -  at 28.8s,	estimator xgboost's best error=3.2661,	best estimator xgboost's best error=3.2661
[flaml.automl: 09-18 01:47:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:47:17] {3072} INFO -  at 30.9s,	estimator xgboost's best error=3.2661,	best estimator xgboost's best error=3.2661
[flaml.automl: 09-18 01:47:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:47:20] {3072} INFO -  at 33.0s,	estimator xgboost's best error=3.2661,	best estimator xgboost's best error=3.2661
[flaml.automl: 09-18 01:47:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:47:30] {3072} INFO -  at 43.4s,	estimator xgboost's best error=3.1071,	best estimator xgboost's best error=3.1071
[flaml.automl: 09-18 01:47:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:47:46] {3072} INFO -  at 59.5s,	estimator xgboost's best error=3.0106,	best estimator xgboost's best error=3.0106
[flaml.automl: 09-18 01:48:04] {3335} INFO - retrain xgboost for 18.3s
[flaml.automl: 09-18 01:48:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:48:04] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:48:04] {2637} INFO - Time taken to find the best model: 59.47742700576782
[flaml.automl: 09-18 01:48:04] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
PM2.5(0)最佳损失：-2.0105575830590148
PM2.5(0)最好结果：{'pred_time': 6.11529724426779e-05, 'wall_clock_time': 59.47742700576782, 'metric_for_logging': {'pred_time': 6.11529724426779e-05}, 'val_loss': 3.0105575830590148, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 17, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 16.039500951766968}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.7863477197945041
PM2.5(0)的mse=18.037748040334755
PM2.5(0)的mae=2.8731159407961737
PM2.5(0)的mar=0.22948118257285705
总共花费的时间为：78.01
蚌埠市
2270A
2271A
2274A
2275A
3715A
[flaml.automl: 09-18 02:04:32] {2390} INFO - task = regression
[flaml.automl: 09-18 02:04:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:04:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:04:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:04:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:04:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:04:35] {3025} INFO - Estimated sufficient time budget=120696s. Estimated necessary time budget=121s.
[flaml.automl: 09-18 02:04:35] {3072} INFO -  at 2.5s,	estimator xgboost's best error=22.9676,	best estimator xgboost's best error=22.9676
[flaml.automl: 09-18 02:04:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:04:38] {3072} INFO -  at 6.4s,	estimator xgboost's best error=11.3002,	best estimator xgboost's best error=11.3002
[flaml.automl: 09-18 02:04:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:04:41] {3072} INFO -  at 8.7s,	estimator xgboost's best error=11.3002,	best estimator xgboost's best error=11.3002
[flaml.automl: 09-18 02:04:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:04:45] {3072} INFO -  at 12.7s,	estimator xgboost's best error=11.3002,	best estimator xgboost's best error=11.3002
[flaml.automl: 09-18 02:04:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:04:47] {3072} INFO -  at 14.8s,	estimator xgboost's best error=7.7287,	best estimator xgboost's best error=7.7287
[flaml.automl: 09-18 02:04:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:04:50] {3072} INFO -  at 17.7s,	estimator xgboost's best error=7.7287,	best estimator xgboost's best error=7.7287
[flaml.automl: 09-18 02:04:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:04:53] {3072} INFO -  at 20.9s,	estimator xgboost's best error=6.3597,	best estimator xgboost's best error=6.3597
[flaml.automl: 09-18 02:04:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:04:56] {3072} INFO -  at 24.2s,	estimator xgboost's best error=6.3597,	best estimator xgboost's best error=6.3597
[flaml.automl: 09-18 02:04:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:04:59] {3072} INFO -  at 27.3s,	estimator xgboost's best error=6.3597,	best estimator xgboost's best error=6.3597
[flaml.automl: 09-18 02:04:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:05:02] {3072} INFO -  at 29.9s,	estimator xgboost's best error=6.3597,	best estimator xgboost's best error=6.3597
[flaml.automl: 09-18 02:05:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:05:04] {3072} INFO -  at 32.5s,	estimator xgboost's best error=6.3463,	best estimator xgboost's best error=6.3463
[flaml.automl: 09-18 02:05:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:05:07] {3072} INFO -  at 34.5s,	estimator xgboost's best error=6.3463,	best estimator xgboost's best error=6.3463
[flaml.automl: 09-18 02:05:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:05:09] {3072} INFO -  at 36.7s,	estimator xgboost's best error=6.0330,	best estimator xgboost's best error=6.0330
[flaml.automl: 09-18 02:05:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:05:11] {3072} INFO -  at 38.6s,	estimator xgboost's best error=6.0330,	best estimator xgboost's best error=6.0330
[flaml.automl: 09-18 02:05:11] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:05:12] {3072} INFO -  at 40.3s,	estimator xgboost's best error=6.0330,	best estimator xgboost's best error=6.0330
[flaml.automl: 09-18 02:05:12] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 02:05:14] {3072} INFO -  at 42.3s,	estimator xgboost's best error=6.0330,	best estimator xgboost's best error=6.0330
[flaml.automl: 09-18 02:05:14] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 02:05:16] {3072} INFO -  at 43.7s,	estimator xgboost's best error=6.0330,	best estimator xgboost's best error=6.0330
[flaml.automl: 09-18 02:05:16] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 02:05:27] {3072} INFO -  at 54.6s,	estimator xgboost's best error=5.6951,	best estimator xgboost's best error=5.6951
[flaml.automl: 09-18 02:05:36] {3335} INFO - retrain xgboost for 9.0s
[flaml.automl: 09-18 02:05:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:05:36] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:05:36] {2637} INFO - Time taken to find the best model: 54.58943295478821
[flaml.automl: 09-18 02:05:36] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 54449}
PM2.5(0)最佳损失：-4.695113128630583
PM2.5(0)最好结果：{'pred_time': 1.1650511055938469e-05, 'wall_clock_time': 54.58943295478821, 'metric_for_logging': {'pred_time': 1.1650511055938469e-05}, 'val_loss': 5.695113128630583, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 54449}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'config/FLAML_sample_size': 54449, 'experiment_tag': 'exp', 'time_total_s': 10.89686393737793}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8936288600118265
PM2.5(0)的mse=57.77987935957538
PM2.5(0)的mae=5.6368833022087115
PM2.5(0)的mar=0.2955499679965876
总共花费的时间为：64.47
淮南市
2278A
2279A
2280A
2281A
[flaml.automl: 09-18 02:18:11] {2390} INFO - task = regression
[flaml.automl: 09-18 02:18:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:18:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:18:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:18:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:18:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:18:15] {3025} INFO - Estimated sufficient time budget=137048s. Estimated necessary time budget=137s.
[flaml.automl: 09-18 02:18:15] {3072} INFO -  at 3.5s,	estimator xgboost's best error=24.2418,	best estimator xgboost's best error=24.2418
[flaml.automl: 09-18 02:18:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:18:21] {3072} INFO -  at 9.5s,	estimator xgboost's best error=11.6950,	best estimator xgboost's best error=11.6950
[flaml.automl: 09-18 02:18:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:18:24] {3072} INFO -  at 12.8s,	estimator xgboost's best error=11.6950,	best estimator xgboost's best error=11.6950
[flaml.automl: 09-18 02:18:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:18:29] {3072} INFO -  at 17.4s,	estimator xgboost's best error=11.6950,	best estimator xgboost's best error=11.6950
[flaml.automl: 09-18 02:18:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:18:31] {3072} INFO -  at 19.6s,	estimator xgboost's best error=7.9320,	best estimator xgboost's best error=7.9320
[flaml.automl: 09-18 02:18:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:18:34] {3072} INFO -  at 22.5s,	estimator xgboost's best error=7.9320,	best estimator xgboost's best error=7.9320
[flaml.automl: 09-18 02:18:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:18:37] {3072} INFO -  at 25.5s,	estimator xgboost's best error=6.6915,	best estimator xgboost's best error=6.6915
[flaml.automl: 09-18 02:18:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:18:40] {3072} INFO -  at 29.0s,	estimator xgboost's best error=6.6915,	best estimator xgboost's best error=6.6915
[flaml.automl: 09-18 02:18:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:18:44] {3072} INFO -  at 32.4s,	estimator xgboost's best error=6.6915,	best estimator xgboost's best error=6.6915
[flaml.automl: 09-18 02:18:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:18:46] {3072} INFO -  at 35.2s,	estimator xgboost's best error=6.6915,	best estimator xgboost's best error=6.6915
[flaml.automl: 09-18 02:18:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:18:49] {3072} INFO -  at 37.9s,	estimator xgboost's best error=6.6915,	best estimator xgboost's best error=6.6915
[flaml.automl: 09-18 02:18:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:18:54] {3072} INFO -  at 42.8s,	estimator xgboost's best error=6.6915,	best estimator xgboost's best error=6.6915
[flaml.automl: 09-18 02:18:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:18:57] {3072} INFO -  at 46.1s,	estimator xgboost's best error=6.6915,	best estimator xgboost's best error=6.6915
[flaml.automl: 09-18 02:18:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:19:10] {3072} INFO -  at 58.7s,	estimator xgboost's best error=6.1923,	best estimator xgboost's best error=6.1923
[flaml.automl: 09-18 02:19:21] {3335} INFO - retrain xgboost for 11.2s
[flaml.automl: 09-18 02:19:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:19:21] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:19:21] {2637} INFO - Time taken to find the best model: 58.73336887359619
[flaml.automl: 09-18 02:19:21] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41204}
PM2.5(0)最佳损失：-5.192275798224445
PM2.5(0)最好结果：{'pred_time': 2.590192147909124e-05, 'wall_clock_time': 58.73336887359619, 'metric_for_logging': {'pred_time': 2.590192147909124e-05}, 'val_loss': 6.192275798224445, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41204}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 41204, 'experiment_tag': 'exp', 'time_total_s': 12.61655044555664}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8891065581915683
PM2.5(0)的mse=70.33702480924491
PM2.5(0)的mae=6.195348043865461
PM2.5(0)的mar=0.31090173044898384
总共花费的时间为：70.62
淮北市
2282A
2283A
2284A
3330A
[flaml.automl: 09-18 02:32:51] {2390} INFO - task = regression
[flaml.automl: 09-18 02:32:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:32:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:32:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:32:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:32:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:32:53] {3025} INFO - Estimated sufficient time budget=96658s. Estimated necessary time budget=97s.
[flaml.automl: 09-18 02:32:53] {3072} INFO -  at 2.4s,	estimator xgboost's best error=25.0854,	best estimator xgboost's best error=25.0854
[flaml.automl: 09-18 02:32:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:32:57] {3072} INFO -  at 6.2s,	estimator xgboost's best error=11.6561,	best estimator xgboost's best error=11.6561
[flaml.automl: 09-18 02:32:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:32:59] {3072} INFO -  at 8.5s,	estimator xgboost's best error=11.6561,	best estimator xgboost's best error=11.6561
[flaml.automl: 09-18 02:32:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:33:04] {3072} INFO -  at 13.6s,	estimator xgboost's best error=11.6561,	best estimator xgboost's best error=11.6561
[flaml.automl: 09-18 02:33:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:33:06] {3072} INFO -  at 15.8s,	estimator xgboost's best error=7.1207,	best estimator xgboost's best error=7.1207
[flaml.automl: 09-18 02:33:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:33:09] {3072} INFO -  at 18.7s,	estimator xgboost's best error=6.5940,	best estimator xgboost's best error=6.5940
[flaml.automl: 09-18 02:33:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:33:12] {3072} INFO -  at 21.7s,	estimator xgboost's best error=5.5733,	best estimator xgboost's best error=5.5733
[flaml.automl: 09-18 02:33:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:33:17] {3072} INFO -  at 26.0s,	estimator xgboost's best error=5.5733,	best estimator xgboost's best error=5.5733
[flaml.automl: 09-18 02:33:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:33:20] {3072} INFO -  at 29.0s,	estimator xgboost's best error=5.5733,	best estimator xgboost's best error=5.5733
[flaml.automl: 09-18 02:33:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:33:23] {3072} INFO -  at 32.3s,	estimator xgboost's best error=5.5733,	best estimator xgboost's best error=5.5733
[flaml.automl: 09-18 02:33:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:33:26] {3072} INFO -  at 34.9s,	estimator xgboost's best error=5.2397,	best estimator xgboost's best error=5.2397
[flaml.automl: 09-18 02:33:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:33:28] {3072} INFO -  at 37.0s,	estimator xgboost's best error=5.2397,	best estimator xgboost's best error=5.2397
[flaml.automl: 09-18 02:33:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:33:30] {3072} INFO -  at 39.3s,	estimator xgboost's best error=4.5429,	best estimator xgboost's best error=4.5429
[flaml.automl: 09-18 02:33:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:33:32] {3072} INFO -  at 41.2s,	estimator xgboost's best error=4.5429,	best estimator xgboost's best error=4.5429
[flaml.automl: 09-18 02:33:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:33:33] {3072} INFO -  at 42.8s,	estimator xgboost's best error=4.5429,	best estimator xgboost's best error=4.5429
[flaml.automl: 09-18 02:33:33] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 02:33:35] {3072} INFO -  at 44.8s,	estimator xgboost's best error=4.5429,	best estimator xgboost's best error=4.5429
[flaml.automl: 09-18 02:33:35] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 02:33:37] {3072} INFO -  at 46.2s,	estimator xgboost's best error=4.5429,	best estimator xgboost's best error=4.5429
[flaml.automl: 09-18 02:33:37] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 02:33:48] {3072} INFO -  at 57.5s,	estimator xgboost's best error=4.0752,	best estimator xgboost's best error=4.0752
[flaml.automl: 09-18 02:33:54] {3335} INFO - retrain xgboost for 6.0s
[flaml.automl: 09-18 02:33:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:33:54] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:33:54] {2637} INFO - Time taken to find the best model: 57.497716188430786
[flaml.automl: 09-18 02:33:54] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024, 'FLAML_sample_size': 42961}
PM2.5(0)最佳损失：-3.0752475509395847
PM2.5(0)最好结果：{'pred_time': 1.9175565837065844e-05, 'wall_clock_time': 57.497716188430786, 'metric_for_logging': {'pred_time': 1.9175565837065844e-05}, 'val_loss': 4.075247550939585, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024, 'FLAML_sample_size': 42961}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'config/FLAML_sample_size': 42961, 'experiment_tag': 'exp', 'time_total_s': 11.253809452056885}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9407849943518674
PM2.5(0)的mse=37.45647166989471
PM2.5(0)的mae=4.176009077292222
PM2.5(0)的mar=0.15091600556218132
总共花费的时间为：64.24
铜陵市
2285A
2286A
2287A
2288A
2289A
2290A
[flaml.automl: 09-18 02:53:00] {2390} INFO - task = regression
[flaml.automl: 09-18 02:53:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:53:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:53:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:53:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:53:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:53:04] {3025} INFO - Estimated sufficient time budget=215788s. Estimated necessary time budget=216s.
[flaml.automl: 09-18 02:53:04] {3072} INFO -  at 3.6s,	estimator xgboost's best error=20.4158,	best estimator xgboost's best error=20.4158
[flaml.automl: 09-18 02:53:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:53:08] {3072} INFO -  at 7.7s,	estimator xgboost's best error=14.8548,	best estimator xgboost's best error=14.8548
[flaml.automl: 09-18 02:53:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:53:11] {3072} INFO -  at 11.0s,	estimator xgboost's best error=14.8548,	best estimator xgboost's best error=14.8548
[flaml.automl: 09-18 02:53:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:53:14] {3072} INFO -  at 14.5s,	estimator xgboost's best error=14.8548,	best estimator xgboost's best error=14.8548
[flaml.automl: 09-18 02:53:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:53:17] {3072} INFO -  at 17.1s,	estimator xgboost's best error=9.3374,	best estimator xgboost's best error=9.3374
[flaml.automl: 09-18 02:53:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:53:19] {3072} INFO -  at 19.6s,	estimator xgboost's best error=9.3374,	best estimator xgboost's best error=9.3374
[flaml.automl: 09-18 02:53:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:53:22] {3072} INFO -  at 22.3s,	estimator xgboost's best error=9.3374,	best estimator xgboost's best error=9.3374
[flaml.automl: 09-18 02:53:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:53:24] {3072} INFO -  at 24.0s,	estimator xgboost's best error=9.3374,	best estimator xgboost's best error=9.3374
[flaml.automl: 09-18 02:53:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:53:26] {3072} INFO -  at 25.9s,	estimator xgboost's best error=7.5030,	best estimator xgboost's best error=7.5030
[flaml.automl: 09-18 02:53:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:53:27] {3072} INFO -  at 27.5s,	estimator xgboost's best error=7.5030,	best estimator xgboost's best error=7.5030
[flaml.automl: 09-18 02:53:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:53:29] {3072} INFO -  at 29.4s,	estimator xgboost's best error=7.5030,	best estimator xgboost's best error=7.5030
[flaml.automl: 09-18 02:53:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:53:31] {3072} INFO -  at 31.2s,	estimator xgboost's best error=7.5030,	best estimator xgboost's best error=7.5030
[flaml.automl: 09-18 02:53:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:53:33] {3072} INFO -  at 33.2s,	estimator xgboost's best error=6.3463,	best estimator xgboost's best error=6.3463
[flaml.automl: 09-18 02:53:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:53:35] {3072} INFO -  at 34.9s,	estimator xgboost's best error=6.3463,	best estimator xgboost's best error=6.3463
[flaml.automl: 09-18 02:53:35] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:53:37] {3072} INFO -  at 37.3s,	estimator xgboost's best error=6.3463,	best estimator xgboost's best error=6.3463
[flaml.automl: 09-18 02:53:37] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 02:53:40] {3072} INFO -  at 40.3s,	estimator xgboost's best error=6.3463,	best estimator xgboost's best error=6.3463
[flaml.automl: 09-18 02:53:40] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 02:53:42] {3072} INFO -  at 41.8s,	estimator xgboost's best error=6.3463,	best estimator xgboost's best error=6.3463
[flaml.automl: 09-18 02:53:42] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 02:53:50] {3072} INFO -  at 50.0s,	estimator xgboost's best error=6.0540,	best estimator xgboost's best error=6.0540
[flaml.automl: 09-18 02:53:50] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 02:53:52] {3072} INFO -  at 52.6s,	estimator xgboost's best error=6.0540,	best estimator xgboost's best error=6.0540
[flaml.automl: 09-18 02:53:57] {3335} INFO - retrain xgboost for 4.3s
[flaml.automl: 09-18 02:53:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 02:53:57] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:53:57] {2637} INFO - Time taken to find the best model: 50.04535627365112
[flaml.automl: 09-18 02:53:57] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 64896}
PM2.5(0)最佳损失：-5.054010826412905
PM2.5(0)最好结果：{'pred_time': 1.1007393337192093e-05, 'wall_clock_time': 50.04535627365112, 'metric_for_logging': {'pred_time': 1.1007393337192093e-05}, 'val_loss': 6.054010826412905, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 64896}, 'config/n_estimators': 8, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609401, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466859, 'config/FLAML_sample_size': 64896, 'experiment_tag': 'exp', 'time_total_s': 8.282982110977173}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8056801382362293
PM2.5(0)的mse=63.28795688784067
PM2.5(0)的mae=6.0460103591918
PM2.5(0)的mar=0.37177068450698375
总共花费的时间为：58.19
安庆市
2291A
2292A
3173A
[flaml.automl: 09-18 03:03:39] {2390} INFO - task = regression
[flaml.automl: 09-18 03:03:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:03:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:03:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:03:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:03:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:03:41] {3025} INFO - Estimated sufficient time budget=22445s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 03:03:41] {3072} INFO -  at 2.4s,	estimator xgboost's best error=19.5341,	best estimator xgboost's best error=19.5341
[flaml.automl: 09-18 03:03:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:03:45] {3072} INFO -  at 6.2s,	estimator xgboost's best error=9.1950,	best estimator xgboost's best error=9.1950
[flaml.automl: 09-18 03:03:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:03:47] {3072} INFO -  at 8.5s,	estimator xgboost's best error=9.1950,	best estimator xgboost's best error=9.1950
[flaml.automl: 09-18 03:03:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:04:06] {3072} INFO -  at 27.1s,	estimator xgboost's best error=9.1950,	best estimator xgboost's best error=9.1950
[flaml.automl: 09-18 03:04:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:04:08] {3072} INFO -  at 29.3s,	estimator xgboost's best error=6.2163,	best estimator xgboost's best error=6.2163
[flaml.automl: 09-18 03:04:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:04:11] {3072} INFO -  at 32.3s,	estimator xgboost's best error=6.2163,	best estimator xgboost's best error=6.2163
[flaml.automl: 09-18 03:04:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:04:14] {3072} INFO -  at 35.5s,	estimator xgboost's best error=3.9068,	best estimator xgboost's best error=3.9068
[flaml.automl: 09-18 03:04:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:04:19] {3072} INFO -  at 40.4s,	estimator xgboost's best error=3.9068,	best estimator xgboost's best error=3.9068
[flaml.automl: 09-18 03:04:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:04:22] {3072} INFO -  at 43.4s,	estimator xgboost's best error=3.9068,	best estimator xgboost's best error=3.9068
[flaml.automl: 09-18 03:04:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:04:28] {3072} INFO -  at 49.1s,	estimator xgboost's best error=3.9068,	best estimator xgboost's best error=3.9068
[flaml.automl: 09-18 03:04:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:04:31] {3072} INFO -  at 51.8s,	estimator xgboost's best error=3.9068,	best estimator xgboost's best error=3.9068
[flaml.automl: 09-18 03:04:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:04:33] {3072} INFO -  at 54.0s,	estimator xgboost's best error=3.9068,	best estimator xgboost's best error=3.9068
[flaml.automl: 09-18 03:04:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:04:38] {3072} INFO -  at 59.2s,	estimator xgboost's best error=3.2711,	best estimator xgboost's best error=3.2711
[flaml.automl: 09-18 03:04:49] {3335} INFO - retrain xgboost for 11.1s
[flaml.automl: 09-18 03:04:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:04:49] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:04:49] {2637} INFO - Time taken to find the best model: 59.23320269584656
[flaml.automl: 09-18 03:04:49] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
PM2.5(0)最佳损失：-2.2711096019498487
PM2.5(0)最好结果：{'pred_time': 2.2871177606922762e-05, 'wall_clock_time': 59.23320269584656, 'metric_for_logging': {'pred_time': 2.2871177606922762e-05}, 'val_loss': 3.2711096019498487, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 5.242441415786743}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9455806334006809
PM2.5(0)的mse=21.176554824231776
PM2.5(0)的mae=3.1047736281905176
PM2.5(0)的mar=0.14265056054428832
总共花费的时间为：70.86
黄山市
2295A
2296A
2297A
[flaml.automl: 09-18 03:15:01] {2390} INFO - task = regression
[flaml.automl: 09-18 03:15:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:15:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:15:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:15:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:15:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:15:03] {3025} INFO - Estimated sufficient time budget=23171s. Estimated necessary time budget=23s.
[flaml.automl: 09-18 03:15:03] {3072} INFO -  at 2.5s,	estimator xgboost's best error=11.5253,	best estimator xgboost's best error=11.5253
[flaml.automl: 09-18 03:15:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:15:08] {3072} INFO -  at 7.1s,	estimator xgboost's best error=6.7578,	best estimator xgboost's best error=6.7578
[flaml.automl: 09-18 03:15:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:15:11] {3072} INFO -  at 10.6s,	estimator xgboost's best error=6.7578,	best estimator xgboost's best error=6.7578
[flaml.automl: 09-18 03:15:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:15:40] {3072} INFO -  at 38.8s,	estimator xgboost's best error=6.7578,	best estimator xgboost's best error=6.7578
[flaml.automl: 09-18 03:15:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:15:43] {3072} INFO -  at 42.0s,	estimator xgboost's best error=5.8005,	best estimator xgboost's best error=5.8005
[flaml.automl: 09-18 03:15:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:15:47] {3072} INFO -  at 46.5s,	estimator xgboost's best error=5.8005,	best estimator xgboost's best error=5.8005
[flaml.automl: 09-18 03:15:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:15:52] {3072} INFO -  at 51.4s,	estimator xgboost's best error=5.3247,	best estimator xgboost's best error=5.3247
[flaml.automl: 09-18 03:15:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:15:59] {3072} INFO -  at 58.3s,	estimator xgboost's best error=5.3247,	best estimator xgboost's best error=5.3247
[flaml.automl: 09-18 03:16:02] {3335} INFO - retrain xgboost for 3.1s
[flaml.automl: 09-18 03:16:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 03:16:02] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:16:02] {2637} INFO - Time taken to find the best model: 51.35916042327881
[flaml.automl: 09-18 03:16:02] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-4.324729931374205
PM2.5(0)最好结果：{'pred_time': 3.1358407104021914e-05, 'wall_clock_time': 51.35916042327881, 'metric_for_logging': {'pred_time': 3.1358407104021914e-05}, 'val_loss': 5.324729931374205, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.8093812465667725}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.6172177740031848
PM2.5(0)的mse=45.1885735504606
PM2.5(0)的mae=5.080841534439514
PM2.5(0)的mar=0.6280902630431003
总共花费的时间为：62.01
滁州市
2298A
2299A
3331A
[flaml.automl: 09-18 03:27:12] {2390} INFO - task = regression
[flaml.automl: 09-18 03:27:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:27:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:27:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:27:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:27:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:27:16] {3025} INFO - Estimated sufficient time budget=42541s. Estimated necessary time budget=43s.
[flaml.automl: 09-18 03:27:16] {3072} INFO -  at 4.5s,	estimator xgboost's best error=20.9180,	best estimator xgboost's best error=20.9180
[flaml.automl: 09-18 03:27:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:27:24] {3072} INFO -  at 12.4s,	estimator xgboost's best error=9.7104,	best estimator xgboost's best error=9.7104
[flaml.automl: 09-18 03:27:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:27:28] {3072} INFO -  at 16.6s,	estimator xgboost's best error=9.7104,	best estimator xgboost's best error=9.7104
[flaml.automl: 09-18 03:27:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:28:02] {3072} INFO -  at 50.5s,	estimator xgboost's best error=9.7104,	best estimator xgboost's best error=9.7104
[flaml.automl: 09-18 03:28:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:28:06] {3072} INFO -  at 53.9s,	estimator xgboost's best error=5.9515,	best estimator xgboost's best error=5.9515
[flaml.automl: 09-18 03:28:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:28:08] {3072} INFO -  at 56.6s,	estimator xgboost's best error=5.9515,	best estimator xgboost's best error=5.9515
[flaml.automl: 09-18 03:28:09] {3335} INFO - retrain xgboost for 1.1s
[flaml.automl: 09-18 03:28:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579228, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.125078938571443, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:28:09] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:28:09] {2637} INFO - Time taken to find the best model: 53.926677227020264
[flaml.automl: 09-18 03:28:09] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579228, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.125078938571443}
PM2.5(0)最佳损失：-4.951523912009063
PM2.5(0)最好结果：{'pred_time': 3.822417989055635e-05, 'wall_clock_time': 53.926677227020264, 'metric_for_logging': {'pred_time': 3.822417989055635e-05}, 'val_loss': 5.951523912009063, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579228, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.125078938571443}, 'config/n_estimators': 4, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579228, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.125078938571443, 'experiment_tag': 'exp', 'time_total_s': 3.442979574203491}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579228, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.125078938571443, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.7564589415103904
PM2.5(0)的mse=67.99424872317593
PM2.5(0)的mae=5.935421374072767
PM2.5(0)的mar=0.26526931313253427
总共花费的时间为：58.66
阜阳市
2301A
2875A
3468A
3469A
[flaml.automl: 09-18 03:42:08] {2390} INFO - task = regression
[flaml.automl: 09-18 03:42:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:42:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:42:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:42:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:42:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:42:10] {3025} INFO - Estimated sufficient time budget=94069s. Estimated necessary time budget=94s.
[flaml.automl: 09-18 03:42:10] {3072} INFO -  at 2.4s,	estimator xgboost's best error=24.9729,	best estimator xgboost's best error=24.9729
[flaml.automl: 09-18 03:42:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:42:14] {3072} INFO -  at 6.3s,	estimator xgboost's best error=11.7905,	best estimator xgboost's best error=11.7905
[flaml.automl: 09-18 03:42:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:42:17] {3072} INFO -  at 9.3s,	estimator xgboost's best error=11.7905,	best estimator xgboost's best error=11.7905
[flaml.automl: 09-18 03:42:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:42:21] {3072} INFO -  at 14.1s,	estimator xgboost's best error=11.7905,	best estimator xgboost's best error=11.7905
[flaml.automl: 09-18 03:42:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:42:25] {3072} INFO -  at 17.3s,	estimator xgboost's best error=8.3142,	best estimator xgboost's best error=8.3142
[flaml.automl: 09-18 03:42:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:42:29] {3072} INFO -  at 21.8s,	estimator xgboost's best error=7.1392,	best estimator xgboost's best error=7.1392
[flaml.automl: 09-18 03:42:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:42:33] {3072} INFO -  at 25.6s,	estimator xgboost's best error=7.1392,	best estimator xgboost's best error=7.1392
[flaml.automl: 09-18 03:42:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:42:35] {3072} INFO -  at 27.7s,	estimator xgboost's best error=7.1392,	best estimator xgboost's best error=7.1392
[flaml.automl: 09-18 03:42:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:42:38] {3072} INFO -  at 30.9s,	estimator xgboost's best error=7.1392,	best estimator xgboost's best error=7.1392
[flaml.automl: 09-18 03:42:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:42:40] {3072} INFO -  at 33.1s,	estimator xgboost's best error=7.1392,	best estimator xgboost's best error=7.1392
[flaml.automl: 09-18 03:42:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:42:45] {3072} INFO -  at 37.7s,	estimator xgboost's best error=7.1392,	best estimator xgboost's best error=7.1392
[flaml.automl: 09-18 03:42:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:42:48] {3072} INFO -  at 41.0s,	estimator xgboost's best error=7.1392,	best estimator xgboost's best error=7.1392
[flaml.automl: 09-18 03:42:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:42:59] {3072} INFO -  at 51.8s,	estimator xgboost's best error=4.8927,	best estimator xgboost's best error=4.8927
[flaml.automl: 09-18 03:43:10] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-18 03:43:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 03:43:10] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:43:10] {2637} INFO - Time taken to find the best model: 51.82979607582092
[flaml.automl: 09-18 03:43:10] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 43617}
PM2.5(0)最佳损失：-3.892692124890327
PM2.5(0)最好结果：{'pred_time': 1.4871475695088054e-05, 'wall_clock_time': 51.82979607582092, 'metric_for_logging': {'pred_time': 1.4871475695088054e-05}, 'val_loss': 4.892692124890327, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 43617}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 43617, 'experiment_tag': 'exp', 'time_total_s': 10.874540090560913}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9400584067133181
PM2.5(0)的mse=47.274219259912805
PM2.5(0)的mae=4.787675466909382
PM2.5(0)的mar=0.19587594739607836
总共花费的时间为：63.23
宿州市
3463A
3634A
3701A
[flaml.automl: 09-18 03:52:55] {2390} INFO - task = regression
[flaml.automl: 09-18 03:52:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:52:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:52:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:52:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:52:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:52:57] {3025} INFO - Estimated sufficient time budget=22232s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 03:52:57] {3072} INFO -  at 2.4s,	estimator xgboost's best error=24.1009,	best estimator xgboost's best error=24.1009
[flaml.automl: 09-18 03:52:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:53:01] {3072} INFO -  at 6.3s,	estimator xgboost's best error=11.2190,	best estimator xgboost's best error=11.2190
[flaml.automl: 09-18 03:53:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:53:03] {3072} INFO -  at 8.5s,	estimator xgboost's best error=11.2190,	best estimator xgboost's best error=11.2190
[flaml.automl: 09-18 03:53:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:53:22] {3072} INFO -  at 27.3s,	estimator xgboost's best error=11.2190,	best estimator xgboost's best error=11.2190
[flaml.automl: 09-18 03:53:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:53:25] {3072} INFO -  at 30.6s,	estimator xgboost's best error=7.8516,	best estimator xgboost's best error=7.8516
[flaml.automl: 09-18 03:53:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:53:30] {3072} INFO -  at 35.1s,	estimator xgboost's best error=6.7225,	best estimator xgboost's best error=6.7225
[flaml.automl: 09-18 03:53:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:53:34] {3072} INFO -  at 39.7s,	estimator xgboost's best error=5.9584,	best estimator xgboost's best error=5.9584
[flaml.automl: 09-18 03:53:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:53:42] {3072} INFO -  at 47.3s,	estimator xgboost's best error=5.9584,	best estimator xgboost's best error=5.9584
[flaml.automl: 09-18 03:53:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:53:46] {3072} INFO -  at 51.9s,	estimator xgboost's best error=5.9584,	best estimator xgboost's best error=5.9584
[flaml.automl: 09-18 03:53:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:53:54] {3072} INFO -  at 59.5s,	estimator xgboost's best error=5.2541,	best estimator xgboost's best error=5.2541
[flaml.automl: 09-18 03:54:03] {3335} INFO - retrain xgboost for 8.7s
[flaml.automl: 09-18 03:54:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 03:54:03] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:54:03] {2637} INFO - Time taken to find the best model: 59.453749895095825
[flaml.automl: 09-18 03:54:03] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-4.254059215771538
PM2.5(0)最好结果：{'pred_time': 3.455644427047755e-05, 'wall_clock_time': 59.453749895095825, 'metric_for_logging': {'pred_time': 3.455644427047755e-05}, 'val_loss': 5.254059215771538, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 7.556680917739868}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9216975085063139
PM2.5(0)的mse=47.50042965578212
PM2.5(0)的mae=4.679091113497061
PM2.5(0)的mar=0.19067121513559895
总共花费的时间为：68.79
六安市
2307A
2308A
2309A
2310A
[flaml.automl: 09-18 04:06:57] {2390} INFO - task = regression
[flaml.automl: 09-18 04:06:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:06:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:06:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:06:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:06:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:06:58] {3025} INFO - Estimated sufficient time budget=52274s. Estimated necessary time budget=52s.
[flaml.automl: 09-18 04:06:58] {3072} INFO -  at 1.4s,	estimator xgboost's best error=19.0294,	best estimator xgboost's best error=19.0294
[flaml.automl: 09-18 04:06:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:07:00] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.2101,	best estimator xgboost's best error=9.2101
[flaml.automl: 09-18 04:07:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:07:01] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.2101,	best estimator xgboost's best error=9.2101
[flaml.automl: 09-18 04:07:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:07:08] {3072} INFO -  at 11.3s,	estimator xgboost's best error=9.2101,	best estimator xgboost's best error=9.2101
[flaml.automl: 09-18 04:07:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:07:10] {3072} INFO -  at 13.4s,	estimator xgboost's best error=7.0109,	best estimator xgboost's best error=7.0109
[flaml.automl: 09-18 04:07:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:07:13] {3072} INFO -  at 16.2s,	estimator xgboost's best error=7.0109,	best estimator xgboost's best error=7.0109
[flaml.automl: 09-18 04:07:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:07:16] {3072} INFO -  at 19.4s,	estimator xgboost's best error=4.7592,	best estimator xgboost's best error=4.7592
[flaml.automl: 09-18 04:07:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:07:20] {3072} INFO -  at 23.7s,	estimator xgboost's best error=4.7592,	best estimator xgboost's best error=4.7592
[flaml.automl: 09-18 04:07:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:07:23] {3072} INFO -  at 26.7s,	estimator xgboost's best error=4.7592,	best estimator xgboost's best error=4.7592
[flaml.automl: 09-18 04:07:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:07:27] {3072} INFO -  at 30.1s,	estimator xgboost's best error=4.7592,	best estimator xgboost's best error=4.7592
[flaml.automl: 09-18 04:07:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:07:29] {3072} INFO -  at 32.7s,	estimator xgboost's best error=4.7592,	best estimator xgboost's best error=4.7592
[flaml.automl: 09-18 04:07:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:07:33] {3072} INFO -  at 35.9s,	estimator xgboost's best error=4.7592,	best estimator xgboost's best error=4.7592
[flaml.automl: 09-18 04:07:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:07:35] {3072} INFO -  at 38.8s,	estimator xgboost's best error=4.7592,	best estimator xgboost's best error=4.7592
[flaml.automl: 09-18 04:07:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:07:55] {3072} INFO -  at 58.4s,	estimator xgboost's best error=4.3015,	best estimator xgboost's best error=4.3015
[flaml.automl: 09-18 04:08:15] {3335} INFO - retrain xgboost for 19.9s
[flaml.automl: 09-18 04:08:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:08:15] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:08:15] {2637} INFO - Time taken to find the best model: 58.415207862854004
[flaml.automl: 09-18 04:08:15] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 43326}
PM2.5(0)最佳损失：-3.3014669112327315
PM2.5(0)最好结果：{'pred_time': 2.5483653305228007e-05, 'wall_clock_time': 58.415207862854004, 'metric_for_logging': {'pred_time': 2.5483653305228007e-05}, 'val_loss': 4.3014669112327315, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 43326}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 43326, 'experiment_tag': 'exp', 'time_total_s': 19.60651421546936}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9225706059614832
PM2.5(0)的mse=34.35221423580177
PM2.5(0)的mae=4.226333037339141
PM2.5(0)的mar=0.2774168159379426
总共花费的时间为：79.24
亳州市
2311A
2312A
3332A
[flaml.automl: 09-18 04:17:54] {2390} INFO - task = regression
[flaml.automl: 09-18 04:17:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:17:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:17:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:17:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:17:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:17:56] {3025} INFO - Estimated sufficient time budget=12105s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 04:17:56] {3072} INFO -  at 1.4s,	estimator xgboost's best error=22.4408,	best estimator xgboost's best error=22.4408
[flaml.automl: 09-18 04:17:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:17:58] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.4606,	best estimator xgboost's best error=10.4606
[flaml.automl: 09-18 04:17:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:17:59] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.4606,	best estimator xgboost's best error=10.4606
[flaml.automl: 09-18 04:17:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:18:09] {3072} INFO -  at 14.7s,	estimator xgboost's best error=10.4606,	best estimator xgboost's best error=10.4606
[flaml.automl: 09-18 04:18:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:18:10] {3072} INFO -  at 15.9s,	estimator xgboost's best error=7.4575,	best estimator xgboost's best error=7.4575
[flaml.automl: 09-18 04:18:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:18:12] {3072} INFO -  at 17.5s,	estimator xgboost's best error=7.4143,	best estimator xgboost's best error=7.4143
[flaml.automl: 09-18 04:18:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:18:15] {3072} INFO -  at 20.4s,	estimator xgboost's best error=5.4008,	best estimator xgboost's best error=5.4008
[flaml.automl: 09-18 04:18:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:18:20] {3072} INFO -  at 25.4s,	estimator xgboost's best error=5.4008,	best estimator xgboost's best error=5.4008
[flaml.automl: 09-18 04:18:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:18:23] {3072} INFO -  at 28.4s,	estimator xgboost's best error=5.3062,	best estimator xgboost's best error=5.3062
[flaml.automl: 09-18 04:18:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:18:28] {3072} INFO -  at 33.8s,	estimator xgboost's best error=4.5354,	best estimator xgboost's best error=4.5354
[flaml.automl: 09-18 04:18:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:18:30] {3072} INFO -  at 36.0s,	estimator xgboost's best error=4.5354,	best estimator xgboost's best error=4.5354
[flaml.automl: 09-18 04:18:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:18:54] {3072} INFO -  at 60.1s,	estimator xgboost's best error=3.5905,	best estimator xgboost's best error=3.5905
[flaml.automl: 09-18 04:19:26] {3335} INFO - retrain xgboost for 31.6s
[flaml.automl: 09-18 04:19:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:19:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:19:26] {2637} INFO - Time taken to find the best model: 60.14605951309204
[flaml.automl: 09-18 04:19:26] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
PM2.5(0)最佳损失：-2.5904511441265976
PM2.5(0)最好结果：{'pred_time': 2.8599311072661616e-05, 'wall_clock_time': 60.14605951309204, 'metric_for_logging': {'pred_time': 2.8599311072661616e-05}, 'val_loss': 3.5904511441265976, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 24.187053203582764}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9602756775772225
PM2.5(0)的mse=27.358975885264776
PM2.5(0)的mae=3.493345729865275
PM2.5(0)的mar=0.14604267839575086
总共花费的时间为：92.32
池州市
3237A
3333A
3334A
[flaml.automl: 09-18 04:29:42] {2390} INFO - task = regression
[flaml.automl: 09-18 04:29:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:29:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:29:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:29:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:29:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:29:43] {3025} INFO - Estimated sufficient time budget=11779s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 04:29:43] {3072} INFO -  at 1.3s,	estimator xgboost's best error=17.8984,	best estimator xgboost's best error=17.8984
[flaml.automl: 09-18 04:29:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:29:45] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.3234,	best estimator xgboost's best error=8.3234
[flaml.automl: 09-18 04:29:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:29:46] {3072} INFO -  at 4.6s,	estimator xgboost's best error=8.3234,	best estimator xgboost's best error=8.3234
[flaml.automl: 09-18 04:29:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:29:56] {3072} INFO -  at 14.5s,	estimator xgboost's best error=8.3234,	best estimator xgboost's best error=8.3234
[flaml.automl: 09-18 04:29:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:29:57] {3072} INFO -  at 15.6s,	estimator xgboost's best error=5.9278,	best estimator xgboost's best error=5.9278
[flaml.automl: 09-18 04:29:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:29:59] {3072} INFO -  at 17.2s,	estimator xgboost's best error=5.0104,	best estimator xgboost's best error=5.0104
[flaml.automl: 09-18 04:29:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:30:01] {3072} INFO -  at 18.9s,	estimator xgboost's best error=4.4835,	best estimator xgboost's best error=4.4835
[flaml.automl: 09-18 04:30:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:30:03] {3072} INFO -  at 21.5s,	estimator xgboost's best error=4.4835,	best estimator xgboost's best error=4.4835
[flaml.automl: 09-18 04:30:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:30:05] {3072} INFO -  at 23.2s,	estimator xgboost's best error=4.4835,	best estimator xgboost's best error=4.4835
[flaml.automl: 09-18 04:30:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:30:08] {3072} INFO -  at 26.2s,	estimator xgboost's best error=3.6153,	best estimator xgboost's best error=3.6153
[flaml.automl: 09-18 04:30:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:30:10] {3072} INFO -  at 27.8s,	estimator xgboost's best error=3.6153,	best estimator xgboost's best error=3.6153
[flaml.automl: 09-18 04:30:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:30:11] {3072} INFO -  at 28.9s,	estimator xgboost's best error=3.6153,	best estimator xgboost's best error=3.6153
[flaml.automl: 09-18 04:30:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:30:24] {3072} INFO -  at 42.6s,	estimator xgboost's best error=3.1743,	best estimator xgboost's best error=3.1743
[flaml.automl: 09-18 04:30:24] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:30:42] {3072} INFO -  at 59.8s,	estimator xgboost's best error=2.9094,	best estimator xgboost's best error=2.9094
[flaml.automl: 09-18 04:31:05] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-18 04:31:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:31:05] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:31:05] {2637} INFO - Time taken to find the best model: 59.76606297492981
[flaml.automl: 09-18 04:31:05] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-1.9094267429290568
PM2.5(0)最好结果：{'pred_time': 1.2628002045328123e-05, 'wall_clock_time': 59.76606297492981, 'metric_for_logging': {'pred_time': 1.2628002045328123e-05}, 'val_loss': 2.909426742929057, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 17.206231594085693}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9281376239622376
PM2.5(0)的mse=20.565111004279267
PM2.5(0)的mae=2.9931210342870758
PM2.5(0)的mar=0.13669458108974442
总共花费的时间为：84.17
宣城市
2316A
2317A
2318A
3470A
[flaml.automl: 09-18 04:43:47] {2390} INFO - task = regression
[flaml.automl: 09-18 04:43:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:43:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:43:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:43:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:43:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:43:48] {3025} INFO - Estimated sufficient time budget=50576s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 04:43:48] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.2119,	best estimator xgboost's best error=18.2119
[flaml.automl: 09-18 04:43:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:43:50] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.5515,	best estimator xgboost's best error=8.5515
[flaml.automl: 09-18 04:43:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:43:51] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.5515,	best estimator xgboost's best error=8.5515
[flaml.automl: 09-18 04:43:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:43:57] {3072} INFO -  at 11.0s,	estimator xgboost's best error=8.5515,	best estimator xgboost's best error=8.5515
[flaml.automl: 09-18 04:43:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:43:59] {3072} INFO -  at 12.1s,	estimator xgboost's best error=5.3476,	best estimator xgboost's best error=5.3476
[flaml.automl: 09-18 04:43:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:44:00] {3072} INFO -  at 13.7s,	estimator xgboost's best error=5.1503,	best estimator xgboost's best error=5.1503
[flaml.automl: 09-18 04:44:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:44:02] {3072} INFO -  at 15.3s,	estimator xgboost's best error=4.2146,	best estimator xgboost's best error=4.2146
[flaml.automl: 09-18 04:44:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:44:04] {3072} INFO -  at 18.0s,	estimator xgboost's best error=4.2146,	best estimator xgboost's best error=4.2146
[flaml.automl: 09-18 04:44:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:44:06] {3072} INFO -  at 19.6s,	estimator xgboost's best error=4.2146,	best estimator xgboost's best error=4.2146
[flaml.automl: 09-18 04:44:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:44:09] {3072} INFO -  at 22.6s,	estimator xgboost's best error=3.8933,	best estimator xgboost's best error=3.8933
[flaml.automl: 09-18 04:44:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:44:11] {3072} INFO -  at 24.3s,	estimator xgboost's best error=3.8933,	best estimator xgboost's best error=3.8933
[flaml.automl: 09-18 04:44:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:44:12] {3072} INFO -  at 25.4s,	estimator xgboost's best error=3.8933,	best estimator xgboost's best error=3.8933
[flaml.automl: 09-18 04:44:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:44:16] {3072} INFO -  at 29.1s,	estimator xgboost's best error=3.3988,	best estimator xgboost's best error=3.3988
[flaml.automl: 09-18 04:44:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:44:19] {3072} INFO -  at 32.0s,	estimator xgboost's best error=3.3988,	best estimator xgboost's best error=3.3988
[flaml.automl: 09-18 04:44:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 04:44:21] {3072} INFO -  at 34.6s,	estimator xgboost's best error=3.3988,	best estimator xgboost's best error=3.3988
[flaml.automl: 09-18 04:44:21] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 04:44:23] {3072} INFO -  at 36.3s,	estimator xgboost's best error=3.3988,	best estimator xgboost's best error=3.3988
[flaml.automl: 09-18 04:44:23] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 04:44:25] {3072} INFO -  at 38.6s,	estimator xgboost's best error=3.3104,	best estimator xgboost's best error=3.3104
[flaml.automl: 09-18 04:44:25] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 04:44:27] {3072} INFO -  at 40.9s,	estimator xgboost's best error=3.3104,	best estimator xgboost's best error=3.3104
[flaml.automl: 09-18 04:44:27] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 04:44:29] {3072} INFO -  at 42.2s,	estimator xgboost's best error=3.3104,	best estimator xgboost's best error=3.3104
[flaml.automl: 09-18 04:44:29] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 04:44:31] {3072} INFO -  at 44.2s,	estimator xgboost's best error=3.3104,	best estimator xgboost's best error=3.3104
[flaml.automl: 09-18 04:44:31] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 04:44:32] {3072} INFO -  at 45.4s,	estimator xgboost's best error=3.3104,	best estimator xgboost's best error=3.3104
[flaml.automl: 09-18 04:44:32] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 04:44:40] {3072} INFO -  at 53.3s,	estimator xgboost's best error=3.1430,	best estimator xgboost's best error=3.1430
[flaml.automl: 09-18 04:44:48] {3335} INFO - retrain xgboost for 7.9s
[flaml.automl: 09-18 04:44:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9508280259547836, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8344084316033451,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=0.0066459383199444525, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0031424921315017056, reg_lambda=0.10503395230912585,
             scale_pos_weight=1, subsample=0.8568446847279476,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 04:44:48] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:44:48] {2637} INFO - Time taken to find the best model: 53.26432180404663
[flaml.automl: 09-18 04:44:48] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 9, 'min_child_weight': 0.0066459383199444525, 'learning_rate': 0.8344084316033451, 'subsample': 0.8568446847279476, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9508280259547836, 'reg_alpha': 0.0031424921315017056, 'reg_lambda': 0.10503395230912585, 'FLAML_sample_size': 42572}
PM2.5(0)最佳损失：-2.1430457925423587
PM2.5(0)最好结果：{'pred_time': 8.370603675495567e-06, 'wall_clock_time': 53.26432180404663, 'metric_for_logging': {'pred_time': 8.370603675495567e-06}, 'val_loss': 3.1430457925423587, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 9, 'min_child_weight': 0.0066459383199444525, 'learning_rate': 0.8344084316033451, 'subsample': 0.8568446847279476, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9508280259547836, 'reg_alpha': 0.0031424921315017056, 'reg_lambda': 0.10503395230912585, 'FLAML_sample_size': 42572}, 'config/n_estimators': 15, 'config/max_leaves': 9, 'config/min_child_weight': 0.0066459383199444525, 'config/learning_rate': 0.8344084316033451, 'config/subsample': 0.8568446847279476, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9508280259547836, 'config/reg_alpha': 0.0031424921315017056, 'config/reg_lambda': 0.10503395230912585, 'config/FLAML_sample_size': 42572, 'experiment_tag': 'exp', 'time_total_s': 7.907909631729126}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9508280259547836, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8344084316033451,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=0.0066459383199444525, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0031424921315017056, reg_lambda=0.10503395230912585,
             scale_pos_weight=1, subsample=0.8568446847279476,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9250922456364913
PM2.5(0)的mse=24.293778086094562
PM2.5(0)的mae=3.198620307844705
PM2.5(0)的mar=0.1436129656383206
总共花费的时间为：61.83
莆田市
2319A
2320A
2321A
2322A
2323A
[flaml.automl: 09-18 04:59:55] {2390} INFO - task = regression
[flaml.automl: 09-18 04:59:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:59:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:59:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:59:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:59:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:59:57] {3025} INFO - Estimated sufficient time budget=104202s. Estimated necessary time budget=104s.
[flaml.automl: 09-18 04:59:57] {3072} INFO -  at 2.2s,	estimator xgboost's best error=12.6050,	best estimator xgboost's best error=12.6050
[flaml.automl: 09-18 04:59:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:00:00] {3072} INFO -  at 5.5s,	estimator xgboost's best error=6.0317,	best estimator xgboost's best error=6.0317
[flaml.automl: 09-18 05:00:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:00:02] {3072} INFO -  at 7.4s,	estimator xgboost's best error=6.0317,	best estimator xgboost's best error=6.0317
[flaml.automl: 09-18 05:00:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:00:06] {3072} INFO -  at 11.5s,	estimator xgboost's best error=6.0317,	best estimator xgboost's best error=6.0317
[flaml.automl: 09-18 05:00:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:00:08] {3072} INFO -  at 13.4s,	estimator xgboost's best error=4.5650,	best estimator xgboost's best error=4.5650
[flaml.automl: 09-18 05:00:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:00:10] {3072} INFO -  at 15.6s,	estimator xgboost's best error=4.5650,	best estimator xgboost's best error=4.5650
[flaml.automl: 09-18 05:00:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:00:13] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.2255,	best estimator xgboost's best error=3.2255
[flaml.automl: 09-18 05:00:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:00:17] {3072} INFO -  at 22.2s,	estimator xgboost's best error=3.2255,	best estimator xgboost's best error=3.2255
[flaml.automl: 09-18 05:00:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:00:20] {3072} INFO -  at 24.8s,	estimator xgboost's best error=3.2255,	best estimator xgboost's best error=3.2255
[flaml.automl: 09-18 05:00:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:00:23] {3072} INFO -  at 27.9s,	estimator xgboost's best error=3.2255,	best estimator xgboost's best error=3.2255
[flaml.automl: 09-18 05:00:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:00:25] {3072} INFO -  at 30.4s,	estimator xgboost's best error=3.2255,	best estimator xgboost's best error=3.2255
[flaml.automl: 09-18 05:00:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:00:28] {3072} INFO -  at 33.5s,	estimator xgboost's best error=3.1964,	best estimator xgboost's best error=3.1964
[flaml.automl: 09-18 05:00:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:00:30] {3072} INFO -  at 35.4s,	estimator xgboost's best error=3.1964,	best estimator xgboost's best error=3.1964
[flaml.automl: 09-18 05:00:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:00:42] {3072} INFO -  at 47.0s,	estimator xgboost's best error=2.8874,	best estimator xgboost's best error=2.8874
[flaml.automl: 09-18 05:00:42] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:00:54] {3072} INFO -  at 59.7s,	estimator xgboost's best error=2.8498,	best estimator xgboost's best error=2.8498
[flaml.automl: 09-18 05:01:09] {3335} INFO - retrain xgboost for 14.3s
[flaml.automl: 09-18 05:01:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:01:09] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:01:09] {2637} INFO - Time taken to find the best model: 59.70962977409363
[flaml.automl: 09-18 05:01:09] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52625}
PM2.5(0)最佳损失：-1.8498067305653922
PM2.5(0)最好结果：{'pred_time': 1.2678625482540939e-05, 'wall_clock_time': 59.70962977409363, 'metric_for_logging': {'pred_time': 1.2678625482540939e-05}, 'val_loss': 2.8498067305653922, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52625}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 52625, 'experiment_tag': 'exp', 'time_total_s': 12.698323726654053}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8735281062874922
PM2.5(0)的mse=19.431397039452513
PM2.5(0)的mae=2.7978525923016586
PM2.5(0)的mar=0.18286700371432063
总共花费的时间为：75.16
三明市
2324A
2325A
2326A
2327A
[flaml.automl: 09-18 05:13:06] {2390} INFO - task = regression
[flaml.automl: 09-18 05:13:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:13:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:13:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:13:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:13:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:13:08] {3025} INFO - Estimated sufficient time budget=51203s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 05:13:08] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.9715,	best estimator xgboost's best error=13.9715
[flaml.automl: 09-18 05:13:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:13:10] {3072} INFO -  at 4.2s,	estimator xgboost's best error=6.5752,	best estimator xgboost's best error=6.5752
[flaml.automl: 09-18 05:13:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:13:12] {3072} INFO -  at 6.2s,	estimator xgboost's best error=6.5752,	best estimator xgboost's best error=6.5752
[flaml.automl: 09-18 05:13:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:13:18] {3072} INFO -  at 11.6s,	estimator xgboost's best error=6.5752,	best estimator xgboost's best error=6.5752
[flaml.automl: 09-18 05:13:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:13:20] {3072} INFO -  at 13.4s,	estimator xgboost's best error=4.5249,	best estimator xgboost's best error=4.5249
[flaml.automl: 09-18 05:13:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:13:22] {3072} INFO -  at 15.5s,	estimator xgboost's best error=4.5249,	best estimator xgboost's best error=4.5249
[flaml.automl: 09-18 05:13:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:13:24] {3072} INFO -  at 18.2s,	estimator xgboost's best error=3.3353,	best estimator xgboost's best error=3.3353
[flaml.automl: 09-18 05:13:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:13:28] {3072} INFO -  at 22.2s,	estimator xgboost's best error=3.3353,	best estimator xgboost's best error=3.3353
[flaml.automl: 09-18 05:13:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:13:32] {3072} INFO -  at 25.7s,	estimator xgboost's best error=3.3353,	best estimator xgboost's best error=3.3353
[flaml.automl: 09-18 05:13:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:13:35] {3072} INFO -  at 28.9s,	estimator xgboost's best error=3.3353,	best estimator xgboost's best error=3.3353
[flaml.automl: 09-18 05:13:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:13:38] {3072} INFO -  at 32.2s,	estimator xgboost's best error=3.2736,	best estimator xgboost's best error=3.2736
[flaml.automl: 09-18 05:13:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:13:41] {3072} INFO -  at 34.9s,	estimator xgboost's best error=3.2736,	best estimator xgboost's best error=3.2736
[flaml.automl: 09-18 05:13:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:13:44] {3072} INFO -  at 37.6s,	estimator xgboost's best error=3.2579,	best estimator xgboost's best error=3.2579
[flaml.automl: 09-18 05:13:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:13:46] {3072} INFO -  at 39.8s,	estimator xgboost's best error=3.2579,	best estimator xgboost's best error=3.2579
[flaml.automl: 09-18 05:13:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:13:48] {3072} INFO -  at 41.8s,	estimator xgboost's best error=3.2579,	best estimator xgboost's best error=3.2579
[flaml.automl: 09-18 05:13:48] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 05:13:51] {3072} INFO -  at 44.4s,	estimator xgboost's best error=3.2579,	best estimator xgboost's best error=3.2579
[flaml.automl: 09-18 05:13:51] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 05:13:52] {3072} INFO -  at 45.6s,	estimator xgboost's best error=3.2579,	best estimator xgboost's best error=3.2579
[flaml.automl: 09-18 05:13:52] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 05:14:06] {3072} INFO -  at 59.4s,	estimator xgboost's best error=2.9906,	best estimator xgboost's best error=2.9906
[flaml.automl: 09-18 05:14:18] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 05:14:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:14:18] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:14:18] {2637} INFO - Time taken to find the best model: 59.38081669807434
[flaml.automl: 09-18 05:14:18] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 42852}
PM2.5(0)最佳损失：-1.990623592391629
PM2.5(0)最好结果：{'pred_time': 2.4398501707395053e-05, 'wall_clock_time': 59.38081669807434, 'metric_for_logging': {'pred_time': 2.4398501707395053e-05}, 'val_loss': 2.990623592391629, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 42852}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'config/FLAML_sample_size': 42852, 'experiment_tag': 'exp', 'time_total_s': 13.78028917312622}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8423713940779455
PM2.5(0)的mse=23.126416142253845
PM2.5(0)的mae=3.136952110193916
PM2.5(0)的mar=0.17777151600580335
总共花费的时间为：72.04
南平市
2331A
2332A
2333A
2334A
[flaml.automl: 09-18 05:26:34] {2390} INFO - task = regression
[flaml.automl: 09-18 05:26:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:26:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:26:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:26:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:26:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:26:35] {3025} INFO - Estimated sufficient time budget=50101s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 05:26:35] {3072} INFO -  at 1.3s,	estimator xgboost's best error=10.7908,	best estimator xgboost's best error=10.7908
[flaml.automl: 09-18 05:26:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:26:37] {3072} INFO -  at 3.4s,	estimator xgboost's best error=5.1476,	best estimator xgboost's best error=5.1476
[flaml.automl: 09-18 05:26:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:26:38] {3072} INFO -  at 4.6s,	estimator xgboost's best error=5.1476,	best estimator xgboost's best error=5.1476
[flaml.automl: 09-18 05:26:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:26:44] {3072} INFO -  at 11.0s,	estimator xgboost's best error=5.1476,	best estimator xgboost's best error=5.1476
[flaml.automl: 09-18 05:26:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:26:46] {3072} INFO -  at 12.1s,	estimator xgboost's best error=3.6603,	best estimator xgboost's best error=3.6603
[flaml.automl: 09-18 05:26:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:26:47] {3072} INFO -  at 13.7s,	estimator xgboost's best error=3.6603,	best estimator xgboost's best error=3.6603
[flaml.automl: 09-18 05:26:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:26:49] {3072} INFO -  at 15.4s,	estimator xgboost's best error=2.3630,	best estimator xgboost's best error=2.3630
[flaml.automl: 09-18 05:26:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:26:52] {3072} INFO -  at 18.1s,	estimator xgboost's best error=2.3630,	best estimator xgboost's best error=2.3630
[flaml.automl: 09-18 05:26:52] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:26:53] {3072} INFO -  at 19.7s,	estimator xgboost's best error=2.3630,	best estimator xgboost's best error=2.3630
[flaml.automl: 09-18 05:26:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:26:56] {3072} INFO -  at 22.7s,	estimator xgboost's best error=2.3630,	best estimator xgboost's best error=2.3630
[flaml.automl: 09-18 05:26:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:26:58] {3072} INFO -  at 24.2s,	estimator xgboost's best error=2.3630,	best estimator xgboost's best error=2.3630
[flaml.automl: 09-18 05:26:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:26:59] {3072} INFO -  at 25.9s,	estimator xgboost's best error=2.3630,	best estimator xgboost's best error=2.3630
[flaml.automl: 09-18 05:26:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:27:01] {3072} INFO -  at 27.0s,	estimator xgboost's best error=2.3630,	best estimator xgboost's best error=2.3630
[flaml.automl: 09-18 05:27:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:27:08] {3072} INFO -  at 34.0s,	estimator xgboost's best error=2.0387,	best estimator xgboost's best error=2.0387
[flaml.automl: 09-18 05:27:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:27:20] {3072} INFO -  at 46.8s,	estimator xgboost's best error=1.9745,	best estimator xgboost's best error=1.9745
[flaml.automl: 09-18 05:27:20] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 05:27:27] {3072} INFO -  at 53.8s,	estimator xgboost's best error=1.9745,	best estimator xgboost's best error=1.9745
[flaml.automl: 09-18 05:27:40] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 05:27:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:27:40] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:27:40] {2637} INFO - Time taken to find the best model: 46.81885623931885
[flaml.automl: 09-18 05:27:40] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 42513}
PM2.5(0)最佳损失：-0.9745152062853952
PM2.5(0)最好结果：{'pred_time': 8.568432652072116e-06, 'wall_clock_time': 46.81885623931885, 'metric_for_logging': {'pred_time': 8.568432652072116e-06}, 'val_loss': 1.9745152062853952, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 42513}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 42513, 'experiment_tag': 'exp', 'time_total_s': 12.768616437911987}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9110312202682339
PM2.5(0)的mse=10.058845222012755
PM2.5(0)的mae=1.9565897873411362
PM2.5(0)的mar=0.16489975537290968
总共花费的时间为：67.28
龙岩市
2335A
2336A
2337A
2338A
[flaml.automl: 09-18 05:39:41] {2390} INFO - task = regression
[flaml.automl: 09-18 05:39:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:39:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:39:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:39:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:39:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:39:45] {3025} INFO - Estimated sufficient time budget=143352s. Estimated necessary time budget=143s.
[flaml.automl: 09-18 05:39:45] {3072} INFO -  at 3.5s,	estimator xgboost's best error=12.1584,	best estimator xgboost's best error=12.1584
[flaml.automl: 09-18 05:39:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:39:50] {3072} INFO -  at 9.3s,	estimator xgboost's best error=5.8131,	best estimator xgboost's best error=5.8131
[flaml.automl: 09-18 05:39:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:39:54] {3072} INFO -  at 12.7s,	estimator xgboost's best error=5.8131,	best estimator xgboost's best error=5.8131
[flaml.automl: 09-18 05:39:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:39:59] {3072} INFO -  at 17.5s,	estimator xgboost's best error=5.8131,	best estimator xgboost's best error=5.8131
[flaml.automl: 09-18 05:39:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:40:02] {3072} INFO -  at 20.7s,	estimator xgboost's best error=4.1357,	best estimator xgboost's best error=4.1357
[flaml.automl: 09-18 05:40:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:40:05] {3072} INFO -  at 24.3s,	estimator xgboost's best error=4.1357,	best estimator xgboost's best error=4.1357
[flaml.automl: 09-18 05:40:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:40:08] {3072} INFO -  at 27.4s,	estimator xgboost's best error=2.8809,	best estimator xgboost's best error=2.8809
[flaml.automl: 09-18 05:40:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:40:12] {3072} INFO -  at 30.8s,	estimator xgboost's best error=2.8809,	best estimator xgboost's best error=2.8809
[flaml.automl: 09-18 05:40:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:40:15] {3072} INFO -  at 33.8s,	estimator xgboost's best error=2.8809,	best estimator xgboost's best error=2.8809
[flaml.automl: 09-18 05:40:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:40:17] {3072} INFO -  at 36.5s,	estimator xgboost's best error=2.8809,	best estimator xgboost's best error=2.8809
[flaml.automl: 09-18 05:40:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:40:20] {3072} INFO -  at 38.7s,	estimator xgboost's best error=2.8809,	best estimator xgboost's best error=2.8809
[flaml.automl: 09-18 05:40:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:40:23] {3072} INFO -  at 41.5s,	estimator xgboost's best error=2.8348,	best estimator xgboost's best error=2.8348
[flaml.automl: 09-18 05:40:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:40:24] {3072} INFO -  at 42.7s,	estimator xgboost's best error=2.8348,	best estimator xgboost's best error=2.8348
[flaml.automl: 09-18 05:40:24] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:40:31] {3072} INFO -  at 49.7s,	estimator xgboost's best error=2.3950,	best estimator xgboost's best error=2.3950
[flaml.automl: 09-18 05:40:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:40:41] {3072} INFO -  at 59.6s,	estimator xgboost's best error=2.3566,	best estimator xgboost's best error=2.3566
[flaml.automl: 09-18 05:40:53] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 05:40:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:40:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:40:53] {2637} INFO - Time taken to find the best model: 59.63065028190613
[flaml.automl: 09-18 05:40:53] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43665}
PM2.5(0)最佳损失：-1.3566037332431797
PM2.5(0)最好结果：{'pred_time': 8.404254913330078e-06, 'wall_clock_time': 59.63065028190613, 'metric_for_logging': {'pred_time': 8.404254913330078e-06}, 'val_loss': 2.3566037332431797, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43665}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 43665, 'experiment_tag': 'exp', 'time_total_s': 9.908264875411987}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9026683964159081
PM2.5(0)的mse=12.973884576815689
PM2.5(0)的mae=2.27734938983107
PM2.5(0)的mar=0.16261271240758057
总共花费的时间为：73.27
宁德市
2339A
3209A
[flaml.automl: 09-18 05:46:52] {2390} INFO - task = regression
[flaml.automl: 09-18 05:46:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:46:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:46:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:46:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:46:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:46:53] {3025} INFO - Estimated sufficient time budget=11818s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 05:46:53] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.6809,	best estimator xgboost's best error=11.6809
[flaml.automl: 09-18 05:46:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:46:55] {3072} INFO -  at 3.1s,	estimator xgboost's best error=6.2356,	best estimator xgboost's best error=6.2356
[flaml.automl: 09-18 05:46:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:46:56] {3072} INFO -  at 4.3s,	estimator xgboost's best error=6.2356,	best estimator xgboost's best error=6.2356
[flaml.automl: 09-18 05:46:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:47:04] {3072} INFO -  at 12.6s,	estimator xgboost's best error=6.2356,	best estimator xgboost's best error=6.2356
[flaml.automl: 09-18 05:47:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:47:05] {3072} INFO -  at 13.7s,	estimator xgboost's best error=4.0811,	best estimator xgboost's best error=4.0811
[flaml.automl: 09-18 05:47:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:47:07] {3072} INFO -  at 15.3s,	estimator xgboost's best error=3.3670,	best estimator xgboost's best error=3.3670
[flaml.automl: 09-18 05:47:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:47:09] {3072} INFO -  at 16.9s,	estimator xgboost's best error=3.1036,	best estimator xgboost's best error=3.1036
[flaml.automl: 09-18 05:47:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:47:11] {3072} INFO -  at 19.2s,	estimator xgboost's best error=3.1036,	best estimator xgboost's best error=3.1036
[flaml.automl: 09-18 05:47:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:47:12] {3072} INFO -  at 20.8s,	estimator xgboost's best error=3.1036,	best estimator xgboost's best error=3.1036
[flaml.automl: 09-18 05:47:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:47:15] {3072} INFO -  at 23.8s,	estimator xgboost's best error=2.6714,	best estimator xgboost's best error=2.6714
[flaml.automl: 09-18 05:47:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:47:17] {3072} INFO -  at 25.5s,	estimator xgboost's best error=2.6714,	best estimator xgboost's best error=2.6714
[flaml.automl: 09-18 05:47:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:47:18] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.6714,	best estimator xgboost's best error=2.6714
[flaml.automl: 09-18 05:47:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:47:30] {3072} INFO -  at 38.6s,	estimator xgboost's best error=2.5511,	best estimator xgboost's best error=2.5511
[flaml.automl: 09-18 05:47:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:47:51] {3072} INFO -  at 59.8s,	estimator xgboost's best error=2.3702,	best estimator xgboost's best error=2.3702
[flaml.automl: 09-18 05:48:27] {3335} INFO - retrain xgboost for 35.4s
[flaml.automl: 09-18 05:48:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:48:27] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:48:27] {2637} INFO - Time taken to find the best model: 59.827348947525024
[flaml.automl: 09-18 05:48:27] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-1.3702370950988696
PM2.5(0)最好结果：{'pred_time': 1.747871677718273e-05, 'wall_clock_time': 59.827348947525024, 'metric_for_logging': {'pred_time': 1.747871677718273e-05}, 'val_loss': 2.3702370950988696, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 21.259587049484253}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8811128547071415
PM2.5(0)的mse=15.838362478708882
PM2.5(0)的mae=2.412168603894034
PM2.5(0)的mar=0.1676741278731708
总共花费的时间为：95.67
景德镇市
2342A
2343A
2344A
2345A
2346A
[flaml.automl: 09-18 06:03:27] {2390} INFO - task = regression
[flaml.automl: 09-18 06:03:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:03:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:03:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:03:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:03:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:03:29] {3025} INFO - Estimated sufficient time budget=117804s. Estimated necessary time budget=118s.
[flaml.automl: 09-18 06:03:29] {3072} INFO -  at 2.6s,	estimator xgboost's best error=14.2299,	best estimator xgboost's best error=14.2299
[flaml.automl: 09-18 06:03:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:03:33] {3072} INFO -  at 6.5s,	estimator xgboost's best error=7.2197,	best estimator xgboost's best error=7.2197
[flaml.automl: 09-18 06:03:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:03:35] {3072} INFO -  at 8.7s,	estimator xgboost's best error=7.2197,	best estimator xgboost's best error=7.2197
[flaml.automl: 09-18 06:03:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:03:39] {3072} INFO -  at 12.9s,	estimator xgboost's best error=7.2197,	best estimator xgboost's best error=7.2197
[flaml.automl: 09-18 06:03:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:03:41] {3072} INFO -  at 15.1s,	estimator xgboost's best error=5.4232,	best estimator xgboost's best error=5.4232
[flaml.automl: 09-18 06:03:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:03:44] {3072} INFO -  at 17.5s,	estimator xgboost's best error=5.4232,	best estimator xgboost's best error=5.4232
[flaml.automl: 09-18 06:03:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:03:46] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.7138,	best estimator xgboost's best error=4.7138
[flaml.automl: 09-18 06:03:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:03:48] {3072} INFO -  at 21.8s,	estimator xgboost's best error=4.7138,	best estimator xgboost's best error=4.7138
[flaml.automl: 09-18 06:03:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:03:50] {3072} INFO -  at 23.4s,	estimator xgboost's best error=4.7138,	best estimator xgboost's best error=4.7138
[flaml.automl: 09-18 06:03:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:03:53] {3072} INFO -  at 26.3s,	estimator xgboost's best error=4.7138,	best estimator xgboost's best error=4.7138
[flaml.automl: 09-18 06:03:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:03:54] {3072} INFO -  at 27.7s,	estimator xgboost's best error=4.6642,	best estimator xgboost's best error=4.6642
[flaml.automl: 09-18 06:03:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:03:55] {3072} INFO -  at 28.9s,	estimator xgboost's best error=4.6642,	best estimator xgboost's best error=4.6642
[flaml.automl: 09-18 06:03:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:03:58] {3072} INFO -  at 31.6s,	estimator xgboost's best error=4.5596,	best estimator xgboost's best error=4.5596
[flaml.automl: 09-18 06:03:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:04:01] {3072} INFO -  at 34.2s,	estimator xgboost's best error=4.5026,	best estimator xgboost's best error=4.5026
[flaml.automl: 09-18 06:04:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:04:03] {3072} INFO -  at 36.5s,	estimator xgboost's best error=4.5026,	best estimator xgboost's best error=4.5026
[flaml.automl: 09-18 06:04:03] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 06:04:04] {3072} INFO -  at 38.0s,	estimator xgboost's best error=4.5026,	best estimator xgboost's best error=4.5026
[flaml.automl: 09-18 06:04:04] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 06:04:06] {3072} INFO -  at 39.8s,	estimator xgboost's best error=4.5026,	best estimator xgboost's best error=4.5026
[flaml.automl: 09-18 06:04:06] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 06:04:08] {3072} INFO -  at 41.6s,	estimator xgboost's best error=4.5026,	best estimator xgboost's best error=4.5026
[flaml.automl: 09-18 06:04:08] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 06:04:19] {3072} INFO -  at 52.3s,	estimator xgboost's best error=4.4161,	best estimator xgboost's best error=4.4161
[flaml.automl: 09-18 06:04:29] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-18 06:04:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:04:29] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:04:29] {2637} INFO - Time taken to find the best model: 52.33914399147034
[flaml.automl: 09-18 06:04:29] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52970}
PM2.5(0)最佳损失：-3.4160757696713633
PM2.5(0)最好结果：{'pred_time': 6.7954155769309256e-06, 'wall_clock_time': 52.33914399147034, 'metric_for_logging': {'pred_time': 6.7954155769309256e-06}, 'val_loss': 4.416075769671363, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52970}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 52970, 'experiment_tag': 'exp', 'time_total_s': 10.720422983169556}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.7439438706838358
PM2.5(0)的mse=32.21226972125608
PM2.5(0)的mae=4.3763657433541905
PM2.5(0)的mar=0.32090334135355775
总共花费的时间为：63.97
萍乡市
2347A
2348A
2349A
2350A
2351A
[flaml.automl: 09-18 06:20:59] {2390} INFO - task = regression
[flaml.automl: 09-18 06:20:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:20:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:20:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:20:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:20:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:21:01] {3025} INFO - Estimated sufficient time budget=95337s. Estimated necessary time budget=95s.
[flaml.automl: 09-18 06:21:01] {3072} INFO -  at 2.1s,	estimator xgboost's best error=18.5663,	best estimator xgboost's best error=18.5663
[flaml.automl: 09-18 06:21:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:21:04] {3072} INFO -  at 5.4s,	estimator xgboost's best error=9.3294,	best estimator xgboost's best error=9.3294
[flaml.automl: 09-18 06:21:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:21:06] {3072} INFO -  at 7.6s,	estimator xgboost's best error=9.3294,	best estimator xgboost's best error=9.3294
[flaml.automl: 09-18 06:21:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:21:11] {3072} INFO -  at 12.3s,	estimator xgboost's best error=9.3294,	best estimator xgboost's best error=9.3294
[flaml.automl: 09-18 06:21:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:21:13] {3072} INFO -  at 14.3s,	estimator xgboost's best error=6.9146,	best estimator xgboost's best error=6.9146
[flaml.automl: 09-18 06:21:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:21:16] {3072} INFO -  at 17.0s,	estimator xgboost's best error=6.9146,	best estimator xgboost's best error=6.9146
[flaml.automl: 09-18 06:21:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:21:19] {3072} INFO -  at 19.8s,	estimator xgboost's best error=5.3188,	best estimator xgboost's best error=5.3188
[flaml.automl: 09-18 06:21:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:21:22] {3072} INFO -  at 23.0s,	estimator xgboost's best error=5.3188,	best estimator xgboost's best error=5.3188
[flaml.automl: 09-18 06:21:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:21:25] {3072} INFO -  at 25.7s,	estimator xgboost's best error=5.3188,	best estimator xgboost's best error=5.3188
[flaml.automl: 09-18 06:21:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:21:28] {3072} INFO -  at 28.7s,	estimator xgboost's best error=5.3188,	best estimator xgboost's best error=5.3188
[flaml.automl: 09-18 06:21:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:21:30] {3072} INFO -  at 30.9s,	estimator xgboost's best error=5.2679,	best estimator xgboost's best error=5.2679
[flaml.automl: 09-18 06:21:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:21:32] {3072} INFO -  at 32.9s,	estimator xgboost's best error=5.2679,	best estimator xgboost's best error=5.2679
[flaml.automl: 09-18 06:21:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:21:34] {3072} INFO -  at 34.9s,	estimator xgboost's best error=5.0738,	best estimator xgboost's best error=5.0738
[flaml.automl: 09-18 06:21:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:21:36] {3072} INFO -  at 36.7s,	estimator xgboost's best error=5.0738,	best estimator xgboost's best error=5.0738
[flaml.automl: 09-18 06:21:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:21:37] {3072} INFO -  at 38.2s,	estimator xgboost's best error=5.0738,	best estimator xgboost's best error=5.0738
[flaml.automl: 09-18 06:21:37] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 06:21:39] {3072} INFO -  at 39.9s,	estimator xgboost's best error=5.0738,	best estimator xgboost's best error=5.0738
[flaml.automl: 09-18 06:21:39] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 06:21:40] {3072} INFO -  at 41.2s,	estimator xgboost's best error=5.0738,	best estimator xgboost's best error=5.0738
[flaml.automl: 09-18 06:21:40] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 06:21:49] {3072} INFO -  at 50.5s,	estimator xgboost's best error=4.7518,	best estimator xgboost's best error=4.7518
[flaml.automl: 09-18 06:21:49] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 06:21:53] {3072} INFO -  at 54.4s,	estimator xgboost's best error=4.7518,	best estimator xgboost's best error=4.7518
[flaml.automl: 09-18 06:22:03] {3335} INFO - retrain xgboost for 9.4s
[flaml.automl: 09-18 06:22:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:22:03] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:22:03] {2637} INFO - Time taken to find the best model: 50.523041009902954
[flaml.automl: 09-18 06:22:03] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 52338}
PM2.5(0)最佳损失：-3.7517899094864298
PM2.5(0)最好结果：{'pred_time': 1.1346117174445026e-05, 'wall_clock_time': 50.523041009902954, 'metric_for_logging': {'pred_time': 1.1346117174445026e-05}, 'val_loss': 4.75178990948643, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 52338}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'config/FLAML_sample_size': 52338, 'experiment_tag': 'exp', 'time_total_s': 9.346291542053223}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.898532630442311
PM2.5(0)的mse=41.12408487318793
PM2.5(0)的mae=4.641105408345314
PM2.5(0)的mar=0.3192655265410738
总共花费的时间为：64.80
新余市
2352A
2353A
2354A
2355A
[flaml.automl: 09-18 06:34:12] {2390} INFO - task = regression
[flaml.automl: 09-18 06:34:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:34:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:34:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:34:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:34:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:34:13] {3025} INFO - Estimated sufficient time budget=48631s. Estimated necessary time budget=49s.
[flaml.automl: 09-18 06:34:13] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.8270,	best estimator xgboost's best error=18.8270
[flaml.automl: 09-18 06:34:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:34:15] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.7386,	best estimator xgboost's best error=8.7386
[flaml.automl: 09-18 06:34:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:34:16] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.7386,	best estimator xgboost's best error=8.7386
[flaml.automl: 09-18 06:34:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:34:23] {3072} INFO -  at 11.4s,	estimator xgboost's best error=8.7386,	best estimator xgboost's best error=8.7386
[flaml.automl: 09-18 06:34:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:34:24] {3072} INFO -  at 12.5s,	estimator xgboost's best error=5.4855,	best estimator xgboost's best error=5.4855
[flaml.automl: 09-18 06:34:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:34:26] {3072} INFO -  at 14.1s,	estimator xgboost's best error=4.6745,	best estimator xgboost's best error=4.6745
[flaml.automl: 09-18 06:34:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:34:28] {3072} INFO -  at 15.7s,	estimator xgboost's best error=4.1095,	best estimator xgboost's best error=4.1095
[flaml.automl: 09-18 06:34:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:34:30] {3072} INFO -  at 18.4s,	estimator xgboost's best error=4.1095,	best estimator xgboost's best error=4.1095
[flaml.automl: 09-18 06:34:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:34:32] {3072} INFO -  at 20.0s,	estimator xgboost's best error=4.1095,	best estimator xgboost's best error=4.1095
[flaml.automl: 09-18 06:34:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:34:35] {3072} INFO -  at 23.0s,	estimator xgboost's best error=3.7571,	best estimator xgboost's best error=3.7571
[flaml.automl: 09-18 06:34:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:34:36] {3072} INFO -  at 24.6s,	estimator xgboost's best error=3.7571,	best estimator xgboost's best error=3.7571
[flaml.automl: 09-18 06:34:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:34:38] {3072} INFO -  at 25.7s,	estimator xgboost's best error=3.7571,	best estimator xgboost's best error=3.7571
[flaml.automl: 09-18 06:34:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:34:41] {3072} INFO -  at 29.5s,	estimator xgboost's best error=3.6016,	best estimator xgboost's best error=3.6016
[flaml.automl: 09-18 06:34:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:34:45] {3072} INFO -  at 33.1s,	estimator xgboost's best error=3.5488,	best estimator xgboost's best error=3.5488
[flaml.automl: 09-18 06:34:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:34:48] {3072} INFO -  at 35.8s,	estimator xgboost's best error=3.5488,	best estimator xgboost's best error=3.5488
[flaml.automl: 09-18 06:34:48] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 06:34:50] {3072} INFO -  at 38.3s,	estimator xgboost's best error=3.5488,	best estimator xgboost's best error=3.5488
[flaml.automl: 09-18 06:34:50] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 06:34:53] {3072} INFO -  at 40.9s,	estimator xgboost's best error=3.4222,	best estimator xgboost's best error=3.4222
[flaml.automl: 09-18 06:34:53] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 06:34:55] {3072} INFO -  at 43.0s,	estimator xgboost's best error=3.4222,	best estimator xgboost's best error=3.4222
[flaml.automl: 09-18 06:34:55] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 06:34:57] {3072} INFO -  at 44.9s,	estimator xgboost's best error=3.4222,	best estimator xgboost's best error=3.4222
[flaml.automl: 09-18 06:34:57] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 06:34:58] {3072} INFO -  at 46.6s,	estimator xgboost's best error=3.4222,	best estimator xgboost's best error=3.4222
[flaml.automl: 09-18 06:34:58] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 06:34:59] {3072} INFO -  at 47.5s,	estimator xgboost's best error=3.4222,	best estimator xgboost's best error=3.4222
[flaml.automl: 09-18 06:34:59] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 06:35:12] {3072} INFO -  at 60.2s,	estimator xgboost's best error=3.2773,	best estimator xgboost's best error=3.2773
[flaml.automl: 09-18 06:35:35] {3335} INFO - retrain xgboost for 22.6s
[flaml.automl: 09-18 06:35:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4512592128754277,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.003827601076117227, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010065608612606477, reg_lambda=0.020722552365849516,
             scale_pos_weight=1, subsample=0.749001334635897,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 06:35:35] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:35:35] {2637} INFO - Time taken to find the best model: 60.18921613693237
[flaml.automl: 09-18 06:35:35] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.003827601076117227, 'learning_rate': 0.4512592128754277, 'subsample': 0.749001334635897, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0010065608612606477, 'reg_lambda': 0.020722552365849516, 'FLAML_sample_size': 40553}
PM2.5(0)最佳损失：-2.2773210715570293
PM2.5(0)最好结果：{'pred_time': 1.8214417519804322e-05, 'wall_clock_time': 60.18921613693237, 'metric_for_logging': {'pred_time': 1.8214417519804322e-05}, 'val_loss': 3.2773210715570293, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.003827601076117227, 'learning_rate': 0.4512592128754277, 'subsample': 0.749001334635897, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0010065608612606477, 'reg_lambda': 0.020722552365849516, 'FLAML_sample_size': 40553}, 'config/n_estimators': 35, 'config/max_leaves': 7, 'config/min_child_weight': 0.003827601076117227, 'config/learning_rate': 0.4512592128754277, 'config/subsample': 0.749001334635897, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0010065608612606477, 'config/reg_lambda': 0.020722552365849516, 'config/FLAML_sample_size': 40553, 'experiment_tag': 'exp', 'time_total_s': 12.653905391693115}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4512592128754277,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.003827601076117227, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010065608612606477, reg_lambda=0.020722552365849516,
             scale_pos_weight=1, subsample=0.749001334635897,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9043533008771087
PM2.5(0)的mse=20.938823605453376
PM2.5(0)的mae=3.2814478957060214
PM2.5(0)的mar=0.15771088682852286
总共花费的时间为：83.53
鹰潭市
2357A
2358A
2359A
2360A
2361A
[flaml.automl: 09-18 06:50:56] {2390} INFO - task = regression
[flaml.automl: 09-18 06:50:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:50:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:50:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:50:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:50:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:50:58] {3025} INFO - Estimated sufficient time budget=96780s. Estimated necessary time budget=97s.
[flaml.automl: 09-18 06:50:58] {3072} INFO -  at 2.1s,	estimator xgboost's best error=14.5210,	best estimator xgboost's best error=14.5210
[flaml.automl: 09-18 06:50:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:51:01] {3072} INFO -  at 5.7s,	estimator xgboost's best error=7.3162,	best estimator xgboost's best error=7.3162
[flaml.automl: 09-18 06:51:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:51:03] {3072} INFO -  at 7.9s,	estimator xgboost's best error=7.3162,	best estimator xgboost's best error=7.3162
[flaml.automl: 09-18 06:51:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:51:07] {3072} INFO -  at 11.8s,	estimator xgboost's best error=7.3162,	best estimator xgboost's best error=7.3162
[flaml.automl: 09-18 06:51:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:51:09] {3072} INFO -  at 13.8s,	estimator xgboost's best error=5.9093,	best estimator xgboost's best error=5.9093
[flaml.automl: 09-18 06:51:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:51:12] {3072} INFO -  at 16.4s,	estimator xgboost's best error=5.9093,	best estimator xgboost's best error=5.9093
[flaml.automl: 09-18 06:51:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:51:15] {3072} INFO -  at 19.4s,	estimator xgboost's best error=4.4577,	best estimator xgboost's best error=4.4577
[flaml.automl: 09-18 06:51:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:51:18] {3072} INFO -  at 22.5s,	estimator xgboost's best error=4.4577,	best estimator xgboost's best error=4.4577
[flaml.automl: 09-18 06:51:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:51:21] {3072} INFO -  at 25.2s,	estimator xgboost's best error=4.4577,	best estimator xgboost's best error=4.4577
[flaml.automl: 09-18 06:51:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:51:24] {3072} INFO -  at 28.1s,	estimator xgboost's best error=4.4577,	best estimator xgboost's best error=4.4577
[flaml.automl: 09-18 06:51:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:51:26] {3072} INFO -  at 30.8s,	estimator xgboost's best error=4.4577,	best estimator xgboost's best error=4.4577
[flaml.automl: 09-18 06:51:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:51:29] {3072} INFO -  at 33.8s,	estimator xgboost's best error=4.4497,	best estimator xgboost's best error=4.4497
[flaml.automl: 09-18 06:51:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:51:31] {3072} INFO -  at 35.9s,	estimator xgboost's best error=4.4497,	best estimator xgboost's best error=4.4497
[flaml.automl: 09-18 06:51:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:51:43] {3072} INFO -  at 47.3s,	estimator xgboost's best error=4.1095,	best estimator xgboost's best error=4.1095
[flaml.automl: 09-18 06:51:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:51:55] {3072} INFO -  at 59.2s,	estimator xgboost's best error=4.0720,	best estimator xgboost's best error=4.0720
[flaml.automl: 09-18 06:52:08] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 06:52:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:52:08] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:52:08] {2637} INFO - Time taken to find the best model: 59.15543842315674
[flaml.automl: 09-18 06:52:08] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 51703}
PM2.5(0)最佳损失：-3.072004269848086
PM2.5(0)最好结果：{'pred_time': 8.1212132572609e-06, 'wall_clock_time': 59.15543842315674, 'metric_for_logging': {'pred_time': 8.1212132572609e-06}, 'val_loss': 4.072004269848086, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 51703}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 51703, 'experiment_tag': 'exp', 'time_total_s': 11.882084131240845}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8755443470084172
PM2.5(0)的mse=32.29561253774329
PM2.5(0)的mae=3.984359153908185
PM2.5(0)的mar=0.30726141922034234
总共花费的时间为：72.99
赣州市
2362A
2363A
2364A
2365A
2366A
3109A
[flaml.automl: 09-18 07:10:14] {2390} INFO - task = regression
[flaml.automl: 09-18 07:10:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:10:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:10:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:10:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:10:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:10:15] {3025} INFO - Estimated sufficient time budget=76345s. Estimated necessary time budget=76s.
[flaml.automl: 09-18 07:10:15] {3072} INFO -  at 1.5s,	estimator xgboost's best error=13.1044,	best estimator xgboost's best error=13.1044
[flaml.automl: 09-18 07:10:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:10:17] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.5139,	best estimator xgboost's best error=6.5139
[flaml.automl: 09-18 07:10:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:10:18] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.5139,	best estimator xgboost's best error=6.5139
[flaml.automl: 09-18 07:10:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:10:22] {3072} INFO -  at 8.5s,	estimator xgboost's best error=6.5139,	best estimator xgboost's best error=6.5139
[flaml.automl: 09-18 07:10:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:10:23] {3072} INFO -  at 9.7s,	estimator xgboost's best error=4.7382,	best estimator xgboost's best error=4.7382
[flaml.automl: 09-18 07:10:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:10:25] {3072} INFO -  at 11.2s,	estimator xgboost's best error=4.1565,	best estimator xgboost's best error=4.1565
[flaml.automl: 09-18 07:10:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:10:26] {3072} INFO -  at 12.9s,	estimator xgboost's best error=3.9021,	best estimator xgboost's best error=3.9021
[flaml.automl: 09-18 07:10:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:10:29] {3072} INFO -  at 15.6s,	estimator xgboost's best error=3.9021,	best estimator xgboost's best error=3.9021
[flaml.automl: 09-18 07:10:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:10:31] {3072} INFO -  at 17.2s,	estimator xgboost's best error=3.9021,	best estimator xgboost's best error=3.9021
[flaml.automl: 09-18 07:10:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:10:34] {3072} INFO -  at 20.2s,	estimator xgboost's best error=3.7327,	best estimator xgboost's best error=3.7327
[flaml.automl: 09-18 07:10:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:10:35] {3072} INFO -  at 21.9s,	estimator xgboost's best error=3.7327,	best estimator xgboost's best error=3.7327
[flaml.automl: 09-18 07:10:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:10:37] {3072} INFO -  at 23.0s,	estimator xgboost's best error=3.7327,	best estimator xgboost's best error=3.7327
[flaml.automl: 09-18 07:10:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:10:39] {3072} INFO -  at 25.0s,	estimator xgboost's best error=3.6590,	best estimator xgboost's best error=3.6590
[flaml.automl: 09-18 07:10:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:10:41] {3072} INFO -  at 27.2s,	estimator xgboost's best error=3.6590,	best estimator xgboost's best error=3.6590
[flaml.automl: 09-18 07:10:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:10:42] {3072} INFO -  at 28.6s,	estimator xgboost's best error=3.6590,	best estimator xgboost's best error=3.6590
[flaml.automl: 09-18 07:10:42] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 07:10:44] {3072} INFO -  at 30.4s,	estimator xgboost's best error=3.6590,	best estimator xgboost's best error=3.6590
[flaml.automl: 09-18 07:10:44] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 07:10:46] {3072} INFO -  at 32.6s,	estimator xgboost's best error=3.5844,	best estimator xgboost's best error=3.5844
[flaml.automl: 09-18 07:10:46] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 07:10:48] {3072} INFO -  at 34.5s,	estimator xgboost's best error=3.5844,	best estimator xgboost's best error=3.5844
[flaml.automl: 09-18 07:10:48] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 07:10:49] {3072} INFO -  at 35.8s,	estimator xgboost's best error=3.5729,	best estimator xgboost's best error=3.5729
[flaml.automl: 09-18 07:10:49] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 07:10:51] {3072} INFO -  at 37.1s,	estimator xgboost's best error=3.5729,	best estimator xgboost's best error=3.5729
[flaml.automl: 09-18 07:10:51] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 07:10:53] {3072} INFO -  at 39.4s,	estimator xgboost's best error=3.5729,	best estimator xgboost's best error=3.5729
[flaml.automl: 09-18 07:10:53] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 07:10:55] {3072} INFO -  at 41.7s,	estimator xgboost's best error=3.5729,	best estimator xgboost's best error=3.5729
[flaml.automl: 09-18 07:10:55] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-18 07:10:56] {3072} INFO -  at 42.5s,	estimator xgboost's best error=3.5729,	best estimator xgboost's best error=3.5729
[flaml.automl: 09-18 07:10:56] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-18 07:11:13] {3072} INFO -  at 59.4s,	estimator xgboost's best error=3.4264,	best estimator xgboost's best error=3.4264
[flaml.automl: 09-18 07:11:33] {3335} INFO - retrain xgboost for 20.3s
[flaml.automl: 09-18 07:11:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8193436379278645, colsample_bynode=1,
             colsample_bytree=0.9324261831198668, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=20, min_child_weight=0.002301726356589306,
             missing=nan, monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010669077022962851, reg_lambda=0.0799156556628313,
             scale_pos_weight=1, subsample=0.9199833031469933,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 07:11:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:11:33] {2637} INFO - Time taken to find the best model: 59.43144154548645
[flaml.automl: 09-18 07:11:33] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 18, 'max_leaves': 20, 'min_child_weight': 0.002301726356589306, 'learning_rate': 1.0, 'subsample': 0.9199833031469933, 'colsample_bylevel': 0.8193436379278645, 'colsample_bytree': 0.9324261831198668, 'reg_alpha': 0.0010669077022962851, 'reg_lambda': 0.0799156556628313, 'FLAML_sample_size': 63445}
PM2.5(0)最佳损失：-2.4263814275315467
PM2.5(0)最好结果：{'pred_time': 5.789039828253131e-06, 'wall_clock_time': 59.43144154548645, 'metric_for_logging': {'pred_time': 5.789039828253131e-06}, 'val_loss': 3.4263814275315467, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 20, 'min_child_weight': 0.002301726356589306, 'learning_rate': 1.0, 'subsample': 0.9199833031469933, 'colsample_bylevel': 0.8193436379278645, 'colsample_bytree': 0.9324261831198668, 'reg_alpha': 0.0010669077022962851, 'reg_lambda': 0.0799156556628313, 'FLAML_sample_size': 63445}, 'config/n_estimators': 18, 'config/max_leaves': 20, 'config/min_child_weight': 0.002301726356589306, 'config/learning_rate': 1.0, 'config/subsample': 0.9199833031469933, 'config/colsample_bylevel': 0.8193436379278645, 'config/colsample_bytree': 0.9324261831198668, 'config/reg_alpha': 0.0010669077022962851, 'config/reg_lambda': 0.0799156556628313, 'config/FLAML_sample_size': 63445, 'experiment_tag': 'exp', 'time_total_s': 16.968786001205444}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8193436379278645, colsample_bynode=1,
             colsample_bytree=0.9324261831198668, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=20, min_child_weight=0.002301726356589306,
             missing=nan, monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010669077022962851, reg_lambda=0.0799156556628313,
             scale_pos_weight=1, subsample=0.9199833031469933,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8449975937792078
PM2.5(0)的mse=20.37412753038066
PM2.5(0)的mae=3.4427164782665707
PM2.5(0)的mar=0.265994688822632
总共花费的时间为：80.75
吉安市
2367A
2368A
2369A
2370A
[flaml.automl: 09-18 07:24:13] {2390} INFO - task = regression
[flaml.automl: 09-18 07:24:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:24:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:24:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:24:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:24:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:24:14] {3025} INFO - Estimated sufficient time budget=51988s. Estimated necessary time budget=52s.
[flaml.automl: 09-18 07:24:14] {3072} INFO -  at 1.4s,	estimator xgboost's best error=16.2917,	best estimator xgboost's best error=16.2917
[flaml.automl: 09-18 07:24:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:24:16] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.4651,	best estimator xgboost's best error=7.4651
[flaml.automl: 09-18 07:24:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:24:17] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.4651,	best estimator xgboost's best error=7.4651
[flaml.automl: 09-18 07:24:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:24:23] {3072} INFO -  at 10.5s,	estimator xgboost's best error=7.4651,	best estimator xgboost's best error=7.4651
[flaml.automl: 09-18 07:24:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:24:24] {3072} INFO -  at 11.6s,	estimator xgboost's best error=4.5145,	best estimator xgboost's best error=4.5145
[flaml.automl: 09-18 07:24:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:24:26] {3072} INFO -  at 13.1s,	estimator xgboost's best error=3.8867,	best estimator xgboost's best error=3.8867
[flaml.automl: 09-18 07:24:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:24:27] {3072} INFO -  at 14.7s,	estimator xgboost's best error=3.3569,	best estimator xgboost's best error=3.3569
[flaml.automl: 09-18 07:24:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:24:30] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.3569,	best estimator xgboost's best error=3.3569
[flaml.automl: 09-18 07:24:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:24:32] {3072} INFO -  at 19.0s,	estimator xgboost's best error=3.3569,	best estimator xgboost's best error=3.3569
[flaml.automl: 09-18 07:24:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:24:35] {3072} INFO -  at 21.9s,	estimator xgboost's best error=2.9785,	best estimator xgboost's best error=2.9785
[flaml.automl: 09-18 07:24:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:24:36] {3072} INFO -  at 23.5s,	estimator xgboost's best error=2.9785,	best estimator xgboost's best error=2.9785
[flaml.automl: 09-18 07:24:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:24:37] {3072} INFO -  at 24.7s,	estimator xgboost's best error=2.9785,	best estimator xgboost's best error=2.9785
[flaml.automl: 09-18 07:24:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:24:41] {3072} INFO -  at 28.4s,	estimator xgboost's best error=2.7413,	best estimator xgboost's best error=2.7413
[flaml.automl: 09-18 07:24:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:24:44] {3072} INFO -  at 31.9s,	estimator xgboost's best error=2.7413,	best estimator xgboost's best error=2.7413
[flaml.automl: 09-18 07:24:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:24:47] {3072} INFO -  at 34.4s,	estimator xgboost's best error=2.6743,	best estimator xgboost's best error=2.6743
[flaml.automl: 09-18 07:24:47] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 07:24:49] {3072} INFO -  at 36.6s,	estimator xgboost's best error=2.6743,	best estimator xgboost's best error=2.6743
[flaml.automl: 09-18 07:24:49] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 07:24:51] {3072} INFO -  at 38.8s,	estimator xgboost's best error=2.6267,	best estimator xgboost's best error=2.6267
[flaml.automl: 09-18 07:24:51] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 07:24:53] {3072} INFO -  at 40.5s,	estimator xgboost's best error=2.6267,	best estimator xgboost's best error=2.6267
[flaml.automl: 09-18 07:24:53] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 07:24:55] {3072} INFO -  at 42.2s,	estimator xgboost's best error=2.6267,	best estimator xgboost's best error=2.6267
[flaml.automl: 09-18 07:24:55] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 07:24:56] {3072} INFO -  at 43.8s,	estimator xgboost's best error=2.6267,	best estimator xgboost's best error=2.6267
[flaml.automl: 09-18 07:24:56] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 07:24:58] {3072} INFO -  at 45.3s,	estimator xgboost's best error=2.5559,	best estimator xgboost's best error=2.5559
[flaml.automl: 09-18 07:24:58] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 07:25:01] {3072} INFO -  at 47.9s,	estimator xgboost's best error=2.5559,	best estimator xgboost's best error=2.5559
[flaml.automl: 09-18 07:25:01] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-18 07:25:01] {3072} INFO -  at 48.8s,	estimator xgboost's best error=2.5559,	best estimator xgboost's best error=2.5559
[flaml.automl: 09-18 07:25:01] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-18 07:25:05] {3072} INFO -  at 52.2s,	estimator xgboost's best error=2.5559,	best estimator xgboost's best error=2.5559
[flaml.automl: 09-18 07:25:05] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-18 07:25:06] {3072} INFO -  at 52.9s,	estimator xgboost's best error=2.5559,	best estimator xgboost's best error=2.5559
[flaml.automl: 09-18 07:25:06] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-18 07:25:12] {3072} INFO -  at 59.9s,	estimator xgboost's best error=2.4594,	best estimator xgboost's best error=2.4594
[flaml.automl: 09-18 07:25:26] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-18 07:25:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8732721209051209, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=24, min_child_weight=0.0023588925015011544,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009451087049979049, reg_lambda=0.08827632666156185,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:25:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:25:26] {2637} INFO - Time taken to find the best model: 59.88079309463501
[flaml.automl: 09-18 07:25:26] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 10, 'max_leaves': 24, 'min_child_weight': 0.0023588925015011544, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8732721209051209, 'reg_alpha': 0.009451087049979049, 'reg_lambda': 0.08827632666156185, 'FLAML_sample_size': 43605}
PM2.5(0)最佳损失：-1.459385311935708
PM2.5(0)最好结果：{'pred_time': 9.486402151385328e-06, 'wall_clock_time': 59.88079309463501, 'metric_for_logging': {'pred_time': 9.486402151385328e-06}, 'val_loss': 2.459385311935708, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 24, 'min_child_weight': 0.0023588925015011544, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8732721209051209, 'reg_alpha': 0.009451087049979049, 'reg_lambda': 0.08827632666156185, 'FLAML_sample_size': 43605}, 'config/n_estimators': 10, 'config/max_leaves': 24, 'config/min_child_weight': 0.0023588925015011544, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8732721209051209, 'config/reg_alpha': 0.009451087049979049, 'config/reg_lambda': 0.08827632666156185, 'config/FLAML_sample_size': 43605, 'experiment_tag': 'exp', 'time_total_s': 6.931365489959717}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8732721209051209, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=24, min_child_weight=0.0023588925015011544,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009451087049979049, reg_lambda=0.08827632666156185,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.94612657335835
PM2.5(0)的mse=9.693854669749797
PM2.5(0)的mae=2.3952497891074302
PM2.5(0)的mar=0.13109602980554394
总共花费的时间为：74.10
宜春市
2371A
2374A
2375A
3151A
3415A
[flaml.automl: 09-18 07:40:11] {2390} INFO - task = regression
[flaml.automl: 09-18 07:40:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:40:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:40:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:40:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:40:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:40:13] {3025} INFO - Estimated sufficient time budget=62505s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 07:40:13] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.5380,	best estimator xgboost's best error=17.5380
[flaml.automl: 09-18 07:40:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:40:15] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.2007,	best estimator xgboost's best error=8.2007
[flaml.automl: 09-18 07:40:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:40:16] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.2007,	best estimator xgboost's best error=8.2007
[flaml.automl: 09-18 07:40:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:40:21] {3072} INFO -  at 9.8s,	estimator xgboost's best error=8.2007,	best estimator xgboost's best error=8.2007
[flaml.automl: 09-18 07:40:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:40:22] {3072} INFO -  at 11.0s,	estimator xgboost's best error=5.4721,	best estimator xgboost's best error=5.4721
[flaml.automl: 09-18 07:40:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:40:24] {3072} INFO -  at 12.5s,	estimator xgboost's best error=4.6743,	best estimator xgboost's best error=4.6743
[flaml.automl: 09-18 07:40:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:40:25] {3072} INFO -  at 14.1s,	estimator xgboost's best error=4.1274,	best estimator xgboost's best error=4.1274
[flaml.automl: 09-18 07:40:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:40:28] {3072} INFO -  at 16.8s,	estimator xgboost's best error=4.1274,	best estimator xgboost's best error=4.1274
[flaml.automl: 09-18 07:40:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:40:30] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.8514,	best estimator xgboost's best error=3.8514
[flaml.automl: 09-18 07:40:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:40:33] {3072} INFO -  at 21.5s,	estimator xgboost's best error=3.8514,	best estimator xgboost's best error=3.8514
[flaml.automl: 09-18 07:40:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:40:34] {3072} INFO -  at 22.9s,	estimator xgboost's best error=3.8514,	best estimator xgboost's best error=3.8514
[flaml.automl: 09-18 07:40:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:40:35] {3072} INFO -  at 24.1s,	estimator xgboost's best error=3.8514,	best estimator xgboost's best error=3.8514
[flaml.automl: 09-18 07:40:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:40:39] {3072} INFO -  at 27.3s,	estimator xgboost's best error=3.3500,	best estimator xgboost's best error=3.3500
[flaml.automl: 09-18 07:40:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:40:42] {3072} INFO -  at 30.3s,	estimator xgboost's best error=3.3500,	best estimator xgboost's best error=3.3500
[flaml.automl: 09-18 07:40:42] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:40:44] {3072} INFO -  at 32.6s,	estimator xgboost's best error=3.3500,	best estimator xgboost's best error=3.3500
[flaml.automl: 09-18 07:40:44] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 07:40:46] {3072} INFO -  at 35.1s,	estimator xgboost's best error=3.3500,	best estimator xgboost's best error=3.3500
[flaml.automl: 09-18 07:40:46] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 07:40:49] {3072} INFO -  at 37.3s,	estimator xgboost's best error=3.3500,	best estimator xgboost's best error=3.3500
[flaml.automl: 09-18 07:40:49] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 07:40:55] {3072} INFO -  at 44.2s,	estimator xgboost's best error=3.2137,	best estimator xgboost's best error=3.2137
[flaml.automl: 09-18 07:40:55] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 07:40:58] {3072} INFO -  at 47.1s,	estimator xgboost's best error=3.2137,	best estimator xgboost's best error=3.2137
[flaml.automl: 09-18 07:40:58] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 07:41:11] {3072} INFO -  at 59.7s,	estimator xgboost's best error=3.2137,	best estimator xgboost's best error=3.2137
[flaml.automl: 09-18 07:41:18] {3335} INFO - retrain xgboost for 6.9s
[flaml.automl: 09-18 07:41:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466852,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 07:41:18] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:41:18] {2637} INFO - Time taken to find the best model: 44.227304458618164
[flaml.automl: 09-18 07:41:18] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466852, 'FLAML_sample_size': 51864}
PM2.5(0)最佳损失：-2.213665746388989
PM2.5(0)最好结果：{'pred_time': 7.191776670806424e-06, 'wall_clock_time': 44.227304458618164, 'metric_for_logging': {'pred_time': 7.191776670806424e-06}, 'val_loss': 3.213665746388989, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466852, 'FLAML_sample_size': 51864}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609401, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466852, 'config/FLAML_sample_size': 51864, 'experiment_tag': 'exp', 'time_total_s': 6.912285566329956}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466852,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9174808692188091
PM2.5(0)的mse=19.283638318645586
PM2.5(0)的mae=3.182294938855854
PM2.5(0)的mar=0.16402862901701137
总共花费的时间为：67.52
抚州市
2376A
2377A
2378A
2379A
2380A
[flaml.automl: 09-18 07:58:26] {2390} INFO - task = regression
[flaml.automl: 09-18 07:58:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:58:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:58:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:58:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:58:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:58:27] {3025} INFO - Estimated sufficient time budget=63927s. Estimated necessary time budget=64s.
[flaml.automl: 09-18 07:58:27] {3072} INFO -  at 1.5s,	estimator xgboost's best error=15.4489,	best estimator xgboost's best error=15.4489
[flaml.automl: 09-18 07:58:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:58:29] {3072} INFO -  at 3.6s,	estimator xgboost's best error=7.0778,	best estimator xgboost's best error=7.0778
[flaml.automl: 09-18 07:58:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:58:31] {3072} INFO -  at 4.8s,	estimator xgboost's best error=7.0778,	best estimator xgboost's best error=7.0778
[flaml.automl: 09-18 07:58:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:58:35] {3072} INFO -  at 9.6s,	estimator xgboost's best error=7.0778,	best estimator xgboost's best error=7.0778
[flaml.automl: 09-18 07:58:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:58:36] {3072} INFO -  at 10.7s,	estimator xgboost's best error=4.4447,	best estimator xgboost's best error=4.4447
[flaml.automl: 09-18 07:58:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:58:38] {3072} INFO -  at 12.3s,	estimator xgboost's best error=4.4447,	best estimator xgboost's best error=4.4447
[flaml.automl: 09-18 07:58:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:58:40] {3072} INFO -  at 14.0s,	estimator xgboost's best error=2.8635,	best estimator xgboost's best error=2.8635
[flaml.automl: 09-18 07:58:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:58:42] {3072} INFO -  at 16.7s,	estimator xgboost's best error=2.8635,	best estimator xgboost's best error=2.8635
[flaml.automl: 09-18 07:58:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:58:44] {3072} INFO -  at 18.3s,	estimator xgboost's best error=2.8635,	best estimator xgboost's best error=2.8635
[flaml.automl: 09-18 07:58:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:58:47] {3072} INFO -  at 21.3s,	estimator xgboost's best error=2.8635,	best estimator xgboost's best error=2.8635
[flaml.automl: 09-18 07:58:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:58:49] {3072} INFO -  at 22.8s,	estimator xgboost's best error=2.8635,	best estimator xgboost's best error=2.8635
[flaml.automl: 09-18 07:58:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:58:50] {3072} INFO -  at 24.5s,	estimator xgboost's best error=2.8470,	best estimator xgboost's best error=2.8470
[flaml.automl: 09-18 07:58:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:58:51] {3072} INFO -  at 25.6s,	estimator xgboost's best error=2.8470,	best estimator xgboost's best error=2.8470
[flaml.automl: 09-18 07:58:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:58:58] {3072} INFO -  at 32.7s,	estimator xgboost's best error=2.3142,	best estimator xgboost's best error=2.3142
[flaml.automl: 09-18 07:58:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:59:11] {3072} INFO -  at 45.5s,	estimator xgboost's best error=2.2614,	best estimator xgboost's best error=2.2614
[flaml.automl: 09-18 07:59:11] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 07:59:18] {3072} INFO -  at 52.6s,	estimator xgboost's best error=2.2614,	best estimator xgboost's best error=2.2614
[flaml.automl: 09-18 07:59:31] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-18 07:59:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:59:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:59:31] {2637} INFO - Time taken to find the best model: 45.51883864402771
[flaml.automl: 09-18 07:59:31] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 53192}
PM2.5(0)最佳损失：-1.26136297240634
PM2.5(0)最好结果：{'pred_time': 6.758487077357099e-06, 'wall_clock_time': 45.51883864402771, 'metric_for_logging': {'pred_time': 6.758487077357099e-06}, 'val_loss': 2.26136297240634, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 53192}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 53192, 'experiment_tag': 'exp', 'time_total_s': 12.800424575805664}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9414565162251418
PM2.5(0)的mse=10.42517096932427
PM2.5(0)的mae=2.3160272988986983
PM2.5(0)的mar=0.12334479667956065
总共花费的时间为：66.19
上饶市
2381A
2382A
2383A
3685A
[flaml.automl: 09-18 08:11:43] {2390} INFO - task = regression
[flaml.automl: 09-18 08:11:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:11:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:11:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:11:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:11:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:11:44] {3025} INFO - Estimated sufficient time budget=49414s. Estimated necessary time budget=49s.
[flaml.automl: 09-18 08:11:44] {3072} INFO -  at 1.4s,	estimator xgboost's best error=16.2348,	best estimator xgboost's best error=16.2348
[flaml.automl: 09-18 08:11:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:11:46] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.7469,	best estimator xgboost's best error=7.7469
[flaml.automl: 09-18 08:11:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:11:48] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.7469,	best estimator xgboost's best error=7.7469
[flaml.automl: 09-18 08:11:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:11:54] {3072} INFO -  at 11.0s,	estimator xgboost's best error=7.7469,	best estimator xgboost's best error=7.7469
[flaml.automl: 09-18 08:11:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:11:55] {3072} INFO -  at 12.2s,	estimator xgboost's best error=5.2858,	best estimator xgboost's best error=5.2858
[flaml.automl: 09-18 08:11:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:11:57] {3072} INFO -  at 13.7s,	estimator xgboost's best error=5.2858,	best estimator xgboost's best error=5.2858
[flaml.automl: 09-18 08:11:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:11:58] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.0883,	best estimator xgboost's best error=4.0883
[flaml.automl: 09-18 08:11:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:12:01] {3072} INFO -  at 18.1s,	estimator xgboost's best error=4.0883,	best estimator xgboost's best error=4.0883
[flaml.automl: 09-18 08:12:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:12:03] {3072} INFO -  at 19.7s,	estimator xgboost's best error=4.0883,	best estimator xgboost's best error=4.0883
[flaml.automl: 09-18 08:12:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:12:06] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.0883,	best estimator xgboost's best error=4.0883
[flaml.automl: 09-18 08:12:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:12:07] {3072} INFO -  at 24.2s,	estimator xgboost's best error=4.0190,	best estimator xgboost's best error=4.0190
[flaml.automl: 09-18 08:12:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:12:08] {3072} INFO -  at 25.3s,	estimator xgboost's best error=4.0190,	best estimator xgboost's best error=4.0190
[flaml.automl: 09-18 08:12:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:12:12] {3072} INFO -  at 29.2s,	estimator xgboost's best error=3.7282,	best estimator xgboost's best error=3.7282
[flaml.automl: 09-18 08:12:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:12:16] {3072} INFO -  at 32.6s,	estimator xgboost's best error=3.7057,	best estimator xgboost's best error=3.7057
[flaml.automl: 09-18 08:12:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:12:18] {3072} INFO -  at 35.4s,	estimator xgboost's best error=3.7057,	best estimator xgboost's best error=3.7057
[flaml.automl: 09-18 08:12:18] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 08:12:21] {3072} INFO -  at 38.3s,	estimator xgboost's best error=3.7057,	best estimator xgboost's best error=3.7057
[flaml.automl: 09-18 08:12:21] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 08:12:24] {3072} INFO -  at 40.7s,	estimator xgboost's best error=3.7057,	best estimator xgboost's best error=3.7057
[flaml.automl: 09-18 08:12:24] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 08:12:26] {3072} INFO -  at 42.8s,	estimator xgboost's best error=3.7057,	best estimator xgboost's best error=3.7057
[flaml.automl: 09-18 08:12:26] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 08:12:42] {3072} INFO -  at 58.7s,	estimator xgboost's best error=3.6122,	best estimator xgboost's best error=3.6122
[flaml.automl: 09-18 08:13:05] {3335} INFO - retrain xgboost for 23.8s
[flaml.automl: 09-18 08:13:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:13:05] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:13:05] {2637} INFO - Time taken to find the best model: 58.69061231613159
[flaml.automl: 09-18 08:13:05] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 41215}
PM2.5(0)最佳损失：-2.612182701727188
PM2.5(0)最好结果：{'pred_time': 2.563343297966703e-05, 'wall_clock_time': 58.69061231613159, 'metric_for_logging': {'pred_time': 2.563343297966703e-05}, 'val_loss': 3.612182701727188, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 41215}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 41215, 'experiment_tag': 'exp', 'time_total_s': 15.937593221664429}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8548245033040043
PM2.5(0)的mse=23.44388217925109
PM2.5(0)的mae=3.6301359603718426
PM2.5(0)的mar=0.2042041764935819
总共花费的时间为：83.28
鹤壁市
2385A
2386A
2387A
3474A
3596A
[flaml.automl: 09-18 08:28:26] {2390} INFO - task = regression
[flaml.automl: 09-18 08:28:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:28:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:28:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:28:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:28:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:28:27] {3025} INFO - Estimated sufficient time budget=63807s. Estimated necessary time budget=64s.
[flaml.automl: 09-18 08:28:27] {3072} INFO -  at 1.4s,	estimator xgboost's best error=30.5309,	best estimator xgboost's best error=30.5309
[flaml.automl: 09-18 08:28:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:28:29] {3072} INFO -  at 3.5s,	estimator xgboost's best error=14.6529,	best estimator xgboost's best error=14.6529
[flaml.automl: 09-18 08:28:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:28:30] {3072} INFO -  at 4.7s,	estimator xgboost's best error=14.6529,	best estimator xgboost's best error=14.6529
[flaml.automl: 09-18 08:28:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:28:35] {3072} INFO -  at 9.5s,	estimator xgboost's best error=14.6529,	best estimator xgboost's best error=14.6529
[flaml.automl: 09-18 08:28:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:28:36] {3072} INFO -  at 10.7s,	estimator xgboost's best error=10.8966,	best estimator xgboost's best error=10.8966
[flaml.automl: 09-18 08:28:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:28:38] {3072} INFO -  at 12.2s,	estimator xgboost's best error=10.8966,	best estimator xgboost's best error=10.8966
[flaml.automl: 09-18 08:28:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:28:39] {3072} INFO -  at 13.9s,	estimator xgboost's best error=7.3717,	best estimator xgboost's best error=7.3717
[flaml.automl: 09-18 08:28:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:28:42] {3072} INFO -  at 16.6s,	estimator xgboost's best error=7.3717,	best estimator xgboost's best error=7.3717
[flaml.automl: 09-18 08:28:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:28:44] {3072} INFO -  at 18.3s,	estimator xgboost's best error=7.3717,	best estimator xgboost's best error=7.3717
[flaml.automl: 09-18 08:28:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:28:47] {3072} INFO -  at 21.3s,	estimator xgboost's best error=7.3717,	best estimator xgboost's best error=7.3717
[flaml.automl: 09-18 08:28:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:28:48] {3072} INFO -  at 22.7s,	estimator xgboost's best error=7.3717,	best estimator xgboost's best error=7.3717
[flaml.automl: 09-18 08:28:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:28:50] {3072} INFO -  at 24.4s,	estimator xgboost's best error=7.3717,	best estimator xgboost's best error=7.3717
[flaml.automl: 09-18 08:28:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:28:51] {3072} INFO -  at 25.6s,	estimator xgboost's best error=7.3717,	best estimator xgboost's best error=7.3717
[flaml.automl: 09-18 08:28:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:28:58] {3072} INFO -  at 32.6s,	estimator xgboost's best error=6.3946,	best estimator xgboost's best error=6.3946
[flaml.automl: 09-18 08:28:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:29:11] {3072} INFO -  at 45.4s,	estimator xgboost's best error=6.2791,	best estimator xgboost's best error=6.2791
[flaml.automl: 09-18 08:29:11] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 08:29:18] {3072} INFO -  at 52.5s,	estimator xgboost's best error=6.2791,	best estimator xgboost's best error=6.2791
[flaml.automl: 09-18 08:29:31] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 08:29:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:29:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:29:31] {2637} INFO - Time taken to find the best model: 45.443535566329956
[flaml.automl: 09-18 08:29:31] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 53244}
PM2.5(0)最佳损失：-5.279100115685643
PM2.5(0)最好结果：{'pred_time': 6.668467325839477e-06, 'wall_clock_time': 45.443535566329956, 'metric_for_logging': {'pred_time': 6.668467325839477e-06}, 'val_loss': 6.279100115685643, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 53244}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 53244, 'experiment_tag': 'exp', 'time_total_s': 12.807082891464233}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9114023839348103
PM2.5(0)的mse=83.29708119306504
PM2.5(0)的mae=6.419329957864841
PM2.5(0)的mar=0.24804324873889094
总共花费的时间为：66.21
新乡市
2390A
2391A
3054A
3475A
3476A
[flaml.automl: 09-18 08:45:36] {2390} INFO - task = regression
[flaml.automl: 09-18 08:45:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:45:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:45:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:45:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:45:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:45:37] {3025} INFO - Estimated sufficient time budget=53986s. Estimated necessary time budget=54s.
[flaml.automl: 09-18 08:45:37] {3072} INFO -  at 1.4s,	estimator xgboost's best error=28.1326,	best estimator xgboost's best error=28.1326
[flaml.automl: 09-18 08:45:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:45:39] {3072} INFO -  at 3.5s,	estimator xgboost's best error=13.4027,	best estimator xgboost's best error=13.4027
[flaml.automl: 09-18 08:45:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:45:40] {3072} INFO -  at 4.7s,	estimator xgboost's best error=13.4027,	best estimator xgboost's best error=13.4027
[flaml.automl: 09-18 08:45:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:45:46] {3072} INFO -  at 10.5s,	estimator xgboost's best error=13.4027,	best estimator xgboost's best error=13.4027
[flaml.automl: 09-18 08:45:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:45:47] {3072} INFO -  at 11.7s,	estimator xgboost's best error=9.6518,	best estimator xgboost's best error=9.6518
[flaml.automl: 09-18 08:45:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:45:49] {3072} INFO -  at 13.2s,	estimator xgboost's best error=9.6518,	best estimator xgboost's best error=9.6518
[flaml.automl: 09-18 08:45:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:45:51] {3072} INFO -  at 14.9s,	estimator xgboost's best error=6.2200,	best estimator xgboost's best error=6.2200
[flaml.automl: 09-18 08:45:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:45:53] {3072} INFO -  at 17.6s,	estimator xgboost's best error=6.2200,	best estimator xgboost's best error=6.2200
[flaml.automl: 09-18 08:45:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:45:55] {3072} INFO -  at 19.3s,	estimator xgboost's best error=6.2200,	best estimator xgboost's best error=6.2200
[flaml.automl: 09-18 08:45:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:45:58] {3072} INFO -  at 22.3s,	estimator xgboost's best error=6.2200,	best estimator xgboost's best error=6.2200
[flaml.automl: 09-18 08:45:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:45:59] {3072} INFO -  at 23.7s,	estimator xgboost's best error=6.1993,	best estimator xgboost's best error=6.1993
[flaml.automl: 09-18 08:45:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:46:00] {3072} INFO -  at 24.9s,	estimator xgboost's best error=6.1993,	best estimator xgboost's best error=6.1993
[flaml.automl: 09-18 08:46:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:46:04] {3072} INFO -  at 28.2s,	estimator xgboost's best error=5.4831,	best estimator xgboost's best error=5.4831
[flaml.automl: 09-18 08:46:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:46:07] {3072} INFO -  at 31.7s,	estimator xgboost's best error=5.4064,	best estimator xgboost's best error=5.4064
[flaml.automl: 09-18 08:46:07] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:46:10] {3072} INFO -  at 34.5s,	estimator xgboost's best error=5.4064,	best estimator xgboost's best error=5.4064
[flaml.automl: 09-18 08:46:10] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 08:46:12] {3072} INFO -  at 36.7s,	estimator xgboost's best error=5.4064,	best estimator xgboost's best error=5.4064
[flaml.automl: 09-18 08:46:12] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 08:46:15] {3072} INFO -  at 39.0s,	estimator xgboost's best error=5.4064,	best estimator xgboost's best error=5.4064
[flaml.automl: 09-18 08:46:15] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 08:46:17] {3072} INFO -  at 41.1s,	estimator xgboost's best error=5.4064,	best estimator xgboost's best error=5.4064
[flaml.automl: 09-18 08:46:17] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 08:46:28] {3072} INFO -  at 51.9s,	estimator xgboost's best error=5.2505,	best estimator xgboost's best error=5.2505
[flaml.automl: 09-18 08:46:38] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-18 08:46:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:46:38] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:46:38] {2637} INFO - Time taken to find the best model: 51.88455891609192
[flaml.automl: 09-18 08:46:38] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 44902}
PM2.5(0)最佳损失：-4.250486192340125
PM2.5(0)最好结果：{'pred_time': 9.22221220089104e-06, 'wall_clock_time': 51.88455891609192, 'metric_for_logging': {'pred_time': 9.22221220089104e-06}, 'val_loss': 5.250486192340125, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 44902}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 44902, 'experiment_tag': 'exp', 'time_total_s': 10.751988649368286}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9346718576176112
PM2.5(0)的mse=56.78766478986608
PM2.5(0)的mae=5.154272561293488
PM2.5(0)的mar=0.1770253785143991
总共花费的时间为：63.75
濮阳市
2392A
2395A
3021A
[flaml.automl: 09-18 08:55:56] {2390} INFO - task = regression
[flaml.automl: 09-18 08:55:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:55:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:55:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:55:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:55:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:55:57] {3025} INFO - Estimated sufficient time budget=12122s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 08:55:57] {3072} INFO -  at 1.3s,	estimator xgboost's best error=29.4718,	best estimator xgboost's best error=29.4718
[flaml.automl: 09-18 08:55:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:55:59] {3072} INFO -  at 3.5s,	estimator xgboost's best error=14.0196,	best estimator xgboost's best error=14.0196
[flaml.automl: 09-18 08:55:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:56:01] {3072} INFO -  at 4.7s,	estimator xgboost's best error=14.0196,	best estimator xgboost's best error=14.0196
[flaml.automl: 09-18 08:56:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:56:11] {3072} INFO -  at 14.7s,	estimator xgboost's best error=14.0196,	best estimator xgboost's best error=14.0196
[flaml.automl: 09-18 08:56:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:56:12] {3072} INFO -  at 15.9s,	estimator xgboost's best error=9.8237,	best estimator xgboost's best error=9.8237
[flaml.automl: 09-18 08:56:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:56:13] {3072} INFO -  at 17.4s,	estimator xgboost's best error=9.8237,	best estimator xgboost's best error=9.8237
[flaml.automl: 09-18 08:56:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:56:15] {3072} INFO -  at 19.2s,	estimator xgboost's best error=6.0484,	best estimator xgboost's best error=6.0484
[flaml.automl: 09-18 08:56:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:56:18] {3072} INFO -  at 21.9s,	estimator xgboost's best error=6.0484,	best estimator xgboost's best error=6.0484
[flaml.automl: 09-18 08:56:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:56:19] {3072} INFO -  at 23.5s,	estimator xgboost's best error=6.0484,	best estimator xgboost's best error=6.0484
[flaml.automl: 09-18 08:56:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:56:22] {3072} INFO -  at 26.5s,	estimator xgboost's best error=6.0484,	best estimator xgboost's best error=6.0484
[flaml.automl: 09-18 08:56:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:56:24] {3072} INFO -  at 28.0s,	estimator xgboost's best error=6.0397,	best estimator xgboost's best error=6.0397
[flaml.automl: 09-18 08:56:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:56:25] {3072} INFO -  at 29.2s,	estimator xgboost's best error=6.0397,	best estimator xgboost's best error=6.0397
[flaml.automl: 09-18 08:56:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:56:31] {3072} INFO -  at 34.6s,	estimator xgboost's best error=4.6180,	best estimator xgboost's best error=4.6180
[flaml.automl: 09-18 08:56:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:56:40] {3072} INFO -  at 44.2s,	estimator xgboost's best error=4.5288,	best estimator xgboost's best error=4.5288
[flaml.automl: 09-18 08:56:40] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:56:46] {3072} INFO -  at 49.6s,	estimator xgboost's best error=4.5288,	best estimator xgboost's best error=4.5288
[flaml.automl: 09-18 08:56:46] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 08:56:55] {3072} INFO -  at 59.3s,	estimator xgboost's best error=4.5288,	best estimator xgboost's best error=4.5288
[flaml.automl: 09-18 08:57:05] {3335} INFO - retrain xgboost for 9.4s
[flaml.automl: 09-18 08:57:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:57:05] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:57:05] {2637} INFO - Time taken to find the best model: 44.16508460044861
[flaml.automl: 09-18 08:57:05] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 23, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-3.528776647725823
PM2.5(0)最好结果：{'pred_time': 1.1541028949452244e-05, 'wall_clock_time': 44.16508460044861, 'metric_for_logging': {'pred_time': 1.1541028949452244e-05}, 'val_loss': 4.528776647725823, 'training_iteration': 1, 'config': {'n_estimators': 23, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 23, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 9.540007591247559}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9572531132346285
PM2.5(0)的mse=49.85455539583644
PM2.5(0)的mae=4.508537665509174
PM2.5(0)的mar=0.14391021429085935
总共花费的时间为：69.27
许昌市
2396A
3134A
3337A
3338A
3597A
[flaml.automl: 09-18 09:12:26] {2390} INFO - task = regression
[flaml.automl: 09-18 09:12:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:12:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:12:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:12:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:12:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:12:27] {3025} INFO - Estimated sufficient time budget=61971s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 09:12:27] {3072} INFO -  at 1.4s,	estimator xgboost's best error=26.7942,	best estimator xgboost's best error=26.7942
[flaml.automl: 09-18 09:12:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:12:29] {3072} INFO -  at 3.5s,	estimator xgboost's best error=12.7042,	best estimator xgboost's best error=12.7042
[flaml.automl: 09-18 09:12:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:12:30] {3072} INFO -  at 4.7s,	estimator xgboost's best error=12.7042,	best estimator xgboost's best error=12.7042
[flaml.automl: 09-18 09:12:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:12:35] {3072} INFO -  at 9.6s,	estimator xgboost's best error=12.7042,	best estimator xgboost's best error=12.7042
[flaml.automl: 09-18 09:12:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:12:36] {3072} INFO -  at 10.7s,	estimator xgboost's best error=8.9454,	best estimator xgboost's best error=8.9454
[flaml.automl: 09-18 09:12:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:12:38] {3072} INFO -  at 12.3s,	estimator xgboost's best error=8.9454,	best estimator xgboost's best error=8.9454
[flaml.automl: 09-18 09:12:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:12:40] {3072} INFO -  at 14.0s,	estimator xgboost's best error=5.4307,	best estimator xgboost's best error=5.4307
[flaml.automl: 09-18 09:12:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:12:42] {3072} INFO -  at 16.7s,	estimator xgboost's best error=5.4307,	best estimator xgboost's best error=5.4307
[flaml.automl: 09-18 09:12:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:12:44] {3072} INFO -  at 18.3s,	estimator xgboost's best error=5.4307,	best estimator xgboost's best error=5.4307
[flaml.automl: 09-18 09:12:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:12:47] {3072} INFO -  at 21.3s,	estimator xgboost's best error=5.4307,	best estimator xgboost's best error=5.4307
[flaml.automl: 09-18 09:12:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:12:48] {3072} INFO -  at 22.7s,	estimator xgboost's best error=5.4307,	best estimator xgboost's best error=5.4307
[flaml.automl: 09-18 09:12:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:12:50] {3072} INFO -  at 24.5s,	estimator xgboost's best error=5.4307,	best estimator xgboost's best error=5.4307
[flaml.automl: 09-18 09:12:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:12:51] {3072} INFO -  at 25.6s,	estimator xgboost's best error=5.4307,	best estimator xgboost's best error=5.4307
[flaml.automl: 09-18 09:12:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:12:58] {3072} INFO -  at 32.6s,	estimator xgboost's best error=4.3212,	best estimator xgboost's best error=4.3212
[flaml.automl: 09-18 09:12:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 09:13:11] {3072} INFO -  at 45.4s,	estimator xgboost's best error=4.0563,	best estimator xgboost's best error=4.0563
[flaml.automl: 09-18 09:13:11] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 09:13:18] {3072} INFO -  at 52.5s,	estimator xgboost's best error=4.0563,	best estimator xgboost's best error=4.0563
[flaml.automl: 09-18 09:13:31] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 09:13:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 09:13:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:13:31] {2637} INFO - Time taken to find the best model: 45.41904878616333
[flaml.automl: 09-18 09:13:31] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52453}
PM2.5(0)最佳损失：-3.056321457905229
PM2.5(0)最好结果：{'pred_time': 8.344609366413823e-06, 'wall_clock_time': 45.41904878616333, 'metric_for_logging': {'pred_time': 8.344609366413823e-06}, 'val_loss': 4.056321457905229, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52453}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 52453, 'experiment_tag': 'exp', 'time_total_s': 12.77166748046875}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9514691352620949
PM2.5(0)的mse=44.291157232768654
PM2.5(0)的mae=4.170089280756261
PM2.5(0)的mar=0.14885138739455658
总共花费的时间为：66.06
漯河市
2399A
2400A
2401A
2402A
3478A
3479A
[flaml.automl: 09-18 09:31:51] {2390} INFO - task = regression
[flaml.automl: 09-18 09:31:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:31:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:31:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:31:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:31:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:31:53] {3025} INFO - Estimated sufficient time budget=74971s. Estimated necessary time budget=75s.
[flaml.automl: 09-18 09:31:53] {3072} INFO -  at 1.5s,	estimator xgboost's best error=28.3258,	best estimator xgboost's best error=28.3258
[flaml.automl: 09-18 09:31:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:31:55] {3072} INFO -  at 3.6s,	estimator xgboost's best error=13.4849,	best estimator xgboost's best error=13.4849
[flaml.automl: 09-18 09:31:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:31:56] {3072} INFO -  at 4.8s,	estimator xgboost's best error=13.4849,	best estimator xgboost's best error=13.4849
[flaml.automl: 09-18 09:31:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:32:00] {3072} INFO -  at 9.1s,	estimator xgboost's best error=13.4849,	best estimator xgboost's best error=13.4849
[flaml.automl: 09-18 09:32:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:32:01] {3072} INFO -  at 10.2s,	estimator xgboost's best error=9.4001,	best estimator xgboost's best error=9.4001
[flaml.automl: 09-18 09:32:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:32:03] {3072} INFO -  at 11.8s,	estimator xgboost's best error=9.4001,	best estimator xgboost's best error=9.4001
[flaml.automl: 09-18 09:32:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:32:04] {3072} INFO -  at 13.5s,	estimator xgboost's best error=6.0284,	best estimator xgboost's best error=6.0284
[flaml.automl: 09-18 09:32:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:32:07] {3072} INFO -  at 16.1s,	estimator xgboost's best error=6.0284,	best estimator xgboost's best error=6.0284
[flaml.automl: 09-18 09:32:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:32:09] {3072} INFO -  at 17.8s,	estimator xgboost's best error=6.0284,	best estimator xgboost's best error=6.0284
[flaml.automl: 09-18 09:32:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:32:12] {3072} INFO -  at 20.8s,	estimator xgboost's best error=6.0284,	best estimator xgboost's best error=6.0284
[flaml.automl: 09-18 09:32:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:32:13] {3072} INFO -  at 22.2s,	estimator xgboost's best error=6.0284,	best estimator xgboost's best error=6.0284
[flaml.automl: 09-18 09:32:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:32:15] {3072} INFO -  at 24.0s,	estimator xgboost's best error=6.0008,	best estimator xgboost's best error=6.0008
[flaml.automl: 09-18 09:32:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:32:16] {3072} INFO -  at 25.1s,	estimator xgboost's best error=6.0008,	best estimator xgboost's best error=6.0008
[flaml.automl: 09-18 09:32:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:32:23] {3072} INFO -  at 32.2s,	estimator xgboost's best error=4.8441,	best estimator xgboost's best error=4.8441
[flaml.automl: 09-18 09:32:23] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 09:32:36] {3072} INFO -  at 45.0s,	estimator xgboost's best error=4.6865,	best estimator xgboost's best error=4.6865
[flaml.automl: 09-18 09:32:36] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 09:32:43] {3072} INFO -  at 52.0s,	estimator xgboost's best error=4.6865,	best estimator xgboost's best error=4.6865
[flaml.automl: 09-18 09:32:56] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 09:32:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 09:32:56] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:32:56] {2637} INFO - Time taken to find the best model: 44.95886540412903
[flaml.automl: 09-18 09:32:56] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 62304}
PM2.5(0)最佳损失：-3.68653767304873
PM2.5(0)最好结果：{'pred_time': 5.957916019521799e-06, 'wall_clock_time': 44.95886540412903, 'metric_for_logging': {'pred_time': 5.957916019521799e-06}, 'val_loss': 4.68653767304873, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 62304}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 62304, 'experiment_tag': 'exp', 'time_total_s': 12.763392686843872}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9579919205831163
PM2.5(0)的mse=43.76134699795027
PM2.5(0)的mae=4.591542914951462
PM2.5(0)的mar=0.17215369037292538
总共花费的时间为：65.79
南阳市
2403A
2404A
2405A
2406A
2407A
[flaml.automl: 09-18 09:48:22] {2390} INFO - task = regression
[flaml.automl: 09-18 09:48:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:48:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:48:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:48:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:48:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:48:23] {3025} INFO - Estimated sufficient time budget=63852s. Estimated necessary time budget=64s.
[flaml.automl: 09-18 09:48:23] {3072} INFO -  at 1.4s,	estimator xgboost's best error=27.1558,	best estimator xgboost's best error=27.1558
[flaml.automl: 09-18 09:48:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:48:25] {3072} INFO -  at 3.6s,	estimator xgboost's best error=12.8688,	best estimator xgboost's best error=12.8688
[flaml.automl: 09-18 09:48:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:48:26] {3072} INFO -  at 4.8s,	estimator xgboost's best error=12.8688,	best estimator xgboost's best error=12.8688
[flaml.automl: 09-18 09:48:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:48:31] {3072} INFO -  at 9.6s,	estimator xgboost's best error=12.8688,	best estimator xgboost's best error=12.8688
[flaml.automl: 09-18 09:48:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:48:32] {3072} INFO -  at 10.8s,	estimator xgboost's best error=8.5144,	best estimator xgboost's best error=8.5144
[flaml.automl: 09-18 09:48:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:48:34] {3072} INFO -  at 12.3s,	estimator xgboost's best error=8.5144,	best estimator xgboost's best error=8.5144
[flaml.automl: 09-18 09:48:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:48:36] {3072} INFO -  at 14.0s,	estimator xgboost's best error=5.7006,	best estimator xgboost's best error=5.7006
[flaml.automl: 09-18 09:48:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:48:38] {3072} INFO -  at 16.7s,	estimator xgboost's best error=5.7006,	best estimator xgboost's best error=5.7006
[flaml.automl: 09-18 09:48:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:48:40] {3072} INFO -  at 18.3s,	estimator xgboost's best error=5.7006,	best estimator xgboost's best error=5.7006
[flaml.automl: 09-18 09:48:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:48:43] {3072} INFO -  at 21.3s,	estimator xgboost's best error=5.7006,	best estimator xgboost's best error=5.7006
[flaml.automl: 09-18 09:48:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:48:44] {3072} INFO -  at 22.8s,	estimator xgboost's best error=5.7006,	best estimator xgboost's best error=5.7006
[flaml.automl: 09-18 09:48:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:48:46] {3072} INFO -  at 24.5s,	estimator xgboost's best error=5.6739,	best estimator xgboost's best error=5.6739
[flaml.automl: 09-18 09:48:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:48:47] {3072} INFO -  at 25.7s,	estimator xgboost's best error=5.6739,	best estimator xgboost's best error=5.6739
[flaml.automl: 09-18 09:48:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:48:54] {3072} INFO -  at 32.7s,	estimator xgboost's best error=4.5143,	best estimator xgboost's best error=4.5143
[flaml.automl: 09-18 09:48:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 09:49:07] {3072} INFO -  at 45.5s,	estimator xgboost's best error=4.4213,	best estimator xgboost's best error=4.4213
[flaml.automl: 09-18 09:49:07] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 09:49:14] {3072} INFO -  at 52.6s,	estimator xgboost's best error=4.4213,	best estimator xgboost's best error=4.4213
[flaml.automl: 09-18 09:49:27] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-18 09:49:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 09:49:27] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:49:27] {2637} INFO - Time taken to find the best model: 45.54011058807373
[flaml.automl: 09-18 09:49:27] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52581}
PM2.5(0)最佳损失：-3.421290924425466
PM2.5(0)最好结果：{'pred_time': 6.833469012859605e-06, 'wall_clock_time': 45.54011058807373, 'metric_for_logging': {'pred_time': 6.833469012859605e-06}, 'val_loss': 4.421290924425466, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52581}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 52581, 'experiment_tag': 'exp', 'time_total_s': 12.793885707855225}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9583582195728774
PM2.5(0)的mse=40.38210640596367
PM2.5(0)的mae=4.420178310524441
PM2.5(0)的mar=0.17833149879823645
总共花费的时间为：66.18
商丘市
2408A
2409A
2410A
[flaml.automl: 09-18 09:59:05] {2390} INFO - task = regression
[flaml.automl: 09-18 09:59:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:59:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:59:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:59:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:59:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:59:06] {3025} INFO - Estimated sufficient time budget=12081s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 09:59:06] {3072} INFO -  at 1.3s,	estimator xgboost's best error=26.7053,	best estimator xgboost's best error=26.7053
[flaml.automl: 09-18 09:59:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:59:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=12.4464,	best estimator xgboost's best error=12.4464
[flaml.automl: 09-18 09:59:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:59:09] {3072} INFO -  at 4.7s,	estimator xgboost's best error=12.4464,	best estimator xgboost's best error=12.4464
[flaml.automl: 09-18 09:59:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:59:19] {3072} INFO -  at 14.6s,	estimator xgboost's best error=12.4464,	best estimator xgboost's best error=12.4464
[flaml.automl: 09-18 09:59:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:59:20] {3072} INFO -  at 15.8s,	estimator xgboost's best error=8.5942,	best estimator xgboost's best error=8.5942
[flaml.automl: 09-18 09:59:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:59:22] {3072} INFO -  at 17.4s,	estimator xgboost's best error=8.5942,	best estimator xgboost's best error=8.5942
[flaml.automl: 09-18 09:59:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:59:24] {3072} INFO -  at 19.1s,	estimator xgboost's best error=5.1417,	best estimator xgboost's best error=5.1417
[flaml.automl: 09-18 09:59:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:59:26] {3072} INFO -  at 21.8s,	estimator xgboost's best error=5.1417,	best estimator xgboost's best error=5.1417
[flaml.automl: 09-18 09:59:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:59:28] {3072} INFO -  at 23.4s,	estimator xgboost's best error=5.1417,	best estimator xgboost's best error=5.1417
[flaml.automl: 09-18 09:59:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:59:31] {3072} INFO -  at 26.4s,	estimator xgboost's best error=5.1417,	best estimator xgboost's best error=5.1417
[flaml.automl: 09-18 09:59:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:59:32] {3072} INFO -  at 27.8s,	estimator xgboost's best error=5.1417,	best estimator xgboost's best error=5.1417
[flaml.automl: 09-18 09:59:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:59:34] {3072} INFO -  at 29.0s,	estimator xgboost's best error=5.1417,	best estimator xgboost's best error=5.1417
[flaml.automl: 09-18 09:59:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:59:41] {3072} INFO -  at 36.0s,	estimator xgboost's best error=3.9311,	best estimator xgboost's best error=3.9311
[flaml.automl: 09-18 09:59:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:59:53] {3072} INFO -  at 48.3s,	estimator xgboost's best error=3.8033,	best estimator xgboost's best error=3.8033
[flaml.automl: 09-18 10:00:05] {3335} INFO - retrain xgboost for 12.4s
[flaml.automl: 09-18 10:00:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:00:05] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:00:05] {2637} INFO - Time taken to find the best model: 48.336918115615845
[flaml.automl: 09-18 10:00:05] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-2.8033084620796673
PM2.5(0)最好结果：{'pred_time': 1.2693741310264263e-05, 'wall_clock_time': 48.336918115615845, 'metric_for_logging': {'pred_time': 1.2693741310264263e-05}, 'val_loss': 3.8033084620796673, 'training_iteration': 1, 'config': {'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 30, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 12.355087995529175}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.967646023473656
PM2.5(0)的mse=31.840383778787874
PM2.5(0)的mae=3.647939884513259
PM2.5(0)的mar=0.12586519216712763
总共花费的时间为：61.24
驻马店市
2420A
2421A
2422A
3339A
[flaml.automl: 09-18 10:13:04] {2390} INFO - task = regression
[flaml.automl: 09-18 10:13:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:13:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:13:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:13:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:13:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:13:05] {3025} INFO - Estimated sufficient time budget=50228s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 10:13:05] {3072} INFO -  at 1.4s,	estimator xgboost's best error=23.8651,	best estimator xgboost's best error=23.8651
[flaml.automl: 09-18 10:13:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:13:07] {3072} INFO -  at 3.5s,	estimator xgboost's best error=11.3403,	best estimator xgboost's best error=11.3403
[flaml.automl: 09-18 10:13:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:13:09] {3072} INFO -  at 4.7s,	estimator xgboost's best error=11.3403,	best estimator xgboost's best error=11.3403
[flaml.automl: 09-18 10:13:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:13:15] {3072} INFO -  at 11.0s,	estimator xgboost's best error=11.3403,	best estimator xgboost's best error=11.3403
[flaml.automl: 09-18 10:13:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:13:16] {3072} INFO -  at 12.2s,	estimator xgboost's best error=7.9906,	best estimator xgboost's best error=7.9906
[flaml.automl: 09-18 10:13:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:13:18] {3072} INFO -  at 13.7s,	estimator xgboost's best error=7.9906,	best estimator xgboost's best error=7.9906
[flaml.automl: 09-18 10:13:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:13:19] {3072} INFO -  at 15.4s,	estimator xgboost's best error=5.6666,	best estimator xgboost's best error=5.6666
[flaml.automl: 09-18 10:13:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:13:22] {3072} INFO -  at 18.1s,	estimator xgboost's best error=5.6666,	best estimator xgboost's best error=5.6666
[flaml.automl: 09-18 10:13:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:13:24] {3072} INFO -  at 19.7s,	estimator xgboost's best error=5.6666,	best estimator xgboost's best error=5.6666
[flaml.automl: 09-18 10:13:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:13:27] {3072} INFO -  at 22.7s,	estimator xgboost's best error=5.6666,	best estimator xgboost's best error=5.6666
[flaml.automl: 09-18 10:13:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:13:28] {3072} INFO -  at 24.2s,	estimator xgboost's best error=5.6666,	best estimator xgboost's best error=5.6666
[flaml.automl: 09-18 10:13:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:13:30] {3072} INFO -  at 25.9s,	estimator xgboost's best error=5.6228,	best estimator xgboost's best error=5.6228
[flaml.automl: 09-18 10:13:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:13:31] {3072} INFO -  at 27.1s,	estimator xgboost's best error=5.6228,	best estimator xgboost's best error=5.6228
[flaml.automl: 09-18 10:13:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:13:38] {3072} INFO -  at 34.1s,	estimator xgboost's best error=4.9533,	best estimator xgboost's best error=4.9533
[flaml.automl: 09-18 10:13:38] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 10:13:51] {3072} INFO -  at 46.9s,	estimator xgboost's best error=4.7847,	best estimator xgboost's best error=4.7847
[flaml.automl: 09-18 10:13:51] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 10:13:58] {3072} INFO -  at 54.0s,	estimator xgboost's best error=4.7847,	best estimator xgboost's best error=4.7847
[flaml.automl: 09-18 10:14:11] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 10:14:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:14:11] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:14:11] {2637} INFO - Time taken to find the best model: 46.93567657470703
[flaml.automl: 09-18 10:14:11] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 42030}
PM2.5(0)最佳损失：-3.784673188738445
PM2.5(0)最好结果：{'pred_time': 9.619143075554988e-06, 'wall_clock_time': 46.93567657470703, 'metric_for_logging': {'pred_time': 9.619143075554988e-06}, 'val_loss': 4.784673188738445, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 42030}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 42030, 'experiment_tag': 'exp', 'time_total_s': 12.812861442565918}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9224323926655474
PM2.5(0)的mse=52.22397075935445
PM2.5(0)的mae=4.8303041623538805
PM2.5(0)的mar=0.18940702466544726
总共花费的时间为：67.48
黄石市
2423A
2424A
2427A
3149A
[flaml.automl: 09-18 10:26:41] {2390} INFO - task = regression
[flaml.automl: 09-18 10:26:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:26:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:26:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:26:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:26:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:26:42] {3025} INFO - Estimated sufficient time budget=52354s. Estimated necessary time budget=52s.
[flaml.automl: 09-18 10:26:42] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.7160,	best estimator xgboost's best error=18.7160
[flaml.automl: 09-18 10:26:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:26:44] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.8823,	best estimator xgboost's best error=8.8823
[flaml.automl: 09-18 10:26:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:26:45] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.8823,	best estimator xgboost's best error=8.8823
[flaml.automl: 09-18 10:26:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:26:51] {3072} INFO -  at 10.5s,	estimator xgboost's best error=8.8823,	best estimator xgboost's best error=8.8823
[flaml.automl: 09-18 10:26:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:26:52] {3072} INFO -  at 11.7s,	estimator xgboost's best error=6.0265,	best estimator xgboost's best error=6.0265
[flaml.automl: 09-18 10:26:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:26:54] {3072} INFO -  at 13.2s,	estimator xgboost's best error=6.0265,	best estimator xgboost's best error=6.0265
[flaml.automl: 09-18 10:26:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:26:56] {3072} INFO -  at 14.9s,	estimator xgboost's best error=4.0056,	best estimator xgboost's best error=4.0056
[flaml.automl: 09-18 10:26:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:26:58] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.0056,	best estimator xgboost's best error=4.0056
[flaml.automl: 09-18 10:26:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:27:00] {3072} INFO -  at 19.3s,	estimator xgboost's best error=4.0056,	best estimator xgboost's best error=4.0056
[flaml.automl: 09-18 10:27:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:27:03] {3072} INFO -  at 22.3s,	estimator xgboost's best error=4.0056,	best estimator xgboost's best error=4.0056
[flaml.automl: 09-18 10:27:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:27:04] {3072} INFO -  at 23.7s,	estimator xgboost's best error=4.0056,	best estimator xgboost's best error=4.0056
[flaml.automl: 09-18 10:27:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:27:06] {3072} INFO -  at 25.4s,	estimator xgboost's best error=4.0056,	best estimator xgboost's best error=4.0056
[flaml.automl: 09-18 10:27:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:27:07] {3072} INFO -  at 26.6s,	estimator xgboost's best error=4.0056,	best estimator xgboost's best error=4.0056
[flaml.automl: 09-18 10:27:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:27:14] {3072} INFO -  at 33.6s,	estimator xgboost's best error=3.3523,	best estimator xgboost's best error=3.3523
[flaml.automl: 09-18 10:27:14] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 10:27:27] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.2615,	best estimator xgboost's best error=3.2615
[flaml.automl: 09-18 10:27:27] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 10:27:34] {3072} INFO -  at 53.4s,	estimator xgboost's best error=3.2615,	best estimator xgboost's best error=3.2615
[flaml.automl: 09-18 10:27:47] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-18 10:27:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:27:47] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:27:47] {2637} INFO - Time taken to find the best model: 46.390642166137695
[flaml.automl: 09-18 10:27:47] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43801}
PM2.5(0)最佳损失：-2.2614509675300463
PM2.5(0)最好结果：{'pred_time': 8.342592824477788e-06, 'wall_clock_time': 46.390642166137695, 'metric_for_logging': {'pred_time': 8.342592824477788e-06}, 'val_loss': 3.2614509675300463, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43801}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 43801, 'experiment_tag': 'exp', 'time_total_s': 12.75665807723999}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9386136194765891
PM2.5(0)的mse=19.733912299011585
PM2.5(0)的mae=3.240768727554372
PM2.5(0)的mar=0.1836245990562474
总共花费的时间为：66.75
十堰市
2428A
2429A
2430A
2431A
3545A
[flaml.automl: 09-18 10:43:44] {2390} INFO - task = regression
[flaml.automl: 09-18 10:43:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:43:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:43:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:43:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:43:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:43:45] {3025} INFO - Estimated sufficient time budget=91912s. Estimated necessary time budget=92s.
[flaml.automl: 09-18 10:43:45] {3072} INFO -  at 2.1s,	estimator xgboost's best error=18.2851,	best estimator xgboost's best error=18.2851
[flaml.automl: 09-18 10:43:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:43:49] {3072} INFO -  at 5.6s,	estimator xgboost's best error=8.8407,	best estimator xgboost's best error=8.8407
[flaml.automl: 09-18 10:43:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:43:51] {3072} INFO -  at 7.4s,	estimator xgboost's best error=8.8407,	best estimator xgboost's best error=8.8407
[flaml.automl: 09-18 10:43:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:43:55] {3072} INFO -  at 12.0s,	estimator xgboost's best error=8.8407,	best estimator xgboost's best error=8.8407
[flaml.automl: 09-18 10:43:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:43:57] {3072} INFO -  at 14.0s,	estimator xgboost's best error=6.2776,	best estimator xgboost's best error=6.2776
[flaml.automl: 09-18 10:43:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:44:00] {3072} INFO -  at 16.8s,	estimator xgboost's best error=6.2776,	best estimator xgboost's best error=6.2776
[flaml.automl: 09-18 10:44:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:44:03] {3072} INFO -  at 19.8s,	estimator xgboost's best error=4.4744,	best estimator xgboost's best error=4.4744
[flaml.automl: 09-18 10:44:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:44:07] {3072} INFO -  at 23.7s,	estimator xgboost's best error=4.4744,	best estimator xgboost's best error=4.4744
[flaml.automl: 09-18 10:44:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:44:10] {3072} INFO -  at 26.5s,	estimator xgboost's best error=4.4744,	best estimator xgboost's best error=4.4744
[flaml.automl: 09-18 10:44:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:44:13] {3072} INFO -  at 29.9s,	estimator xgboost's best error=4.4744,	best estimator xgboost's best error=4.4744
[flaml.automl: 09-18 10:44:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:44:16] {3072} INFO -  at 32.5s,	estimator xgboost's best error=4.4744,	best estimator xgboost's best error=4.4744
[flaml.automl: 09-18 10:44:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:44:19] {3072} INFO -  at 35.4s,	estimator xgboost's best error=4.4744,	best estimator xgboost's best error=4.4744
[flaml.automl: 09-18 10:44:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:44:21] {3072} INFO -  at 37.6s,	estimator xgboost's best error=4.4744,	best estimator xgboost's best error=4.4744
[flaml.automl: 09-18 10:44:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:44:36] {3072} INFO -  at 53.1s,	estimator xgboost's best error=3.9285,	best estimator xgboost's best error=3.9285
[flaml.automl: 09-18 10:44:51] {3335} INFO - retrain xgboost for 14.6s
[flaml.automl: 09-18 10:44:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:44:51] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:44:51] {2637} INFO - Time taken to find the best model: 53.13805031776428
[flaml.automl: 09-18 10:44:51] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 47366}
PM2.5(0)最佳损失：-2.9284762701969194
PM2.5(0)最好结果：{'pred_time': 2.5768821827929983e-05, 'wall_clock_time': 53.13805031776428, 'metric_for_logging': {'pred_time': 2.5768821827929983e-05}, 'val_loss': 3.9284762701969194, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 47366}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 47366, 'experiment_tag': 'exp', 'time_total_s': 15.54441237449646}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9178650698349508
PM2.5(0)的mse=28.139221976737495
PM2.5(0)的mae=3.94290260752477
PM2.5(0)的mar=0.21757798773387685
总共花费的时间为：68.74
襄阳市
2432A
2433A
2434A
2435A
3396A
3397A
[flaml.automl: 09-18 11:03:53] {2390} INFO - task = regression
[flaml.automl: 09-18 11:03:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:03:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:03:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:03:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:03:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:03:55] {3025} INFO - Estimated sufficient time budget=76180s. Estimated necessary time budget=76s.
[flaml.automl: 09-18 11:03:55] {3072} INFO -  at 1.4s,	estimator xgboost's best error=28.8553,	best estimator xgboost's best error=28.8553
[flaml.automl: 09-18 11:03:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:03:57] {3072} INFO -  at 3.5s,	estimator xgboost's best error=13.4424,	best estimator xgboost's best error=13.4424
[flaml.automl: 09-18 11:03:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:03:58] {3072} INFO -  at 4.7s,	estimator xgboost's best error=13.4424,	best estimator xgboost's best error=13.4424
[flaml.automl: 09-18 11:03:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:04:02] {3072} INFO -  at 9.0s,	estimator xgboost's best error=13.4424,	best estimator xgboost's best error=13.4424
[flaml.automl: 09-18 11:04:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:04:03] {3072} INFO -  at 10.1s,	estimator xgboost's best error=9.2586,	best estimator xgboost's best error=9.2586
[flaml.automl: 09-18 11:04:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:04:05] {3072} INFO -  at 11.7s,	estimator xgboost's best error=9.2586,	best estimator xgboost's best error=9.2586
[flaml.automl: 09-18 11:04:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:04:07] {3072} INFO -  at 13.4s,	estimator xgboost's best error=5.7111,	best estimator xgboost's best error=5.7111
[flaml.automl: 09-18 11:04:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:04:09] {3072} INFO -  at 16.0s,	estimator xgboost's best error=5.7111,	best estimator xgboost's best error=5.7111
[flaml.automl: 09-18 11:04:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:04:11] {3072} INFO -  at 17.7s,	estimator xgboost's best error=5.7111,	best estimator xgboost's best error=5.7111
[flaml.automl: 09-18 11:04:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:04:14] {3072} INFO -  at 20.7s,	estimator xgboost's best error=5.7111,	best estimator xgboost's best error=5.7111
[flaml.automl: 09-18 11:04:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:04:15] {3072} INFO -  at 22.1s,	estimator xgboost's best error=5.7111,	best estimator xgboost's best error=5.7111
[flaml.automl: 09-18 11:04:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:04:17] {3072} INFO -  at 23.9s,	estimator xgboost's best error=5.7111,	best estimator xgboost's best error=5.7111
[flaml.automl: 09-18 11:04:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:04:18] {3072} INFO -  at 25.1s,	estimator xgboost's best error=5.7111,	best estimator xgboost's best error=5.7111
[flaml.automl: 09-18 11:04:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:04:25] {3072} INFO -  at 32.1s,	estimator xgboost's best error=4.6232,	best estimator xgboost's best error=4.6232
[flaml.automl: 09-18 11:04:25] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 11:04:38] {3072} INFO -  at 44.9s,	estimator xgboost's best error=4.4605,	best estimator xgboost's best error=4.4605
[flaml.automl: 09-18 11:04:38] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 11:04:45] {3072} INFO -  at 51.9s,	estimator xgboost's best error=4.4605,	best estimator xgboost's best error=4.4605
[flaml.automl: 09-18 11:04:58] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 11:04:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:04:58] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:04:58] {2637} INFO - Time taken to find the best model: 44.8612163066864
[flaml.automl: 09-18 11:04:58] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64141}
PM2.5(0)最佳损失：-3.4604763961597698
PM2.5(0)最好结果：{'pred_time': 5.962371290926012e-06, 'wall_clock_time': 44.8612163066864, 'metric_for_logging': {'pred_time': 5.962371290926012e-06}, 'val_loss': 4.46047639615977, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 64141}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 64141, 'experiment_tag': 'exp', 'time_total_s': 12.781892538070679}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9534385958343832
PM2.5(0)的mse=46.45197905663627
PM2.5(0)的mae=4.596144444940367
PM2.5(0)的mar=0.14376117287796814
总共花费的时间为：65.61
鄂州市
2436A
2437A
[flaml.automl: 09-18 11:11:00] {2390} INFO - task = regression
[flaml.automl: 09-18 11:11:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:11:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:11:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:11:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:11:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:11:01] {3025} INFO - Estimated sufficient time budget=12210s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 11:11:01] {3072} INFO -  at 1.3s,	estimator xgboost's best error=21.1287,	best estimator xgboost's best error=21.1287
[flaml.automl: 09-18 11:11:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:11:03] {3072} INFO -  at 3.4s,	estimator xgboost's best error=10.1199,	best estimator xgboost's best error=10.1199
[flaml.automl: 09-18 11:11:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:11:05] {3072} INFO -  at 4.6s,	estimator xgboost's best error=10.1199,	best estimator xgboost's best error=10.1199
[flaml.automl: 09-18 11:11:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:11:14] {3072} INFO -  at 14.1s,	estimator xgboost's best error=10.1199,	best estimator xgboost's best error=10.1199
[flaml.automl: 09-18 11:11:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:11:15] {3072} INFO -  at 15.2s,	estimator xgboost's best error=6.7213,	best estimator xgboost's best error=6.7213
[flaml.automl: 09-18 11:11:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:11:17] {3072} INFO -  at 16.8s,	estimator xgboost's best error=5.7872,	best estimator xgboost's best error=5.7872
[flaml.automl: 09-18 11:11:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:11:18] {3072} INFO -  at 18.4s,	estimator xgboost's best error=5.2654,	best estimator xgboost's best error=5.2654
[flaml.automl: 09-18 11:11:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:11:21] {3072} INFO -  at 21.1s,	estimator xgboost's best error=5.2654,	best estimator xgboost's best error=5.2654
[flaml.automl: 09-18 11:11:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:11:23] {3072} INFO -  at 22.8s,	estimator xgboost's best error=5.0298,	best estimator xgboost's best error=5.0298
[flaml.automl: 09-18 11:11:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:11:26] {3072} INFO -  at 25.8s,	estimator xgboost's best error=4.8405,	best estimator xgboost's best error=4.8405
[flaml.automl: 09-18 11:11:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:11:27] {3072} INFO -  at 26.9s,	estimator xgboost's best error=4.8405,	best estimator xgboost's best error=4.8405
[flaml.automl: 09-18 11:11:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:11:39] {3072} INFO -  at 38.8s,	estimator xgboost's best error=3.9415,	best estimator xgboost's best error=3.9415
[flaml.automl: 09-18 11:11:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:12:00] {3072} INFO -  at 59.9s,	estimator xgboost's best error=3.8357,	best estimator xgboost's best error=3.8357
[flaml.automl: 09-18 11:12:22] {3335} INFO - retrain xgboost for 22.0s
[flaml.automl: 09-18 11:12:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 11:12:22] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:12:22] {2637} INFO - Time taken to find the best model: 59.908291816711426
[flaml.automl: 09-18 11:12:22] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}
PM2.5(0)最佳损失：-2.835718432234048
PM2.5(0)最好结果：{'pred_time': 1.7772072700223807e-05, 'wall_clock_time': 59.908291816711426, 'metric_for_logging': {'pred_time': 1.7772072700223807e-05}, 'val_loss': 3.835718432234048, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.006967902231194494, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7520708370056695, 'config/colsample_bylevel': 0.948506134666318, 'config/colsample_bytree': 0.8223167301060249, 'config/reg_alpha': 0.0038027739156058313, 'config/reg_lambda': 0.5403518701157697, 'experiment_tag': 'exp', 'time_total_s': 21.14927864074707}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9403179839777689
PM2.5(0)的mse=26.13905495965967
PM2.5(0)的mae=3.7208901579811084
PM2.5(0)的mar=0.17913106358071113
总共花费的时间为：82.30
荆门市
2439A
2440A
2441A
3547A
[flaml.automl: 09-18 11:24:03] {2390} INFO - task = regression
[flaml.automl: 09-18 11:24:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:24:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:24:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:24:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:24:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:24:05] {3025} INFO - Estimated sufficient time budget=50363s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 11:24:05] {3072} INFO -  at 1.4s,	estimator xgboost's best error=25.9000,	best estimator xgboost's best error=25.9000
[flaml.automl: 09-18 11:24:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:24:07] {3072} INFO -  at 3.4s,	estimator xgboost's best error=11.9890,	best estimator xgboost's best error=11.9890
[flaml.automl: 09-18 11:24:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:24:08] {3072} INFO -  at 4.7s,	estimator xgboost's best error=11.9890,	best estimator xgboost's best error=11.9890
[flaml.automl: 09-18 11:24:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:24:14] {3072} INFO -  at 11.0s,	estimator xgboost's best error=11.9890,	best estimator xgboost's best error=11.9890
[flaml.automl: 09-18 11:24:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:24:15] {3072} INFO -  at 12.1s,	estimator xgboost's best error=7.6899,	best estimator xgboost's best error=7.6899
[flaml.automl: 09-18 11:24:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:24:17] {3072} INFO -  at 13.7s,	estimator xgboost's best error=7.6899,	best estimator xgboost's best error=7.6899
[flaml.automl: 09-18 11:24:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:24:19] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.9979,	best estimator xgboost's best error=4.9979
[flaml.automl: 09-18 11:24:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:24:21] {3072} INFO -  at 18.1s,	estimator xgboost's best error=4.9979,	best estimator xgboost's best error=4.9979
[flaml.automl: 09-18 11:24:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:24:23] {3072} INFO -  at 19.7s,	estimator xgboost's best error=4.9979,	best estimator xgboost's best error=4.9979
[flaml.automl: 09-18 11:24:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:24:26] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.9979,	best estimator xgboost's best error=4.9979
[flaml.automl: 09-18 11:24:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:24:27] {3072} INFO -  at 24.1s,	estimator xgboost's best error=4.9979,	best estimator xgboost's best error=4.9979
[flaml.automl: 09-18 11:24:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:24:29] {3072} INFO -  at 25.9s,	estimator xgboost's best error=4.9979,	best estimator xgboost's best error=4.9979
[flaml.automl: 09-18 11:24:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:24:30] {3072} INFO -  at 27.0s,	estimator xgboost's best error=4.9979,	best estimator xgboost's best error=4.9979
[flaml.automl: 09-18 11:24:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:24:37] {3072} INFO -  at 34.1s,	estimator xgboost's best error=4.0874,	best estimator xgboost's best error=4.0874
[flaml.automl: 09-18 11:24:37] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 11:24:50] {3072} INFO -  at 46.9s,	estimator xgboost's best error=3.8592,	best estimator xgboost's best error=3.8592
[flaml.automl: 09-18 11:24:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 11:24:57] {3072} INFO -  at 54.0s,	estimator xgboost's best error=3.8592,	best estimator xgboost's best error=3.8592
[flaml.automl: 09-18 11:25:10] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 11:25:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:25:10] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:25:10] {2637} INFO - Time taken to find the best model: 46.89969205856323
[flaml.automl: 09-18 11:25:10] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 42540}
PM2.5(0)最佳损失：-2.859183339959652
PM2.5(0)最好结果：{'pred_time': 8.617013358706674e-06, 'wall_clock_time': 46.89969205856323, 'metric_for_logging': {'pred_time': 8.617013358706674e-06}, 'val_loss': 3.859183339959652, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 42540}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 42540, 'experiment_tag': 'exp', 'time_total_s': 12.814987659454346}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9503953873781978
PM2.5(0)的mse=40.09749321503957
PM2.5(0)的mae=4.152695646652808
PM2.5(0)的mar=0.13604042262438856
总共花费的时间为：67.42
孝感市
2443A
2444A
[flaml.automl: 09-18 11:31:42] {2390} INFO - task = regression
[flaml.automl: 09-18 11:31:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:31:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:31:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:31:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:31:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:31:44] {3025} INFO - Estimated sufficient time budget=22708s. Estimated necessary time budget=23s.
[flaml.automl: 09-18 11:31:44] {3072} INFO -  at 2.4s,	estimator xgboost's best error=21.6111,	best estimator xgboost's best error=21.6111
[flaml.automl: 09-18 11:31:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:31:48] {3072} INFO -  at 6.4s,	estimator xgboost's best error=10.4322,	best estimator xgboost's best error=10.4322
[flaml.automl: 09-18 11:31:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:31:50] {3072} INFO -  at 8.6s,	estimator xgboost's best error=10.4322,	best estimator xgboost's best error=10.4322
[flaml.automl: 09-18 11:31:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:32:07] {3072} INFO -  at 25.0s,	estimator xgboost's best error=10.4322,	best estimator xgboost's best error=10.4322
[flaml.automl: 09-18 11:32:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:32:09] {3072} INFO -  at 27.1s,	estimator xgboost's best error=7.1362,	best estimator xgboost's best error=7.1362
[flaml.automl: 09-18 11:32:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:32:12] {3072} INFO -  at 29.8s,	estimator xgboost's best error=7.1362,	best estimator xgboost's best error=7.1362
[flaml.automl: 09-18 11:32:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:32:15] {3072} INFO -  at 32.9s,	estimator xgboost's best error=5.1396,	best estimator xgboost's best error=5.1396
[flaml.automl: 09-18 11:32:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:32:19] {3072} INFO -  at 37.5s,	estimator xgboost's best error=5.1396,	best estimator xgboost's best error=5.1396
[flaml.automl: 09-18 11:32:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:32:21] {3072} INFO -  at 39.3s,	estimator xgboost's best error=5.1396,	best estimator xgboost's best error=5.1396
[flaml.automl: 09-18 11:32:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:32:24] {3072} INFO -  at 42.3s,	estimator xgboost's best error=5.1396,	best estimator xgboost's best error=5.1396
[flaml.automl: 09-18 11:32:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:32:26] {3072} INFO -  at 44.0s,	estimator xgboost's best error=4.8793,	best estimator xgboost's best error=4.8793
[flaml.automl: 09-18 11:32:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:32:27] {3072} INFO -  at 45.2s,	estimator xgboost's best error=4.8793,	best estimator xgboost's best error=4.8793
[flaml.automl: 09-18 11:32:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:32:33] {3072} INFO -  at 51.7s,	estimator xgboost's best error=4.3931,	best estimator xgboost's best error=4.3931
[flaml.automl: 09-18 11:32:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:32:41] {3072} INFO -  at 59.6s,	estimator xgboost's best error=4.3488,	best estimator xgboost's best error=4.3488
[flaml.automl: 09-18 11:32:52] {3335} INFO - retrain xgboost for 11.1s
[flaml.automl: 09-18 11:32:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:32:52] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:32:52] {2637} INFO - Time taken to find the best model: 59.551421880722046
[flaml.automl: 09-18 11:32:52] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-3.348830132565583
PM2.5(0)最好结果：{'pred_time': 1.6024375464193632e-05, 'wall_clock_time': 59.551421880722046, 'metric_for_logging': {'pred_time': 1.6024375464193632e-05}, 'val_loss': 4.348830132565583, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 7.887096643447876}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9420512126029639
PM2.5(0)的mse=31.07799913359981
PM2.5(0)的mae=4.2034101780099125
PM2.5(0)的mar=0.2300145484966888
总共花费的时间为：71.06
黄冈市
2929A
3398A
[flaml.automl: 09-18 11:38:51] {2390} INFO - task = regression
[flaml.automl: 09-18 11:38:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:38:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:38:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:38:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:38:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:38:52] {3025} INFO - Estimated sufficient time budget=12114s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 11:38:52] {3072} INFO -  at 1.3s,	estimator xgboost's best error=18.2333,	best estimator xgboost's best error=18.2333
[flaml.automl: 09-18 11:38:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:38:55] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.6462,	best estimator xgboost's best error=8.6462
[flaml.automl: 09-18 11:38:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:38:56] {3072} INFO -  at 4.6s,	estimator xgboost's best error=8.6462,	best estimator xgboost's best error=8.6462
[flaml.automl: 09-18 11:38:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:39:05] {3072} INFO -  at 14.1s,	estimator xgboost's best error=8.6462,	best estimator xgboost's best error=8.6462
[flaml.automl: 09-18 11:39:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:39:06] {3072} INFO -  at 15.2s,	estimator xgboost's best error=5.8604,	best estimator xgboost's best error=5.8604
[flaml.automl: 09-18 11:39:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:39:08] {3072} INFO -  at 16.8s,	estimator xgboost's best error=5.8604,	best estimator xgboost's best error=5.8604
[flaml.automl: 09-18 11:39:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:39:10] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.8244,	best estimator xgboost's best error=3.8244
[flaml.automl: 09-18 11:39:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:39:12] {3072} INFO -  at 21.2s,	estimator xgboost's best error=3.8244,	best estimator xgboost's best error=3.8244
[flaml.automl: 09-18 11:39:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:39:14] {3072} INFO -  at 22.9s,	estimator xgboost's best error=3.8244,	best estimator xgboost's best error=3.8244
[flaml.automl: 09-18 11:39:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:39:17] {3072} INFO -  at 25.9s,	estimator xgboost's best error=3.8244,	best estimator xgboost's best error=3.8244
[flaml.automl: 09-18 11:39:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:39:19] {3072} INFO -  at 27.6s,	estimator xgboost's best error=3.6603,	best estimator xgboost's best error=3.6603
[flaml.automl: 09-18 11:39:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:39:20] {3072} INFO -  at 28.8s,	estimator xgboost's best error=3.6603,	best estimator xgboost's best error=3.6603
[flaml.automl: 09-18 11:39:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:39:26] {3072} INFO -  at 35.3s,	estimator xgboost's best error=3.1232,	best estimator xgboost's best error=3.1232
[flaml.automl: 09-18 11:39:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:39:38] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.0484,	best estimator xgboost's best error=3.0484
[flaml.automl: 09-18 11:39:38] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 11:39:44] {3072} INFO -  at 52.9s,	estimator xgboost's best error=3.0484,	best estimator xgboost's best error=3.0484
[flaml.automl: 09-18 11:39:55] {3335} INFO - retrain xgboost for 11.1s
[flaml.automl: 09-18 11:39:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:39:55] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:39:55] {2637} INFO - Time taken to find the best model: 46.42641282081604
[flaml.automl: 09-18 11:39:55] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-2.0484366569218424
PM2.5(0)最好结果：{'pred_time': 1.655936191705983e-05, 'wall_clock_time': 46.42641282081604, 'metric_for_logging': {'pred_time': 1.655936191705983e-05}, 'val_loss': 3.0484366569218424, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 11.131808042526245}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9421884289743685
PM2.5(0)的mse=19.396378935507745
PM2.5(0)的mae=3.089290612227394
PM2.5(0)的mar=0.18201363644763113
总共花费的时间为：64.41
咸宁市
2447A
2448A
2449A
[flaml.automl: 09-18 11:49:30] {2390} INFO - task = regression
[flaml.automl: 09-18 11:49:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:49:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:49:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:49:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:49:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:49:32] {3025} INFO - Estimated sufficient time budget=20277s. Estimated necessary time budget=20s.
[flaml.automl: 09-18 11:49:32] {3072} INFO -  at 2.2s,	estimator xgboost's best error=15.6803,	best estimator xgboost's best error=15.6803
[flaml.automl: 09-18 11:49:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:49:36] {3072} INFO -  at 5.6s,	estimator xgboost's best error=7.2330,	best estimator xgboost's best error=7.2330
[flaml.automl: 09-18 11:49:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:49:38] {3072} INFO -  at 7.6s,	estimator xgboost's best error=7.2330,	best estimator xgboost's best error=7.2330
[flaml.automl: 09-18 11:49:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:49:55] {3072} INFO -  at 25.0s,	estimator xgboost's best error=7.2330,	best estimator xgboost's best error=7.2330
[flaml.automl: 09-18 11:49:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:49:57] {3072} INFO -  at 26.8s,	estimator xgboost's best error=4.8085,	best estimator xgboost's best error=4.8085
[flaml.automl: 09-18 11:49:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:49:59] {3072} INFO -  at 29.3s,	estimator xgboost's best error=4.0418,	best estimator xgboost's best error=4.0418
[flaml.automl: 09-18 11:49:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:50:02] {3072} INFO -  at 32.1s,	estimator xgboost's best error=3.5762,	best estimator xgboost's best error=3.5762
[flaml.automl: 09-18 11:50:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:50:06] {3072} INFO -  at 36.5s,	estimator xgboost's best error=3.5762,	best estimator xgboost's best error=3.5762
[flaml.automl: 09-18 11:50:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:50:09] {3072} INFO -  at 39.2s,	estimator xgboost's best error=3.5762,	best estimator xgboost's best error=3.5762
[flaml.automl: 09-18 11:50:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:50:15] {3072} INFO -  at 44.8s,	estimator xgboost's best error=2.9710,	best estimator xgboost's best error=2.9710
[flaml.automl: 09-18 11:50:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:50:18] {3072} INFO -  at 47.7s,	estimator xgboost's best error=2.9710,	best estimator xgboost's best error=2.9710
[flaml.automl: 09-18 11:50:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:50:20] {3072} INFO -  at 49.8s,	estimator xgboost's best error=2.9710,	best estimator xgboost's best error=2.9710
[flaml.automl: 09-18 11:50:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:50:29] {3072} INFO -  at 58.9s,	estimator xgboost's best error=2.6547,	best estimator xgboost's best error=2.6547
[flaml.automl: 09-18 11:50:43] {3335} INFO - retrain xgboost for 13.7s
[flaml.automl: 09-18 11:50:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 11:50:43] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:50:43] {2637} INFO - Time taken to find the best model: 58.902509450912476
[flaml.automl: 09-18 11:50:43] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-1.6547471233742441
PM2.5(0)最好结果：{'pred_time': 1.1438483557205944e-05, 'wall_clock_time': 58.902509450912476, 'metric_for_logging': {'pred_time': 1.1438483557205944e-05}, 'val_loss': 2.654747123374244, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 9.111671447753906}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9572971730962595
PM2.5(0)的mse=10.194137818774538
PM2.5(0)的mae=2.449306082897846
PM2.5(0)的mar=0.14333030816204687
总共花费的时间为：73.22
随州市
2451A
2452A
2453A
[flaml.automl: 09-18 11:59:43] {2390} INFO - task = regression
[flaml.automl: 09-18 11:59:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:59:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:59:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:59:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:59:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:59:44] {3025} INFO - Estimated sufficient time budget=11901s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 11:59:44] {3072} INFO -  at 1.4s,	estimator xgboost's best error=20.8786,	best estimator xgboost's best error=20.8786
[flaml.automl: 09-18 11:59:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:59:46] {3072} INFO -  at 3.4s,	estimator xgboost's best error=9.9768,	best estimator xgboost's best error=9.9768
[flaml.automl: 09-18 11:59:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:59:48] {3072} INFO -  at 4.6s,	estimator xgboost's best error=9.9768,	best estimator xgboost's best error=9.9768
[flaml.automl: 09-18 11:59:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:59:57] {3072} INFO -  at 14.5s,	estimator xgboost's best error=9.9768,	best estimator xgboost's best error=9.9768
[flaml.automl: 09-18 11:59:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:59:59] {3072} INFO -  at 15.6s,	estimator xgboost's best error=6.8132,	best estimator xgboost's best error=6.8132
[flaml.automl: 09-18 11:59:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:00:00] {3072} INFO -  at 17.2s,	estimator xgboost's best error=6.8132,	best estimator xgboost's best error=6.8132
[flaml.automl: 09-18 12:00:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:00:02] {3072} INFO -  at 18.9s,	estimator xgboost's best error=4.7535,	best estimator xgboost's best error=4.7535
[flaml.automl: 09-18 12:00:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:00:05] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.7535,	best estimator xgboost's best error=4.7535
[flaml.automl: 09-18 12:00:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:00:06] {3072} INFO -  at 23.3s,	estimator xgboost's best error=4.7535,	best estimator xgboost's best error=4.7535
[flaml.automl: 09-18 12:00:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:00:09] {3072} INFO -  at 26.3s,	estimator xgboost's best error=4.7535,	best estimator xgboost's best error=4.7535
[flaml.automl: 09-18 12:00:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:00:11] {3072} INFO -  at 27.8s,	estimator xgboost's best error=4.7476,	best estimator xgboost's best error=4.7476
[flaml.automl: 09-18 12:00:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:00:12] {3072} INFO -  at 28.9s,	estimator xgboost's best error=4.7476,	best estimator xgboost's best error=4.7476
[flaml.automl: 09-18 12:00:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:00:18] {3072} INFO -  at 34.9s,	estimator xgboost's best error=3.9817,	best estimator xgboost's best error=3.9817
[flaml.automl: 09-18 12:00:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:00:29] {3072} INFO -  at 45.6s,	estimator xgboost's best error=3.9349,	best estimator xgboost's best error=3.9349
[flaml.automl: 09-18 12:00:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 12:00:34] {3072} INFO -  at 51.5s,	estimator xgboost's best error=3.9349,	best estimator xgboost's best error=3.9349
[flaml.automl: 09-18 12:00:45] {3335} INFO - retrain xgboost for 10.6s
[flaml.automl: 09-18 12:00:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 12:00:45] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:00:45] {2637} INFO - Time taken to find the best model: 45.581669330596924
[flaml.automl: 09-18 12:00:45] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-2.9349054666625993
PM2.5(0)最好结果：{'pred_time': 1.0767405677451071e-05, 'wall_clock_time': 45.581669330596924, 'metric_for_logging': {'pred_time': 1.0767405677451071e-05}, 'val_loss': 3.9349054666625993, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 10.702521562576294}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9381123610979596
PM2.5(0)的mse=30.40429476326959
PM2.5(0)的mae=3.898089836063926
PM2.5(0)的mar=0.19827312902855632
总共花费的时间为：62.62
恩施土家族苗族自治州
2454A
2455A
3549A
[flaml.automl: 09-18 12:10:05] {2390} INFO - task = regression
[flaml.automl: 09-18 12:10:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:10:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:10:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:10:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:10:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:10:08] {3025} INFO - Estimated sufficient time budget=30675s. Estimated necessary time budget=31s.
[flaml.automl: 09-18 12:10:08] {3072} INFO -  at 3.2s,	estimator xgboost's best error=13.6810,	best estimator xgboost's best error=13.6810
[flaml.automl: 09-18 12:10:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:10:12] {3072} INFO -  at 7.4s,	estimator xgboost's best error=6.6178,	best estimator xgboost's best error=6.6178
[flaml.automl: 09-18 12:10:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:10:14] {3072} INFO -  at 9.5s,	estimator xgboost's best error=6.6178,	best estimator xgboost's best error=6.6178
[flaml.automl: 09-18 12:10:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:10:35] {3072} INFO -  at 30.5s,	estimator xgboost's best error=6.6178,	best estimator xgboost's best error=6.6178
[flaml.automl: 09-18 12:10:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:10:37] {3072} INFO -  at 32.1s,	estimator xgboost's best error=4.7407,	best estimator xgboost's best error=4.7407
[flaml.automl: 09-18 12:10:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:10:39] {3072} INFO -  at 34.5s,	estimator xgboost's best error=4.0447,	best estimator xgboost's best error=4.0447
[flaml.automl: 09-18 12:10:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:10:42] {3072} INFO -  at 37.0s,	estimator xgboost's best error=3.8006,	best estimator xgboost's best error=3.8006
[flaml.automl: 09-18 12:10:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:10:47] {3072} INFO -  at 41.9s,	estimator xgboost's best error=3.8006,	best estimator xgboost's best error=3.8006
[flaml.automl: 09-18 12:10:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:10:50] {3072} INFO -  at 44.8s,	estimator xgboost's best error=3.8006,	best estimator xgboost's best error=3.8006
[flaml.automl: 09-18 12:10:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:10:55] {3072} INFO -  at 50.0s,	estimator xgboost's best error=3.3322,	best estimator xgboost's best error=3.3322
[flaml.automl: 09-18 12:10:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:10:58] {3072} INFO -  at 52.9s,	estimator xgboost's best error=3.3322,	best estimator xgboost's best error=3.3322
[flaml.automl: 09-18 12:10:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:11:00] {3072} INFO -  at 54.8s,	estimator xgboost's best error=3.3322,	best estimator xgboost's best error=3.3322
[flaml.automl: 09-18 12:11:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:11:04] {3072} INFO -  at 59.5s,	estimator xgboost's best error=3.2790,	best estimator xgboost's best error=3.2790
[flaml.automl: 09-18 12:11:24] {3335} INFO - retrain xgboost for 19.4s
[flaml.automl: 09-18 12:11:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 12:11:24] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:11:24] {2637} INFO - Time taken to find the best model: 59.50327706336975
[flaml.automl: 09-18 12:11:24] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-2.279038235258837
PM2.5(0)最好结果：{'pred_time': 1.327024134245798e-05, 'wall_clock_time': 59.50327706336975, 'metric_for_logging': {'pred_time': 1.327024134245798e-05}, 'val_loss': 3.279038235258837, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 4.6941423416137695}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9315345369112031
PM2.5(0)的mse=16.982812572135337
PM2.5(0)的mae=2.881520822621826
PM2.5(0)的mar=0.2227685373626529
总共花费的时间为：79.59
衡阳市
2456A
2457A
2458A
2459A
2460A
2461A
3399A
[flaml.automl: 09-18 12:33:18] {2390} INFO - task = regression
[flaml.automl: 09-18 12:33:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:33:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:33:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:33:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:33:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:33:21] {3025} INFO - Estimated sufficient time budget=230462s. Estimated necessary time budget=230s.
[flaml.automl: 09-18 12:33:21] {3072} INFO -  at 3.5s,	estimator xgboost's best error=19.4522,	best estimator xgboost's best error=19.4522
[flaml.automl: 09-18 12:33:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:33:24] {3072} INFO -  at 6.6s,	estimator xgboost's best error=16.0827,	best estimator xgboost's best error=16.0827
[flaml.automl: 09-18 12:33:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:33:27] {3072} INFO -  at 9.8s,	estimator xgboost's best error=16.0827,	best estimator xgboost's best error=16.0827
[flaml.automl: 09-18 12:33:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:33:29] {3072} INFO -  at 11.7s,	estimator xgboost's best error=16.0827,	best estimator xgboost's best error=16.0827
[flaml.automl: 09-18 12:33:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:33:31] {3072} INFO -  at 14.1s,	estimator xgboost's best error=8.4829,	best estimator xgboost's best error=8.4829
[flaml.automl: 09-18 12:33:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:33:34] {3072} INFO -  at 16.6s,	estimator xgboost's best error=8.3102,	best estimator xgboost's best error=8.3102
[flaml.automl: 09-18 12:33:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:33:36] {3072} INFO -  at 19.0s,	estimator xgboost's best error=5.5095,	best estimator xgboost's best error=5.5095
[flaml.automl: 09-18 12:33:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:33:39] {3072} INFO -  at 21.6s,	estimator xgboost's best error=5.5095,	best estimator xgboost's best error=5.5095
[flaml.automl: 09-18 12:33:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:33:41] {3072} INFO -  at 23.6s,	estimator xgboost's best error=5.5095,	best estimator xgboost's best error=5.5095
[flaml.automl: 09-18 12:33:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:33:43] {3072} INFO -  at 25.9s,	estimator xgboost's best error=5.5095,	best estimator xgboost's best error=5.5095
[flaml.automl: 09-18 12:33:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:33:45] {3072} INFO -  at 28.1s,	estimator xgboost's best error=5.5095,	best estimator xgboost's best error=5.5095
[flaml.automl: 09-18 12:33:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:33:48] {3072} INFO -  at 31.2s,	estimator xgboost's best error=5.0528,	best estimator xgboost's best error=5.0528
[flaml.automl: 09-18 12:33:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:33:50] {3072} INFO -  at 32.9s,	estimator xgboost's best error=5.0528,	best estimator xgboost's best error=5.0528
[flaml.automl: 09-18 12:33:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:33:57] {3072} INFO -  at 39.8s,	estimator xgboost's best error=3.8841,	best estimator xgboost's best error=3.8841
[flaml.automl: 09-18 12:33:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 12:34:10] {3072} INFO -  at 52.6s,	estimator xgboost's best error=3.8042,	best estimator xgboost's best error=3.8042
[flaml.automl: 09-18 12:34:23] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 12:34:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 12:34:23] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:34:23] {2637} INFO - Time taken to find the best model: 52.58063888549805
[flaml.automl: 09-18 12:34:23] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 73688}
PM2.5(0)最佳损失：-2.804183908523905
PM2.5(0)最好结果：{'pred_time': 5.144139994745669e-06, 'wall_clock_time': 52.58063888549805, 'metric_for_logging': {'pred_time': 5.144139994745669e-06}, 'val_loss': 3.804183908523905, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 73688}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 73688, 'experiment_tag': 'exp', 'time_total_s': 12.773732900619507}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.920543809287756
PM2.5(0)的mse=33.5183480518044
PM2.5(0)的mae=3.8741874596553205
PM2.5(0)的mar=0.1684118662732247
总共花费的时间为：66.65
邵阳市
2462A
2463A
2464A
2465A
2466A
[flaml.automl: 09-18 12:50:29] {2390} INFO - task = regression
[flaml.automl: 09-18 12:50:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:50:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:50:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:50:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:50:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:50:33] {3025} INFO - Estimated sufficient time budget=228629s. Estimated necessary time budget=229s.
[flaml.automl: 09-18 12:50:33] {3072} INFO -  at 5.0s,	estimator xgboost's best error=20.2683,	best estimator xgboost's best error=20.2683
[flaml.automl: 09-18 12:50:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:50:38] {3072} INFO -  at 9.3s,	estimator xgboost's best error=16.7178,	best estimator xgboost's best error=16.7178
[flaml.automl: 09-18 12:50:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:50:42] {3072} INFO -  at 13.5s,	estimator xgboost's best error=16.7178,	best estimator xgboost's best error=16.7178
[flaml.automl: 09-18 12:50:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:50:45] {3072} INFO -  at 16.8s,	estimator xgboost's best error=16.7178,	best estimator xgboost's best error=16.7178
[flaml.automl: 09-18 12:50:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:50:48] {3072} INFO -  at 19.9s,	estimator xgboost's best error=6.0488,	best estimator xgboost's best error=6.0488
[flaml.automl: 09-18 12:50:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:50:51] {3072} INFO -  at 23.2s,	estimator xgboost's best error=6.0488,	best estimator xgboost's best error=6.0488
[flaml.automl: 09-18 12:50:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:50:55] {3072} INFO -  at 26.5s,	estimator xgboost's best error=5.9900,	best estimator xgboost's best error=5.9900
[flaml.automl: 09-18 12:50:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:50:57] {3072} INFO -  at 29.2s,	estimator xgboost's best error=5.9900,	best estimator xgboost's best error=5.9900
[flaml.automl: 09-18 12:50:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:51:00] {3072} INFO -  at 32.0s,	estimator xgboost's best error=5.2936,	best estimator xgboost's best error=5.2936
[flaml.automl: 09-18 12:51:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:51:03] {3072} INFO -  at 34.6s,	estimator xgboost's best error=5.0112,	best estimator xgboost's best error=5.0112
[flaml.automl: 09-18 12:51:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:51:05] {3072} INFO -  at 36.6s,	estimator xgboost's best error=5.0112,	best estimator xgboost's best error=5.0112
[flaml.automl: 09-18 12:51:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:51:07] {3072} INFO -  at 38.6s,	estimator xgboost's best error=3.7241,	best estimator xgboost's best error=3.7241
[flaml.automl: 09-18 12:51:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:51:08] {3072} INFO -  at 40.2s,	estimator xgboost's best error=3.7241,	best estimator xgboost's best error=3.7241
[flaml.automl: 09-18 12:51:08] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:51:11] {3072} INFO -  at 42.7s,	estimator xgboost's best error=3.7241,	best estimator xgboost's best error=3.7241
[flaml.automl: 09-18 12:51:11] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 12:51:14] {3072} INFO -  at 45.9s,	estimator xgboost's best error=3.7241,	best estimator xgboost's best error=3.7241
[flaml.automl: 09-18 12:51:14] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 12:51:15] {3072} INFO -  at 47.2s,	estimator xgboost's best error=3.7241,	best estimator xgboost's best error=3.7241
[flaml.automl: 09-18 12:51:15] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 12:51:27] {3072} INFO -  at 58.9s,	estimator xgboost's best error=2.9056,	best estimator xgboost's best error=2.9056
[flaml.automl: 09-18 12:51:41] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-18 12:51:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8479900785102675, colsample_bynode=1,
             colsample_bytree=0.8056718854106218, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.023690869250810015,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=0.49559887056368973,
             scale_pos_weight=1, subsample=0.8984298717939952,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 12:51:41] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:51:41] {2637} INFO - Time taken to find the best model: 58.89391279220581
[flaml.automl: 09-18 12:51:41] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.023690869250810015, 'learning_rate': 1.0, 'subsample': 0.8984298717939952, 'colsample_bylevel': 0.8479900785102675, 'colsample_bytree': 0.8056718854106218, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 0.49559887056368973, 'FLAML_sample_size': 50878}
PM2.5(0)最佳损失：-1.9055819046088316
PM2.5(0)最好结果：{'pred_time': 7.167105376193327e-06, 'wall_clock_time': 58.89391279220581, 'metric_for_logging': {'pred_time': 7.167105376193327e-06}, 'val_loss': 2.9055819046088316, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.023690869250810015, 'learning_rate': 1.0, 'subsample': 0.8984298717939952, 'colsample_bylevel': 0.8479900785102675, 'colsample_bytree': 0.8056718854106218, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 0.49559887056368973, 'FLAML_sample_size': 50878}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.023690869250810015, 'config/learning_rate': 1.0, 'config/subsample': 0.8984298717939952, 'config/colsample_bylevel': 0.8479900785102675, 'config/colsample_bytree': 0.8056718854106218, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 0.49559887056368973, 'config/FLAML_sample_size': 50878, 'experiment_tag': 'exp', 'time_total_s': 11.72123670578003}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8479900785102675, colsample_bynode=1,
             colsample_bytree=0.8056718854106218, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.023690869250810015,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=0.49559887056368973,
             scale_pos_weight=1, subsample=0.8984298717939952,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9433275171015245
PM2.5(0)的mse=19.71926964247301
PM2.5(0)的mae=2.99510784114633
PM2.5(0)的mar=0.11999712308656925
总共花费的时间为：73.63
益阳市
2467A
2468A
2469A
2470A
2471A
[flaml.automl: 09-18 13:07:15] {2390} INFO - task = regression
[flaml.automl: 09-18 13:07:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:07:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:07:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:07:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:07:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:07:17] {3025} INFO - Estimated sufficient time budget=115256s. Estimated necessary time budget=115s.
[flaml.automl: 09-18 13:07:17] {3072} INFO -  at 2.5s,	estimator xgboost's best error=20.9717,	best estimator xgboost's best error=20.9717
[flaml.automl: 09-18 13:07:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:07:21] {3072} INFO -  at 6.3s,	estimator xgboost's best error=10.2466,	best estimator xgboost's best error=10.2466
[flaml.automl: 09-18 13:07:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:07:23] {3072} INFO -  at 8.6s,	estimator xgboost's best error=10.2466,	best estimator xgboost's best error=10.2466
[flaml.automl: 09-18 13:07:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:07:27] {3072} INFO -  at 12.6s,	estimator xgboost's best error=10.2466,	best estimator xgboost's best error=10.2466
[flaml.automl: 09-18 13:07:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:07:30] {3072} INFO -  at 14.8s,	estimator xgboost's best error=7.7470,	best estimator xgboost's best error=7.7470
[flaml.automl: 09-18 13:07:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:07:32] {3072} INFO -  at 17.7s,	estimator xgboost's best error=7.7470,	best estimator xgboost's best error=7.7470
[flaml.automl: 09-18 13:07:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:07:36] {3072} INFO -  at 21.0s,	estimator xgboost's best error=5.6632,	best estimator xgboost's best error=5.6632
[flaml.automl: 09-18 13:07:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:07:38] {3072} INFO -  at 23.7s,	estimator xgboost's best error=5.6632,	best estimator xgboost's best error=5.6632
[flaml.automl: 09-18 13:07:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:07:41] {3072} INFO -  at 26.7s,	estimator xgboost's best error=5.6632,	best estimator xgboost's best error=5.6632
[flaml.automl: 09-18 13:07:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:07:44] {3072} INFO -  at 29.2s,	estimator xgboost's best error=5.6632,	best estimator xgboost's best error=5.6632
[flaml.automl: 09-18 13:07:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:07:46] {3072} INFO -  at 31.0s,	estimator xgboost's best error=5.6286,	best estimator xgboost's best error=5.6286
[flaml.automl: 09-18 13:07:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:07:47] {3072} INFO -  at 32.1s,	estimator xgboost's best error=5.6286,	best estimator xgboost's best error=5.6286
[flaml.automl: 09-18 13:07:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:07:49] {3072} INFO -  at 34.4s,	estimator xgboost's best error=5.2836,	best estimator xgboost's best error=5.2836
[flaml.automl: 09-18 13:07:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:07:51] {3072} INFO -  at 36.6s,	estimator xgboost's best error=5.1634,	best estimator xgboost's best error=5.1634
[flaml.automl: 09-18 13:07:51] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:07:53] {3072} INFO -  at 38.4s,	estimator xgboost's best error=5.1634,	best estimator xgboost's best error=5.1634
[flaml.automl: 09-18 13:07:53] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 13:07:55] {3072} INFO -  at 40.4s,	estimator xgboost's best error=5.1634,	best estimator xgboost's best error=5.1634
[flaml.automl: 09-18 13:07:55] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 13:07:56] {3072} INFO -  at 41.8s,	estimator xgboost's best error=5.1634,	best estimator xgboost's best error=5.1634
[flaml.automl: 09-18 13:07:56] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 13:07:58] {3072} INFO -  at 43.1s,	estimator xgboost's best error=5.1634,	best estimator xgboost's best error=5.1634
[flaml.automl: 09-18 13:07:58] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 13:08:14] {3072} INFO -  at 59.5s,	estimator xgboost's best error=4.9736,	best estimator xgboost's best error=4.9736
[flaml.automl: 09-18 13:08:32] {3335} INFO - retrain xgboost for 17.9s
[flaml.automl: 09-18 13:08:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:08:32] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:08:32] {2637} INFO - Time taken to find the best model: 59.4945170879364
[flaml.automl: 09-18 13:08:32] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52124}
PM2.5(0)最佳损失：-3.973575802962424
PM2.5(0)最好结果：{'pred_time': 1.5525569243984328e-05, 'wall_clock_time': 59.4945170879364, 'metric_for_logging': {'pred_time': 1.5525569243984328e-05}, 'val_loss': 4.973575802962424, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52124}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 52124, 'experiment_tag': 'exp', 'time_total_s': 16.396664142608643}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9318115366303399
PM2.5(0)的mse=47.596110256905654
PM2.5(0)的mae=4.9572287117049445
PM2.5(0)的mar=0.2797015722611459
总共花费的时间为：78.37
郴州市
2472A
2473A
2474A
2475A
2476A
[flaml.automl: 09-18 13:24:04] {2390} INFO - task = regression
[flaml.automl: 09-18 13:24:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:24:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:24:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:24:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:24:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:24:05] {3025} INFO - Estimated sufficient time budget=62380s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 13:24:05] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.0735,	best estimator xgboost's best error=15.0735
[flaml.automl: 09-18 13:24:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:24:07] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.2687,	best estimator xgboost's best error=7.2687
[flaml.automl: 09-18 13:24:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:24:09] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.2687,	best estimator xgboost's best error=7.2687
[flaml.automl: 09-18 13:24:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:24:13] {3072} INFO -  at 9.5s,	estimator xgboost's best error=7.2687,	best estimator xgboost's best error=7.2687
[flaml.automl: 09-18 13:24:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:24:15] {3072} INFO -  at 10.6s,	estimator xgboost's best error=5.2612,	best estimator xgboost's best error=5.2612
[flaml.automl: 09-18 13:24:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:24:16] {3072} INFO -  at 12.2s,	estimator xgboost's best error=5.2612,	best estimator xgboost's best error=5.2612
[flaml.automl: 09-18 13:24:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:24:18] {3072} INFO -  at 13.9s,	estimator xgboost's best error=3.8763,	best estimator xgboost's best error=3.8763
[flaml.automl: 09-18 13:24:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:24:21] {3072} INFO -  at 16.6s,	estimator xgboost's best error=3.8763,	best estimator xgboost's best error=3.8763
[flaml.automl: 09-18 13:24:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:24:22] {3072} INFO -  at 18.2s,	estimator xgboost's best error=3.8763,	best estimator xgboost's best error=3.8763
[flaml.automl: 09-18 13:24:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:24:25] {3072} INFO -  at 21.2s,	estimator xgboost's best error=3.8763,	best estimator xgboost's best error=3.8763
[flaml.automl: 09-18 13:24:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:24:27] {3072} INFO -  at 22.6s,	estimator xgboost's best error=3.8763,	best estimator xgboost's best error=3.8763
[flaml.automl: 09-18 13:24:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:24:28] {3072} INFO -  at 24.3s,	estimator xgboost's best error=3.8550,	best estimator xgboost's best error=3.8550
[flaml.automl: 09-18 13:24:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:24:29] {3072} INFO -  at 25.5s,	estimator xgboost's best error=3.8550,	best estimator xgboost's best error=3.8550
[flaml.automl: 09-18 13:24:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:24:36] {3072} INFO -  at 32.5s,	estimator xgboost's best error=3.3798,	best estimator xgboost's best error=3.3798
[flaml.automl: 09-18 13:24:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:24:49] {3072} INFO -  at 45.3s,	estimator xgboost's best error=3.3172,	best estimator xgboost's best error=3.3172
[flaml.automl: 09-18 13:24:49] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 13:24:56] {3072} INFO -  at 52.3s,	estimator xgboost's best error=3.3172,	best estimator xgboost's best error=3.3172
[flaml.automl: 09-18 13:25:09] {3335} INFO - retrain xgboost for 13.1s
[flaml.automl: 09-18 13:25:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:25:09] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:25:09] {2637} INFO - Time taken to find the best model: 45.34829878807068
[flaml.automl: 09-18 13:25:09] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52364}
PM2.5(0)最佳损失：-2.31719725933819
PM2.5(0)最好结果：{'pred_time': 6.885130312418606e-06, 'wall_clock_time': 45.34829878807068, 'metric_for_logging': {'pred_time': 6.885130312418606e-06}, 'val_loss': 3.31719725933819, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52364}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 52364, 'experiment_tag': 'exp', 'time_total_s': 12.81104040145874}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9145306942800787
PM2.5(0)的mse=19.925929699159518
PM2.5(0)的mae=3.366094351220481
PM2.5(0)的mar=0.22511628752364074
总共花费的时间为：66.30
永州市
2477A
2478A
2479A
2480A
2481A
[flaml.automl: 09-18 13:40:21] {2390} INFO - task = regression
[flaml.automl: 09-18 13:40:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:40:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:40:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:40:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:40:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:40:23] {3025} INFO - Estimated sufficient time budget=113317s. Estimated necessary time budget=113s.
[flaml.automl: 09-18 13:40:23] {3072} INFO -  at 2.4s,	estimator xgboost's best error=19.4103,	best estimator xgboost's best error=19.4103
[flaml.automl: 09-18 13:40:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:40:27] {3072} INFO -  at 6.3s,	estimator xgboost's best error=9.1976,	best estimator xgboost's best error=9.1976
[flaml.automl: 09-18 13:40:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:40:29] {3072} INFO -  at 8.5s,	estimator xgboost's best error=9.1976,	best estimator xgboost's best error=9.1976
[flaml.automl: 09-18 13:40:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:40:33] {3072} INFO -  at 12.8s,	estimator xgboost's best error=9.1976,	best estimator xgboost's best error=9.1976
[flaml.automl: 09-18 13:40:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:40:35] {3072} INFO -  at 14.8s,	estimator xgboost's best error=6.2643,	best estimator xgboost's best error=6.2643
[flaml.automl: 09-18 13:40:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:40:38] {3072} INFO -  at 17.8s,	estimator xgboost's best error=6.2643,	best estimator xgboost's best error=6.2643
[flaml.automl: 09-18 13:40:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:40:41] {3072} INFO -  at 20.8s,	estimator xgboost's best error=4.6027,	best estimator xgboost's best error=4.6027
[flaml.automl: 09-18 13:40:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:40:45] {3072} INFO -  at 24.2s,	estimator xgboost's best error=4.6027,	best estimator xgboost's best error=4.6027
[flaml.automl: 09-18 13:40:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:40:48] {3072} INFO -  at 27.2s,	estimator xgboost's best error=4.6027,	best estimator xgboost's best error=4.6027
[flaml.automl: 09-18 13:40:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:40:50] {3072} INFO -  at 29.7s,	estimator xgboost's best error=4.6027,	best estimator xgboost's best error=4.6027
[flaml.automl: 09-18 13:40:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:40:52] {3072} INFO -  at 31.1s,	estimator xgboost's best error=4.6027,	best estimator xgboost's best error=4.6027
[flaml.automl: 09-18 13:40:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:40:53] {3072} INFO -  at 32.8s,	estimator xgboost's best error=4.5924,	best estimator xgboost's best error=4.5924
[flaml.automl: 09-18 13:40:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:40:54] {3072} INFO -  at 34.0s,	estimator xgboost's best error=4.5924,	best estimator xgboost's best error=4.5924
[flaml.automl: 09-18 13:40:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:41:01] {3072} INFO -  at 41.0s,	estimator xgboost's best error=4.0252,	best estimator xgboost's best error=4.0252
[flaml.automl: 09-18 13:41:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:41:14] {3072} INFO -  at 53.8s,	estimator xgboost's best error=3.9783,	best estimator xgboost's best error=3.9783
[flaml.automl: 09-18 13:41:27] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 13:41:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:41:27] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:41:27] {2637} INFO - Time taken to find the best model: 53.83305788040161
[flaml.automl: 09-18 13:41:27] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 51974}
PM2.5(0)最佳损失：-2.9782763352332178
PM2.5(0)最好结果：{'pred_time': 7.031139357265456e-06, 'wall_clock_time': 53.83305788040161, 'metric_for_logging': {'pred_time': 7.031139357265456e-06}, 'val_loss': 3.9782763352332178, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 51974}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 51974, 'experiment_tag': 'exp', 'time_total_s': 12.795032739639282}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9118969820538214
PM2.5(0)的mse=31.763684508036352
PM2.5(0)的mae=4.119154037299029
PM2.5(0)的mar=0.19335645265812826
总共花费的时间为：67.52
怀化市
2482A
2483A
2484A
2485A
2486A
[flaml.automl: 09-18 13:56:21] {2390} INFO - task = regression
[flaml.automl: 09-18 13:56:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:56:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:56:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:56:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:56:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:56:24] {3025} INFO - Estimated sufficient time budget=120697s. Estimated necessary time budget=121s.
[flaml.automl: 09-18 13:56:24] {3072} INFO -  at 2.5s,	estimator xgboost's best error=16.1888,	best estimator xgboost's best error=16.1888
[flaml.automl: 09-18 13:56:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:56:27] {3072} INFO -  at 6.5s,	estimator xgboost's best error=7.6683,	best estimator xgboost's best error=7.6683
[flaml.automl: 09-18 13:56:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:56:30] {3072} INFO -  at 8.7s,	estimator xgboost's best error=7.6683,	best estimator xgboost's best error=7.6683
[flaml.automl: 09-18 13:56:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:56:34] {3072} INFO -  at 12.8s,	estimator xgboost's best error=7.6683,	best estimator xgboost's best error=7.6683
[flaml.automl: 09-18 13:56:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:56:36] {3072} INFO -  at 14.9s,	estimator xgboost's best error=5.4251,	best estimator xgboost's best error=5.4251
[flaml.automl: 09-18 13:56:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:56:39] {3072} INFO -  at 17.8s,	estimator xgboost's best error=4.7119,	best estimator xgboost's best error=4.7119
[flaml.automl: 09-18 13:56:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:56:42] {3072} INFO -  at 20.8s,	estimator xgboost's best error=4.2475,	best estimator xgboost's best error=4.2475
[flaml.automl: 09-18 13:56:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:56:45] {3072} INFO -  at 24.3s,	estimator xgboost's best error=4.2475,	best estimator xgboost's best error=4.2475
[flaml.automl: 09-18 13:56:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:56:48] {3072} INFO -  at 27.2s,	estimator xgboost's best error=4.2475,	best estimator xgboost's best error=4.2475
[flaml.automl: 09-18 13:56:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:56:51] {3072} INFO -  at 29.6s,	estimator xgboost's best error=4.2475,	best estimator xgboost's best error=4.2475
[flaml.automl: 09-18 13:56:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:56:53] {3072} INFO -  at 32.2s,	estimator xgboost's best error=4.2322,	best estimator xgboost's best error=4.2322
[flaml.automl: 09-18 13:56:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:56:55] {3072} INFO -  at 33.5s,	estimator xgboost's best error=4.2322,	best estimator xgboost's best error=4.2322
[flaml.automl: 09-18 13:56:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:56:57] {3072} INFO -  at 35.8s,	estimator xgboost's best error=3.4648,	best estimator xgboost's best error=3.4648
[flaml.automl: 09-18 13:56:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:56:59] {3072} INFO -  at 38.0s,	estimator xgboost's best error=3.4578,	best estimator xgboost's best error=3.4578
[flaml.automl: 09-18 13:56:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:57:01] {3072} INFO -  at 39.8s,	estimator xgboost's best error=3.4578,	best estimator xgboost's best error=3.4578
[flaml.automl: 09-18 13:57:01] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 13:57:02] {3072} INFO -  at 41.3s,	estimator xgboost's best error=3.4578,	best estimator xgboost's best error=3.4578
[flaml.automl: 09-18 13:57:02] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 13:57:04] {3072} INFO -  at 42.9s,	estimator xgboost's best error=3.4578,	best estimator xgboost's best error=3.4578
[flaml.automl: 09-18 13:57:04] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 13:57:05] {3072} INFO -  at 44.3s,	estimator xgboost's best error=3.4578,	best estimator xgboost's best error=3.4578
[flaml.automl: 09-18 13:57:05] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 13:57:16] {3072} INFO -  at 55.0s,	estimator xgboost's best error=3.2657,	best estimator xgboost's best error=3.2657
[flaml.automl: 09-18 13:57:27] {3335} INFO - retrain xgboost for 10.8s
[flaml.automl: 09-18 13:57:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:57:27] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:57:27] {2637} INFO - Time taken to find the best model: 55.022674560546875
[flaml.automl: 09-18 13:57:27] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463, 'FLAML_sample_size': 52368}
PM2.5(0)最佳损失：-2.2656741756039973
PM2.5(0)最好结果：{'pred_time': 6.992969752218453e-06, 'wall_clock_time': 55.022674560546875, 'metric_for_logging': {'pred_time': 6.992969752218453e-06}, 'val_loss': 3.2656741756039973, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463, 'FLAML_sample_size': 52368}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 6.532042959250319, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8835265175628242, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.10065328594609463, 'config/FLAML_sample_size': 52368, 'experiment_tag': 'exp', 'time_total_s': 10.758126497268677}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9164614400267547
PM2.5(0)的mse=20.131133444914077
PM2.5(0)的mae=3.306293660481574
PM2.5(0)的mar=0.1924173750641704
总共花费的时间为：66.74
娄底市
2487A
2488A
2489A
2490A
2491A
[flaml.automl: 09-18 14:12:15] {2390} INFO - task = regression
[flaml.automl: 09-18 14:12:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:12:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:12:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:12:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:12:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:12:17] {3025} INFO - Estimated sufficient time budget=113514s. Estimated necessary time budget=114s.
[flaml.automl: 09-18 14:12:17] {3072} INFO -  at 2.4s,	estimator xgboost's best error=20.7810,	best estimator xgboost's best error=20.7810
[flaml.automl: 09-18 14:12:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:12:21] {3072} INFO -  at 6.3s,	estimator xgboost's best error=10.0758,	best estimator xgboost's best error=10.0758
[flaml.automl: 09-18 14:12:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:12:23] {3072} INFO -  at 8.5s,	estimator xgboost's best error=10.0758,	best estimator xgboost's best error=10.0758
[flaml.automl: 09-18 14:12:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:12:27] {3072} INFO -  at 12.6s,	estimator xgboost's best error=10.0758,	best estimator xgboost's best error=10.0758
[flaml.automl: 09-18 14:12:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:12:29] {3072} INFO -  at 14.7s,	estimator xgboost's best error=7.3154,	best estimator xgboost's best error=7.3154
[flaml.automl: 09-18 14:12:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:12:32] {3072} INFO -  at 17.6s,	estimator xgboost's best error=6.3454,	best estimator xgboost's best error=6.3454
[flaml.automl: 09-18 14:12:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:12:35] {3072} INFO -  at 20.6s,	estimator xgboost's best error=5.8538,	best estimator xgboost's best error=5.8538
[flaml.automl: 09-18 14:12:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:12:39] {3072} INFO -  at 24.0s,	estimator xgboost's best error=5.8538,	best estimator xgboost's best error=5.8538
[flaml.automl: 09-18 14:12:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:12:41] {3072} INFO -  at 26.6s,	estimator xgboost's best error=5.4734,	best estimator xgboost's best error=5.4734
[flaml.automl: 09-18 14:12:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:12:44] {3072} INFO -  at 29.5s,	estimator xgboost's best error=5.4734,	best estimator xgboost's best error=5.4734
[flaml.automl: 09-18 14:12:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:12:46] {3072} INFO -  at 31.0s,	estimator xgboost's best error=5.4734,	best estimator xgboost's best error=5.4734
[flaml.automl: 09-18 14:12:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:12:47] {3072} INFO -  at 32.1s,	estimator xgboost's best error=5.4734,	best estimator xgboost's best error=5.4734
[flaml.automl: 09-18 14:12:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:12:49] {3072} INFO -  at 34.3s,	estimator xgboost's best error=5.1378,	best estimator xgboost's best error=5.1378
[flaml.automl: 09-18 14:12:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:12:51] {3072} INFO -  at 36.5s,	estimator xgboost's best error=5.0028,	best estimator xgboost's best error=5.0028
[flaml.automl: 09-18 14:12:51] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 14:12:53] {3072} INFO -  at 38.2s,	estimator xgboost's best error=5.0028,	best estimator xgboost's best error=5.0028
[flaml.automl: 09-18 14:12:53] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 14:12:54] {3072} INFO -  at 39.8s,	estimator xgboost's best error=5.0028,	best estimator xgboost's best error=5.0028
[flaml.automl: 09-18 14:12:54] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 14:12:56] {3072} INFO -  at 41.6s,	estimator xgboost's best error=5.0028,	best estimator xgboost's best error=5.0028
[flaml.automl: 09-18 14:12:56] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 14:12:58] {3072} INFO -  at 43.2s,	estimator xgboost's best error=5.0028,	best estimator xgboost's best error=5.0028
[flaml.automl: 09-18 14:12:58] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 14:13:11] {3072} INFO -  at 55.9s,	estimator xgboost's best error=4.7374,	best estimator xgboost's best error=4.7374
[flaml.automl: 09-18 14:13:23] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 14:13:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 14:13:23] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:13:23] {2637} INFO - Time taken to find the best model: 55.94168257713318
[flaml.automl: 09-18 14:13:23] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 52094}
PM2.5(0)最佳损失：-3.737435293724797
PM2.5(0)最好结果：{'pred_time': 7.248600234761307e-06, 'wall_clock_time': 55.94168257713318, 'metric_for_logging': {'pred_time': 7.248600234761307e-06}, 'val_loss': 4.737435293724797, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 52094}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'config/FLAML_sample_size': 52094, 'experiment_tag': 'exp', 'time_total_s': 12.720310688018799}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8852730173868284
PM2.5(0)的mse=48.30270690747232
PM2.5(0)的mae=4.916800322780265
PM2.5(0)的mar=0.2424660538332824
总共花费的时间为：69.83
湘西州
2492A
2493A
2494A
[flaml.automl: 09-18 14:22:30] {2390} INFO - task = regression
[flaml.automl: 09-18 14:22:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:22:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:22:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:22:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:22:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:22:34] {3025} INFO - Estimated sufficient time budget=34271s. Estimated necessary time budget=34s.
[flaml.automl: 09-18 14:22:34] {3072} INFO -  at 3.6s,	estimator xgboost's best error=13.3709,	best estimator xgboost's best error=13.3709
[flaml.automl: 09-18 14:22:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:22:40] {3072} INFO -  at 9.6s,	estimator xgboost's best error=6.3584,	best estimator xgboost's best error=6.3584
[flaml.automl: 09-18 14:22:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:22:43] {3072} INFO -  at 12.8s,	estimator xgboost's best error=6.3584,	best estimator xgboost's best error=6.3584
[flaml.automl: 09-18 14:22:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:23:11] {3072} INFO -  at 40.7s,	estimator xgboost's best error=6.3584,	best estimator xgboost's best error=6.3584
[flaml.automl: 09-18 14:23:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:23:14] {3072} INFO -  at 43.8s,	estimator xgboost's best error=4.3605,	best estimator xgboost's best error=4.3605
[flaml.automl: 09-18 14:23:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:23:19] {3072} INFO -  at 48.3s,	estimator xgboost's best error=3.8034,	best estimator xgboost's best error=3.8034
[flaml.automl: 09-18 14:23:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:23:23] {3072} INFO -  at 52.9s,	estimator xgboost's best error=3.4190,	best estimator xgboost's best error=3.4190
[flaml.automl: 09-18 14:23:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:23:30] {3072} INFO -  at 59.3s,	estimator xgboost's best error=3.4190,	best estimator xgboost's best error=3.4190
[flaml.automl: 09-18 14:23:33] {3335} INFO - retrain xgboost for 3.7s
[flaml.automl: 09-18 14:23:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:23:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:23:33] {2637} INFO - Time taken to find the best model: 52.898640394210815
[flaml.automl: 09-18 14:23:33] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-2.4190425801526625
PM2.5(0)最好结果：{'pred_time': 3.561389107697035e-05, 'wall_clock_time': 52.898640394210815, 'metric_for_logging': {'pred_time': 3.561389107697035e-05}, 'val_loss': 3.4190425801526625, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.5656304359436035}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8643096427176319
PM2.5(0)的mse=19.76890327592311
PM2.5(0)的mae=3.3632872379817145
PM2.5(0)的mar=0.27160729660403377
总共花费的时间为：63.60
梧州市
2495A
2496A
2497A
2498A
[flaml.automl: 09-18 14:36:48] {2390} INFO - task = regression
[flaml.automl: 09-18 14:36:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:36:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:36:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:36:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:36:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:36:52] {3025} INFO - Estimated sufficient time budget=147915s. Estimated necessary time budget=148s.
[flaml.automl: 09-18 14:36:52] {3072} INFO -  at 3.6s,	estimator xgboost's best error=15.5421,	best estimator xgboost's best error=15.5421
[flaml.automl: 09-18 14:36:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:36:57] {3072} INFO -  at 9.1s,	estimator xgboost's best error=7.1465,	best estimator xgboost's best error=7.1465
[flaml.automl: 09-18 14:36:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:36:59] {3072} INFO -  at 11.2s,	estimator xgboost's best error=7.1465,	best estimator xgboost's best error=7.1465
[flaml.automl: 09-18 14:36:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:37:04] {3072} INFO -  at 16.4s,	estimator xgboost's best error=7.1465,	best estimator xgboost's best error=7.1465
[flaml.automl: 09-18 14:37:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:37:06] {3072} INFO -  at 18.2s,	estimator xgboost's best error=4.5732,	best estimator xgboost's best error=4.5732
[flaml.automl: 09-18 14:37:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:37:09] {3072} INFO -  at 20.7s,	estimator xgboost's best error=3.7241,	best estimator xgboost's best error=3.7241
[flaml.automl: 09-18 14:37:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:37:11] {3072} INFO -  at 23.2s,	estimator xgboost's best error=3.3353,	best estimator xgboost's best error=3.3353
[flaml.automl: 09-18 14:37:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:37:15] {3072} INFO -  at 27.0s,	estimator xgboost's best error=3.3353,	best estimator xgboost's best error=3.3353
[flaml.automl: 09-18 14:37:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:37:18] {3072} INFO -  at 29.7s,	estimator xgboost's best error=3.3353,	best estimator xgboost's best error=3.3353
[flaml.automl: 09-18 14:37:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:37:21] {3072} INFO -  at 32.5s,	estimator xgboost's best error=3.3353,	best estimator xgboost's best error=3.3353
[flaml.automl: 09-18 14:37:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:37:23] {3072} INFO -  at 34.9s,	estimator xgboost's best error=3.2877,	best estimator xgboost's best error=3.2877
[flaml.automl: 09-18 14:37:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:37:25] {3072} INFO -  at 36.7s,	estimator xgboost's best error=3.2877,	best estimator xgboost's best error=3.2877
[flaml.automl: 09-18 14:37:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:37:27] {3072} INFO -  at 39.0s,	estimator xgboost's best error=2.6945,	best estimator xgboost's best error=2.6945
[flaml.automl: 09-18 14:37:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:37:29] {3072} INFO -  at 41.2s,	estimator xgboost's best error=2.5895,	best estimator xgboost's best error=2.5895
[flaml.automl: 09-18 14:37:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 14:37:31] {3072} INFO -  at 43.0s,	estimator xgboost's best error=2.5895,	best estimator xgboost's best error=2.5895
[flaml.automl: 09-18 14:37:31] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 14:37:33] {3072} INFO -  at 44.5s,	estimator xgboost's best error=2.5895,	best estimator xgboost's best error=2.5895
[flaml.automl: 09-18 14:37:33] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 14:37:34] {3072} INFO -  at 46.2s,	estimator xgboost's best error=2.5895,	best estimator xgboost's best error=2.5895
[flaml.automl: 09-18 14:37:34] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 14:37:36] {3072} INFO -  at 47.5s,	estimator xgboost's best error=2.5895,	best estimator xgboost's best error=2.5895
[flaml.automl: 09-18 14:37:36] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 14:37:46] {3072} INFO -  at 58.3s,	estimator xgboost's best error=2.4101,	best estimator xgboost's best error=2.4101
[flaml.automl: 09-18 14:37:57] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-18 14:37:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:37:57] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:37:57] {2637} INFO - Time taken to find the best model: 58.32757782936096
[flaml.automl: 09-18 14:37:57] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463, 'FLAML_sample_size': 44682}
PM2.5(0)最佳损失：-1.4100615554705849
PM2.5(0)最好结果：{'pred_time': 8.203856054269536e-06, 'wall_clock_time': 58.32757782936096, 'metric_for_logging': {'pred_time': 8.203856054269536e-06}, 'val_loss': 2.410061555470585, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463, 'FLAML_sample_size': 44682}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 6.532042959250319, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8835265175628242, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.10065328594609463, 'config/FLAML_sample_size': 44682, 'experiment_tag': 'exp', 'time_total_s': 10.787240266799927}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9483353692484193
PM2.5(0)的mse=10.25500312537403
PM2.5(0)的mae=2.413639282605474
PM2.5(0)的mar=0.13475280527899233
总共花费的时间为：69.86
防城港市
2499A
2500A
2501A
[flaml.automl: 09-18 14:47:11] {2390} INFO - task = regression
[flaml.automl: 09-18 14:47:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:47:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:47:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:47:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:47:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:47:12] {3025} INFO - Estimated sufficient time budget=12151s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 14:47:12] {3072} INFO -  at 1.3s,	estimator xgboost's best error=12.6419,	best estimator xgboost's best error=12.6419
[flaml.automl: 09-18 14:47:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:47:14] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.0252,	best estimator xgboost's best error=6.0252
[flaml.automl: 09-18 14:47:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:47:15] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.0252,	best estimator xgboost's best error=6.0252
[flaml.automl: 09-18 14:47:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:47:26] {3072} INFO -  at 14.7s,	estimator xgboost's best error=6.0252,	best estimator xgboost's best error=6.0252
[flaml.automl: 09-18 14:47:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:47:27] {3072} INFO -  at 15.9s,	estimator xgboost's best error=3.9824,	best estimator xgboost's best error=3.9824
[flaml.automl: 09-18 14:47:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:47:28] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.8794,	best estimator xgboost's best error=3.8794
[flaml.automl: 09-18 14:47:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:47:30] {3072} INFO -  at 19.2s,	estimator xgboost's best error=3.2094,	best estimator xgboost's best error=3.2094
[flaml.automl: 09-18 14:47:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:47:33] {3072} INFO -  at 21.9s,	estimator xgboost's best error=3.2094,	best estimator xgboost's best error=3.2094
[flaml.automl: 09-18 14:47:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:47:34] {3072} INFO -  at 23.5s,	estimator xgboost's best error=3.2094,	best estimator xgboost's best error=3.2094
[flaml.automl: 09-18 14:47:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:47:37] {3072} INFO -  at 26.5s,	estimator xgboost's best error=2.9855,	best estimator xgboost's best error=2.9855
[flaml.automl: 09-18 14:47:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:47:39] {3072} INFO -  at 28.2s,	estimator xgboost's best error=2.9855,	best estimator xgboost's best error=2.9855
[flaml.automl: 09-18 14:47:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:47:40] {3072} INFO -  at 29.3s,	estimator xgboost's best error=2.9855,	best estimator xgboost's best error=2.9855
[flaml.automl: 09-18 14:47:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:47:54] {3072} INFO -  at 43.0s,	estimator xgboost's best error=2.7601,	best estimator xgboost's best error=2.7601
[flaml.automl: 09-18 14:47:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:48:10] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.5547,	best estimator xgboost's best error=2.5547
[flaml.automl: 09-18 14:48:34] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-18 14:48:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:48:34] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:48:34] {2637} INFO - Time taken to find the best model: 59.523584842681885
[flaml.automl: 09-18 14:48:34] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-1.5546654157375546
PM2.5(0)最好结果：{'pred_time': 1.1097504877075162e-05, 'wall_clock_time': 59.523584842681885, 'metric_for_logging': {'pred_time': 1.1097504877075162e-05}, 'val_loss': 2.5546654157375546, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.52160930633545}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9245938539592871
PM2.5(0)的mse=14.84815422350369
PM2.5(0)的mae=2.4236102793815273
PM2.5(0)的mar=0.16436997424122113
总共花费的时间为：83.99
钦州市
2502A
2503A
2504A
3404A
[flaml.automl: 09-18 15:01:20] {2390} INFO - task = regression
[flaml.automl: 09-18 15:01:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:01:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:01:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:01:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:01:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:01:22] {3025} INFO - Estimated sufficient time budget=98257s. Estimated necessary time budget=98s.
[flaml.automl: 09-18 15:01:22] {3072} INFO -  at 2.5s,	estimator xgboost's best error=14.9608,	best estimator xgboost's best error=14.9608
[flaml.automl: 09-18 15:01:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:01:26] {3072} INFO -  at 6.3s,	estimator xgboost's best error=7.3325,	best estimator xgboost's best error=7.3325
[flaml.automl: 09-18 15:01:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:01:28] {3072} INFO -  at 8.1s,	estimator xgboost's best error=7.3325,	best estimator xgboost's best error=7.3325
[flaml.automl: 09-18 15:01:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:01:33] {3072} INFO -  at 13.7s,	estimator xgboost's best error=7.3325,	best estimator xgboost's best error=7.3325
[flaml.automl: 09-18 15:01:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:01:36] {3072} INFO -  at 16.0s,	estimator xgboost's best error=5.4094,	best estimator xgboost's best error=5.4094
[flaml.automl: 09-18 15:01:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:01:38] {3072} INFO -  at 18.6s,	estimator xgboost's best error=4.5212,	best estimator xgboost's best error=4.5212
[flaml.automl: 09-18 15:01:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:01:41] {3072} INFO -  at 21.4s,	estimator xgboost's best error=4.3014,	best estimator xgboost's best error=4.3014
[flaml.automl: 09-18 15:01:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:01:45] {3072} INFO -  at 25.0s,	estimator xgboost's best error=4.3014,	best estimator xgboost's best error=4.3014
[flaml.automl: 09-18 15:01:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:01:46] {3072} INFO -  at 26.7s,	estimator xgboost's best error=4.3014,	best estimator xgboost's best error=4.3014
[flaml.automl: 09-18 15:01:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:01:49] {3072} INFO -  at 29.7s,	estimator xgboost's best error=3.6996,	best estimator xgboost's best error=3.6996
[flaml.automl: 09-18 15:01:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:01:51] {3072} INFO -  at 31.3s,	estimator xgboost's best error=3.6996,	best estimator xgboost's best error=3.6996
[flaml.automl: 09-18 15:01:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:01:52] {3072} INFO -  at 32.4s,	estimator xgboost's best error=3.6996,	best estimator xgboost's best error=3.6996
[flaml.automl: 09-18 15:01:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:01:55] {3072} INFO -  at 35.3s,	estimator xgboost's best error=3.6496,	best estimator xgboost's best error=3.6496
[flaml.automl: 09-18 15:01:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:01:57] {3072} INFO -  at 37.5s,	estimator xgboost's best error=3.6496,	best estimator xgboost's best error=3.6496
[flaml.automl: 09-18 15:01:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:01:59] {3072} INFO -  at 38.9s,	estimator xgboost's best error=3.6322,	best estimator xgboost's best error=3.6322
[flaml.automl: 09-18 15:01:59] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 15:02:01] {3072} INFO -  at 41.1s,	estimator xgboost's best error=3.6322,	best estimator xgboost's best error=3.6322
[flaml.automl: 09-18 15:02:01] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 15:02:02] {3072} INFO -  at 42.6s,	estimator xgboost's best error=3.5808,	best estimator xgboost's best error=3.5808
[flaml.automl: 09-18 15:02:02] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 15:02:04] {3072} INFO -  at 44.3s,	estimator xgboost's best error=3.5808,	best estimator xgboost's best error=3.5808
[flaml.automl: 09-18 15:02:04] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 15:02:06] {3072} INFO -  at 46.0s,	estimator xgboost's best error=3.5808,	best estimator xgboost's best error=3.5808
[flaml.automl: 09-18 15:02:06] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 15:02:07] {3072} INFO -  at 47.2s,	estimator xgboost's best error=3.5808,	best estimator xgboost's best error=3.5808
[flaml.automl: 09-18 15:02:07] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 15:02:08] {3072} INFO -  at 48.7s,	estimator xgboost's best error=3.4275,	best estimator xgboost's best error=3.4275
[flaml.automl: 09-18 15:02:08] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 15:02:11] {3072} INFO -  at 51.3s,	estimator xgboost's best error=3.4275,	best estimator xgboost's best error=3.4275
[flaml.automl: 09-18 15:02:11] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-18 15:02:12] {3072} INFO -  at 52.2s,	estimator xgboost's best error=3.4275,	best estimator xgboost's best error=3.4275
[flaml.automl: 09-18 15:02:12] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-18 15:02:15] {3072} INFO -  at 55.6s,	estimator xgboost's best error=3.4275,	best estimator xgboost's best error=3.4275
[flaml.automl: 09-18 15:02:15] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-18 15:02:16] {3072} INFO -  at 56.3s,	estimator xgboost's best error=3.4275,	best estimator xgboost's best error=3.4275
[flaml.automl: 09-18 15:02:16] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-18 15:02:19] {3072} INFO -  at 59.2s,	estimator xgboost's best error=3.3025,	best estimator xgboost's best error=3.3025
[flaml.automl: 09-18 15:02:33] {3335} INFO - retrain xgboost for 13.7s
[flaml.automl: 09-18 15:02:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8732721209051209, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=24, min_child_weight=0.0023588925015011544,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009451087049979049, reg_lambda=0.08827632666156185,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:02:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:02:33] {2637} INFO - Time taken to find the best model: 59.22079133987427
[flaml.automl: 09-18 15:02:33] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 10, 'max_leaves': 24, 'min_child_weight': 0.0023588925015011544, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8732721209051209, 'reg_alpha': 0.009451087049979049, 'reg_lambda': 0.08827632666156185, 'FLAML_sample_size': 43767}
PM2.5(0)最佳损失：-2.3024697560620937
PM2.5(0)最好结果：{'pred_time': 9.319695987199483e-06, 'wall_clock_time': 59.22079133987427, 'metric_for_logging': {'pred_time': 9.319695987199483e-06}, 'val_loss': 3.3024697560620937, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 24, 'min_child_weight': 0.0023588925015011544, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8732721209051209, 'reg_alpha': 0.009451087049979049, 'reg_lambda': 0.08827632666156185, 'FLAML_sample_size': 43767}, 'config/n_estimators': 10, 'config/max_leaves': 24, 'config/min_child_weight': 0.0023588925015011544, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8732721209051209, 'config/reg_alpha': 0.009451087049979049, 'config/reg_lambda': 0.08827632666156185, 'config/FLAML_sample_size': 43767, 'experiment_tag': 'exp', 'time_total_s': 2.9081225395202637}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8732721209051209, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=24, min_child_weight=0.0023588925015011544,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009451087049979049, reg_lambda=0.08827632666156185,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.917126923157908
PM2.5(0)的mse=24.302994424022646
PM2.5(0)的mae=3.2275845948013884
PM2.5(0)的mar=0.19746102286008602
总共花费的时间为：73.73
贵港市
2505A
2506A
2507A
2508A
3405A
[flaml.automl: 09-18 15:17:50] {2390} INFO - task = regression
[flaml.automl: 09-18 15:17:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:17:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:17:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:17:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:17:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:17:53] {3025} INFO - Estimated sufficient time budget=107471s. Estimated necessary time budget=107s.
[flaml.automl: 09-18 15:17:53] {3072} INFO -  at 2.4s,	estimator xgboost's best error=16.6995,	best estimator xgboost's best error=16.6995
[flaml.automl: 09-18 15:17:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:17:56] {3072} INFO -  at 6.0s,	estimator xgboost's best error=7.9540,	best estimator xgboost's best error=7.9540
[flaml.automl: 09-18 15:17:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:17:58] {3072} INFO -  at 8.0s,	estimator xgboost's best error=7.9540,	best estimator xgboost's best error=7.9540
[flaml.automl: 09-18 15:17:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:18:03] {3072} INFO -  at 12.6s,	estimator xgboost's best error=7.9540,	best estimator xgboost's best error=7.9540
[flaml.automl: 09-18 15:18:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:18:05] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.6150,	best estimator xgboost's best error=5.6150
[flaml.automl: 09-18 15:18:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:18:07] {3072} INFO -  at 17.3s,	estimator xgboost's best error=5.6150,	best estimator xgboost's best error=5.6150
[flaml.automl: 09-18 15:18:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:18:10] {3072} INFO -  at 20.2s,	estimator xgboost's best error=3.9629,	best estimator xgboost's best error=3.9629
[flaml.automl: 09-18 15:18:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:18:13] {3072} INFO -  at 23.1s,	estimator xgboost's best error=3.9629,	best estimator xgboost's best error=3.9629
[flaml.automl: 09-18 15:18:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:18:16] {3072} INFO -  at 26.1s,	estimator xgboost's best error=3.9629,	best estimator xgboost's best error=3.9629
[flaml.automl: 09-18 15:18:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:18:19] {3072} INFO -  at 28.8s,	estimator xgboost's best error=3.9629,	best estimator xgboost's best error=3.9629
[flaml.automl: 09-18 15:18:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:18:22] {3072} INFO -  at 31.5s,	estimator xgboost's best error=3.9629,	best estimator xgboost's best error=3.9629
[flaml.automl: 09-18 15:18:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:18:25] {3072} INFO -  at 34.8s,	estimator xgboost's best error=3.9629,	best estimator xgboost's best error=3.9629
[flaml.automl: 09-18 15:18:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:18:27] {3072} INFO -  at 36.7s,	estimator xgboost's best error=3.9629,	best estimator xgboost's best error=3.9629
[flaml.automl: 09-18 15:18:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:18:38] {3072} INFO -  at 47.8s,	estimator xgboost's best error=3.4725,	best estimator xgboost's best error=3.4725
[flaml.automl: 09-18 15:18:38] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:18:50] {3072} INFO -  at 59.6s,	estimator xgboost's best error=3.3627,	best estimator xgboost's best error=3.3627
[flaml.automl: 09-18 15:19:02] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-18 15:19:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:19:02] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:19:02] {2637} INFO - Time taken to find the best model: 59.60049366950989
[flaml.automl: 09-18 15:19:02] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52852}
PM2.5(0)最佳损失：-2.362657946418218
PM2.5(0)最好结果：{'pred_time': 7.22335628364676e-06, 'wall_clock_time': 59.60049366950989, 'metric_for_logging': {'pred_time': 7.22335628364676e-06}, 'val_loss': 3.362657946418218, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52852}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 52852, 'experiment_tag': 'exp', 'time_total_s': 11.847244262695312}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9032746510684626
PM2.5(0)的mse=24.579862027374645
PM2.5(0)的mae=3.328643710599368
PM2.5(0)的mar=0.19565244291077186
总共花费的时间为：73.23
玉林市
2509A
2510A
2511A
3532A
3533A
[flaml.automl: 09-18 15:35:32] {2390} INFO - task = regression
[flaml.automl: 09-18 15:35:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:35:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:35:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:35:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:35:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:35:34] {3025} INFO - Estimated sufficient time budget=101077s. Estimated necessary time budget=101s.
[flaml.automl: 09-18 15:35:34] {3072} INFO -  at 2.1s,	estimator xgboost's best error=17.0094,	best estimator xgboost's best error=17.0094
[flaml.automl: 09-18 15:35:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:35:38] {3072} INFO -  at 5.8s,	estimator xgboost's best error=8.2130,	best estimator xgboost's best error=8.2130
[flaml.automl: 09-18 15:35:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:35:39] {3072} INFO -  at 7.6s,	estimator xgboost's best error=8.2130,	best estimator xgboost's best error=8.2130
[flaml.automl: 09-18 15:35:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:35:44] {3072} INFO -  at 12.6s,	estimator xgboost's best error=8.2130,	best estimator xgboost's best error=8.2130
[flaml.automl: 09-18 15:35:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:35:46] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.6933,	best estimator xgboost's best error=5.6933
[flaml.automl: 09-18 15:35:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:35:50] {3072} INFO -  at 17.7s,	estimator xgboost's best error=4.8604,	best estimator xgboost's best error=4.8604
[flaml.automl: 09-18 15:35:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:35:52] {3072} INFO -  at 20.7s,	estimator xgboost's best error=4.4966,	best estimator xgboost's best error=4.4966
[flaml.automl: 09-18 15:35:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:35:55] {3072} INFO -  at 23.6s,	estimator xgboost's best error=4.4966,	best estimator xgboost's best error=4.4966
[flaml.automl: 09-18 15:35:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:35:57] {3072} INFO -  at 25.2s,	estimator xgboost's best error=4.4966,	best estimator xgboost's best error=4.4966
[flaml.automl: 09-18 15:35:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:36:00] {3072} INFO -  at 28.2s,	estimator xgboost's best error=4.0152,	best estimator xgboost's best error=4.0152
[flaml.automl: 09-18 15:36:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:36:02] {3072} INFO -  at 29.8s,	estimator xgboost's best error=4.0152,	best estimator xgboost's best error=4.0152
[flaml.automl: 09-18 15:36:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:36:03] {3072} INFO -  at 30.9s,	estimator xgboost's best error=4.0152,	best estimator xgboost's best error=4.0152
[flaml.automl: 09-18 15:36:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:36:05] {3072} INFO -  at 32.9s,	estimator xgboost's best error=4.0152,	best estimator xgboost's best error=4.0152
[flaml.automl: 09-18 15:36:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:36:07] {3072} INFO -  at 35.1s,	estimator xgboost's best error=4.0152,	best estimator xgboost's best error=4.0152
[flaml.automl: 09-18 15:36:07] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:36:10] {3072} INFO -  at 38.2s,	estimator xgboost's best error=4.0126,	best estimator xgboost's best error=4.0126
[flaml.automl: 09-18 15:36:10] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 15:36:15] {3072} INFO -  at 43.2s,	estimator xgboost's best error=4.0126,	best estimator xgboost's best error=4.0126
[flaml.automl: 09-18 15:36:15] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 15:36:17] {3072} INFO -  at 45.0s,	estimator xgboost's best error=3.9606,	best estimator xgboost's best error=3.9606
[flaml.automl: 09-18 15:36:17] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 15:36:18] {3072} INFO -  at 46.7s,	estimator xgboost's best error=3.9606,	best estimator xgboost's best error=3.9606
[flaml.automl: 09-18 15:36:18] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 15:36:23] {3072} INFO -  at 51.0s,	estimator xgboost's best error=3.6609,	best estimator xgboost's best error=3.6609
[flaml.automl: 09-18 15:36:23] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 15:36:25] {3072} INFO -  at 52.7s,	estimator xgboost's best error=3.6609,	best estimator xgboost's best error=3.6609
[flaml.automl: 09-18 15:36:25] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 15:36:31] {3072} INFO -  at 59.0s,	estimator xgboost's best error=3.5515,	best estimator xgboost's best error=3.5515
[flaml.automl: 09-18 15:36:44] {3335} INFO - retrain xgboost for 13.3s
[flaml.automl: 09-18 15:36:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:36:44] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:36:44] {2637} INFO - Time taken to find the best model: 58.985387086868286
[flaml.automl: 09-18 15:36:44] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 53367}
PM2.5(0)最佳损失：-2.5514818175245093
PM2.5(0)最好结果：{'pred_time': 6.646531026238736e-06, 'wall_clock_time': 58.985387086868286, 'metric_for_logging': {'pred_time': 6.646531026238736e-06}, 'val_loss': 3.5514818175245093, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 53367}, 'config/n_estimators': 13, 'config/max_leaves': 18, 'config/min_child_weight': 0.002212987346630184, 'config/learning_rate': 1.0, 'config/subsample': 0.9736629142498529, 'config/colsample_bylevel': 0.8717925041332704, 'config/colsample_bytree': 0.9857450191529009, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.04619111740498511, 'config/FLAML_sample_size': 53367, 'experiment_tag': 'exp', 'time_total_s': 6.2619500160217285}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8987135336703328
PM2.5(0)的mse=24.74847220493175
PM2.5(0)的mae=3.404649742018889
PM2.5(0)的mar=0.1841575428462998
总共花费的时间为：73.21
百色市
2512A
2513A
3406A
[flaml.automl: 09-18 15:46:23] {2390} INFO - task = regression
[flaml.automl: 09-18 15:46:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:46:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:46:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:46:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:46:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:46:25] {3025} INFO - Estimated sufficient time budget=12191s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 15:46:25] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.4126,	best estimator xgboost's best error=17.4126
[flaml.automl: 09-18 15:46:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:46:27] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.2412,	best estimator xgboost's best error=8.2412
[flaml.automl: 09-18 15:46:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:46:28] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.2412,	best estimator xgboost's best error=8.2412
[flaml.automl: 09-18 15:46:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:46:38] {3072} INFO -  at 14.7s,	estimator xgboost's best error=8.2412,	best estimator xgboost's best error=8.2412
[flaml.automl: 09-18 15:46:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:46:39] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.1512,	best estimator xgboost's best error=5.1512
[flaml.automl: 09-18 15:46:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:46:41] {3072} INFO -  at 17.5s,	estimator xgboost's best error=5.1512,	best estimator xgboost's best error=5.1512
[flaml.automl: 09-18 15:46:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:46:42] {3072} INFO -  at 19.2s,	estimator xgboost's best error=3.8847,	best estimator xgboost's best error=3.8847
[flaml.automl: 09-18 15:46:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:46:45] {3072} INFO -  at 21.9s,	estimator xgboost's best error=3.8847,	best estimator xgboost's best error=3.8847
[flaml.automl: 09-18 15:46:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:46:47] {3072} INFO -  at 23.5s,	estimator xgboost's best error=3.8847,	best estimator xgboost's best error=3.8847
[flaml.automl: 09-18 15:46:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:46:50] {3072} INFO -  at 26.6s,	estimator xgboost's best error=3.8847,	best estimator xgboost's best error=3.8847
[flaml.automl: 09-18 15:46:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:46:51] {3072} INFO -  at 28.1s,	estimator xgboost's best error=3.8847,	best estimator xgboost's best error=3.8847
[flaml.automl: 09-18 15:46:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:46:52] {3072} INFO -  at 29.2s,	estimator xgboost's best error=3.8847,	best estimator xgboost's best error=3.8847
[flaml.automl: 09-18 15:46:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:47:00] {3072} INFO -  at 36.3s,	estimator xgboost's best error=3.3948,	best estimator xgboost's best error=3.3948
[flaml.automl: 09-18 15:47:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:47:12] {3072} INFO -  at 49.0s,	estimator xgboost's best error=3.2629,	best estimator xgboost's best error=3.2629
[flaml.automl: 09-18 15:47:25] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 15:47:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:47:25] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:47:25] {2637} INFO - Time taken to find the best model: 49.04745411872864
[flaml.automl: 09-18 15:47:25] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-2.26288083298453
PM2.5(0)最好结果：{'pred_time': 1.2537396126514813e-05, 'wall_clock_time': 49.04745411872864, 'metric_for_logging': {'pred_time': 1.2537396126514813e-05}, 'val_loss': 3.26288083298453, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 12.771950006484985}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9151035793401694
PM2.5(0)的mse=23.54302252933767
PM2.5(0)的mae=3.128500410565385
PM2.5(0)的mar=0.15814850723571017
总共花费的时间为：62.33
贺州市
2514A
2515A
3534A
[flaml.automl: 09-18 15:57:13] {2390} INFO - task = regression
[flaml.automl: 09-18 15:57:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:57:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:57:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:57:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:57:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:57:15] {3025} INFO - Estimated sufficient time budget=12159s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 15:57:15] {3072} INFO -  at 1.3s,	estimator xgboost's best error=15.7059,	best estimator xgboost's best error=15.7059
[flaml.automl: 09-18 15:57:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:57:17] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.7717,	best estimator xgboost's best error=7.7717
[flaml.automl: 09-18 15:57:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:57:18] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.7717,	best estimator xgboost's best error=7.7717
[flaml.automl: 09-18 15:57:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:57:28] {3072} INFO -  at 14.8s,	estimator xgboost's best error=7.7717,	best estimator xgboost's best error=7.7717
[flaml.automl: 09-18 15:57:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:57:29] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.6352,	best estimator xgboost's best error=5.6352
[flaml.automl: 09-18 15:57:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:57:31] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.9706,	best estimator xgboost's best error=4.9706
[flaml.automl: 09-18 15:57:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:57:32] {3072} INFO -  at 19.2s,	estimator xgboost's best error=4.5739,	best estimator xgboost's best error=4.5739
[flaml.automl: 09-18 15:57:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:57:35] {3072} INFO -  at 21.9s,	estimator xgboost's best error=4.5739,	best estimator xgboost's best error=4.5739
[flaml.automl: 09-18 15:57:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:57:37] {3072} INFO -  at 23.5s,	estimator xgboost's best error=4.5739,	best estimator xgboost's best error=4.5739
[flaml.automl: 09-18 15:57:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:57:40] {3072} INFO -  at 26.5s,	estimator xgboost's best error=4.2706,	best estimator xgboost's best error=4.2706
[flaml.automl: 09-18 15:57:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:57:41] {3072} INFO -  at 28.2s,	estimator xgboost's best error=4.2706,	best estimator xgboost's best error=4.2706
[flaml.automl: 09-18 15:57:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:57:43] {3072} INFO -  at 29.3s,	estimator xgboost's best error=4.2706,	best estimator xgboost's best error=4.2706
[flaml.automl: 09-18 15:57:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:57:56] {3072} INFO -  at 43.0s,	estimator xgboost's best error=4.0446,	best estimator xgboost's best error=4.0446
[flaml.automl: 09-18 15:57:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:58:13] {3072} INFO -  at 59.5s,	estimator xgboost's best error=3.8776,	best estimator xgboost's best error=3.8776
[flaml.automl: 09-18 15:58:37] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-18 15:58:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:58:37] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:58:37] {2637} INFO - Time taken to find the best model: 59.51325845718384
[flaml.automl: 09-18 15:58:37] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-2.877637132320099
PM2.5(0)最好结果：{'pred_time': 1.251644938280092e-05, 'wall_clock_time': 59.51325845718384, 'metric_for_logging': {'pred_time': 1.251644938280092e-05}, 'val_loss': 3.877637132320099, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.48769998550415}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8960222985112354
PM2.5(0)的mse=27.7546178373421
PM2.5(0)的mae=3.8942484981104175
PM2.5(0)的mar=0.2816541600479623
总共花费的时间为：83.97
河池市
2516A
2517A
2518A
[flaml.automl: 09-18 16:08:19] {2390} INFO - task = regression
[flaml.automl: 09-18 16:08:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:08:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:08:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:08:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:08:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:08:20] {3025} INFO - Estimated sufficient time budget=12183s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:08:20] {3072} INFO -  at 1.3s,	estimator xgboost's best error=14.7421,	best estimator xgboost's best error=14.7421
[flaml.automl: 09-18 16:08:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:08:23] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.8802,	best estimator xgboost's best error=6.8802
[flaml.automl: 09-18 16:08:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:08:24] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.8802,	best estimator xgboost's best error=6.8802
[flaml.automl: 09-18 16:08:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:08:34] {3072} INFO -  at 14.7s,	estimator xgboost's best error=6.8802,	best estimator xgboost's best error=6.8802
[flaml.automl: 09-18 16:08:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:08:35] {3072} INFO -  at 15.8s,	estimator xgboost's best error=4.3682,	best estimator xgboost's best error=4.3682
[flaml.automl: 09-18 16:08:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:08:37] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.3682,	best estimator xgboost's best error=4.3682
[flaml.automl: 09-18 16:08:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:08:38] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.8705,	best estimator xgboost's best error=2.8705
[flaml.automl: 09-18 16:08:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:08:41] {3072} INFO -  at 21.8s,	estimator xgboost's best error=2.8705,	best estimator xgboost's best error=2.8705
[flaml.automl: 09-18 16:08:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:08:43] {3072} INFO -  at 23.5s,	estimator xgboost's best error=2.8705,	best estimator xgboost's best error=2.8705
[flaml.automl: 09-18 16:08:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:08:46] {3072} INFO -  at 26.5s,	estimator xgboost's best error=2.8705,	best estimator xgboost's best error=2.8705
[flaml.automl: 09-18 16:08:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:08:47] {3072} INFO -  at 27.9s,	estimator xgboost's best error=2.8705,	best estimator xgboost's best error=2.8705
[flaml.automl: 09-18 16:08:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:08:48] {3072} INFO -  at 29.1s,	estimator xgboost's best error=2.8705,	best estimator xgboost's best error=2.8705
[flaml.automl: 09-18 16:08:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:08:55] {3072} INFO -  at 36.1s,	estimator xgboost's best error=2.2908,	best estimator xgboost's best error=2.2908
[flaml.automl: 09-18 16:08:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:09:08] {3072} INFO -  at 48.5s,	estimator xgboost's best error=2.2382,	best estimator xgboost's best error=2.2382
[flaml.automl: 09-18 16:09:20] {3335} INFO - retrain xgboost for 12.3s
[flaml.automl: 09-18 16:09:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:09:20] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:09:20] {2637} INFO - Time taken to find the best model: 48.49363946914673
[flaml.automl: 09-18 16:09:20] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-1.2382078285862628
PM2.5(0)最好结果：{'pred_time': 1.2162072317940849e-05, 'wall_clock_time': 48.49363946914673, 'metric_for_logging': {'pred_time': 1.2162072317940849e-05}, 'val_loss': 2.2382078285862628, 'training_iteration': 1, 'config': {'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 30, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 12.379183292388916}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9394427703812375
PM2.5(0)的mse=17.406789674667394
PM2.5(0)的mae=2.410445713593608
PM2.5(0)的mar=0.1419529290302348
总共花费的时间为：61.39
来宾市
2519A
2520A
3535A
[flaml.automl: 09-18 16:18:36] {2390} INFO - task = regression
[flaml.automl: 09-18 16:18:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:18:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:18:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:18:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:18:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:18:37] {3025} INFO - Estimated sufficient time budget=12182s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:18:37] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.3846,	best estimator xgboost's best error=18.3846
[flaml.automl: 09-18 16:18:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:18:39] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.7012,	best estimator xgboost's best error=8.7012
[flaml.automl: 09-18 16:18:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:18:41] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.7012,	best estimator xgboost's best error=8.7012
[flaml.automl: 09-18 16:18:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:18:51] {3072} INFO -  at 14.7s,	estimator xgboost's best error=8.7012,	best estimator xgboost's best error=8.7012
[flaml.automl: 09-18 16:18:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:18:52] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.8247,	best estimator xgboost's best error=5.8247
[flaml.automl: 09-18 16:18:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:18:53] {3072} INFO -  at 17.5s,	estimator xgboost's best error=5.8247,	best estimator xgboost's best error=5.8247
[flaml.automl: 09-18 16:18:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:18:55] {3072} INFO -  at 19.2s,	estimator xgboost's best error=4.4815,	best estimator xgboost's best error=4.4815
[flaml.automl: 09-18 16:18:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:18:58] {3072} INFO -  at 21.9s,	estimator xgboost's best error=4.4815,	best estimator xgboost's best error=4.4815
[flaml.automl: 09-18 16:18:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:19:00] {3072} INFO -  at 23.5s,	estimator xgboost's best error=4.4815,	best estimator xgboost's best error=4.4815
[flaml.automl: 09-18 16:19:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:19:03] {3072} INFO -  at 26.6s,	estimator xgboost's best error=4.4815,	best estimator xgboost's best error=4.4815
[flaml.automl: 09-18 16:19:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:19:04] {3072} INFO -  at 28.0s,	estimator xgboost's best error=4.4815,	best estimator xgboost's best error=4.4815
[flaml.automl: 09-18 16:19:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:19:05] {3072} INFO -  at 29.2s,	estimator xgboost's best error=4.4815,	best estimator xgboost's best error=4.4815
[flaml.automl: 09-18 16:19:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:19:12] {3072} INFO -  at 36.2s,	estimator xgboost's best error=4.0073,	best estimator xgboost's best error=4.0073
[flaml.automl: 09-18 16:19:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:19:25] {3072} INFO -  at 49.0s,	estimator xgboost's best error=3.9047,	best estimator xgboost's best error=3.9047
[flaml.automl: 09-18 16:19:38] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-18 16:19:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:19:38] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:19:38] {2637} INFO - Time taken to find the best model: 49.01817750930786
[flaml.automl: 09-18 16:19:38] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
PM2.5(0)最佳损失：-2.904732491086689
PM2.5(0)最好结果：{'pred_time': 1.1107791266607224e-05, 'wall_clock_time': 49.01817750930786, 'metric_for_logging': {'pred_time': 1.1107791266607224e-05}, 'val_loss': 3.904732491086689, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 12.780752897262573}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9027044298875705
PM2.5(0)的mse=40.65931170736984
PM2.5(0)的mae=4.155931634132907
PM2.5(0)的mar=0.21778113925511033
总共花费的时间为：62.26
崇左市
2521A
2522A
[flaml.automl: 09-18 16:25:43] {2390} INFO - task = regression
[flaml.automl: 09-18 16:25:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:25:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:25:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:25:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:25:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:25:44] {3025} INFO - Estimated sufficient time budget=12045s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:25:44] {3072} INFO -  at 1.3s,	estimator xgboost's best error=16.3440,	best estimator xgboost's best error=16.3440
[flaml.automl: 09-18 16:25:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:25:47] {3072} INFO -  at 3.4s,	estimator xgboost's best error=7.7059,	best estimator xgboost's best error=7.7059
[flaml.automl: 09-18 16:25:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:25:48] {3072} INFO -  at 4.6s,	estimator xgboost's best error=7.7059,	best estimator xgboost's best error=7.7059
[flaml.automl: 09-18 16:25:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:25:57] {3072} INFO -  at 14.0s,	estimator xgboost's best error=7.7059,	best estimator xgboost's best error=7.7059
[flaml.automl: 09-18 16:25:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:25:58] {3072} INFO -  at 15.2s,	estimator xgboost's best error=5.4904,	best estimator xgboost's best error=5.4904
[flaml.automl: 09-18 16:25:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:26:00] {3072} INFO -  at 16.8s,	estimator xgboost's best error=4.6545,	best estimator xgboost's best error=4.6545
[flaml.automl: 09-18 16:26:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:26:02] {3072} INFO -  at 18.4s,	estimator xgboost's best error=4.0526,	best estimator xgboost's best error=4.0526
[flaml.automl: 09-18 16:26:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:26:04] {3072} INFO -  at 20.8s,	estimator xgboost's best error=4.0526,	best estimator xgboost's best error=4.0526
[flaml.automl: 09-18 16:26:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:26:06] {3072} INFO -  at 22.4s,	estimator xgboost's best error=4.0526,	best estimator xgboost's best error=4.0526
[flaml.automl: 09-18 16:26:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:26:09] {3072} INFO -  at 25.5s,	estimator xgboost's best error=3.4963,	best estimator xgboost's best error=3.4963
[flaml.automl: 09-18 16:26:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:26:10] {3072} INFO -  at 27.1s,	estimator xgboost's best error=3.4963,	best estimator xgboost's best error=3.4963
[flaml.automl: 09-18 16:26:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:26:11] {3072} INFO -  at 28.2s,	estimator xgboost's best error=3.4963,	best estimator xgboost's best error=3.4963
[flaml.automl: 09-18 16:26:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:26:23] {3072} INFO -  at 40.3s,	estimator xgboost's best error=3.1054,	best estimator xgboost's best error=3.1054
[flaml.automl: 09-18 16:26:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:26:43] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.9036,	best estimator xgboost's best error=2.9036
[flaml.automl: 09-18 16:27:05] {3335} INFO - retrain xgboost for 21.9s
[flaml.automl: 09-18 16:27:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:27:05] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:27:05] {2637} INFO - Time taken to find the best model: 59.45835614204407
[flaml.automl: 09-18 16:27:05] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-1.9036178116166789
PM2.5(0)最好结果：{'pred_time': 1.703043181766053e-05, 'wall_clock_time': 59.45835614204407, 'metric_for_logging': {'pred_time': 1.703043181766053e-05}, 'val_loss': 2.903617811616679, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 19.204291105270386}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9348265319611413
PM2.5(0)的mse=20.594887497530898
PM2.5(0)的mae=2.782343237536294
PM2.5(0)的mar=0.1354351101406443
总共花费的时间为：81.70
广元市
2523A
2524A
3617A
[flaml.automl: 09-18 16:36:34] {2390} INFO - task = regression
[flaml.automl: 09-18 16:36:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:36:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:36:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:36:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:36:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:36:35] {3025} INFO - Estimated sufficient time budget=12703s. Estimated necessary time budget=13s.
[flaml.automl: 09-18 16:36:35] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.8075,	best estimator xgboost's best error=15.8075
[flaml.automl: 09-18 16:36:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:36:37] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.6271,	best estimator xgboost's best error=7.6271
[flaml.automl: 09-18 16:36:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:36:38] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.6271,	best estimator xgboost's best error=7.6271
[flaml.automl: 09-18 16:36:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:36:48] {3072} INFO -  at 14.8s,	estimator xgboost's best error=7.6271,	best estimator xgboost's best error=7.6271
[flaml.automl: 09-18 16:36:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:36:50] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.2961,	best estimator xgboost's best error=5.2961
[flaml.automl: 09-18 16:36:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:36:51] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.5904,	best estimator xgboost's best error=4.5904
[flaml.automl: 09-18 16:36:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:36:53] {3072} INFO -  at 19.2s,	estimator xgboost's best error=4.2080,	best estimator xgboost's best error=4.2080
[flaml.automl: 09-18 16:36:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:36:55] {3072} INFO -  at 21.9s,	estimator xgboost's best error=4.2080,	best estimator xgboost's best error=4.2080
[flaml.automl: 09-18 16:36:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:36:57] {3072} INFO -  at 23.5s,	estimator xgboost's best error=4.2080,	best estimator xgboost's best error=4.2080
[flaml.automl: 09-18 16:36:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:37:00] {3072} INFO -  at 26.6s,	estimator xgboost's best error=3.8149,	best estimator xgboost's best error=3.8149
[flaml.automl: 09-18 16:37:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:37:02] {3072} INFO -  at 28.2s,	estimator xgboost's best error=3.8149,	best estimator xgboost's best error=3.8149
[flaml.automl: 09-18 16:37:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:37:03] {3072} INFO -  at 29.4s,	estimator xgboost's best error=3.8149,	best estimator xgboost's best error=3.8149
[flaml.automl: 09-18 16:37:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:37:17] {3072} INFO -  at 43.0s,	estimator xgboost's best error=3.4233,	best estimator xgboost's best error=3.4233
[flaml.automl: 09-18 16:37:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:37:33] {3072} INFO -  at 59.5s,	estimator xgboost's best error=3.3561,	best estimator xgboost's best error=3.3561
[flaml.automl: 09-18 16:37:57] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-18 16:37:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:37:57] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:37:57] {2637} INFO - Time taken to find the best model: 59.51985502243042
[flaml.automl: 09-18 16:37:57] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-2.3561294854315418
PM2.5(0)最好结果：{'pred_time': 1.1690183773708968e-05, 'wall_clock_time': 59.51985502243042, 'metric_for_logging': {'pred_time': 1.1690183773708968e-05}, 'val_loss': 3.3561294854315418, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.49378514289856}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9196028159021432
PM2.5(0)的mse=20.55395315464503
PM2.5(0)的mae=3.3323578657293527
PM2.5(0)的mar=0.2130575178438322
总共花费的时间为：83.95
遂宁市
2527A
2528A
2529A
2530A
[flaml.automl: 09-18 16:50:54] {2390} INFO - task = regression
[flaml.automl: 09-18 16:50:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:50:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:50:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:50:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:50:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:50:55] {3025} INFO - Estimated sufficient time budget=50874s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 16:50:55] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.0406,	best estimator xgboost's best error=18.0406
[flaml.automl: 09-18 16:50:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:50:57] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.5829,	best estimator xgboost's best error=8.5829
[flaml.automl: 09-18 16:50:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:50:58] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.5829,	best estimator xgboost's best error=8.5829
[flaml.automl: 09-18 16:50:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:51:05] {3072} INFO -  at 11.1s,	estimator xgboost's best error=8.5829,	best estimator xgboost's best error=8.5829
[flaml.automl: 09-18 16:51:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:51:06] {3072} INFO -  at 12.2s,	estimator xgboost's best error=5.7468,	best estimator xgboost's best error=5.7468
[flaml.automl: 09-18 16:51:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:51:07] {3072} INFO -  at 13.8s,	estimator xgboost's best error=5.6816,	best estimator xgboost's best error=5.6816
[flaml.automl: 09-18 16:51:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:51:09] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.3094,	best estimator xgboost's best error=4.3094
[flaml.automl: 09-18 16:51:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:51:12] {3072} INFO -  at 18.1s,	estimator xgboost's best error=4.3094,	best estimator xgboost's best error=4.3094
[flaml.automl: 09-18 16:51:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:51:13] {3072} INFO -  at 19.7s,	estimator xgboost's best error=4.3094,	best estimator xgboost's best error=4.3094
[flaml.automl: 09-18 16:51:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:51:16] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.5986,	best estimator xgboost's best error=3.5986
[flaml.automl: 09-18 16:51:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:51:18] {3072} INFO -  at 24.4s,	estimator xgboost's best error=3.5986,	best estimator xgboost's best error=3.5986
[flaml.automl: 09-18 16:51:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:51:19] {3072} INFO -  at 25.5s,	estimator xgboost's best error=3.5986,	best estimator xgboost's best error=3.5986
[flaml.automl: 09-18 16:51:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:51:23] {3072} INFO -  at 29.3s,	estimator xgboost's best error=3.4072,	best estimator xgboost's best error=3.4072
[flaml.automl: 09-18 16:51:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:51:26] {3072} INFO -  at 32.2s,	estimator xgboost's best error=3.4072,	best estimator xgboost's best error=3.4072
[flaml.automl: 09-18 16:51:26] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 16:51:28] {3072} INFO -  at 34.7s,	estimator xgboost's best error=3.4072,	best estimator xgboost's best error=3.4072
[flaml.automl: 09-18 16:51:28] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 16:51:30] {3072} INFO -  at 36.5s,	estimator xgboost's best error=3.4072,	best estimator xgboost's best error=3.4072
[flaml.automl: 09-18 16:51:30] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 16:51:32] {3072} INFO -  at 38.8s,	estimator xgboost's best error=3.2132,	best estimator xgboost's best error=3.2132
[flaml.automl: 09-18 16:51:32] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 16:51:35] {3072} INFO -  at 41.1s,	estimator xgboost's best error=3.2132,	best estimator xgboost's best error=3.2132
[flaml.automl: 09-18 16:51:35] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 16:51:36] {3072} INFO -  at 42.4s,	estimator xgboost's best error=3.2132,	best estimator xgboost's best error=3.2132
[flaml.automl: 09-18 16:51:36] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 16:51:38] {3072} INFO -  at 44.4s,	estimator xgboost's best error=3.2132,	best estimator xgboost's best error=3.2132
[flaml.automl: 09-18 16:51:38] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 16:51:39] {3072} INFO -  at 45.5s,	estimator xgboost's best error=3.2132,	best estimator xgboost's best error=3.2132
[flaml.automl: 09-18 16:51:39] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 16:51:47] {3072} INFO -  at 53.5s,	estimator xgboost's best error=3.0032,	best estimator xgboost's best error=3.0032
[flaml.automl: 09-18 16:51:55] {3335} INFO - retrain xgboost for 7.9s
[flaml.automl: 09-18 16:51:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9508280259547836, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8344084316033451,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=0.0066459383199444525, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0031424921315017056, reg_lambda=0.10503395230912585,
             scale_pos_weight=1, subsample=0.8568446847279476,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 16:51:55] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:51:55] {2637} INFO - Time taken to find the best model: 53.493425130844116
[flaml.automl: 09-18 16:51:55] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 9, 'min_child_weight': 0.0066459383199444525, 'learning_rate': 0.8344084316033451, 'subsample': 0.8568446847279476, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9508280259547836, 'reg_alpha': 0.0031424921315017056, 'reg_lambda': 0.10503395230912585, 'FLAML_sample_size': 42695}
PM2.5(0)最佳损失：-2.0031710886985414
PM2.5(0)最好结果：{'pred_time': 8.600709008125189e-06, 'wall_clock_time': 53.493425130844116, 'metric_for_logging': {'pred_time': 8.600709008125189e-06}, 'val_loss': 3.0031710886985414, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 9, 'min_child_weight': 0.0066459383199444525, 'learning_rate': 0.8344084316033451, 'subsample': 0.8568446847279476, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9508280259547836, 'reg_alpha': 0.0031424921315017056, 'reg_lambda': 0.10503395230912585, 'FLAML_sample_size': 42695}, 'config/n_estimators': 15, 'config/max_leaves': 9, 'config/min_child_weight': 0.0066459383199444525, 'config/learning_rate': 0.8344084316033451, 'config/subsample': 0.8568446847279476, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9508280259547836, 'config/reg_alpha': 0.0031424921315017056, 'config/reg_lambda': 0.10503395230912585, 'config/FLAML_sample_size': 42695, 'experiment_tag': 'exp', 'time_total_s': 7.950473308563232}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9508280259547836, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8344084316033451,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=0.0066459383199444525, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0031424921315017056, reg_lambda=0.10503395230912585,
             scale_pos_weight=1, subsample=0.8568446847279476,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9552051230740625
PM2.5(0)的mse=18.02415616371309
PM2.5(0)的mae=3.0563769557135965
PM2.5(0)的mar=0.18646679481045436
总共花费的时间为：62.10
内江市
2531A
2532A
2533A
2534A
[flaml.automl: 09-18 17:04:24] {2390} INFO - task = regression
[flaml.automl: 09-18 17:04:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:04:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:04:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:04:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:04:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:04:26] {3025} INFO - Estimated sufficient time budget=100530s. Estimated necessary time budget=101s.
[flaml.automl: 09-18 17:04:26] {3072} INFO -  at 2.6s,	estimator xgboost's best error=19.7025,	best estimator xgboost's best error=19.7025
[flaml.automl: 09-18 17:04:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:04:30] {3072} INFO -  at 6.5s,	estimator xgboost's best error=9.6832,	best estimator xgboost's best error=9.6832
[flaml.automl: 09-18 17:04:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:04:32] {3072} INFO -  at 8.5s,	estimator xgboost's best error=9.6832,	best estimator xgboost's best error=9.6832
[flaml.automl: 09-18 17:04:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:04:37] {3072} INFO -  at 13.5s,	estimator xgboost's best error=9.6832,	best estimator xgboost's best error=9.6832
[flaml.automl: 09-18 17:04:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:04:39] {3072} INFO -  at 15.3s,	estimator xgboost's best error=7.3022,	best estimator xgboost's best error=7.3022
[flaml.automl: 09-18 17:04:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:04:42] {3072} INFO -  at 18.1s,	estimator xgboost's best error=7.3022,	best estimator xgboost's best error=7.3022
[flaml.automl: 09-18 17:04:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:04:45] {3072} INFO -  at 21.1s,	estimator xgboost's best error=5.9694,	best estimator xgboost's best error=5.9694
[flaml.automl: 09-18 17:04:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:04:49] {3072} INFO -  at 25.5s,	estimator xgboost's best error=5.9694,	best estimator xgboost's best error=5.9694
[flaml.automl: 09-18 17:04:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:04:52] {3072} INFO -  at 28.2s,	estimator xgboost's best error=5.9694,	best estimator xgboost's best error=5.9694
[flaml.automl: 09-18 17:04:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:04:54] {3072} INFO -  at 30.9s,	estimator xgboost's best error=5.9694,	best estimator xgboost's best error=5.9694
[flaml.automl: 09-18 17:04:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:04:57] {3072} INFO -  at 33.3s,	estimator xgboost's best error=5.9694,	best estimator xgboost's best error=5.9694
[flaml.automl: 09-18 17:04:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:05:00] {3072} INFO -  at 36.4s,	estimator xgboost's best error=5.9482,	best estimator xgboost's best error=5.9482
[flaml.automl: 09-18 17:05:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:05:02] {3072} INFO -  at 38.6s,	estimator xgboost's best error=5.9482,	best estimator xgboost's best error=5.9482
[flaml.automl: 09-18 17:05:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:05:13] {3072} INFO -  at 49.5s,	estimator xgboost's best error=5.5320,	best estimator xgboost's best error=5.5320
[flaml.automl: 09-18 17:05:25] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-18 17:05:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:05:25] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:05:25] {2637} INFO - Time taken to find the best model: 49.51894927024841
[flaml.automl: 09-18 17:05:25] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 43865}
PM2.5(0)最佳损失：-4.531980169078684
PM2.5(0)最好结果：{'pred_time': 1.6129403315854452e-05, 'wall_clock_time': 49.51894927024841, 'metric_for_logging': {'pred_time': 1.6129403315854452e-05}, 'val_loss': 5.531980169078684, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 43865}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 43865, 'experiment_tag': 'exp', 'time_total_s': 10.925140619277954}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8866845264591492
PM2.5(0)的mse=56.96953424375802
PM2.5(0)的mae=5.261372089650113
PM2.5(0)的mar=0.2392545902134163
总共花费的时间为：62.24
眉山市
2539A
2540A
3137A
3148A
[flaml.automl: 09-18 17:18:38] {2390} INFO - task = regression
[flaml.automl: 09-18 17:18:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:18:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:18:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:18:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:18:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:18:40] {3025} INFO - Estimated sufficient time budget=74817s. Estimated necessary time budget=75s.
[flaml.automl: 09-18 17:18:40] {3072} INFO -  at 2.0s,	estimator xgboost's best error=20.3350,	best estimator xgboost's best error=20.3350
[flaml.automl: 09-18 17:18:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:18:43] {3072} INFO -  at 5.3s,	estimator xgboost's best error=9.8719,	best estimator xgboost's best error=9.8719
[flaml.automl: 09-18 17:18:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:18:45] {3072} INFO -  at 6.9s,	estimator xgboost's best error=9.8719,	best estimator xgboost's best error=9.8719
[flaml.automl: 09-18 17:18:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:18:50] {3072} INFO -  at 12.3s,	estimator xgboost's best error=9.8719,	best estimator xgboost's best error=9.8719
[flaml.automl: 09-18 17:18:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:18:52] {3072} INFO -  at 14.5s,	estimator xgboost's best error=6.8814,	best estimator xgboost's best error=6.8814
[flaml.automl: 09-18 17:18:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:18:55] {3072} INFO -  at 17.3s,	estimator xgboost's best error=6.0350,	best estimator xgboost's best error=6.0350
[flaml.automl: 09-18 17:18:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:18:58] {3072} INFO -  at 20.1s,	estimator xgboost's best error=5.5333,	best estimator xgboost's best error=5.5333
[flaml.automl: 09-18 17:18:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:19:01] {3072} INFO -  at 22.8s,	estimator xgboost's best error=5.5333,	best estimator xgboost's best error=5.5333
[flaml.automl: 09-18 17:19:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:19:02] {3072} INFO -  at 24.4s,	estimator xgboost's best error=5.4339,	best estimator xgboost's best error=5.4339
[flaml.automl: 09-18 17:19:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:19:05] {3072} INFO -  at 27.4s,	estimator xgboost's best error=4.9789,	best estimator xgboost's best error=4.9789
[flaml.automl: 09-18 17:19:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:19:06] {3072} INFO -  at 28.5s,	estimator xgboost's best error=4.9789,	best estimator xgboost's best error=4.9789
[flaml.automl: 09-18 17:19:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:19:09] {3072} INFO -  at 31.4s,	estimator xgboost's best error=4.7895,	best estimator xgboost's best error=4.7895
[flaml.automl: 09-18 17:19:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:19:12] {3072} INFO -  at 34.3s,	estimator xgboost's best error=4.7895,	best estimator xgboost's best error=4.7895
[flaml.automl: 09-18 17:19:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:19:15] {3072} INFO -  at 37.3s,	estimator xgboost's best error=4.7249,	best estimator xgboost's best error=4.7249
[flaml.automl: 09-18 17:19:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 17:19:17] {3072} INFO -  at 39.1s,	estimator xgboost's best error=4.7249,	best estimator xgboost's best error=4.7249
[flaml.automl: 09-18 17:19:17] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 17:19:19] {3072} INFO -  at 41.3s,	estimator xgboost's best error=4.5752,	best estimator xgboost's best error=4.5752
[flaml.automl: 09-18 17:19:19] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 17:19:21] {3072} INFO -  at 43.0s,	estimator xgboost's best error=4.5752,	best estimator xgboost's best error=4.5752
[flaml.automl: 09-18 17:19:21] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 17:19:22] {3072} INFO -  at 44.7s,	estimator xgboost's best error=4.5752,	best estimator xgboost's best error=4.5752
[flaml.automl: 09-18 17:19:22] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 17:19:24] {3072} INFO -  at 46.1s,	estimator xgboost's best error=4.5752,	best estimator xgboost's best error=4.5752
[flaml.automl: 09-18 17:19:24] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 17:19:25] {3072} INFO -  at 47.7s,	estimator xgboost's best error=4.3863,	best estimator xgboost's best error=4.3863
[flaml.automl: 09-18 17:19:25] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 17:19:28] {3072} INFO -  at 50.3s,	estimator xgboost's best error=4.3863,	best estimator xgboost's best error=4.3863
[flaml.automl: 09-18 17:19:28] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 17:19:29] {3072} INFO -  at 51.2s,	estimator xgboost's best error=4.3863,	best estimator xgboost's best error=4.3863
[flaml.automl: 09-18 17:19:29] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-18 17:19:32] {3072} INFO -  at 54.6s,	estimator xgboost's best error=4.3863,	best estimator xgboost's best error=4.3863
[flaml.automl: 09-18 17:19:32] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-18 17:19:33] {3072} INFO -  at 55.3s,	estimator xgboost's best error=4.3863,	best estimator xgboost's best error=4.3863
[flaml.automl: 09-18 17:19:33] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-18 17:19:37] {3072} INFO -  at 59.5s,	estimator xgboost's best error=4.3273,	best estimator xgboost's best error=4.3273
[flaml.automl: 09-18 17:19:51] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-18 17:19:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9541264657782791, colsample_bynode=1,
             colsample_bytree=0.7429904345764219, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=24, min_child_weight=0.0010757113963760995,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.04580371554194545,
             reg_lambda=1.1840031021583481, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 17:19:51] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:19:51] {2637} INFO - Time taken to find the best model: 59.54519605636597
[flaml.automl: 09-18 17:19:51] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 10, 'max_leaves': 24, 'min_child_weight': 0.0010757113963760995, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.9541264657782791, 'colsample_bytree': 0.7429904345764219, 'reg_alpha': 0.04580371554194545, 'reg_lambda': 1.1840031021583481, 'FLAML_sample_size': 41686}
PM2.5(0)最佳损失：-3.3273311789068742
PM2.5(0)最好结果：{'pred_time': 8.628312787861404e-06, 'wall_clock_time': 59.54519605636597, 'metric_for_logging': {'pred_time': 8.628312787861404e-06}, 'val_loss': 4.327331178906874, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 24, 'min_child_weight': 0.0010757113963760995, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.9541264657782791, 'colsample_bytree': 0.7429904345764219, 'reg_alpha': 0.04580371554194545, 'reg_lambda': 1.1840031021583481, 'FLAML_sample_size': 41686}, 'config/n_estimators': 10, 'config/max_leaves': 24, 'config/min_child_weight': 0.0010757113963760995, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9541264657782791, 'config/colsample_bytree': 0.7429904345764219, 'config/reg_alpha': 0.04580371554194545, 'config/reg_lambda': 1.1840031021583481, 'config/FLAML_sample_size': 41686, 'experiment_tag': 'exp', 'time_total_s': 4.245647668838501}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9541264657782791, colsample_bynode=1,
             colsample_bytree=0.7429904345764219, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=24, min_child_weight=0.0010757113963760995,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.04580371554194545,
             reg_lambda=1.1840031021583481, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9204428885832826
PM2.5(0)的mse=37.516870167268735
PM2.5(0)的mae=4.3811036962339065
PM2.5(0)的mar=0.19982542933304986
总共花费的时间为：73.93
广安市
2543A
2544A
2545A
2902A
[flaml.automl: 09-18 17:32:14] {2390} INFO - task = regression
[flaml.automl: 09-18 17:32:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:32:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:32:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:32:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:32:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:32:16] {3025} INFO - Estimated sufficient time budget=74653s. Estimated necessary time budget=75s.
[flaml.automl: 09-18 17:32:16] {3072} INFO -  at 2.1s,	estimator xgboost's best error=19.5598,	best estimator xgboost's best error=19.5598
[flaml.automl: 09-18 17:32:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:32:19] {3072} INFO -  at 5.3s,	estimator xgboost's best error=9.3189,	best estimator xgboost's best error=9.3189
[flaml.automl: 09-18 17:32:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:32:21] {3072} INFO -  at 7.5s,	estimator xgboost's best error=9.3189,	best estimator xgboost's best error=9.3189
[flaml.automl: 09-18 17:32:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:32:27] {3072} INFO -  at 13.5s,	estimator xgboost's best error=9.3189,	best estimator xgboost's best error=9.3189
[flaml.automl: 09-18 17:32:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:32:29] {3072} INFO -  at 15.6s,	estimator xgboost's best error=6.4458,	best estimator xgboost's best error=6.4458
[flaml.automl: 09-18 17:32:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:32:32] {3072} INFO -  at 18.1s,	estimator xgboost's best error=5.3510,	best estimator xgboost's best error=5.3510
[flaml.automl: 09-18 17:32:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:32:34] {3072} INFO -  at 20.3s,	estimator xgboost's best error=4.9047,	best estimator xgboost's best error=4.9047
[flaml.automl: 09-18 17:32:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:32:37] {3072} INFO -  at 23.0s,	estimator xgboost's best error=4.9047,	best estimator xgboost's best error=4.9047
[flaml.automl: 09-18 17:32:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:32:38] {3072} INFO -  at 24.6s,	estimator xgboost's best error=4.9047,	best estimator xgboost's best error=4.9047
[flaml.automl: 09-18 17:32:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:32:41] {3072} INFO -  at 27.6s,	estimator xgboost's best error=4.1334,	best estimator xgboost's best error=4.1334
[flaml.automl: 09-18 17:32:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:32:43] {3072} INFO -  at 29.2s,	estimator xgboost's best error=4.1334,	best estimator xgboost's best error=4.1334
[flaml.automl: 09-18 17:32:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:32:44] {3072} INFO -  at 30.3s,	estimator xgboost's best error=4.1334,	best estimator xgboost's best error=4.1334
[flaml.automl: 09-18 17:32:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:32:47] {3072} INFO -  at 33.2s,	estimator xgboost's best error=4.0474,	best estimator xgboost's best error=4.0474
[flaml.automl: 09-18 17:32:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:32:50] {3072} INFO -  at 36.1s,	estimator xgboost's best error=4.0474,	best estimator xgboost's best error=4.0474
[flaml.automl: 09-18 17:32:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 17:32:52] {3072} INFO -  at 38.6s,	estimator xgboost's best error=3.9462,	best estimator xgboost's best error=3.9462
[flaml.automl: 09-18 17:32:52] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 17:32:54] {3072} INFO -  at 40.8s,	estimator xgboost's best error=3.9462,	best estimator xgboost's best error=3.9462
[flaml.automl: 09-18 17:32:54] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 17:32:57] {3072} INFO -  at 43.1s,	estimator xgboost's best error=3.7119,	best estimator xgboost's best error=3.7119
[flaml.automl: 09-18 17:32:57] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 17:32:58] {3072} INFO -  at 44.7s,	estimator xgboost's best error=3.7119,	best estimator xgboost's best error=3.7119
[flaml.automl: 09-18 17:32:58] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 17:33:00] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.7119,	best estimator xgboost's best error=3.7119
[flaml.automl: 09-18 17:33:00] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 17:33:02] {3072} INFO -  at 48.0s,	estimator xgboost's best error=3.7119,	best estimator xgboost's best error=3.7119
[flaml.automl: 09-18 17:33:02] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 17:33:03] {3072} INFO -  at 49.5s,	estimator xgboost's best error=3.6753,	best estimator xgboost's best error=3.6753
[flaml.automl: 09-18 17:33:03] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 17:33:06] {3072} INFO -  at 52.1s,	estimator xgboost's best error=3.6753,	best estimator xgboost's best error=3.6753
[flaml.automl: 09-18 17:33:06] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-18 17:33:07] {3072} INFO -  at 53.1s,	estimator xgboost's best error=3.6753,	best estimator xgboost's best error=3.6753
[flaml.automl: 09-18 17:33:07] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-18 17:33:10] {3072} INFO -  at 56.4s,	estimator xgboost's best error=3.6522,	best estimator xgboost's best error=3.6522
[flaml.automl: 09-18 17:33:10] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-18 17:33:11] {3072} INFO -  at 57.9s,	estimator xgboost's best error=3.6522,	best estimator xgboost's best error=3.6522
[flaml.automl: 09-18 17:33:59] {3335} INFO - retrain xgboost for 47.5s
[flaml.automl: 09-18 17:33:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 17:33:59] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:33:59] {2637} INFO - Time taken to find the best model: 56.39788317680359
[flaml.automl: 09-18 17:33:59] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}
PM2.5(0)最佳损失：-2.6522407751446884
PM2.5(0)最好结果：{'pred_time': 9.113576914697484e-06, 'wall_clock_time': 56.39788317680359, 'metric_for_logging': {'pred_time': 9.113576914697484e-06}, 'val_loss': 3.6522407751446884, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}, 'config/n_estimators': 15, 'config/max_leaves': 57, 'config/min_child_weight': 0.0011006053677836264, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532848210950454, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.0763912614998318, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.3461849689483643}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.946137375379913
PM2.5(0)的mse=26.97633692539295
PM2.5(0)的mae=3.5843775726110496
PM2.5(0)的mar=0.18563794317350216
总共花费的时间为：106.15
达州市
2548A
2549A
2550A
2551A
2552A
[flaml.automl: 09-18 17:48:56] {2390} INFO - task = regression
[flaml.automl: 09-18 17:48:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:48:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:48:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:48:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:48:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:48:57] {3025} INFO - Estimated sufficient time budget=62148s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 17:48:57] {3072} INFO -  at 1.4s,	estimator xgboost's best error=19.8219,	best estimator xgboost's best error=19.8219
[flaml.automl: 09-18 17:48:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:48:59] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.0473,	best estimator xgboost's best error=10.0473
[flaml.automl: 09-18 17:48:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:49:00] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.0473,	best estimator xgboost's best error=10.0473
[flaml.automl: 09-18 17:49:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:49:05] {3072} INFO -  at 9.5s,	estimator xgboost's best error=10.0473,	best estimator xgboost's best error=10.0473
[flaml.automl: 09-18 17:49:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:49:06] {3072} INFO -  at 10.6s,	estimator xgboost's best error=7.4222,	best estimator xgboost's best error=7.4222
[flaml.automl: 09-18 17:49:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:49:08] {3072} INFO -  at 12.1s,	estimator xgboost's best error=6.6172,	best estimator xgboost's best error=6.6172
[flaml.automl: 09-18 17:49:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:49:09] {3072} INFO -  at 13.8s,	estimator xgboost's best error=6.3645,	best estimator xgboost's best error=6.3645
[flaml.automl: 09-18 17:49:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:49:12] {3072} INFO -  at 16.5s,	estimator xgboost's best error=6.3645,	best estimator xgboost's best error=6.3645
[flaml.automl: 09-18 17:49:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:49:14] {3072} INFO -  at 18.1s,	estimator xgboost's best error=6.3645,	best estimator xgboost's best error=6.3645
[flaml.automl: 09-18 17:49:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:49:17] {3072} INFO -  at 21.1s,	estimator xgboost's best error=5.9060,	best estimator xgboost's best error=5.9060
[flaml.automl: 09-18 17:49:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:49:18] {3072} INFO -  at 22.7s,	estimator xgboost's best error=5.9060,	best estimator xgboost's best error=5.9060
[flaml.automl: 09-18 17:49:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:49:19] {3072} INFO -  at 23.8s,	estimator xgboost's best error=5.9060,	best estimator xgboost's best error=5.9060
[flaml.automl: 09-18 17:49:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:49:22] {3072} INFO -  at 26.7s,	estimator xgboost's best error=5.8678,	best estimator xgboost's best error=5.8678
[flaml.automl: 09-18 17:49:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:49:25] {3072} INFO -  at 29.6s,	estimator xgboost's best error=5.8678,	best estimator xgboost's best error=5.8678
[flaml.automl: 09-18 17:49:25] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 17:49:28] {3072} INFO -  at 32.1s,	estimator xgboost's best error=5.7878,	best estimator xgboost's best error=5.7878
[flaml.automl: 09-18 17:49:28] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 17:49:30] {3072} INFO -  at 34.4s,	estimator xgboost's best error=5.7878,	best estimator xgboost's best error=5.7878
[flaml.automl: 09-18 17:49:30] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 17:49:32] {3072} INFO -  at 36.6s,	estimator xgboost's best error=5.7878,	best estimator xgboost's best error=5.7878
[flaml.automl: 09-18 17:49:32] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 17:49:34] {3072} INFO -  at 38.3s,	estimator xgboost's best error=5.7878,	best estimator xgboost's best error=5.7878
[flaml.automl: 09-18 17:49:34] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 17:49:37] {3072} INFO -  at 41.1s,	estimator xgboost's best error=5.7878,	best estimator xgboost's best error=5.7878
[flaml.automl: 09-18 17:49:37] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 17:49:44] {3072} INFO -  at 48.4s,	estimator xgboost's best error=5.7163,	best estimator xgboost's best error=5.7163
[flaml.automl: 09-18 17:49:44] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 17:49:47] {3072} INFO -  at 51.1s,	estimator xgboost's best error=5.7163,	best estimator xgboost's best error=5.7163
[flaml.automl: 09-18 17:49:47] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 17:49:54] {3072} INFO -  at 58.1s,	estimator xgboost's best error=5.5431,	best estimator xgboost's best error=5.5431
[flaml.automl: 09-18 17:50:16] {3335} INFO - retrain xgboost for 22.8s
[flaml.automl: 09-18 17:50:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7869551594064775, colsample_bynode=1,
             colsample_bytree=0.7933397443475808, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=41, min_child_weight=0.009416638758491828,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009169417918441369, reg_lambda=0.1716204668378346,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:50:16] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:50:16] {2637} INFO - Time taken to find the best model: 58.136544704437256
[flaml.automl: 09-18 17:50:16] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 10, 'max_leaves': 41, 'min_child_weight': 0.009416638758491828, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7869551594064775, 'colsample_bytree': 0.7933397443475808, 'reg_alpha': 0.009169417918441369, 'reg_lambda': 0.1716204668378346, 'FLAML_sample_size': 52685}
PM2.5(0)最佳损失：-4.5431057936020345
PM2.5(0)最好结果：{'pred_time': 6.842947185426431e-06, 'wall_clock_time': 58.136544704437256, 'metric_for_logging': {'pred_time': 6.842947185426431e-06}, 'val_loss': 5.5431057936020345, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 41, 'min_child_weight': 0.009416638758491828, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7869551594064775, 'colsample_bytree': 0.7933397443475808, 'reg_alpha': 0.009169417918441369, 'reg_lambda': 0.1716204668378346, 'FLAML_sample_size': 52685}, 'config/n_estimators': 10, 'config/max_leaves': 41, 'config/min_child_weight': 0.009416638758491828, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7869551594064775, 'config/colsample_bytree': 0.7933397443475808, 'config/reg_alpha': 0.009169417918441369, 'config/reg_lambda': 0.1716204668378346, 'config/FLAML_sample_size': 52685, 'experiment_tag': 'exp', 'time_total_s': 7.01950216293335}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7869551594064775, colsample_bynode=1,
             colsample_bytree=0.7933397443475808, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=41, min_child_weight=0.009416638758491828,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009169417918441369, reg_lambda=0.1716204668378346,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8819292290743668
PM2.5(0)的mse=56.49599128870988
PM2.5(0)的mae=5.557886635569955
PM2.5(0)的mar=0.42787959244398704
总共花费的时间为：81.71
雅安市
2555A
2556A
[flaml.automl: 09-18 17:56:45] {2390} INFO - task = regression
[flaml.automl: 09-18 17:56:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:56:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:56:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:56:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:56:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:56:46] {3025} INFO - Estimated sufficient time budget=12302s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 17:56:46] {3072} INFO -  at 1.3s,	estimator xgboost's best error=16.2474,	best estimator xgboost's best error=16.2474
[flaml.automl: 09-18 17:56:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:56:48] {3072} INFO -  at 3.4s,	estimator xgboost's best error=7.6399,	best estimator xgboost's best error=7.6399
[flaml.automl: 09-18 17:56:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:56:50] {3072} INFO -  at 4.6s,	estimator xgboost's best error=7.6399,	best estimator xgboost's best error=7.6399
[flaml.automl: 09-18 17:56:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:57:01] {3072} INFO -  at 16.4s,	estimator xgboost's best error=7.6399,	best estimator xgboost's best error=7.6399
[flaml.automl: 09-18 17:57:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:57:03] {3072} INFO -  at 18.1s,	estimator xgboost's best error=5.1467,	best estimator xgboost's best error=5.1467
[flaml.automl: 09-18 17:57:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:57:05] {3072} INFO -  at 20.1s,	estimator xgboost's best error=5.1467,	best estimator xgboost's best error=5.1467
[flaml.automl: 09-18 17:57:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:57:07] {3072} INFO -  at 22.2s,	estimator xgboost's best error=3.0183,	best estimator xgboost's best error=3.0183
[flaml.automl: 09-18 17:57:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:57:11] {3072} INFO -  at 26.0s,	estimator xgboost's best error=3.0183,	best estimator xgboost's best error=3.0183
[flaml.automl: 09-18 17:57:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:57:13] {3072} INFO -  at 28.3s,	estimator xgboost's best error=3.0183,	best estimator xgboost's best error=3.0183
[flaml.automl: 09-18 17:57:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:57:18] {3072} INFO -  at 32.7s,	estimator xgboost's best error=3.0183,	best estimator xgboost's best error=3.0183
[flaml.automl: 09-18 17:57:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:57:21] {3072} INFO -  at 35.6s,	estimator xgboost's best error=2.9975,	best estimator xgboost's best error=2.9975
[flaml.automl: 09-18 17:57:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:57:23] {3072} INFO -  at 37.6s,	estimator xgboost's best error=2.9975,	best estimator xgboost's best error=2.9975
[flaml.automl: 09-18 17:57:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:57:33] {3072} INFO -  at 48.2s,	estimator xgboost's best error=2.4267,	best estimator xgboost's best error=2.4267
[flaml.automl: 09-18 17:57:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:57:44] {3072} INFO -  at 59.2s,	estimator xgboost's best error=2.3654,	best estimator xgboost's best error=2.3654
[flaml.automl: 09-18 17:57:59] {3335} INFO - retrain xgboost for 14.3s
[flaml.automl: 09-18 17:57:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:57:59] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:57:59] {2637} INFO - Time taken to find the best model: 59.21393871307373
[flaml.automl: 09-18 17:57:59] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-1.365425554233671
PM2.5(0)最好结果：{'pred_time': 3.005228689964244e-05, 'wall_clock_time': 59.21393871307373, 'metric_for_logging': {'pred_time': 3.005228689964244e-05}, 'val_loss': 2.365425554233671, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 10.994518995285034}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.957157102113702
PM2.5(0)的mse=12.572070387049893
PM2.5(0)的mae=2.318445482111954
PM2.5(0)的mar=0.12793363366423358
总共花费的时间为：74.00
巴中市
2914A
3183A
3616A
[flaml.automl: 09-18 18:07:52] {2390} INFO - task = regression
[flaml.automl: 09-18 18:07:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:07:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:07:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:07:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:07:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:07:53] {3025} INFO - Estimated sufficient time budget=12282s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 18:07:53] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.2146,	best estimator xgboost's best error=15.2146
[flaml.automl: 09-18 18:07:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:07:55] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.1040,	best estimator xgboost's best error=7.1040
[flaml.automl: 09-18 18:07:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:07:57] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.1040,	best estimator xgboost's best error=7.1040
[flaml.automl: 09-18 18:07:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:08:07] {3072} INFO -  at 14.6s,	estimator xgboost's best error=7.1040,	best estimator xgboost's best error=7.1040
[flaml.automl: 09-18 18:08:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:08:08] {3072} INFO -  at 15.8s,	estimator xgboost's best error=5.1968,	best estimator xgboost's best error=5.1968
[flaml.automl: 09-18 18:08:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:08:09] {3072} INFO -  at 17.4s,	estimator xgboost's best error=5.1762,	best estimator xgboost's best error=5.1762
[flaml.automl: 09-18 18:08:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:08:11] {3072} INFO -  at 19.0s,	estimator xgboost's best error=3.9543,	best estimator xgboost's best error=3.9543
[flaml.automl: 09-18 18:08:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:08:14] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.9543,	best estimator xgboost's best error=3.9543
[flaml.automl: 09-18 18:08:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:08:15] {3072} INFO -  at 23.3s,	estimator xgboost's best error=3.9543,	best estimator xgboost's best error=3.9543
[flaml.automl: 09-18 18:08:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:08:18] {3072} INFO -  at 26.3s,	estimator xgboost's best error=3.0479,	best estimator xgboost's best error=3.0479
[flaml.automl: 09-18 18:08:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:08:20] {3072} INFO -  at 27.9s,	estimator xgboost's best error=3.0479,	best estimator xgboost's best error=3.0479
[flaml.automl: 09-18 18:08:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:08:21] {3072} INFO -  at 29.0s,	estimator xgboost's best error=3.0479,	best estimator xgboost's best error=3.0479
[flaml.automl: 09-18 18:08:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:08:35] {3072} INFO -  at 42.5s,	estimator xgboost's best error=2.6975,	best estimator xgboost's best error=2.6975
[flaml.automl: 09-18 18:08:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:08:52] {3072} INFO -  at 59.6s,	estimator xgboost's best error=2.5345,	best estimator xgboost's best error=2.5345
[flaml.automl: 09-18 18:09:16] {3335} INFO - retrain xgboost for 24.0s
[flaml.automl: 09-18 18:09:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:09:16] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:09:16] {2637} INFO - Time taken to find the best model: 59.61457300186157
[flaml.automl: 09-18 18:09:16] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
PM2.5(0)最佳损失：-1.534494767190374
PM2.5(0)最好结果：{'pred_time': 1.0985454530206346e-05, 'wall_clock_time': 59.61457300186157, 'metric_for_logging': {'pred_time': 1.0985454530206346e-05}, 'val_loss': 2.534494767190374, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 17.122887134552002}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9427141096668249
PM2.5(0)的mse=16.41568660042083
PM2.5(0)的mae=2.527724282765813
PM2.5(0)的mar=0.1311648227209038
总共花费的时间为：84.20
资阳市
2561A
2562A
2563A
2564A
2565A
[flaml.automl: 09-18 18:24:36] {2390} INFO - task = regression
[flaml.automl: 09-18 18:24:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:24:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:24:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:24:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:24:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:24:37] {3025} INFO - Estimated sufficient time budget=62771s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 18:24:37] {3072} INFO -  at 1.5s,	estimator xgboost's best error=17.7920,	best estimator xgboost's best error=17.7920
[flaml.automl: 09-18 18:24:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:24:39] {3072} INFO -  at 3.6s,	estimator xgboost's best error=8.5975,	best estimator xgboost's best error=8.5975
[flaml.automl: 09-18 18:24:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:24:41] {3072} INFO -  at 4.8s,	estimator xgboost's best error=8.5975,	best estimator xgboost's best error=8.5975
[flaml.automl: 09-18 18:24:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:24:45] {3072} INFO -  at 9.6s,	estimator xgboost's best error=8.5975,	best estimator xgboost's best error=8.5975
[flaml.automl: 09-18 18:24:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:24:46] {3072} INFO -  at 10.7s,	estimator xgboost's best error=6.1738,	best estimator xgboost's best error=6.1738
[flaml.automl: 09-18 18:24:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:24:48] {3072} INFO -  at 12.3s,	estimator xgboost's best error=5.2868,	best estimator xgboost's best error=5.2868
[flaml.automl: 09-18 18:24:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:24:50] {3072} INFO -  at 13.9s,	estimator xgboost's best error=4.8934,	best estimator xgboost's best error=4.8934
[flaml.automl: 09-18 18:24:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:24:52] {3072} INFO -  at 16.6s,	estimator xgboost's best error=4.8934,	best estimator xgboost's best error=4.8934
[flaml.automl: 09-18 18:24:52] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:24:54] {3072} INFO -  at 18.2s,	estimator xgboost's best error=4.8934,	best estimator xgboost's best error=4.8934
[flaml.automl: 09-18 18:24:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:24:57] {3072} INFO -  at 21.2s,	estimator xgboost's best error=4.4138,	best estimator xgboost's best error=4.4138
[flaml.automl: 09-18 18:24:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:24:59] {3072} INFO -  at 22.8s,	estimator xgboost's best error=4.4138,	best estimator xgboost's best error=4.4138
[flaml.automl: 09-18 18:24:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:25:00] {3072} INFO -  at 23.9s,	estimator xgboost's best error=4.4138,	best estimator xgboost's best error=4.4138
[flaml.automl: 09-18 18:25:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:25:02] {3072} INFO -  at 26.8s,	estimator xgboost's best error=4.2678,	best estimator xgboost's best error=4.2678
[flaml.automl: 09-18 18:25:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:25:05] {3072} INFO -  at 29.6s,	estimator xgboost's best error=4.2678,	best estimator xgboost's best error=4.2678
[flaml.automl: 09-18 18:25:05] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 18:25:08] {3072} INFO -  at 32.1s,	estimator xgboost's best error=4.2180,	best estimator xgboost's best error=4.2180
[flaml.automl: 09-18 18:25:08] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 18:25:10] {3072} INFO -  at 34.3s,	estimator xgboost's best error=4.2180,	best estimator xgboost's best error=4.2180
[flaml.automl: 09-18 18:25:10] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 18:25:12] {3072} INFO -  at 36.5s,	estimator xgboost's best error=4.2104,	best estimator xgboost's best error=4.2104
[flaml.automl: 09-18 18:25:12] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 18:25:14] {3072} INFO -  at 38.2s,	estimator xgboost's best error=4.2104,	best estimator xgboost's best error=4.2104
[flaml.automl: 09-18 18:25:14] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 18:25:16] {3072} INFO -  at 39.8s,	estimator xgboost's best error=4.2104,	best estimator xgboost's best error=4.2104
[flaml.automl: 09-18 18:25:16] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 18:25:17] {3072} INFO -  at 41.4s,	estimator xgboost's best error=4.2104,	best estimator xgboost's best error=4.2104
[flaml.automl: 09-18 18:25:17] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 18:25:19] {3072} INFO -  at 42.9s,	estimator xgboost's best error=4.1300,	best estimator xgboost's best error=4.1300
[flaml.automl: 09-18 18:25:19] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 18:25:21] {3072} INFO -  at 45.5s,	estimator xgboost's best error=4.1300,	best estimator xgboost's best error=4.1300
[flaml.automl: 09-18 18:25:21] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-18 18:25:22] {3072} INFO -  at 46.4s,	estimator xgboost's best error=4.1300,	best estimator xgboost's best error=4.1300
[flaml.automl: 09-18 18:25:22] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-18 18:25:25] {3072} INFO -  at 49.8s,	estimator xgboost's best error=4.1255,	best estimator xgboost's best error=4.1255
[flaml.automl: 09-18 18:25:25] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-18 18:25:27] {3072} INFO -  at 51.3s,	estimator xgboost's best error=4.1255,	best estimator xgboost's best error=4.1255
[flaml.automl: 09-18 18:25:27] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-18 18:25:28] {3072} INFO -  at 52.4s,	estimator xgboost's best error=4.1255,	best estimator xgboost's best error=4.1255
[flaml.automl: 09-18 18:25:28] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-18 18:25:39] {3072} INFO -  at 63.2s,	estimator xgboost's best error=4.1255,	best estimator xgboost's best error=4.1255
[flaml.automl: 09-18 18:26:26] {3335} INFO - retrain xgboost for 47.6s
[flaml.automl: 09-18 18:26:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 18:26:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:26:26] {2637} INFO - Time taken to find the best model: 49.75916242599487
[flaml.automl: 09-18 18:26:26] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}
PM2.5(0)最佳损失：-3.125480701409778
PM2.5(0)最好结果：{'pred_time': 6.856298085663619e-06, 'wall_clock_time': 49.75916242599487, 'metric_for_logging': {'pred_time': 6.856298085663619e-06}, 'val_loss': 4.125480701409778, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 57, 'min_child_weight': 0.0011006053677836264, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532848210950454, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0763912614998318, 'FLAML_sample_size': 10000}, 'config/n_estimators': 15, 'config/max_leaves': 57, 'config/min_child_weight': 0.0011006053677836264, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532848210950454, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.0763912614998318, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.327876329421997}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532848210950454, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.0011006053677836264,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0763912614998318, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9449568239946387
PM2.5(0)的mse=27.903632815852554
PM2.5(0)的mae=3.9137324648771528
PM2.5(0)的mar=0.2336976211238284
总共花费的时间为：111.64
阿坝藏族羌族自治州
2566A
2567A
2568A
[flaml.automl: 09-18 18:36:05] {2390} INFO - task = regression
[flaml.automl: 09-18 18:36:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:36:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:36:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:36:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:36:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:36:09] {3025} INFO - Estimated sufficient time budget=45855s. Estimated necessary time budget=46s.
[flaml.automl: 09-18 18:36:09] {3072} INFO -  at 4.8s,	estimator xgboost's best error=8.4141,	best estimator xgboost's best error=8.4141
[flaml.automl: 09-18 18:36:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:36:18] {3072} INFO -  at 12.9s,	estimator xgboost's best error=4.1111,	best estimator xgboost's best error=4.1111
[flaml.automl: 09-18 18:36:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:36:22] {3072} INFO -  at 17.7s,	estimator xgboost's best error=4.1111,	best estimator xgboost's best error=4.1111
[flaml.automl: 09-18 18:36:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:36:55] {3072} INFO -  at 50.1s,	estimator xgboost's best error=4.1111,	best estimator xgboost's best error=4.1111
[flaml.automl: 09-18 18:36:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:36:58] {3072} INFO -  at 52.9s,	estimator xgboost's best error=3.1054,	best estimator xgboost's best error=3.1054
[flaml.automl: 09-18 18:36:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:37:01] {3072} INFO -  at 56.7s,	estimator xgboost's best error=3.1054,	best estimator xgboost's best error=3.1054
[flaml.automl: 09-18 18:37:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:37:04] {3072} INFO -  at 59.3s,	estimator xgboost's best error=3.0292,	best estimator xgboost's best error=3.0292
[flaml.automl: 09-18 18:37:06] {3335} INFO - retrain xgboost for 2.5s
[flaml.automl: 09-18 18:37:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 18:37:06] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:37:06] {2637} INFO - Time taken to find the best model: 59.25417184829712
[flaml.automl: 09-18 18:37:06] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-2.0292345163685317
PM2.5(0)最好结果：{'pred_time': 1.5245031374098671e-05, 'wall_clock_time': 59.25417184829712, 'metric_for_logging': {'pred_time': 1.5245031374098671e-05}, 'val_loss': 3.0292345163685317, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 2.5071563720703125}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.46526277223018697
PM2.5(0)的mse=14.479117928070483
PM2.5(0)的mae=2.8368207751051635
PM2.5(0)的mar=0.23963658570023974
总共花费的时间为：62.63
甘孜藏族自治州
3065A
[flaml.automl: 09-18 18:40:52] {2390} INFO - task = regression
[flaml.automl: 09-18 18:40:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:40:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:40:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:40:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:40:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:40:53] {3025} INFO - Estimated sufficient time budget=11802s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 18:40:53] {3072} INFO -  at 1.2s,	estimator xgboost's best error=4.0904,	best estimator xgboost's best error=4.0904
[flaml.automl: 09-18 18:40:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:40:55] {3072} INFO -  at 3.1s,	estimator xgboost's best error=3.0609,	best estimator xgboost's best error=3.0609
[flaml.automl: 09-18 18:40:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:40:56] {3072} INFO -  at 4.2s,	estimator xgboost's best error=3.0609,	best estimator xgboost's best error=3.0609
[flaml.automl: 09-18 18:40:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:41:03] {3072} INFO -  at 11.1s,	estimator xgboost's best error=3.0609,	best estimator xgboost's best error=3.0609
[flaml.automl: 09-18 18:41:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:41:04] {3072} INFO -  at 12.3s,	estimator xgboost's best error=2.6297,	best estimator xgboost's best error=2.6297
[flaml.automl: 09-18 18:41:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:41:05] {3072} INFO -  at 13.8s,	estimator xgboost's best error=2.5716,	best estimator xgboost's best error=2.5716
[flaml.automl: 09-18 18:41:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:41:07] {3072} INFO -  at 15.4s,	estimator xgboost's best error=2.5690,	best estimator xgboost's best error=2.5690
[flaml.automl: 09-18 18:41:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:41:09] {3072} INFO -  at 17.8s,	estimator xgboost's best error=2.5690,	best estimator xgboost's best error=2.5690
[flaml.automl: 09-18 18:41:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:41:11] {3072} INFO -  at 19.4s,	estimator xgboost's best error=2.5598,	best estimator xgboost's best error=2.5598
[flaml.automl: 09-18 18:41:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:41:13] {3072} INFO -  at 22.0s,	estimator xgboost's best error=2.5561,	best estimator xgboost's best error=2.5561
[flaml.automl: 09-18 18:41:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:41:15] {3072} INFO -  at 23.1s,	estimator xgboost's best error=2.5561,	best estimator xgboost's best error=2.5561
[flaml.automl: 09-18 18:41:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:41:24] {3072} INFO -  at 32.7s,	estimator xgboost's best error=2.5561,	best estimator xgboost's best error=2.5561
[flaml.automl: 09-18 18:41:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:41:29] {3072} INFO -  at 37.4s,	estimator xgboost's best error=2.5561,	best estimator xgboost's best error=2.5561
[flaml.automl: 09-18 18:41:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:41:31] {3072} INFO -  at 39.5s,	estimator xgboost's best error=2.5561,	best estimator xgboost's best error=2.5561
[flaml.automl: 09-18 18:41:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 18:41:35] {3072} INFO -  at 43.6s,	estimator xgboost's best error=2.5561,	best estimator xgboost's best error=2.5561
[flaml.automl: 09-18 18:41:35] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 18:41:37] {3072} INFO -  at 45.2s,	estimator xgboost's best error=2.5346,	best estimator xgboost's best error=2.5346
[flaml.automl: 09-18 18:41:37] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 18:41:38] {3072} INFO -  at 46.6s,	estimator xgboost's best error=2.5346,	best estimator xgboost's best error=2.5346
[flaml.automl: 09-18 18:41:38] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 18:41:41] {3072} INFO -  at 50.0s,	estimator xgboost's best error=2.5346,	best estimator xgboost's best error=2.5346
[flaml.automl: 09-18 18:41:41] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 18:41:43] {3072} INFO -  at 51.0s,	estimator xgboost's best error=2.5346,	best estimator xgboost's best error=2.5346
[flaml.automl: 09-18 18:41:43] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 18:41:46] {3072} INFO -  at 54.9s,	estimator xgboost's best error=2.5346,	best estimator xgboost's best error=2.5346
[flaml.automl: 09-18 18:41:46] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 18:41:51] {3072} INFO -  at 59.9s,	estimator xgboost's best error=2.5346,	best estimator xgboost's best error=2.5346
[flaml.automl: 09-18 18:41:53] {3335} INFO - retrain xgboost for 1.6s
[flaml.automl: 09-18 18:41:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8971253994836724, colsample_bynode=1,
             colsample_bytree=0.9191287173227136, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6402695867769796,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=0.014254334991933237, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007572884876201064, reg_lambda=4.910625482293322,
             scale_pos_weight=1, subsample=0.7687867539273257,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 18:41:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:41:53] {2637} INFO - Time taken to find the best model: 45.22121000289917
[flaml.automl: 09-18 18:41:53] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 0.014254334991933237, 'learning_rate': 0.6402695867769796, 'subsample': 0.7687867539273257, 'colsample_bylevel': 0.8971253994836724, 'colsample_bytree': 0.9191287173227136, 'reg_alpha': 0.007572884876201064, 'reg_lambda': 4.910625482293322}
PM2.5(0)最佳损失：-1.534638869611523
PM2.5(0)最好结果：{'pred_time': 3.3106783774331655e-05, 'wall_clock_time': 45.22121000289917, 'metric_for_logging': {'pred_time': 3.3106783774331655e-05}, 'val_loss': 2.534638869611523, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 0.014254334991933237, 'learning_rate': 0.6402695867769796, 'subsample': 0.7687867539273257, 'colsample_bylevel': 0.8971253994836724, 'colsample_bytree': 0.9191287173227136, 'reg_alpha': 0.007572884876201064, 'reg_lambda': 4.910625482293322}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 0.014254334991933237, 'config/learning_rate': 0.6402695867769796, 'config/subsample': 0.7687867539273257, 'config/colsample_bylevel': 0.8971253994836724, 'config/colsample_bytree': 0.9191287173227136, 'config/reg_alpha': 0.007572884876201064, 'config/reg_lambda': 4.910625482293322, 'experiment_tag': 'exp', 'time_total_s': 1.617130994796753}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8971253994836724, colsample_bynode=1,
             colsample_bytree=0.9191287173227136, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6402695867769796,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=0.014254334991933237, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007572884876201064, reg_lambda=4.910625482293322,
             scale_pos_weight=1, subsample=0.7687867539273257,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=-2.015085559625395
PM2.5(0)的mse=10.393906214492905
PM2.5(0)的mae=2.5814294014903933
PM2.5(0)的mar=0.7727704676692247
总共花费的时间为：61.64
凉山彝族自治州
2571A
2572A
2573A
2574A
2575A
[flaml.automl: 09-18 18:56:50] {2390} INFO - task = regression
[flaml.automl: 09-18 18:56:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:56:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:56:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:56:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:56:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:56:53] {3025} INFO - Estimated sufficient time budget=150997s. Estimated necessary time budget=151s.
[flaml.automl: 09-18 18:56:53] {3072} INFO -  at 3.2s,	estimator xgboost's best error=12.2567,	best estimator xgboost's best error=12.2567
[flaml.automl: 09-18 18:56:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:56:58] {3072} INFO -  at 8.2s,	estimator xgboost's best error=5.7380,	best estimator xgboost's best error=5.7380
[flaml.automl: 09-18 18:56:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:57:00] {3072} INFO -  at 11.0s,	estimator xgboost's best error=5.7380,	best estimator xgboost's best error=5.7380
[flaml.automl: 09-18 18:57:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:57:05] {3072} INFO -  at 15.2s,	estimator xgboost's best error=5.7380,	best estimator xgboost's best error=5.7380
[flaml.automl: 09-18 18:57:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:57:06] {3072} INFO -  at 16.4s,	estimator xgboost's best error=3.7867,	best estimator xgboost's best error=3.7867
[flaml.automl: 09-18 18:57:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:57:07] {3072} INFO -  at 18.0s,	estimator xgboost's best error=3.7867,	best estimator xgboost's best error=3.7867
[flaml.automl: 09-18 18:57:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:57:09] {3072} INFO -  at 19.7s,	estimator xgboost's best error=2.6446,	best estimator xgboost's best error=2.6446
[flaml.automl: 09-18 18:57:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:57:12] {3072} INFO -  at 22.3s,	estimator xgboost's best error=2.6446,	best estimator xgboost's best error=2.6446
[flaml.automl: 09-18 18:57:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:57:13] {3072} INFO -  at 24.0s,	estimator xgboost's best error=2.6446,	best estimator xgboost's best error=2.6446
[flaml.automl: 09-18 18:57:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:57:16] {3072} INFO -  at 27.0s,	estimator xgboost's best error=2.6446,	best estimator xgboost's best error=2.6446
[flaml.automl: 09-18 18:57:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:57:18] {3072} INFO -  at 28.4s,	estimator xgboost's best error=2.6371,	best estimator xgboost's best error=2.6371
[flaml.automl: 09-18 18:57:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:57:19] {3072} INFO -  at 29.6s,	estimator xgboost's best error=2.6371,	best estimator xgboost's best error=2.6371
[flaml.automl: 09-18 18:57:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:57:22] {3072} INFO -  at 32.4s,	estimator xgboost's best error=2.3498,	best estimator xgboost's best error=2.3498
[flaml.automl: 09-18 18:57:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:57:24] {3072} INFO -  at 34.6s,	estimator xgboost's best error=2.3487,	best estimator xgboost's best error=2.3487
[flaml.automl: 09-18 18:57:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 18:57:26] {3072} INFO -  at 36.9s,	estimator xgboost's best error=2.3487,	best estimator xgboost's best error=2.3487
[flaml.automl: 09-18 18:57:26] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 18:57:28] {3072} INFO -  at 38.4s,	estimator xgboost's best error=2.3487,	best estimator xgboost's best error=2.3487
[flaml.automl: 09-18 18:57:28] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 18:57:30] {3072} INFO -  at 40.3s,	estimator xgboost's best error=2.3487,	best estimator xgboost's best error=2.3487
[flaml.automl: 09-18 18:57:30] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 18:57:31] {3072} INFO -  at 41.9s,	estimator xgboost's best error=2.3487,	best estimator xgboost's best error=2.3487
[flaml.automl: 09-18 18:57:31] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 18:57:42] {3072} INFO -  at 52.6s,	estimator xgboost's best error=2.1984,	best estimator xgboost's best error=2.1984
[flaml.automl: 09-18 18:57:53] {3335} INFO - retrain xgboost for 10.8s
[flaml.automl: 09-18 18:57:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:57:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:57:53] {2637} INFO - Time taken to find the best model: 52.64680528640747
[flaml.automl: 09-18 18:57:53] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52187}
PM2.5(0)最佳损失：-1.198350373414657
PM2.5(0)最好结果：{'pred_time': 7.691188482688119e-06, 'wall_clock_time': 52.64680528640747, 'metric_for_logging': {'pred_time': 7.691188482688119e-06}, 'val_loss': 2.198350373414657, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52187}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 52187, 'experiment_tag': 'exp', 'time_total_s': 10.746083974838257}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9016086047523902
PM2.5(0)的mse=13.634181358949956
PM2.5(0)的mae=2.2418817412533154
PM2.5(0)的mar=0.13512859735495894
总共花费的时间为：64.50
六盘水市
2576A
2577A
2578A
2579A
2580A
[flaml.automl: 09-18 19:12:45] {2390} INFO - task = regression
[flaml.automl: 09-18 19:12:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:12:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:12:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:12:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:12:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:12:47] {3025} INFO - Estimated sufficient time budget=105617s. Estimated necessary time budget=106s.
[flaml.automl: 09-18 19:12:47] {3072} INFO -  at 2.2s,	estimator xgboost's best error=14.2836,	best estimator xgboost's best error=14.2836
[flaml.automl: 09-18 19:12:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:12:51] {3072} INFO -  at 6.0s,	estimator xgboost's best error=7.4745,	best estimator xgboost's best error=7.4745
[flaml.automl: 09-18 19:12:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:12:53] {3072} INFO -  at 8.0s,	estimator xgboost's best error=7.4745,	best estimator xgboost's best error=7.4745
[flaml.automl: 09-18 19:12:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:12:57] {3072} INFO -  at 12.1s,	estimator xgboost's best error=7.4745,	best estimator xgboost's best error=7.4745
[flaml.automl: 09-18 19:12:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:12:59] {3072} INFO -  at 14.0s,	estimator xgboost's best error=5.9471,	best estimator xgboost's best error=5.9471
[flaml.automl: 09-18 19:12:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:13:02] {3072} INFO -  at 16.7s,	estimator xgboost's best error=5.9471,	best estimator xgboost's best error=5.9471
[flaml.automl: 09-18 19:13:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:13:05] {3072} INFO -  at 19.7s,	estimator xgboost's best error=4.6454,	best estimator xgboost's best error=4.6454
[flaml.automl: 09-18 19:13:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:13:08] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.6454,	best estimator xgboost's best error=4.6454
[flaml.automl: 09-18 19:13:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:13:10] {3072} INFO -  at 25.4s,	estimator xgboost's best error=4.6454,	best estimator xgboost's best error=4.6454
[flaml.automl: 09-18 19:13:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:13:13] {3072} INFO -  at 28.1s,	estimator xgboost's best error=4.6454,	best estimator xgboost's best error=4.6454
[flaml.automl: 09-18 19:13:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:13:15] {3072} INFO -  at 30.5s,	estimator xgboost's best error=4.6454,	best estimator xgboost's best error=4.6454
[flaml.automl: 09-18 19:13:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:13:18] {3072} INFO -  at 33.3s,	estimator xgboost's best error=4.6001,	best estimator xgboost's best error=4.6001
[flaml.automl: 09-18 19:13:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:13:20] {3072} INFO -  at 35.4s,	estimator xgboost's best error=4.6001,	best estimator xgboost's best error=4.6001
[flaml.automl: 09-18 19:13:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:13:30] {3072} INFO -  at 44.9s,	estimator xgboost's best error=4.2862,	best estimator xgboost's best error=4.2862
[flaml.automl: 09-18 19:13:30] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 19:13:43] {3072} INFO -  at 57.8s,	estimator xgboost's best error=4.2081,	best estimator xgboost's best error=4.2081
[flaml.automl: 09-18 19:13:55] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 19:13:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:13:55] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:13:55] {2637} INFO - Time taken to find the best model: 57.76603841781616
[flaml.automl: 09-18 19:13:55] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 53824}
PM2.5(0)最佳损失：-3.208136148066888
PM2.5(0)最好结果：{'pred_time': 7.966552923164565e-06, 'wall_clock_time': 57.76603841781616, 'metric_for_logging': {'pred_time': 7.966552923164565e-06}, 'val_loss': 4.208136148066888, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 53824}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 53824, 'experiment_tag': 'exp', 'time_total_s': 12.825485944747925}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8437067383398353
PM2.5(0)的mse=32.80493642229854
PM2.5(0)的mae=4.185755166453552
PM2.5(0)的mar=0.36795376611106845
总共花费的时间为：71.65
安顺市
3122A
3123A
3124A
3125A
[flaml.automl: 09-18 19:26:13] {2390} INFO - task = regression
[flaml.automl: 09-18 19:26:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:26:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:26:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:26:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:26:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:26:18] {3025} INFO - Estimated sufficient time budget=196786s. Estimated necessary time budget=197s.
[flaml.automl: 09-18 19:26:18] {3072} INFO -  at 4.9s,	estimator xgboost's best error=14.1550,	best estimator xgboost's best error=14.1550
[flaml.automl: 09-18 19:26:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:26:24] {3072} INFO -  at 10.9s,	estimator xgboost's best error=8.8631,	best estimator xgboost's best error=8.8631
[flaml.automl: 09-18 19:26:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:26:28] {3072} INFO -  at 15.6s,	estimator xgboost's best error=8.8631,	best estimator xgboost's best error=8.8631
[flaml.automl: 09-18 19:26:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:26:33] {3072} INFO -  at 20.3s,	estimator xgboost's best error=8.8631,	best estimator xgboost's best error=8.8631
[flaml.automl: 09-18 19:26:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:26:36] {3072} INFO -  at 23.8s,	estimator xgboost's best error=6.9018,	best estimator xgboost's best error=6.9018
[flaml.automl: 09-18 19:26:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:26:40] {3072} INFO -  at 27.2s,	estimator xgboost's best error=6.4653,	best estimator xgboost's best error=6.4653
[flaml.automl: 09-18 19:26:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:26:43] {3072} INFO -  at 29.9s,	estimator xgboost's best error=6.4653,	best estimator xgboost's best error=6.4653
[flaml.automl: 09-18 19:26:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:26:45] {3072} INFO -  at 32.6s,	estimator xgboost's best error=6.4653,	best estimator xgboost's best error=6.4653
[flaml.automl: 09-18 19:26:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:26:48] {3072} INFO -  at 35.2s,	estimator xgboost's best error=6.4653,	best estimator xgboost's best error=6.4653
[flaml.automl: 09-18 19:26:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:26:51] {3072} INFO -  at 38.4s,	estimator xgboost's best error=6.4653,	best estimator xgboost's best error=6.4653
[flaml.automl: 09-18 19:26:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:26:56] {3072} INFO -  at 43.2s,	estimator xgboost's best error=4.4578,	best estimator xgboost's best error=4.4578
[flaml.automl: 09-18 19:26:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:26:59] {3072} INFO -  at 46.2s,	estimator xgboost's best error=4.4578,	best estimator xgboost's best error=4.4578
[flaml.automl: 09-18 19:26:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:27:07] {3072} INFO -  at 54.2s,	estimator xgboost's best error=3.4403,	best estimator xgboost's best error=3.4403
[flaml.automl: 09-18 19:27:13] {3335} INFO - retrain xgboost for 6.3s
[flaml.automl: 09-18 19:27:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 19:27:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:27:13] {2637} INFO - Time taken to find the best model: 54.24697422981262
[flaml.automl: 09-18 19:27:13] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 42662}
PM2.5(0)最佳损失：-2.440316500794005
PM2.5(0)最好结果：{'pred_time': 8.16602491673677e-06, 'wall_clock_time': 54.24697422981262, 'metric_for_logging': {'pred_time': 8.16602491673677e-06}, 'val_loss': 3.440316500794005, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 42662}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 42662, 'experiment_tag': 'exp', 'time_total_s': 8.079196453094482}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8887706108638872
PM2.5(0)的mse=24.111822216528587
PM2.5(0)的mae=3.2628935239162513
PM2.5(0)的mar=0.2116407314028634
总共花费的时间为：61.83
铜仁地区
2585A
2586A
[flaml.automl: 09-18 19:34:28] {2390} INFO - task = regression
[flaml.automl: 09-18 19:34:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:34:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:34:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:34:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:34:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:34:31] {3025} INFO - Estimated sufficient time budget=32149s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 19:34:31] {3072} INFO -  at 3.5s,	estimator xgboost's best error=11.3653,	best estimator xgboost's best error=11.3653
[flaml.automl: 09-18 19:34:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:34:36] {3072} INFO -  at 9.2s,	estimator xgboost's best error=5.7693,	best estimator xgboost's best error=5.7693
[flaml.automl: 09-18 19:34:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:34:40] {3072} INFO -  at 12.4s,	estimator xgboost's best error=5.7693,	best estimator xgboost's best error=5.7693
[flaml.automl: 09-18 19:34:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:35:04] {3072} INFO -  at 36.8s,	estimator xgboost's best error=5.7693,	best estimator xgboost's best error=5.7693
[flaml.automl: 09-18 19:35:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:35:06] {3072} INFO -  at 38.8s,	estimator xgboost's best error=4.4051,	best estimator xgboost's best error=4.4051
[flaml.automl: 09-18 19:35:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:35:09] {3072} INFO -  at 41.5s,	estimator xgboost's best error=4.4051,	best estimator xgboost's best error=4.4051
[flaml.automl: 09-18 19:35:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:35:11] {3072} INFO -  at 43.3s,	estimator xgboost's best error=3.5041,	best estimator xgboost's best error=3.5041
[flaml.automl: 09-18 19:35:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:35:13] {3072} INFO -  at 45.9s,	estimator xgboost's best error=3.5041,	best estimator xgboost's best error=3.5041
[flaml.automl: 09-18 19:35:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:35:15] {3072} INFO -  at 47.5s,	estimator xgboost's best error=3.5041,	best estimator xgboost's best error=3.5041
[flaml.automl: 09-18 19:35:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:35:18] {3072} INFO -  at 50.5s,	estimator xgboost's best error=3.5041,	best estimator xgboost's best error=3.5041
[flaml.automl: 09-18 19:35:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:35:19] {3072} INFO -  at 52.1s,	estimator xgboost's best error=3.4196,	best estimator xgboost's best error=3.4196
[flaml.automl: 09-18 19:35:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:35:21] {3072} INFO -  at 53.2s,	estimator xgboost's best error=3.4196,	best estimator xgboost's best error=3.4196
[flaml.automl: 09-18 19:35:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:35:27] {3072} INFO -  at 59.6s,	estimator xgboost's best error=3.2547,	best estimator xgboost's best error=3.2547
[flaml.automl: 09-18 19:35:33] {3335} INFO - retrain xgboost for 6.3s
[flaml.automl: 09-18 19:35:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:35:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:35:33] {2637} INFO - Time taken to find the best model: 59.588804483413696
[flaml.automl: 09-18 19:35:33] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
PM2.5(0)最佳损失：-2.254675633331148
PM2.5(0)最好结果：{'pred_time': 1.61519998775779e-05, 'wall_clock_time': 59.588804483413696, 'metric_for_logging': {'pred_time': 1.61519998775779e-05}, 'val_loss': 3.254675633331148, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 6.350403547286987}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9104861873940339
PM2.5(0)的mse=19.89100905396105
PM2.5(0)的mae=3.1224432403222417
PM2.5(0)的mar=0.27382930391216814
总共花费的时间为：66.27
毕节市
2587A
2588A
3537A
[flaml.automl: 09-18 19:46:20] {2390} INFO - task = regression
[flaml.automl: 09-18 19:46:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:46:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:46:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:46:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:46:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:46:23] {3025} INFO - Estimated sufficient time budget=32445s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 19:46:23] {3072} INFO -  at 3.4s,	estimator xgboost's best error=14.1511,	best estimator xgboost's best error=14.1511
[flaml.automl: 09-18 19:46:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:46:29] {3072} INFO -  at 9.1s,	estimator xgboost's best error=6.8809,	best estimator xgboost's best error=6.8809
[flaml.automl: 09-18 19:46:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:46:32] {3072} INFO -  at 12.2s,	estimator xgboost's best error=6.8809,	best estimator xgboost's best error=6.8809
[flaml.automl: 09-18 19:46:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:46:50] {3072} INFO -  at 30.0s,	estimator xgboost's best error=6.8809,	best estimator xgboost's best error=6.8809
[flaml.automl: 09-18 19:46:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:46:52] {3072} INFO -  at 31.9s,	estimator xgboost's best error=5.4351,	best estimator xgboost's best error=5.4351
[flaml.automl: 09-18 19:46:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:46:54] {3072} INFO -  at 34.6s,	estimator xgboost's best error=5.4351,	best estimator xgboost's best error=5.4351
[flaml.automl: 09-18 19:46:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:46:57] {3072} INFO -  at 37.2s,	estimator xgboost's best error=3.5079,	best estimator xgboost's best error=3.5079
[flaml.automl: 09-18 19:46:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:47:01] {3072} INFO -  at 41.6s,	estimator xgboost's best error=3.5079,	best estimator xgboost's best error=3.5079
[flaml.automl: 09-18 19:47:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:47:04] {3072} INFO -  at 44.5s,	estimator xgboost's best error=3.5079,	best estimator xgboost's best error=3.5079
[flaml.automl: 09-18 19:47:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:47:10] {3072} INFO -  at 49.9s,	estimator xgboost's best error=3.5079,	best estimator xgboost's best error=3.5079
[flaml.automl: 09-18 19:47:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:47:11] {3072} INFO -  at 51.9s,	estimator xgboost's best error=3.5079,	best estimator xgboost's best error=3.5079
[flaml.automl: 09-18 19:47:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:47:13] {3072} INFO -  at 53.0s,	estimator xgboost's best error=3.5079,	best estimator xgboost's best error=3.5079
[flaml.automl: 09-18 19:47:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:47:19] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.9844,	best estimator xgboost's best error=2.9844
[flaml.automl: 09-18 19:47:26] {3335} INFO - retrain xgboost for 6.9s
[flaml.automl: 09-18 19:47:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:47:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:47:26] {2637} INFO - Time taken to find the best model: 59.47699427604675
[flaml.automl: 09-18 19:47:26] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
PM2.5(0)最佳损失：-1.9844313031179177
PM2.5(0)最好结果：{'pred_time': 1.0506117130972106e-05, 'wall_clock_time': 59.47699427604675, 'metric_for_logging': {'pred_time': 1.0506117130972106e-05}, 'val_loss': 2.9844313031179177, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 6.472982168197632}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9036063875563811
PM2.5(0)的mse=24.544951704497944
PM2.5(0)的mae=3.104223757631807
PM2.5(0)的mar=0.20513208718361398
总共花费的时间为：66.97
黔西南布依族苗族自治州
2589A
2590A
[flaml.automl: 09-18 19:54:08] {2390} INFO - task = regression
[flaml.automl: 09-18 19:54:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:54:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:54:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:54:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:54:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:54:13] {3025} INFO - Estimated sufficient time budget=45781s. Estimated necessary time budget=46s.
[flaml.automl: 09-18 19:54:13] {3072} INFO -  at 4.7s,	estimator xgboost's best error=11.1274,	best estimator xgboost's best error=11.1274
[flaml.automl: 09-18 19:54:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:54:21] {3072} INFO -  at 12.7s,	estimator xgboost's best error=5.5946,	best estimator xgboost's best error=5.5946
[flaml.automl: 09-18 19:54:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:54:25] {3072} INFO -  at 16.3s,	estimator xgboost's best error=5.5946,	best estimator xgboost's best error=5.5946
[flaml.automl: 09-18 19:54:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:54:48] {3072} INFO -  at 39.9s,	estimator xgboost's best error=5.5946,	best estimator xgboost's best error=5.5946
[flaml.automl: 09-18 19:54:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:54:50] {3072} INFO -  at 41.9s,	estimator xgboost's best error=4.4533,	best estimator xgboost's best error=4.4533
[flaml.automl: 09-18 19:54:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:54:53] {3072} INFO -  at 44.7s,	estimator xgboost's best error=4.0405,	best estimator xgboost's best error=4.0405
[flaml.automl: 09-18 19:54:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:54:56] {3072} INFO -  at 47.5s,	estimator xgboost's best error=3.8488,	best estimator xgboost's best error=3.8488
[flaml.automl: 09-18 19:54:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:55:00] {3072} INFO -  at 52.2s,	estimator xgboost's best error=3.8488,	best estimator xgboost's best error=3.8488
[flaml.automl: 09-18 19:55:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:55:02] {3072} INFO -  at 54.1s,	estimator xgboost's best error=3.8306,	best estimator xgboost's best error=3.8306
[flaml.automl: 09-18 19:55:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:55:05] {3072} INFO -  at 57.1s,	estimator xgboost's best error=3.7080,	best estimator xgboost's best error=3.7080
[flaml.automl: 09-18 19:55:08] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-18 19:55:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 19:55:08] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:55:08] {2637} INFO - Time taken to find the best model: 57.13513207435608
[flaml.automl: 09-18 19:55:08] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}
PM2.5(0)最佳损失：-2.70800126579247
PM2.5(0)最好结果：{'pred_time': 1.615403514159353e-05, 'wall_clock_time': 57.13513207435608, 'metric_for_logging': {'pred_time': 1.615403514159353e-05}, 'val_loss': 3.70800126579247, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.056902942069697794, 'config/learning_rate': 0.6286550459839113, 'config/subsample': 0.8219353140149964, 'config/colsample_bylevel': 0.6552426578592088, 'config/colsample_bytree': 0.8391963407651736, 'config/reg_alpha': 0.007347191482940116, 'config/reg_lambda': 9.546883854467293, 'experiment_tag': 'exp', 'time_total_s': 2.9983808994293213}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8590424459662505
PM2.5(0)的mse=25.32919505779461
PM2.5(0)的mae=3.6175035403270708
PM2.5(0)的mar=0.31484150173699665
总共花费的时间为：60.53
黔东南苗族侗族自治州
2591A
[flaml.automl: 09-18 19:58:30] {2390} INFO - task = regression
[flaml.automl: 09-18 19:58:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:58:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:58:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:58:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:58:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:58:33] {3025} INFO - Estimated sufficient time budget=32182s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 19:58:33] {3072} INFO -  at 3.3s,	estimator xgboost's best error=13.5765,	best estimator xgboost's best error=13.5765
[flaml.automl: 09-18 19:58:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:58:38] {3072} INFO -  at 8.3s,	estimator xgboost's best error=7.1983,	best estimator xgboost's best error=7.1983
[flaml.automl: 09-18 19:58:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:58:41] {3072} INFO -  at 11.5s,	estimator xgboost's best error=7.1983,	best estimator xgboost's best error=7.1983
[flaml.automl: 09-18 19:58:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:59:01] {3072} INFO -  at 30.6s,	estimator xgboost's best error=7.1983,	best estimator xgboost's best error=7.1983
[flaml.automl: 09-18 19:59:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:59:03] {3072} INFO -  at 33.2s,	estimator xgboost's best error=3.6145,	best estimator xgboost's best error=3.6145
[flaml.automl: 09-18 19:59:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:59:06] {3072} INFO -  at 36.0s,	estimator xgboost's best error=2.8714,	best estimator xgboost's best error=2.8714
[flaml.automl: 09-18 19:59:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:59:09] {3072} INFO -  at 39.0s,	estimator xgboost's best error=2.8587,	best estimator xgboost's best error=2.8587
[flaml.automl: 09-18 19:59:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:59:13] {3072} INFO -  at 43.3s,	estimator xgboost's best error=2.8587,	best estimator xgboost's best error=2.8587
[flaml.automl: 09-18 19:59:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:59:16] {3072} INFO -  at 46.2s,	estimator xgboost's best error=2.8587,	best estimator xgboost's best error=2.8587
[flaml.automl: 09-18 19:59:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:59:21] {3072} INFO -  at 51.1s,	estimator xgboost's best error=2.8587,	best estimator xgboost's best error=2.8587
[flaml.automl: 09-18 19:59:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:59:24] {3072} INFO -  at 54.2s,	estimator xgboost's best error=2.3951,	best estimator xgboost's best error=2.3951
[flaml.automl: 09-18 19:59:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:59:26] {3072} INFO -  at 56.3s,	estimator xgboost's best error=2.3951,	best estimator xgboost's best error=2.3951
[flaml.automl: 09-18 19:59:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:59:29] {3072} INFO -  at 59.3s,	estimator xgboost's best error=2.2593,	best estimator xgboost's best error=2.2593
[flaml.automl: 09-18 19:59:36] {3335} INFO - retrain xgboost for 7.0s
[flaml.automl: 09-18 19:59:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=8, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:59:36] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:59:36] {2637} INFO - Time taken to find the best model: 59.29287147521973
[flaml.automl: 09-18 19:59:36] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 8, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}
PM2.5(0)最佳损失：-1.2592873886168712
PM2.5(0)最好结果：{'pred_time': 6.119441609137614e-05, 'wall_clock_time': 59.29287147521973, 'metric_for_logging': {'pred_time': 6.119441609137614e-05}, 'val_loss': 2.259287388616871, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 8, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}, 'config/n_estimators': 12, 'config/max_leaves': 8, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'experiment_tag': 'exp', 'time_total_s': 2.9840445518493652}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=8, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.944961733557427
PM2.5(0)的mse=10.510975902124985
PM2.5(0)的mae=2.047446365627954
PM2.5(0)的mar=0.10557792297415199
总共花费的时间为：66.50
黔南布依族苗族自治州
2593A
3538A
[flaml.automl: 09-18 20:06:13] {2390} INFO - task = regression
[flaml.automl: 09-18 20:06:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:06:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:06:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:06:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:06:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:06:16] {3025} INFO - Estimated sufficient time budget=32885s. Estimated necessary time budget=33s.
[flaml.automl: 09-18 20:06:16] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.6584,	best estimator xgboost's best error=9.6584
[flaml.automl: 09-18 20:06:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:06:22] {3072} INFO -  at 9.0s,	estimator xgboost's best error=5.0299,	best estimator xgboost's best error=5.0299
[flaml.automl: 09-18 20:06:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:06:25] {3072} INFO -  at 12.2s,	estimator xgboost's best error=5.0299,	best estimator xgboost's best error=5.0299
[flaml.automl: 09-18 20:06:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:06:50] {3072} INFO -  at 37.8s,	estimator xgboost's best error=5.0299,	best estimator xgboost's best error=5.0299
[flaml.automl: 09-18 20:06:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:06:53] {3072} INFO -  at 41.0s,	estimator xgboost's best error=4.2739,	best estimator xgboost's best error=4.2739
[flaml.automl: 09-18 20:06:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:06:58] {3072} INFO -  at 45.1s,	estimator xgboost's best error=3.7860,	best estimator xgboost's best error=3.7860
[flaml.automl: 09-18 20:06:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:07:02] {3072} INFO -  at 49.5s,	estimator xgboost's best error=3.5082,	best estimator xgboost's best error=3.5082
[flaml.automl: 09-18 20:07:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:07:08] {3072} INFO -  at 55.9s,	estimator xgboost's best error=3.5082,	best estimator xgboost's best error=3.5082
[flaml.automl: 09-18 20:07:10] {3335} INFO - retrain xgboost for 1.6s
[flaml.automl: 09-18 20:07:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:07:10] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:07:10] {2637} INFO - Time taken to find the best model: 49.51921486854553
[flaml.automl: 09-18 20:07:10] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-2.5081568833123167
PM2.5(0)最好结果：{'pred_time': 4.6960374754849695e-05, 'wall_clock_time': 49.51921486854553, 'metric_for_logging': {'pred_time': 4.6960374754849695e-05}, 'val_loss': 3.5081568833123167, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.379558801651001}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8291178295778749
PM2.5(0)的mse=21.37971067774026
PM2.5(0)的mae=3.386078476240345
PM2.5(0)的mar=0.45657805120143685
总共花费的时间为：58.00
保山市
2594A
2595A
[flaml.automl: 09-18 20:14:34] {2390} INFO - task = regression
[flaml.automl: 09-18 20:14:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:14:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:14:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:14:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:14:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:14:39] {3025} INFO - Estimated sufficient time budget=45223s. Estimated necessary time budget=45s.
[flaml.automl: 09-18 20:14:39] {3072} INFO -  at 4.7s,	estimator xgboost's best error=11.4351,	best estimator xgboost's best error=11.4351
[flaml.automl: 09-18 20:14:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:14:47] {3072} INFO -  at 12.6s,	estimator xgboost's best error=5.9140,	best estimator xgboost's best error=5.9140
[flaml.automl: 09-18 20:14:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:14:51] {3072} INFO -  at 17.2s,	estimator xgboost's best error=5.9140,	best estimator xgboost's best error=5.9140
[flaml.automl: 09-18 20:14:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:15:26] {3072} INFO -  at 51.7s,	estimator xgboost's best error=5.9140,	best estimator xgboost's best error=5.9140
[flaml.automl: 09-18 20:15:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:15:28] {3072} INFO -  at 54.3s,	estimator xgboost's best error=4.3639,	best estimator xgboost's best error=4.3639
[flaml.automl: 09-18 20:15:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:15:30] {3072} INFO -  at 56.1s,	estimator xgboost's best error=4.0481,	best estimator xgboost's best error=4.0481
[flaml.automl: 09-18 20:15:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:15:32] {3072} INFO -  at 57.7s,	estimator xgboost's best error=3.8569,	best estimator xgboost's best error=3.8569
[flaml.automl: 09-18 20:15:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:15:33] {3072} INFO -  at 59.5s,	estimator xgboost's best error=3.8569,	best estimator xgboost's best error=3.8569
[flaml.automl: 09-18 20:15:35] {3335} INFO - retrain xgboost for 1.5s
[flaml.automl: 09-18 20:15:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:15:35] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:15:35] {2637} INFO - Time taken to find the best model: 57.711185693740845
[flaml.automl: 09-18 20:15:35] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-2.856871181548236
PM2.5(0)最好结果：{'pred_time': 1.6387118849643442e-05, 'wall_clock_time': 57.711185693740845, 'metric_for_logging': {'pred_time': 1.6387118849643442e-05}, 'val_loss': 3.856871181548236, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 1.5861351490020752}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.838397857617974
PM2.5(0)的mse=25.861595484776842
PM2.5(0)的mae=3.8602447529366555
PM2.5(0)的mar=0.40188913320201425
总共花费的时间为：61.57
昭通市
2596A
2597A
[flaml.automl: 09-18 20:22:45] {2390} INFO - task = regression
[flaml.automl: 09-18 20:22:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:22:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:22:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:22:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:22:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:22:48] {3025} INFO - Estimated sufficient time budget=31700s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 20:22:48] {3072} INFO -  at 3.4s,	estimator xgboost's best error=13.9826,	best estimator xgboost's best error=13.9826
[flaml.automl: 09-18 20:22:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:22:56] {3072} INFO -  at 11.5s,	estimator xgboost's best error=6.6036,	best estimator xgboost's best error=6.6036
[flaml.automl: 09-18 20:22:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:23:01] {3072} INFO -  at 16.1s,	estimator xgboost's best error=6.6036,	best estimator xgboost's best error=6.6036
[flaml.automl: 09-18 20:23:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:23:37] {3072} INFO -  at 52.6s,	estimator xgboost's best error=6.6036,	best estimator xgboost's best error=6.6036
[flaml.automl: 09-18 20:23:43] {3335} INFO - retrain xgboost for 5.5s
[flaml.automl: 09-18 20:23:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8620995499446478, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.14347380544831878, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=2.30934327968189, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=4.120667606735053, scale_pos_weight=1,
             subsample=0.9348203829099999, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:23:43] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:23:43] {2637} INFO - Time taken to find the best model: 11.495472192764282
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 4, 'min_child_weight': 2.30934327968189, 'learning_rate': 0.14347380544831878, 'subsample': 0.9348203829099999, 'colsample_bylevel': 0.8620995499446478, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 4.120667606735053}
PM2.5(0)最佳损失：-5.603616373261832
PM2.5(0)最好结果：{'pred_time': 7.853700617375889e-05, 'wall_clock_time': 11.495472192764282, 'metric_for_logging': {'pred_time': 7.853700617375889e-05}, 'val_loss': 6.603616373261832, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 4, 'min_child_weight': 2.30934327968189, 'learning_rate': 0.14347380544831878, 'subsample': 0.9348203829099999, 'colsample_bylevel': 0.8620995499446478, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 4.120667606735053}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/min_child_weight': 2.30934327968189, 'config/learning_rate': 0.14347380544831878, 'config/subsample': 0.9348203829099999, 'config/colsample_bylevel': 0.8620995499446478, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 4.120667606735053, 'experiment_tag': 'exp', 'time_total_s': 8.10042667388916}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8620995499446478, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.14347380544831878, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=2.30934327968189, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=4.120667606735053, scale_pos_weight=1,
             subsample=0.9348203829099999, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=-0.6230579926886799
PM2.5(0)的mse=75.08444980926546
PM2.5(0)的mae=6.652740911296215
PM2.5(0)的mar=0.274835577625145
总共花费的时间为：58.84
丽江市
2598A
2599A
2600A
[flaml.automl: 09-18 20:34:29] {2390} INFO - task = regression
[flaml.automl: 09-18 20:34:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:34:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:34:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:34:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:34:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:34:33] {3025} INFO - Estimated sufficient time budget=32672s. Estimated necessary time budget=33s.
[flaml.automl: 09-18 20:34:33] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.6639,	best estimator xgboost's best error=7.6639
[flaml.automl: 09-18 20:34:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:34:38] {3072} INFO -  at 9.2s,	estimator xgboost's best error=4.3115,	best estimator xgboost's best error=4.3115
[flaml.automl: 09-18 20:34:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:34:42] {3072} INFO -  at 12.4s,	estimator xgboost's best error=4.3115,	best estimator xgboost's best error=4.3115
[flaml.automl: 09-18 20:34:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:35:06] {3072} INFO -  at 36.4s,	estimator xgboost's best error=4.3115,	best estimator xgboost's best error=4.3115
[flaml.automl: 09-18 20:35:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:35:07] {3072} INFO -  at 38.1s,	estimator xgboost's best error=3.4372,	best estimator xgboost's best error=3.4372
[flaml.automl: 09-18 20:35:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:35:09] {3072} INFO -  at 40.2s,	estimator xgboost's best error=3.4327,	best estimator xgboost's best error=3.4327
[flaml.automl: 09-18 20:35:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:35:12] {3072} INFO -  at 42.7s,	estimator xgboost's best error=3.1211,	best estimator xgboost's best error=3.1211
[flaml.automl: 09-18 20:35:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:35:16] {3072} INFO -  at 46.3s,	estimator xgboost's best error=3.1211,	best estimator xgboost's best error=3.1211
[flaml.automl: 09-18 20:35:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:35:18] {3072} INFO -  at 48.7s,	estimator xgboost's best error=3.1211,	best estimator xgboost's best error=3.1211
[flaml.automl: 09-18 20:35:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:35:22] {3072} INFO -  at 53.2s,	estimator xgboost's best error=3.0216,	best estimator xgboost's best error=3.0216
[flaml.automl: 09-18 20:35:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:35:25] {3072} INFO -  at 55.5s,	estimator xgboost's best error=3.0216,	best estimator xgboost's best error=3.0216
[flaml.automl: 09-18 20:35:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:35:26] {3072} INFO -  at 57.1s,	estimator xgboost's best error=3.0216,	best estimator xgboost's best error=3.0216
[flaml.automl: 09-18 20:35:31] {3335} INFO - retrain xgboost for 4.5s
[flaml.automl: 09-18 20:35:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 20:35:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:35:31] {2637} INFO - Time taken to find the best model: 53.18281674385071
[flaml.automl: 09-18 20:35:31] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-2.0216050660692315
PM2.5(0)最好结果：{'pred_time': 1.9711911160633856e-05, 'wall_clock_time': 53.18281674385071, 'metric_for_logging': {'pred_time': 1.9711911160633856e-05}, 'val_loss': 3.0216050660692315, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 4.431598901748657}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.6191313465564214
PM2.5(0)的mse=16.438705220945334
PM2.5(0)的mae=3.0034418324575913
PM2.5(0)的mar=0.47287187684754595
总共花费的时间为：62.27
普洱市
2601A
2602A
[flaml.automl: 09-18 20:41:52] {2390} INFO - task = regression
[flaml.automl: 09-18 20:41:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:41:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:41:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:41:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:41:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:41:55] {3025} INFO - Estimated sufficient time budget=33306s. Estimated necessary time budget=33s.
[flaml.automl: 09-18 20:41:55] {3072} INFO -  at 3.4s,	estimator xgboost's best error=11.0269,	best estimator xgboost's best error=11.0269
[flaml.automl: 09-18 20:41:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:42:01] {3072} INFO -  at 9.2s,	estimator xgboost's best error=5.3439,	best estimator xgboost's best error=5.3439
[flaml.automl: 09-18 20:42:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:42:04] {3072} INFO -  at 12.4s,	estimator xgboost's best error=5.3439,	best estimator xgboost's best error=5.3439
[flaml.automl: 09-18 20:42:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:42:30] {3072} INFO -  at 37.8s,	estimator xgboost's best error=5.3439,	best estimator xgboost's best error=5.3439
[flaml.automl: 09-18 20:42:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:42:33] {3072} INFO -  at 40.8s,	estimator xgboost's best error=3.7513,	best estimator xgboost's best error=3.7513
[flaml.automl: 09-18 20:42:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:42:37] {3072} INFO -  at 45.2s,	estimator xgboost's best error=3.7123,	best estimator xgboost's best error=3.7123
[flaml.automl: 09-18 20:42:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:42:41] {3072} INFO -  at 49.2s,	estimator xgboost's best error=3.1783,	best estimator xgboost's best error=3.1783
[flaml.automl: 09-18 20:42:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:42:46] {3072} INFO -  at 54.4s,	estimator xgboost's best error=3.1783,	best estimator xgboost's best error=3.1783
[flaml.automl: 09-18 20:42:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:42:49] {3072} INFO -  at 56.8s,	estimator xgboost's best error=3.1783,	best estimator xgboost's best error=3.1783
[flaml.automl: 09-18 20:42:51] {3335} INFO - retrain xgboost for 2.2s
[flaml.automl: 09-18 20:42:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:42:51] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:42:51] {2637} INFO - Time taken to find the best model: 49.2228684425354
[flaml.automl: 09-18 20:42:51] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-2.1782809215609498
PM2.5(0)最好结果：{'pred_time': 3.964342332784102e-05, 'wall_clock_time': 49.2228684425354, 'metric_for_logging': {'pred_time': 3.964342332784102e-05}, 'val_loss': 3.1782809215609498, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.062283039093018}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8559613229050881
PM2.5(0)的mse=20.19508187476682
PM2.5(0)的mae=3.3430677846775665
PM2.5(0)的mar=0.2472872751604125
总共花费的时间为：59.41
临沧市
2603A
2604A
[flaml.automl: 09-18 20:50:12] {2390} INFO - task = regression
[flaml.automl: 09-18 20:50:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:50:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:50:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:50:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:50:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:50:15] {3025} INFO - Estimated sufficient time budget=32015s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 20:50:15] {3072} INFO -  at 3.3s,	estimator xgboost's best error=16.8897,	best estimator xgboost's best error=16.8897
[flaml.automl: 09-18 20:50:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:50:21] {3072} INFO -  at 9.1s,	estimator xgboost's best error=8.0096,	best estimator xgboost's best error=8.0096
[flaml.automl: 09-18 20:50:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:50:24] {3072} INFO -  at 12.3s,	estimator xgboost's best error=8.0096,	best estimator xgboost's best error=8.0096
[flaml.automl: 09-18 20:50:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:50:49] {3072} INFO -  at 37.8s,	estimator xgboost's best error=8.0096,	best estimator xgboost's best error=8.0096
[flaml.automl: 09-18 20:50:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:50:53] {3072} INFO -  at 41.0s,	estimator xgboost's best error=5.0070,	best estimator xgboost's best error=5.0070
[flaml.automl: 09-18 20:50:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:50:57] {3072} INFO -  at 45.2s,	estimator xgboost's best error=4.5558,	best estimator xgboost's best error=4.5558
[flaml.automl: 09-18 20:50:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:51:01] {3072} INFO -  at 49.5s,	estimator xgboost's best error=4.3429,	best estimator xgboost's best error=4.3429
[flaml.automl: 09-18 20:51:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:51:04] {3072} INFO -  at 51.9s,	estimator xgboost's best error=4.3429,	best estimator xgboost's best error=4.3429
[flaml.automl: 09-18 20:51:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:51:05] {3072} INFO -  at 53.5s,	estimator xgboost's best error=4.3429,	best estimator xgboost's best error=4.3429
[flaml.automl: 09-18 20:51:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:51:08] {3072} INFO -  at 56.5s,	estimator xgboost's best error=4.1608,	best estimator xgboost's best error=4.1608
[flaml.automl: 09-18 20:51:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:51:10] {3072} INFO -  at 58.1s,	estimator xgboost's best error=4.1608,	best estimator xgboost's best error=4.1608
[flaml.automl: 09-18 20:51:13] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-18 20:51:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 20:51:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:51:13] {2637} INFO - Time taken to find the best model: 56.5187623500824
[flaml.automl: 09-18 20:51:13] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-3.1608302583213614
PM2.5(0)最好结果：{'pred_time': 1.642714027597123e-05, 'wall_clock_time': 56.5187623500824, 'metric_for_logging': {'pred_time': 1.642714027597123e-05}, 'val_loss': 4.160830258321361, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 2.9745092391967773}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8247723835372323
PM2.5(0)的mse=34.618857641571246
PM2.5(0)的mae=4.071388109368744
PM2.5(0)的mar=0.20443769866268205
总共花费的时间为：61.46
楚雄州
2605A
2606A
[flaml.automl: 09-18 20:58:28] {2390} INFO - task = regression
[flaml.automl: 09-18 20:58:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:58:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:58:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:58:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:58:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:58:31] {3025} INFO - Estimated sufficient time budget=32439s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 20:58:31] {3072} INFO -  at 3.3s,	estimator xgboost's best error=11.9761,	best estimator xgboost's best error=11.9761
[flaml.automl: 09-18 20:58:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:58:36] {3072} INFO -  at 8.9s,	estimator xgboost's best error=5.7505,	best estimator xgboost's best error=5.7505
[flaml.automl: 09-18 20:58:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:58:40] {3072} INFO -  at 12.2s,	estimator xgboost's best error=5.7505,	best estimator xgboost's best error=5.7505
[flaml.automl: 09-18 20:58:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:59:05] {3072} INFO -  at 37.6s,	estimator xgboost's best error=5.7505,	best estimator xgboost's best error=5.7505
[flaml.automl: 09-18 20:59:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:59:08] {3072} INFO -  at 40.7s,	estimator xgboost's best error=4.0235,	best estimator xgboost's best error=4.0235
[flaml.automl: 09-18 20:59:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:59:12] {3072} INFO -  at 45.0s,	estimator xgboost's best error=3.5539,	best estimator xgboost's best error=3.5539
[flaml.automl: 09-18 20:59:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:59:17] {3072} INFO -  at 49.3s,	estimator xgboost's best error=3.2942,	best estimator xgboost's best error=3.2942
[flaml.automl: 09-18 20:59:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:59:21] {3072} INFO -  at 53.7s,	estimator xgboost's best error=3.2942,	best estimator xgboost's best error=3.2942
[flaml.automl: 09-18 20:59:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:59:23] {3072} INFO -  at 55.8s,	estimator xgboost's best error=3.2942,	best estimator xgboost's best error=3.2942
[flaml.automl: 09-18 20:59:25] {3335} INFO - retrain xgboost for 2.0s
[flaml.automl: 09-18 20:59:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:59:25] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:59:25] {2637} INFO - Time taken to find the best model: 49.31229853630066
[flaml.automl: 09-18 20:59:25] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-2.2941910585151586
PM2.5(0)最好结果：{'pred_time': 4.841672817482081e-05, 'wall_clock_time': 49.31229853630066, 'metric_for_logging': {'pred_time': 4.841672817482081e-05}, 'val_loss': 3.2941910585151586, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.341655969619751}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8331779787859582
PM2.5(0)的mse=21.668215767723275
PM2.5(0)的mae=3.283690411802651
PM2.5(0)的mar=0.2707249361148168
总共花费的时间为：58.26
红河州
2609A
3038A
[flaml.automl: 09-18 21:06:53] {2390} INFO - task = regression
[flaml.automl: 09-18 21:06:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:06:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:06:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:06:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:06:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:06:56] {3025} INFO - Estimated sufficient time budget=31814s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 21:06:56] {3072} INFO -  at 3.3s,	estimator xgboost's best error=14.6683,	best estimator xgboost's best error=14.6683
[flaml.automl: 09-18 21:06:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:07:02] {3072} INFO -  at 8.9s,	estimator xgboost's best error=6.8832,	best estimator xgboost's best error=6.8832
[flaml.automl: 09-18 21:07:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:07:05] {3072} INFO -  at 12.1s,	estimator xgboost's best error=6.8832,	best estimator xgboost's best error=6.8832
[flaml.automl: 09-18 21:07:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:07:31] {3072} INFO -  at 37.4s,	estimator xgboost's best error=6.8832,	best estimator xgboost's best error=6.8832
[flaml.automl: 09-18 21:07:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:07:34] {3072} INFO -  at 40.5s,	estimator xgboost's best error=4.7592,	best estimator xgboost's best error=4.7592
[flaml.automl: 09-18 21:07:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:07:38] {3072} INFO -  at 44.6s,	estimator xgboost's best error=4.1171,	best estimator xgboost's best error=4.1171
[flaml.automl: 09-18 21:07:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:07:42] {3072} INFO -  at 49.0s,	estimator xgboost's best error=3.9052,	best estimator xgboost's best error=3.9052
[flaml.automl: 09-18 21:07:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:07:49] {3072} INFO -  at 56.2s,	estimator xgboost's best error=3.9052,	best estimator xgboost's best error=3.9052
[flaml.automl: 09-18 21:07:53] {3335} INFO - retrain xgboost for 3.5s
[flaml.automl: 09-18 21:07:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:07:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:07:53] {2637} INFO - Time taken to find the best model: 49.04724860191345
[flaml.automl: 09-18 21:07:53] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-2.9051872297139427
PM2.5(0)最好结果：{'pred_time': 6.045770118141877e-05, 'wall_clock_time': 49.04724860191345, 'metric_for_logging': {'pred_time': 6.045770118141877e-05}, 'val_loss': 3.9051872297139427, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.4375832080841064}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8192934155378492
PM2.5(0)的mse=28.56596828060696
PM2.5(0)的mae=3.979875751797446
PM2.5(0)的mar=0.22488700177188622
总共花费的时间为：60.37
文山州
2610A
2611A
[flaml.automl: 09-18 21:15:19] {2390} INFO - task = regression
[flaml.automl: 09-18 21:15:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:15:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:15:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:15:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:15:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:15:23] {3025} INFO - Estimated sufficient time budget=33021s. Estimated necessary time budget=33s.
[flaml.automl: 09-18 21:15:23] {3072} INFO -  at 3.4s,	estimator xgboost's best error=13.4874,	best estimator xgboost's best error=13.4874
[flaml.automl: 09-18 21:15:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:15:28] {3072} INFO -  at 9.1s,	estimator xgboost's best error=7.0157,	best estimator xgboost's best error=7.0157
[flaml.automl: 09-18 21:15:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:15:31] {3072} INFO -  at 12.4s,	estimator xgboost's best error=7.0157,	best estimator xgboost's best error=7.0157
[flaml.automl: 09-18 21:15:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:16:07] {3072} INFO -  at 48.2s,	estimator xgboost's best error=7.0157,	best estimator xgboost's best error=7.0157
[flaml.automl: 09-18 21:16:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:16:11] {3072} INFO -  at 52.0s,	estimator xgboost's best error=5.2232,	best estimator xgboost's best error=5.2232
[flaml.automl: 09-18 21:16:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:16:15] {3072} INFO -  at 55.5s,	estimator xgboost's best error=5.2232,	best estimator xgboost's best error=5.2232
[flaml.automl: 09-18 21:16:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:16:17] {3072} INFO -  at 57.8s,	estimator xgboost's best error=4.4127,	best estimator xgboost's best error=4.4127
[flaml.automl: 09-18 21:16:19] {3335} INFO - retrain xgboost for 2.4s
[flaml.automl: 09-18 21:16:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 21:16:19] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:16:19] {2637} INFO - Time taken to find the best model: 57.7970666885376
[flaml.automl: 09-18 21:16:19] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-3.412735611678176
PM2.5(0)最好结果：{'pred_time': 2.2395552462163353e-05, 'wall_clock_time': 57.7970666885376, 'metric_for_logging': {'pred_time': 2.2395552462163353e-05}, 'val_loss': 4.412735611678176, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 2.2529525756835938}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.789618765840315
PM2.5(0)的mse=35.99095292065088
PM2.5(0)的mae=4.529079010945692
PM2.5(0)的mar=0.4676114032729033
总共花费的时间为：60.52
西双版纳州
2612A
2613A
[flaml.automl: 09-18 21:23:42] {2390} INFO - task = regression
[flaml.automl: 09-18 21:23:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:23:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:23:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:23:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:23:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:23:46] {3025} INFO - Estimated sufficient time budget=46664s. Estimated necessary time budget=47s.
[flaml.automl: 09-18 21:23:46] {3072} INFO -  at 4.9s,	estimator xgboost's best error=11.8251,	best estimator xgboost's best error=11.8251
[flaml.automl: 09-18 21:23:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:23:55] {3072} INFO -  at 13.1s,	estimator xgboost's best error=5.8308,	best estimator xgboost's best error=5.8308
[flaml.automl: 09-18 21:23:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:23:59] {3072} INFO -  at 17.5s,	estimator xgboost's best error=5.8308,	best estimator xgboost's best error=5.8308
[flaml.automl: 09-18 21:23:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:24:33] {3072} INFO -  at 52.0s,	estimator xgboost's best error=5.8308,	best estimator xgboost's best error=5.8308
[flaml.automl: 09-18 21:24:39] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-18 21:24:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8620995499446478, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.14347380544831878, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=2.30934327968189, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=4.120667606735053, scale_pos_weight=1,
             subsample=0.9348203829099999, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:24:39] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:24:39] {2637} INFO - Time taken to find the best model: 13.130306243896484
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 4, 'min_child_weight': 2.30934327968189, 'learning_rate': 0.14347380544831878, 'subsample': 0.9348203829099999, 'colsample_bylevel': 0.8620995499446478, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 4.120667606735053}
PM2.5(0)最佳损失：-4.830805182405653
PM2.5(0)最好结果：{'pred_time': 6.079530079612124e-05, 'wall_clock_time': 13.130306243896484, 'metric_for_logging': {'pred_time': 6.079530079612124e-05}, 'val_loss': 5.830805182405653, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 4, 'min_child_weight': 2.30934327968189, 'learning_rate': 0.14347380544831878, 'subsample': 0.9348203829099999, 'colsample_bylevel': 0.8620995499446478, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 4.120667606735053}, 'config/n_estimators': 8, 'config/max_leaves': 4, 'config/min_child_weight': 2.30934327968189, 'config/learning_rate': 0.14347380544831878, 'config/subsample': 0.9348203829099999, 'config/colsample_bylevel': 0.8620995499446478, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 4.120667606735053, 'experiment_tag': 'exp', 'time_total_s': 8.189049243927002}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8620995499446478, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.14347380544831878, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=2.30934327968189, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=4.120667606735053, scale_pos_weight=1,
             subsample=0.9348203829099999, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.3397323566452839
PM2.5(0)的mse=72.73466401263377
PM2.5(0)的mae=5.619233443264997
PM2.5(0)的mar=0.32747800151738365
总共花费的时间为：57.75
大理州
2614A
2615A
[flaml.automl: 09-18 21:31:38] {2390} INFO - task = regression
[flaml.automl: 09-18 21:31:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:31:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:31:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:31:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:31:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:31:42] {3025} INFO - Estimated sufficient time budget=44991s. Estimated necessary time budget=45s.
[flaml.automl: 09-18 21:31:42] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.4753,	best estimator xgboost's best error=9.4753
[flaml.automl: 09-18 21:31:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:31:51] {3072} INFO -  at 13.0s,	estimator xgboost's best error=4.6314,	best estimator xgboost's best error=4.6314
[flaml.automl: 09-18 21:31:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:31:55] {3072} INFO -  at 17.9s,	estimator xgboost's best error=4.6314,	best estimator xgboost's best error=4.6314
[flaml.automl: 09-18 21:31:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:32:26] {3072} INFO -  at 48.7s,	estimator xgboost's best error=4.6314,	best estimator xgboost's best error=4.6314
[flaml.automl: 09-18 21:32:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:32:28] {3072} INFO -  at 50.7s,	estimator xgboost's best error=3.4969,	best estimator xgboost's best error=3.4969
[flaml.automl: 09-18 21:32:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:32:31] {3072} INFO -  at 53.5s,	estimator xgboost's best error=3.2483,	best estimator xgboost's best error=3.2483
[flaml.automl: 09-18 21:32:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:32:33] {3072} INFO -  at 55.6s,	estimator xgboost's best error=3.1921,	best estimator xgboost's best error=3.1921
[flaml.automl: 09-18 21:32:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:32:36] {3072} INFO -  at 58.3s,	estimator xgboost's best error=3.1921,	best estimator xgboost's best error=3.1921
[flaml.automl: 09-18 21:32:37] {3335} INFO - retrain xgboost for 1.6s
[flaml.automl: 09-18 21:32:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:32:37] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:32:37] {2637} INFO - Time taken to find the best model: 55.621500968933105
[flaml.automl: 09-18 21:32:37] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-2.1921021304901678
PM2.5(0)最好结果：{'pred_time': 1.5950143927394754e-05, 'wall_clock_time': 55.621500968933105, 'metric_for_logging': {'pred_time': 1.5950143927394754e-05}, 'val_loss': 3.1921021304901678, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 2.093207836151123}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.7814668960928199
PM2.5(0)的mse=16.124859807972914
PM2.5(0)的mae=2.955498844584272
PM2.5(0)的mar=0.25723367187074175
总共花费的时间为：60.48
德宏州
2616A
[flaml.automl: 09-18 21:36:28] {2390} INFO - task = regression
[flaml.automl: 09-18 21:36:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:36:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:36:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:36:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:36:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:36:31] {3025} INFO - Estimated sufficient time budget=32266s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 21:36:31] {3072} INFO -  at 3.3s,	estimator xgboost's best error=15.0118,	best estimator xgboost's best error=15.0118
[flaml.automl: 09-18 21:36:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:36:36] {3072} INFO -  at 8.3s,	estimator xgboost's best error=8.2414,	best estimator xgboost's best error=8.2414
[flaml.automl: 09-18 21:36:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:36:39] {3072} INFO -  at 11.4s,	estimator xgboost's best error=8.2414,	best estimator xgboost's best error=8.2414
[flaml.automl: 09-18 21:36:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:36:58] {3072} INFO -  at 30.6s,	estimator xgboost's best error=8.2414,	best estimator xgboost's best error=8.2414
[flaml.automl: 09-18 21:36:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:37:01] {3072} INFO -  at 33.6s,	estimator xgboost's best error=4.6419,	best estimator xgboost's best error=4.6419
[flaml.automl: 09-18 21:37:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:37:06] {3072} INFO -  at 37.9s,	estimator xgboost's best error=4.2948,	best estimator xgboost's best error=4.2948
[flaml.automl: 09-18 21:37:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:37:10] {3072} INFO -  at 42.2s,	estimator xgboost's best error=4.1306,	best estimator xgboost's best error=4.1306
[flaml.automl: 09-18 21:37:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:37:16] {3072} INFO -  at 48.6s,	estimator xgboost's best error=4.1306,	best estimator xgboost's best error=4.1306
[flaml.automl: 09-18 21:37:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:37:20] {3072} INFO -  at 52.6s,	estimator xgboost's best error=4.1306,	best estimator xgboost's best error=4.1306
[flaml.automl: 09-18 21:37:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:37:24] {3072} INFO -  at 56.2s,	estimator xgboost's best error=4.1306,	best estimator xgboost's best error=4.1306
[flaml.automl: 09-18 21:37:25] {3335} INFO - retrain xgboost for 1.6s
[flaml.automl: 09-18 21:37:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:37:25] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:37:25] {2637} INFO - Time taken to find the best model: 42.202871799468994
[flaml.automl: 09-18 21:37:25] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-3.1305614469027274
PM2.5(0)最好结果：{'pred_time': 9.126354146886754e-05, 'wall_clock_time': 42.202871799468994, 'metric_for_logging': {'pred_time': 9.126354146886754e-05}, 'val_loss': 4.130561446902727, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.265427350997925}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8269380483512593
PM2.5(0)的mse=26.724253401596457
PM2.5(0)的mae=4.009023788481048
PM2.5(0)的mar=0.24874889000937436
总共花费的时间为：58.04
怒江州
2618A
2619A
[flaml.automl: 09-18 21:44:35] {2390} INFO - task = regression
[flaml.automl: 09-18 21:44:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:44:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:44:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:44:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:44:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:44:38] {3025} INFO - Estimated sufficient time budget=32368s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 21:44:38] {3072} INFO -  at 3.3s,	estimator xgboost's best error=13.2537,	best estimator xgboost's best error=13.2537
[flaml.automl: 09-18 21:44:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:44:44] {3072} INFO -  at 9.0s,	estimator xgboost's best error=6.4142,	best estimator xgboost's best error=6.4142
[flaml.automl: 09-18 21:44:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:44:47] {3072} INFO -  at 12.2s,	estimator xgboost's best error=6.4142,	best estimator xgboost's best error=6.4142
[flaml.automl: 09-18 21:44:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:45:18] {3072} INFO -  at 43.6s,	estimator xgboost's best error=6.4142,	best estimator xgboost's best error=6.4142
[flaml.automl: 09-18 21:45:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:45:23] {3072} INFO -  at 48.1s,	estimator xgboost's best error=4.2120,	best estimator xgboost's best error=4.2120
[flaml.automl: 09-18 21:45:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:45:28] {3072} INFO -  at 53.8s,	estimator xgboost's best error=3.7689,	best estimator xgboost's best error=3.7689
[flaml.automl: 09-18 21:45:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:45:31] {3072} INFO -  at 56.1s,	estimator xgboost's best error=3.5595,	best estimator xgboost's best error=3.5595
[flaml.automl: 09-18 21:45:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:45:34] {3072} INFO -  at 59.4s,	estimator xgboost's best error=3.5595,	best estimator xgboost's best error=3.5595
[flaml.automl: 09-18 21:45:36] {3335} INFO - retrain xgboost for 2.1s
[flaml.automl: 09-18 21:45:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:45:36] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:45:36] {2637} INFO - Time taken to find the best model: 56.11403942108154
[flaml.automl: 09-18 21:45:36] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-2.559503397903151
PM2.5(0)最好结果：{'pred_time': 3.228584082388605e-05, 'wall_clock_time': 56.11403942108154, 'metric_for_logging': {'pred_time': 3.228584082388605e-05}, 'val_loss': 3.559503397903151, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 2.3558249473571777}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.809172178107059
PM2.5(0)的mse=25.783820529119573
PM2.5(0)的mae=3.703574422025735
PM2.5(0)的mar=0.27941730850223284
总共花费的时间为：62.03
迪庆州
迪庆州没有数据
昌都市
2622A
[flaml.automl: 09-18 21:49:31] {2390} INFO - task = regression
[flaml.automl: 09-18 21:49:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:49:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:49:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:49:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:49:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:49:35] {3025} INFO - Estimated sufficient time budget=41832s. Estimated necessary time budget=42s.
[flaml.automl: 09-18 21:49:35] {3072} INFO -  at 4.2s,	estimator xgboost's best error=4.5412,	best estimator xgboost's best error=4.5412
[flaml.automl: 09-18 21:49:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:49:42] {3072} INFO -  at 11.4s,	estimator xgboost's best error=3.5120,	best estimator xgboost's best error=3.5120
[flaml.automl: 09-18 21:49:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:49:47] {3072} INFO -  at 16.1s,	estimator xgboost's best error=3.5120,	best estimator xgboost's best error=3.5120
[flaml.automl: 09-18 21:49:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:50:16] {3072} INFO -  at 44.9s,	estimator xgboost's best error=3.5120,	best estimator xgboost's best error=3.5120
[flaml.automl: 09-18 21:50:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:50:18] {3072} INFO -  at 47.2s,	estimator xgboost's best error=3.2036,	best estimator xgboost's best error=3.2036
[flaml.automl: 09-18 21:50:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:50:21] {3072} INFO -  at 50.4s,	estimator xgboost's best error=3.2036,	best estimator xgboost's best error=3.2036
[flaml.automl: 09-18 21:50:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:50:24] {3072} INFO -  at 53.6s,	estimator xgboost's best error=3.1898,	best estimator xgboost's best error=3.1898
[flaml.automl: 09-18 21:50:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:50:29] {3072} INFO -  at 58.0s,	estimator xgboost's best error=3.1898,	best estimator xgboost's best error=3.1898
[flaml.automl: 09-18 21:50:32] {3335} INFO - retrain xgboost for 3.3s
[flaml.automl: 09-18 21:50:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 21:50:32] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:50:32] {2637} INFO - Time taken to find the best model: 53.57769966125488
[flaml.automl: 09-18 21:50:32] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-2.1897832763348712
PM2.5(0)最好结果：{'pred_time': 3.69465373058565e-05, 'wall_clock_time': 53.57769966125488, 'metric_for_logging': {'pred_time': 3.69465373058565e-05}, 'val_loss': 3.1897832763348712, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 3.192354679107666}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=-0.18857682463113856
PM2.5(0)的mse=19.79662771409
PM2.5(0)的mae=3.146336575629006
PM2.5(0)的mar=0.7636414613343875
总共花费的时间为：61.60
山南市
2624A
2625A
[flaml.automl: 09-18 21:57:35] {2390} INFO - task = regression
[flaml.automl: 09-18 21:57:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:57:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:57:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:57:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:57:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:57:38] {3025} INFO - Estimated sufficient time budget=31738s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 21:57:38] {3072} INFO -  at 3.4s,	estimator xgboost's best error=4.6532,	best estimator xgboost's best error=4.6532
[flaml.automl: 09-18 21:57:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:57:45] {3072} INFO -  at 10.5s,	estimator xgboost's best error=2.9485,	best estimator xgboost's best error=2.9485
[flaml.automl: 09-18 21:57:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:57:50] {3072} INFO -  at 15.2s,	estimator xgboost's best error=2.9485,	best estimator xgboost's best error=2.9485
[flaml.automl: 09-18 21:57:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:58:16] {3072} INFO -  at 41.5s,	estimator xgboost's best error=2.9485,	best estimator xgboost's best error=2.9485
[flaml.automl: 09-18 21:58:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:58:18] {3072} INFO -  at 43.1s,	estimator xgboost's best error=2.3934,	best estimator xgboost's best error=2.3934
[flaml.automl: 09-18 21:58:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:58:20] {3072} INFO -  at 45.3s,	estimator xgboost's best error=2.3068,	best estimator xgboost's best error=2.3068
[flaml.automl: 09-18 21:58:20] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:58:23] {3072} INFO -  at 47.9s,	estimator xgboost's best error=2.3068,	best estimator xgboost's best error=2.3068
[flaml.automl: 09-18 21:58:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:58:26] {3072} INFO -  at 51.9s,	estimator xgboost's best error=2.3068,	best estimator xgboost's best error=2.3068
[flaml.automl: 09-18 21:58:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:58:28] {3072} INFO -  at 53.4s,	estimator xgboost's best error=2.3068,	best estimator xgboost's best error=2.3068
[flaml.automl: 09-18 21:58:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:58:31] {3072} INFO -  at 56.7s,	estimator xgboost's best error=2.3022,	best estimator xgboost's best error=2.3022
[flaml.automl: 09-18 21:58:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:58:34] {3072} INFO -  at 58.9s,	estimator xgboost's best error=2.3022,	best estimator xgboost's best error=2.3022
[flaml.automl: 09-18 21:58:37] {3335} INFO - retrain xgboost for 3.4s
[flaml.automl: 09-18 21:58:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:58:37] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:58:37] {2637} INFO - Time taken to find the best model: 56.69669222831726
[flaml.automl: 09-18 21:58:37] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}
PM2.5(0)最佳损失：-1.3021510592423629
PM2.5(0)最好结果：{'pred_time': 2.8731961729665282e-05, 'wall_clock_time': 56.69669222831726, 'metric_for_logging': {'pred_time': 2.8731961729665282e-05}, 'val_loss': 2.302151059242363, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}, 'config/n_estimators': 4, 'config/max_leaves': 10, 'config/min_child_weight': 0.06372323163910634, 'config/learning_rate': 0.4086634770112844, 'config/subsample': 0.770412309752864, 'config/colsample_bylevel': 0.6720329282771347, 'config/colsample_bytree': 0.752099095598003, 'config/reg_alpha': 0.00271725862127768, 'config/reg_lambda': 3.9335607770117846, 'experiment_tag': 'exp', 'time_total_s': 3.2742807865142822}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=-0.48096358704254016
PM2.5(0)的mse=16.159492643171312
PM2.5(0)的mae=2.536890525761105
PM2.5(0)的mar=0.442257431102909
总共花费的时间为：62.88
日喀则市
2626A
[flaml.automl: 09-18 22:02:15] {2390} INFO - task = regression
[flaml.automl: 09-18 22:02:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:02:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:02:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:02:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:02:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:02:18] {3025} INFO - Estimated sufficient time budget=31486s. Estimated necessary time budget=31s.
[flaml.automl: 09-18 22:02:18] {3072} INFO -  at 3.2s,	estimator xgboost's best error=5.1354,	best estimator xgboost's best error=5.1354
[flaml.automl: 09-18 22:02:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:02:24] {3072} INFO -  at 9.6s,	estimator xgboost's best error=3.3706,	best estimator xgboost's best error=3.3706
[flaml.automl: 09-18 22:02:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:02:29] {3072} INFO -  at 14.4s,	estimator xgboost's best error=3.3706,	best estimator xgboost's best error=3.3706
[flaml.automl: 09-18 22:02:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:02:51] {3072} INFO -  at 36.8s,	estimator xgboost's best error=3.3706,	best estimator xgboost's best error=3.3706
[flaml.automl: 09-18 22:02:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:02:53] {3072} INFO -  at 38.4s,	estimator xgboost's best error=2.8584,	best estimator xgboost's best error=2.8584
[flaml.automl: 09-18 22:02:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:02:55] {3072} INFO -  at 40.6s,	estimator xgboost's best error=2.7883,	best estimator xgboost's best error=2.7883
[flaml.automl: 09-18 22:02:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:02:58] {3072} INFO -  at 42.9s,	estimator xgboost's best error=2.7819,	best estimator xgboost's best error=2.7819
[flaml.automl: 09-18 22:02:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:03:01] {3072} INFO -  at 46.0s,	estimator xgboost's best error=2.7819,	best estimator xgboost's best error=2.7819
[flaml.automl: 09-18 22:03:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:03:03] {3072} INFO -  at 48.1s,	estimator xgboost's best error=2.7819,	best estimator xgboost's best error=2.7819
[flaml.automl: 09-18 22:03:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:03:06] {3072} INFO -  at 51.7s,	estimator xgboost's best error=2.7474,	best estimator xgboost's best error=2.7474
[flaml.automl: 09-18 22:03:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:03:08] {3072} INFO -  at 53.9s,	estimator xgboost's best error=2.7474,	best estimator xgboost's best error=2.7474
[flaml.automl: 09-18 22:03:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:03:10] {3072} INFO -  at 55.6s,	estimator xgboost's best error=2.7474,	best estimator xgboost's best error=2.7474
[flaml.automl: 09-18 22:03:10] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:03:14] {3072} INFO -  at 59.4s,	estimator xgboost's best error=2.7127,	best estimator xgboost's best error=2.7127
[flaml.automl: 09-18 22:03:26] {3335} INFO - retrain xgboost for 11.6s
[flaml.automl: 09-18 22:03:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:03:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:03:26] {2637} INFO - Time taken to find the best model: 59.368173599243164
[flaml.automl: 09-18 22:03:26] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-1.7127341729789056
PM2.5(0)最好结果：{'pred_time': 6.624415465996127e-05, 'wall_clock_time': 59.368173599243164, 'metric_for_logging': {'pred_time': 6.624415465996127e-05}, 'val_loss': 2.7127341729789056, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 12, 'config/max_leaves': 14, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 3.718761682510376}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.4866776485099302
PM2.5(0)的mse=23.359339672966534
PM2.5(0)的mae=2.870291133504218
PM2.5(0)的mar=0.46433327607127783
总共花费的时间为：71.17
那曲地区
2628A
[flaml.automl: 09-18 22:06:47] {2390} INFO - task = regression
[flaml.automl: 09-18 22:06:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:06:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:06:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:06:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:06:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:06:50] {3025} INFO - Estimated sufficient time budget=30862s. Estimated necessary time budget=31s.
[flaml.automl: 09-18 22:06:50] {3072} INFO -  at 3.2s,	estimator xgboost's best error=13.0130,	best estimator xgboost's best error=13.0130
[flaml.automl: 09-18 22:06:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:06:55] {3072} INFO -  at 8.0s,	estimator xgboost's best error=8.4901,	best estimator xgboost's best error=8.4901
[flaml.automl: 09-18 22:06:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:06:58] {3072} INFO -  at 11.2s,	estimator xgboost's best error=8.4901,	best estimator xgboost's best error=8.4901
[flaml.automl: 09-18 22:06:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:07:07] {3072} INFO -  at 20.3s,	estimator xgboost's best error=8.4901,	best estimator xgboost's best error=8.4901
[flaml.automl: 09-18 22:07:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:07:08] {3072} INFO -  at 21.4s,	estimator xgboost's best error=7.2498,	best estimator xgboost's best error=7.2498
[flaml.automl: 09-18 22:07:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:07:10] {3072} INFO -  at 23.0s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:07:11] {3072} INFO -  at 24.5s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:07:13] {3072} INFO -  at 26.8s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:07:15] {3072} INFO -  at 27.9s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:07:17] {3072} INFO -  at 30.3s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:07:18] {3072} INFO -  at 31.4s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:07:19] {3072} INFO -  at 32.5s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:07:25] {3072} INFO -  at 38.0s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:07:27] {3072} INFO -  at 40.5s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 22:07:29] {3072} INFO -  at 42.3s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:29] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 22:07:31] {3072} INFO -  at 44.7s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:31] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 22:07:33] {3072} INFO -  at 45.8s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:33] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 22:07:34] {3072} INFO -  at 46.9s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:34] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 22:07:37] {3072} INFO -  at 50.5s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:37] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 22:07:38] {3072} INFO -  at 51.6s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:38] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 22:07:42] {3072} INFO -  at 55.5s,	estimator xgboost's best error=6.9127,	best estimator xgboost's best error=6.9127
[flaml.automl: 09-18 22:07:42] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 22:07:46] {3072} INFO -  at 59.6s,	estimator xgboost's best error=6.8733,	best estimator xgboost's best error=6.8733
[flaml.automl: 09-18 22:07:52] {3335} INFO - retrain xgboost for 5.3s
[flaml.automl: 09-18 22:07:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8492588522931204, colsample_bynode=1,
             colsample_bytree=0.6537360078914272, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.32648040091234304,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=1.728347470404329, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3980257413064903, scale_pos_weight=1,
             subsample=0.8883759987059676, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:07:52] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:07:52] {2637} INFO - Time taken to find the best model: 59.63221478462219
[flaml.automl: 09-18 22:07:52] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 9, 'max_leaves': 10, 'min_child_weight': 1.728347470404329, 'learning_rate': 0.32648040091234304, 'subsample': 0.8883759987059676, 'colsample_bylevel': 0.8492588522931204, 'colsample_bytree': 0.6537360078914272, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3980257413064903}
PM2.5(0)最佳损失：-5.873291850707692
PM2.5(0)最好结果：{'pred_time': 3.6162621822587775e-05, 'wall_clock_time': 59.63221478462219, 'metric_for_logging': {'pred_time': 3.6162621822587775e-05}, 'val_loss': 6.873291850707692, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 10, 'min_child_weight': 1.728347470404329, 'learning_rate': 0.32648040091234304, 'subsample': 0.8883759987059676, 'colsample_bylevel': 0.8492588522931204, 'colsample_bytree': 0.6537360078914272, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3980257413064903}, 'config/n_estimators': 9, 'config/max_leaves': 10, 'config/min_child_weight': 1.728347470404329, 'config/learning_rate': 0.32648040091234304, 'config/subsample': 0.8883759987059676, 'config/colsample_bylevel': 0.8492588522931204, 'config/colsample_bytree': 0.6537360078914272, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.3980257413064903, 'experiment_tag': 'exp', 'time_total_s': 4.164659738540649}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8492588522931204, colsample_bynode=1,
             colsample_bytree=0.6537360078914272, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.32648040091234304,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=1.728347470404329, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3980257413064903, scale_pos_weight=1,
             subsample=0.8883759987059676, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.38271983742055193
PM2.5(0)的mse=99.86892117950444
PM2.5(0)的mae=6.437067686168371
PM2.5(0)的mar=0.5016135585009242
总共花费的时间为：65.11
阿里地区
2630A
[flaml.automl: 09-18 22:11:42] {2390} INFO - task = regression
[flaml.automl: 09-18 22:11:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:11:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:11:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:11:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:11:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:11:45] {3025} INFO - Estimated sufficient time budget=31355s. Estimated necessary time budget=31s.
[flaml.automl: 09-18 22:11:45] {3072} INFO -  at 3.2s,	estimator xgboost's best error=3.8181,	best estimator xgboost's best error=3.8181
[flaml.automl: 09-18 22:11:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:11:50] {3072} INFO -  at 8.2s,	estimator xgboost's best error=2.4348,	best estimator xgboost's best error=2.4348
[flaml.automl: 09-18 22:11:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:11:53] {3072} INFO -  at 11.5s,	estimator xgboost's best error=2.4348,	best estimator xgboost's best error=2.4348
[flaml.automl: 09-18 22:11:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:12:03] {3072} INFO -  at 21.1s,	estimator xgboost's best error=2.4348,	best estimator xgboost's best error=2.4348
[flaml.automl: 09-18 22:12:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:12:04] {3072} INFO -  at 22.3s,	estimator xgboost's best error=1.9136,	best estimator xgboost's best error=1.9136
[flaml.automl: 09-18 22:12:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:12:06] {3072} INFO -  at 23.8s,	estimator xgboost's best error=1.8648,	best estimator xgboost's best error=1.8648
[flaml.automl: 09-18 22:12:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:12:07] {3072} INFO -  at 25.4s,	estimator xgboost's best error=1.8532,	best estimator xgboost's best error=1.8532
[flaml.automl: 09-18 22:12:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:12:10] {3072} INFO -  at 27.7s,	estimator xgboost's best error=1.8532,	best estimator xgboost's best error=1.8532
[flaml.automl: 09-18 22:12:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:12:11] {3072} INFO -  at 29.3s,	estimator xgboost's best error=1.8532,	best estimator xgboost's best error=1.8532
[flaml.automl: 09-18 22:12:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:12:14] {3072} INFO -  at 31.8s,	estimator xgboost's best error=1.8338,	best estimator xgboost's best error=1.8338
[flaml.automl: 09-18 22:12:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:12:15] {3072} INFO -  at 33.4s,	estimator xgboost's best error=1.8338,	best estimator xgboost's best error=1.8338
[flaml.automl: 09-18 22:12:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:12:16] {3072} INFO -  at 34.5s,	estimator xgboost's best error=1.8338,	best estimator xgboost's best error=1.8338
[flaml.automl: 09-18 22:12:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:12:26] {3072} INFO -  at 44.0s,	estimator xgboost's best error=1.8338,	best estimator xgboost's best error=1.8338
[flaml.automl: 09-18 22:12:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:12:31] {3072} INFO -  at 48.7s,	estimator xgboost's best error=1.8288,	best estimator xgboost's best error=1.8288
[flaml.automl: 09-18 22:12:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 22:12:33] {3072} INFO -  at 51.3s,	estimator xgboost's best error=1.8288,	best estimator xgboost's best error=1.8288
[flaml.automl: 09-18 22:12:33] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 22:12:41] {3072} INFO -  at 58.9s,	estimator xgboost's best error=1.8073,	best estimator xgboost's best error=1.8073
[flaml.automl: 09-18 22:12:48] {3335} INFO - retrain xgboost for 7.6s
[flaml.automl: 09-18 22:12:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6465085724781942, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.17818279412327895,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.2868832708255262, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.27301808477516265, scale_pos_weight=1,
             subsample=0.8173195817285152, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:12:48] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:12:48] {2637} INFO - Time taken to find the best model: 58.884166955947876
[flaml.automl: 09-18 22:12:48] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 10, 'min_child_weight': 0.2868832708255262, 'learning_rate': 0.17818279412327895, 'subsample': 0.8173195817285152, 'colsample_bylevel': 0.6465085724781942, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.27301808477516265}
PM2.5(0)最佳损失：-0.8073291389683594
PM2.5(0)最好结果：{'pred_time': 3.369398141497735e-05, 'wall_clock_time': 58.884166955947876, 'metric_for_logging': {'pred_time': 3.369398141497735e-05}, 'val_loss': 1.8073291389683594, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 10, 'min_child_weight': 0.2868832708255262, 'learning_rate': 0.17818279412327895, 'subsample': 0.8173195817285152, 'colsample_bylevel': 0.6465085724781942, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.27301808477516265}, 'config/n_estimators': 13, 'config/max_leaves': 10, 'config/min_child_weight': 0.2868832708255262, 'config/learning_rate': 0.17818279412327895, 'config/subsample': 0.8173195817285152, 'config/colsample_bylevel': 0.6465085724781942, 'config/colsample_bytree': 0.92006762344246, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.27301808477516265, 'experiment_tag': 'exp', 'time_total_s': 7.585163593292236}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6465085724781942, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.17818279412327895,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.2868832708255262, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.27301808477516265, scale_pos_weight=1,
             subsample=0.8173195817285152, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.10439639550659585
PM2.5(0)的mse=6.8144930614761465
PM2.5(0)的mae=1.8573447087468962
PM2.5(0)的mar=0.3882157926098812
总共花费的时间为：66.65
林芝市
2632A
2633A
[flaml.automl: 09-18 22:18:56] {2390} INFO - task = regression
[flaml.automl: 09-18 22:18:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:18:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:18:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:18:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:18:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:18:59] {3025} INFO - Estimated sufficient time budget=31412s. Estimated necessary time budget=31s.
[flaml.automl: 09-18 22:18:59] {3072} INFO -  at 3.2s,	estimator xgboost's best error=4.3449,	best estimator xgboost's best error=4.3449
[flaml.automl: 09-18 22:18:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:19:04] {3072} INFO -  at 8.7s,	estimator xgboost's best error=2.2207,	best estimator xgboost's best error=2.2207
[flaml.automl: 09-18 22:19:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:19:08] {3072} INFO -  at 11.9s,	estimator xgboost's best error=2.2207,	best estimator xgboost's best error=2.2207
[flaml.automl: 09-18 22:19:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:19:32] {3072} INFO -  at 36.7s,	estimator xgboost's best error=2.2207,	best estimator xgboost's best error=2.2207
[flaml.automl: 09-18 22:19:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:19:34] {3072} INFO -  at 38.5s,	estimator xgboost's best error=1.4793,	best estimator xgboost's best error=1.4793
[flaml.automl: 09-18 22:19:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:19:36] {3072} INFO -  at 40.1s,	estimator xgboost's best error=1.3710,	best estimator xgboost's best error=1.3710
[flaml.automl: 09-18 22:19:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:19:37] {3072} INFO -  at 41.7s,	estimator xgboost's best error=1.3045,	best estimator xgboost's best error=1.3045
[flaml.automl: 09-18 22:19:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:19:40] {3072} INFO -  at 44.0s,	estimator xgboost's best error=1.3045,	best estimator xgboost's best error=1.3045
[flaml.automl: 09-18 22:19:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:19:41] {3072} INFO -  at 45.6s,	estimator xgboost's best error=1.3045,	best estimator xgboost's best error=1.3045
[flaml.automl: 09-18 22:19:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:19:44] {3072} INFO -  at 48.6s,	estimator xgboost's best error=1.2642,	best estimator xgboost's best error=1.2642
[flaml.automl: 09-18 22:19:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:19:46] {3072} INFO -  at 50.2s,	estimator xgboost's best error=1.2642,	best estimator xgboost's best error=1.2642
[flaml.automl: 09-18 22:19:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:19:47] {3072} INFO -  at 51.3s,	estimator xgboost's best error=1.2642,	best estimator xgboost's best error=1.2642
[flaml.automl: 09-18 22:19:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:19:55] {3072} INFO -  at 59.8s,	estimator xgboost's best error=1.2456,	best estimator xgboost's best error=1.2456
[flaml.automl: 09-18 22:20:07] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-18 22:20:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:20:07] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:20:07] {2637} INFO - Time taken to find the best model: 59.78821516036987
[flaml.automl: 09-18 22:20:07] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-0.2455909263176841
PM2.5(0)最好结果：{'pred_time': 1.8738679381401772e-05, 'wall_clock_time': 59.78821516036987, 'metric_for_logging': {'pred_time': 1.8738679381401772e-05}, 'val_loss': 1.245590926317684, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 8.498743057250977}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8211252555540143
PM2.5(0)的mse=2.9187243052591647
PM2.5(0)的mae=1.2057546155508234
PM2.5(0)的mar=0.26127242396293626
总共花费的时间为：72.05
汉中市
2634A
2635A
2636A
2637A
[flaml.automl: 09-18 22:33:06] {2390} INFO - task = regression
[flaml.automl: 09-18 22:33:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:33:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:33:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:33:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:33:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:33:10] {3025} INFO - Estimated sufficient time budget=132175s. Estimated necessary time budget=132s.
[flaml.automl: 09-18 22:33:10] {3072} INFO -  at 3.3s,	estimator xgboost's best error=20.1772,	best estimator xgboost's best error=20.1772
[flaml.automl: 09-18 22:33:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:33:15] {3072} INFO -  at 9.1s,	estimator xgboost's best error=9.3666,	best estimator xgboost's best error=9.3666
[flaml.automl: 09-18 22:33:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:33:19] {3072} INFO -  at 12.4s,	estimator xgboost's best error=9.3666,	best estimator xgboost's best error=9.3666
[flaml.automl: 09-18 22:33:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:33:23] {3072} INFO -  at 17.1s,	estimator xgboost's best error=9.3666,	best estimator xgboost's best error=9.3666
[flaml.automl: 09-18 22:33:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:33:26] {3072} INFO -  at 20.1s,	estimator xgboost's best error=6.7567,	best estimator xgboost's best error=6.7567
[flaml.automl: 09-18 22:33:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:33:31] {3072} INFO -  at 24.5s,	estimator xgboost's best error=6.4266,	best estimator xgboost's best error=6.4266
[flaml.automl: 09-18 22:33:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:33:34] {3072} INFO -  at 28.2s,	estimator xgboost's best error=5.6369,	best estimator xgboost's best error=5.6369
[flaml.automl: 09-18 22:33:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:33:37] {3072} INFO -  at 30.9s,	estimator xgboost's best error=5.6369,	best estimator xgboost's best error=5.6369
[flaml.automl: 09-18 22:33:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:33:40] {3072} INFO -  at 33.9s,	estimator xgboost's best error=5.6369,	best estimator xgboost's best error=5.6369
[flaml.automl: 09-18 22:33:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:33:43] {3072} INFO -  at 36.6s,	estimator xgboost's best error=5.6369,	best estimator xgboost's best error=5.6369
[flaml.automl: 09-18 22:33:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:33:45] {3072} INFO -  at 38.5s,	estimator xgboost's best error=5.6369,	best estimator xgboost's best error=5.6369
[flaml.automl: 09-18 22:33:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:33:49] {3072} INFO -  at 42.9s,	estimator xgboost's best error=5.0907,	best estimator xgboost's best error=5.0907
[flaml.automl: 09-18 22:33:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:33:52] {3072} INFO -  at 46.0s,	estimator xgboost's best error=5.0907,	best estimator xgboost's best error=5.0907
[flaml.automl: 09-18 22:33:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:34:05] {3072} INFO -  at 59.2s,	estimator xgboost's best error=3.8360,	best estimator xgboost's best error=3.8360
[flaml.automl: 09-18 22:34:13] {3335} INFO - retrain xgboost for 8.0s
[flaml.automl: 09-18 22:34:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:34:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:34:13] {2637} INFO - Time taken to find the best model: 59.171788692474365
[flaml.automl: 09-18 22:34:13] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 42022}
PM2.5(0)最佳损失：-2.8360263241435018
PM2.5(0)最好结果：{'pred_time': 1.652225437205149e-05, 'wall_clock_time': 59.171788692474365, 'metric_for_logging': {'pred_time': 1.652225437205149e-05}, 'val_loss': 3.8360263241435018, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 42022}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 42022, 'experiment_tag': 'exp', 'time_total_s': 13.186762809753418}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9258604484126385
PM2.5(0)的mse=41.19565144801647
PM2.5(0)的mae=3.890109296202499
PM2.5(0)的mar=0.14158577680017598
总共花费的时间为：67.89
榆林市
2638A
2639A
2640A
2641A
[flaml.automl: 09-18 22:48:15] {2390} INFO - task = regression
[flaml.automl: 09-18 22:48:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:48:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:48:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:48:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:48:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:48:18] {3025} INFO - Estimated sufficient time budget=136241s. Estimated necessary time budget=136s.
[flaml.automl: 09-18 22:48:18] {3072} INFO -  at 3.5s,	estimator xgboost's best error=17.9871,	best estimator xgboost's best error=17.9871
[flaml.automl: 09-18 22:48:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:48:24] {3072} INFO -  at 9.3s,	estimator xgboost's best error=8.4401,	best estimator xgboost's best error=8.4401
[flaml.automl: 09-18 22:48:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:48:27] {3072} INFO -  at 12.7s,	estimator xgboost's best error=8.4401,	best estimator xgboost's best error=8.4401
[flaml.automl: 09-18 22:48:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:48:32] {3072} INFO -  at 17.5s,	estimator xgboost's best error=8.4401,	best estimator xgboost's best error=8.4401
[flaml.automl: 09-18 22:48:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:48:34] {3072} INFO -  at 20.0s,	estimator xgboost's best error=5.4657,	best estimator xgboost's best error=5.4657
[flaml.automl: 09-18 22:48:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:48:37] {3072} INFO -  at 22.9s,	estimator xgboost's best error=5.1778,	best estimator xgboost's best error=5.1778
[flaml.automl: 09-18 22:48:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:48:40] {3072} INFO -  at 25.8s,	estimator xgboost's best error=4.4972,	best estimator xgboost's best error=4.4972
[flaml.automl: 09-18 22:48:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:48:44] {3072} INFO -  at 30.0s,	estimator xgboost's best error=4.4972,	best estimator xgboost's best error=4.4972
[flaml.automl: 09-18 22:48:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:48:48] {3072} INFO -  at 33.3s,	estimator xgboost's best error=4.4972,	best estimator xgboost's best error=4.4972
[flaml.automl: 09-18 22:48:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:48:50] {3072} INFO -  at 36.1s,	estimator xgboost's best error=4.4972,	best estimator xgboost's best error=4.4972
[flaml.automl: 09-18 22:48:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:48:53] {3072} INFO -  at 38.6s,	estimator xgboost's best error=4.4972,	best estimator xgboost's best error=4.4972
[flaml.automl: 09-18 22:48:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:48:58] {3072} INFO -  at 43.2s,	estimator xgboost's best error=4.4946,	best estimator xgboost's best error=4.4946
[flaml.automl: 09-18 22:48:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:49:01] {3072} INFO -  at 46.6s,	estimator xgboost's best error=4.4946,	best estimator xgboost's best error=4.4946
[flaml.automl: 09-18 22:49:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:49:13] {3072} INFO -  at 58.8s,	estimator xgboost's best error=3.8123,	best estimator xgboost's best error=3.8123
[flaml.automl: 09-18 22:49:26] {3335} INFO - retrain xgboost for 13.3s
[flaml.automl: 09-18 22:49:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:49:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:49:26] {2637} INFO - Time taken to find the best model: 58.768166303634644
[flaml.automl: 09-18 22:49:26] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 41441}
PM2.5(0)最佳损失：-2.812291110222037
PM2.5(0)最好结果：{'pred_time': 2.565554764837706e-05, 'wall_clock_time': 58.768166303634644, 'metric_for_logging': {'pred_time': 2.565554764837706e-05}, 'val_loss': 3.812291110222037, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 41441}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 41441, 'experiment_tag': 'exp', 'time_total_s': 12.204699993133545}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8899091315645846
PM2.5(0)的mse=37.12228253797796
PM2.5(0)的mae=3.7003221900895955
PM2.5(0)的mar=0.15120151873536558
总共花费的时间为：72.79
安康市
2642A
2643A
2644A
[flaml.automl: 09-18 22:59:11] {2390} INFO - task = regression
[flaml.automl: 09-18 22:59:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:59:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:59:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:59:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:59:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:59:13] {3025} INFO - Estimated sufficient time budget=22282s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 22:59:13] {3072} INFO -  at 2.4s,	estimator xgboost's best error=17.0695,	best estimator xgboost's best error=17.0695
[flaml.automl: 09-18 22:59:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:59:17] {3072} INFO -  at 6.2s,	estimator xgboost's best error=7.9167,	best estimator xgboost's best error=7.9167
[flaml.automl: 09-18 22:59:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:59:19] {3072} INFO -  at 8.5s,	estimator xgboost's best error=7.9167,	best estimator xgboost's best error=7.9167
[flaml.automl: 09-18 22:59:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:59:37] {3072} INFO -  at 27.0s,	estimator xgboost's best error=7.9167,	best estimator xgboost's best error=7.9167
[flaml.automl: 09-18 22:59:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:59:40] {3072} INFO -  at 29.1s,	estimator xgboost's best error=4.8677,	best estimator xgboost's best error=4.8677
[flaml.automl: 09-18 22:59:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:59:43] {3072} INFO -  at 32.0s,	estimator xgboost's best error=4.8677,	best estimator xgboost's best error=4.8677
[flaml.automl: 09-18 22:59:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:59:46] {3072} INFO -  at 35.2s,	estimator xgboost's best error=3.1382,	best estimator xgboost's best error=3.1382
[flaml.automl: 09-18 22:59:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:59:51] {3072} INFO -  at 40.2s,	estimator xgboost's best error=3.1382,	best estimator xgboost's best error=3.1382
[flaml.automl: 09-18 22:59:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:59:53] {3072} INFO -  at 42.2s,	estimator xgboost's best error=3.1382,	best estimator xgboost's best error=3.1382
[flaml.automl: 09-18 22:59:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:59:56] {3072} INFO -  at 45.2s,	estimator xgboost's best error=3.1382,	best estimator xgboost's best error=3.1382
[flaml.automl: 09-18 22:59:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:59:57] {3072} INFO -  at 46.7s,	estimator xgboost's best error=3.1382,	best estimator xgboost's best error=3.1382
[flaml.automl: 09-18 22:59:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:59:58] {3072} INFO -  at 47.8s,	estimator xgboost's best error=3.1382,	best estimator xgboost's best error=3.1382
[flaml.automl: 09-18 22:59:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:00:05] {3072} INFO -  at 54.8s,	estimator xgboost's best error=2.5447,	best estimator xgboost's best error=2.5447
[flaml.automl: 09-18 23:00:16] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 23:00:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:00:16] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:00:16] {2637} INFO - Time taken to find the best model: 54.795713663101196
[flaml.automl: 09-18 23:00:16] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
PM2.5(0)最佳损失：-1.5446765990175284
PM2.5(0)最好结果：{'pred_time': 1.1482211443619605e-05, 'wall_clock_time': 54.795713663101196, 'metric_for_logging': {'pred_time': 1.1482211443619605e-05}, 'val_loss': 2.5446765990175284, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 6.996200084686279}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.941662448660453
PM2.5(0)的mse=18.0010509021011
PM2.5(0)的mae=2.599322805520245
PM2.5(0)的mar=0.11368452742057802
总共花费的时间为：65.79
商洛市
2645A
[flaml.automl: 09-18 23:03:34] {2390} INFO - task = regression
[flaml.automl: 09-18 23:03:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:03:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:03:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:03:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:03:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:03:36] {3025} INFO - Estimated sufficient time budget=21482s. Estimated necessary time budget=21s.
[flaml.automl: 09-18 23:03:36] {3072} INFO -  at 2.2s,	estimator xgboost's best error=15.3405,	best estimator xgboost's best error=15.3405
[flaml.automl: 09-18 23:03:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:03:40] {3072} INFO -  at 5.6s,	estimator xgboost's best error=8.4127,	best estimator xgboost's best error=8.4127
[flaml.automl: 09-18 23:03:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:03:42] {3072} INFO -  at 7.8s,	estimator xgboost's best error=8.4127,	best estimator xgboost's best error=8.4127
[flaml.automl: 09-18 23:03:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:03:55] {3072} INFO -  at 21.0s,	estimator xgboost's best error=8.4127,	best estimator xgboost's best error=8.4127
[flaml.automl: 09-18 23:03:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:03:57] {3072} INFO -  at 23.1s,	estimator xgboost's best error=4.2945,	best estimator xgboost's best error=4.2945
[flaml.automl: 09-18 23:03:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:04:00] {3072} INFO -  at 26.0s,	estimator xgboost's best error=3.5878,	best estimator xgboost's best error=3.5878
[flaml.automl: 09-18 23:04:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:04:03] {3072} INFO -  at 29.1s,	estimator xgboost's best error=3.5468,	best estimator xgboost's best error=3.5468
[flaml.automl: 09-18 23:04:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:04:08] {3072} INFO -  at 33.5s,	estimator xgboost's best error=3.5468,	best estimator xgboost's best error=3.5468
[flaml.automl: 09-18 23:04:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:04:11] {3072} INFO -  at 36.5s,	estimator xgboost's best error=3.5468,	best estimator xgboost's best error=3.5468
[flaml.automl: 09-18 23:04:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:04:15] {3072} INFO -  at 41.2s,	estimator xgboost's best error=3.5468,	best estimator xgboost's best error=3.5468
[flaml.automl: 09-18 23:04:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:04:19] {3072} INFO -  at 44.4s,	estimator xgboost's best error=3.1967,	best estimator xgboost's best error=3.1967
[flaml.automl: 09-18 23:04:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:04:21] {3072} INFO -  at 46.5s,	estimator xgboost's best error=3.1967,	best estimator xgboost's best error=3.1967
[flaml.automl: 09-18 23:04:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:04:27] {3072} INFO -  at 53.2s,	estimator xgboost's best error=2.8240,	best estimator xgboost's best error=2.8240
[flaml.automl: 09-18 23:04:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 23:04:34] {3072} INFO -  at 59.7s,	estimator xgboost's best error=2.6602,	best estimator xgboost's best error=2.6602
[flaml.automl: 09-18 23:04:46] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-18 23:04:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:04:46] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:04:46] {2637} INFO - Time taken to find the best model: 59.68575382232666
[flaml.automl: 09-18 23:04:46] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463}
PM2.5(0)最佳损失：-1.6601951995066235
PM2.5(0)最好结果：{'pred_time': 3.136481557573591e-05, 'wall_clock_time': 59.68575382232666, 'metric_for_logging': {'pred_time': 3.136481557573591e-05}, 'val_loss': 2.6601951995066235, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463}, 'config/n_estimators': 26, 'config/max_leaves': 6, 'config/min_child_weight': 6.532042959250319, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8835265175628242, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.10065328594609463, 'experiment_tag': 'exp', 'time_total_s': 6.442542791366577}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9107588149036212
PM2.5(0)的mse=21.817514058683873
PM2.5(0)的mae=2.7867932121104095
PM2.5(0)的mar=0.15919849424640986
总共花费的时间为：71.92
白银市
2647A
2648A
[flaml.automl: 09-18 23:11:09] {2390} INFO - task = regression
[flaml.automl: 09-18 23:11:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:11:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:11:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:11:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:11:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:11:13] {3025} INFO - Estimated sufficient time budget=32538s. Estimated necessary time budget=33s.
[flaml.automl: 09-18 23:11:13] {3072} INFO -  at 3.3s,	estimator xgboost's best error=17.4319,	best estimator xgboost's best error=17.4319
[flaml.automl: 09-18 23:11:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:11:18] {3072} INFO -  at 8.9s,	estimator xgboost's best error=8.8657,	best estimator xgboost's best error=8.8657
[flaml.automl: 09-18 23:11:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:11:22] {3072} INFO -  at 12.2s,	estimator xgboost's best error=8.8657,	best estimator xgboost's best error=8.8657
[flaml.automl: 09-18 23:11:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:11:46] {3072} INFO -  at 36.3s,	estimator xgboost's best error=8.8657,	best estimator xgboost's best error=8.8657
[flaml.automl: 09-18 23:11:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:11:48] {3072} INFO -  at 38.4s,	estimator xgboost's best error=7.2757,	best estimator xgboost's best error=7.2757
[flaml.automl: 09-18 23:11:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:11:51] {3072} INFO -  at 41.2s,	estimator xgboost's best error=7.2757,	best estimator xgboost's best error=7.2757
[flaml.automl: 09-18 23:11:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:11:53] {3072} INFO -  at 43.1s,	estimator xgboost's best error=5.0724,	best estimator xgboost's best error=5.0724
[flaml.automl: 09-18 23:11:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:11:55] {3072} INFO -  at 45.8s,	estimator xgboost's best error=5.0724,	best estimator xgboost's best error=5.0724
[flaml.automl: 09-18 23:11:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:11:57] {3072} INFO -  at 47.4s,	estimator xgboost's best error=5.0724,	best estimator xgboost's best error=5.0724
[flaml.automl: 09-18 23:11:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:12:00] {3072} INFO -  at 50.3s,	estimator xgboost's best error=5.0724,	best estimator xgboost's best error=5.0724
[flaml.automl: 09-18 23:12:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:12:01] {3072} INFO -  at 52.0s,	estimator xgboost's best error=5.0724,	best estimator xgboost's best error=5.0724
[flaml.automl: 09-18 23:12:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:12:03] {3072} INFO -  at 53.1s,	estimator xgboost's best error=5.0724,	best estimator xgboost's best error=5.0724
[flaml.automl: 09-18 23:12:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:12:09] {3072} INFO -  at 59.5s,	estimator xgboost's best error=4.8187,	best estimator xgboost's best error=4.8187
[flaml.automl: 09-18 23:12:15] {3335} INFO - retrain xgboost for 6.4s
[flaml.automl: 09-18 23:12:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:12:15] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:12:15] {2637} INFO - Time taken to find the best model: 59.53897976875305
[flaml.automl: 09-18 23:12:15] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
PM2.5(0)最佳损失：-3.818706052523133
PM2.5(0)最好结果：{'pred_time': 1.63458464888667e-05, 'wall_clock_time': 59.53897976875305, 'metric_for_logging': {'pred_time': 1.63458464888667e-05}, 'val_loss': 4.818706052523133, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 6.394288778305054}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8438910345186369
PM2.5(0)的mse=68.25714405281903
PM2.5(0)的mae=4.604457745550272
PM2.5(0)的mar=0.23444646603975725
总共花费的时间为：66.43
天水市
2649A
2650A
2651A
[flaml.automl: 09-18 23:22:30] {2390} INFO - task = regression
[flaml.automl: 09-18 23:22:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:22:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:22:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:22:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:22:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:22:34] {3025} INFO - Estimated sufficient time budget=32786s. Estimated necessary time budget=33s.
[flaml.automl: 09-18 23:22:34] {3072} INFO -  at 3.4s,	estimator xgboost's best error=14.0764,	best estimator xgboost's best error=14.0764
[flaml.automl: 09-18 23:22:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:22:39] {3072} INFO -  at 9.0s,	estimator xgboost's best error=6.5728,	best estimator xgboost's best error=6.5728
[flaml.automl: 09-18 23:22:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:22:42] {3072} INFO -  at 12.2s,	estimator xgboost's best error=6.5728,	best estimator xgboost's best error=6.5728
[flaml.automl: 09-18 23:22:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:23:01] {3072} INFO -  at 30.9s,	estimator xgboost's best error=6.5728,	best estimator xgboost's best error=6.5728
[flaml.automl: 09-18 23:23:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:23:02] {3072} INFO -  at 32.4s,	estimator xgboost's best error=4.7820,	best estimator xgboost's best error=4.7820
[flaml.automl: 09-18 23:23:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:23:04] {3072} INFO -  at 33.9s,	estimator xgboost's best error=4.1649,	best estimator xgboost's best error=4.1649
[flaml.automl: 09-18 23:23:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:23:06] {3072} INFO -  at 35.5s,	estimator xgboost's best error=3.9554,	best estimator xgboost's best error=3.9554
[flaml.automl: 09-18 23:23:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:23:08] {3072} INFO -  at 38.2s,	estimator xgboost's best error=3.9554,	best estimator xgboost's best error=3.9554
[flaml.automl: 09-18 23:23:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:23:10] {3072} INFO -  at 39.8s,	estimator xgboost's best error=3.9282,	best estimator xgboost's best error=3.9282
[flaml.automl: 09-18 23:23:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:23:13] {3072} INFO -  at 42.8s,	estimator xgboost's best error=3.8439,	best estimator xgboost's best error=3.8439
[flaml.automl: 09-18 23:23:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:23:14] {3072} INFO -  at 43.9s,	estimator xgboost's best error=3.8439,	best estimator xgboost's best error=3.8439
[flaml.automl: 09-18 23:23:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:23:28] {3072} INFO -  at 57.6s,	estimator xgboost's best error=3.4381,	best estimator xgboost's best error=3.4381
[flaml.automl: 09-18 23:23:41] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-18 23:23:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:23:41] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:23:41] {2637} INFO - Time taken to find the best model: 57.577916383743286
[flaml.automl: 09-18 23:23:41] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
PM2.5(0)最佳损失：-2.438077465524662
PM2.5(0)最好结果：{'pred_time': 1.3063386416126336e-05, 'wall_clock_time': 57.577916383743286, 'metric_for_logging': {'pred_time': 1.3063386416126336e-05}, 'val_loss': 3.438077465524662, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 13.656431198120117}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9078261414025679
PM2.5(0)的mse=25.878608545690753
PM2.5(0)的mae=3.4180016892360117
PM2.5(0)的mar=0.19646684704983616
总共花费的时间为：71.89
武威市
2652A
[flaml.automl: 09-18 23:26:42] {2390} INFO - task = regression
[flaml.automl: 09-18 23:26:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:26:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:26:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:26:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:26:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:26:45] {3025} INFO - Estimated sufficient time budget=32928s. Estimated necessary time budget=33s.
[flaml.automl: 09-18 23:26:45] {3072} INFO -  at 3.4s,	estimator xgboost's best error=19.1615,	best estimator xgboost's best error=19.1615
[flaml.automl: 09-18 23:26:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:26:50] {3072} INFO -  at 8.3s,	estimator xgboost's best error=10.4232,	best estimator xgboost's best error=10.4232
[flaml.automl: 09-18 23:26:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:26:54] {3072} INFO -  at 11.5s,	estimator xgboost's best error=10.4232,	best estimator xgboost's best error=10.4232
[flaml.automl: 09-18 23:26:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:27:09] {3072} INFO -  at 27.4s,	estimator xgboost's best error=10.4232,	best estimator xgboost's best error=10.4232
[flaml.automl: 09-18 23:27:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:27:12] {3072} INFO -  at 29.5s,	estimator xgboost's best error=6.1414,	best estimator xgboost's best error=6.1414
[flaml.automl: 09-18 23:27:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:27:14] {3072} INFO -  at 32.4s,	estimator xgboost's best error=5.4978,	best estimator xgboost's best error=5.4978
[flaml.automl: 09-18 23:27:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:27:17] {3072} INFO -  at 35.4s,	estimator xgboost's best error=5.4978,	best estimator xgboost's best error=5.4978
[flaml.automl: 09-18 23:27:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:27:22] {3072} INFO -  at 39.6s,	estimator xgboost's best error=5.4978,	best estimator xgboost's best error=5.4978
[flaml.automl: 09-18 23:27:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:27:24] {3072} INFO -  at 41.7s,	estimator xgboost's best error=5.2420,	best estimator xgboost's best error=5.2420
[flaml.automl: 09-18 23:27:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:27:27] {3072} INFO -  at 45.1s,	estimator xgboost's best error=4.9744,	best estimator xgboost's best error=4.9744
[flaml.automl: 09-18 23:27:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:27:29] {3072} INFO -  at 46.7s,	estimator xgboost's best error=4.9744,	best estimator xgboost's best error=4.9744
[flaml.automl: 09-18 23:27:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:27:30] {3072} INFO -  at 47.6s,	estimator xgboost's best error=4.9744,	best estimator xgboost's best error=4.9744
[flaml.automl: 09-18 23:27:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:27:36] {3072} INFO -  at 53.9s,	estimator xgboost's best error=4.7320,	best estimator xgboost's best error=4.7320
[flaml.automl: 09-18 23:27:42] {3335} INFO - retrain xgboost for 6.3s
[flaml.automl: 09-18 23:27:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7321477488407947, colsample_bynode=1,
             colsample_bytree=0.523235031572675, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.006178490416939562,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.026483886059738067, reg_lambda=15.135470774428443,
             scale_pos_weight=1, subsample=0.7583121251176891,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:27:42] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:27:42] {2637} INFO - Time taken to find the best model: 53.89341998100281
[flaml.automl: 09-18 23:27:42] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.006178490416939562, 'learning_rate': 1.0, 'subsample': 0.7583121251176891, 'colsample_bylevel': 0.7321477488407947, 'colsample_bytree': 0.523235031572675, 'reg_alpha': 0.026483886059738067, 'reg_lambda': 15.135470774428443}
PM2.5(0)最佳损失：-3.7320321544749815
PM2.5(0)最好结果：{'pred_time': 3.138400717488905e-05, 'wall_clock_time': 53.89341998100281, 'metric_for_logging': {'pred_time': 3.138400717488905e-05}, 'val_loss': 4.7320321544749815, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.006178490416939562, 'learning_rate': 1.0, 'subsample': 0.7583121251176891, 'colsample_bylevel': 0.7321477488407947, 'colsample_bytree': 0.523235031572675, 'reg_alpha': 0.026483886059738067, 'reg_lambda': 15.135470774428443}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.006178490416939562, 'config/learning_rate': 1.0, 'config/subsample': 0.7583121251176891, 'config/colsample_bylevel': 0.7321477488407947, 'config/colsample_bytree': 0.523235031572675, 'config/reg_alpha': 0.026483886059738067, 'config/reg_lambda': 15.135470774428443, 'experiment_tag': 'exp', 'time_total_s': 6.317140340805054}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7321477488407947, colsample_bynode=1,
             colsample_bytree=0.523235031572675, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.006178490416939562,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.026483886059738067, reg_lambda=15.135470774428443,
             scale_pos_weight=1, subsample=0.7583121251176891,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7447147180629595
PM2.5(0)的mse=127.30167336977753
PM2.5(0)的mae=5.475909877033177
PM2.5(0)的mar=0.1777527619451215
总共花费的时间为：60.46
张掖市
2654A
2655A
[flaml.automl: 09-18 23:34:42] {2390} INFO - task = regression
[flaml.automl: 09-18 23:34:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:34:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:34:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:34:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:34:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:34:45] {3025} INFO - Estimated sufficient time budget=32137s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 23:34:45] {3072} INFO -  at 3.3s,	estimator xgboost's best error=19.8233,	best estimator xgboost's best error=19.8233
[flaml.automl: 09-18 23:34:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:34:51] {3072} INFO -  at 9.0s,	estimator xgboost's best error=9.7181,	best estimator xgboost's best error=9.7181
[flaml.automl: 09-18 23:34:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:34:54] {3072} INFO -  at 12.2s,	estimator xgboost's best error=9.7181,	best estimator xgboost's best error=9.7181
[flaml.automl: 09-18 23:34:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:35:20] {3072} INFO -  at 37.8s,	estimator xgboost's best error=9.7181,	best estimator xgboost's best error=9.7181
[flaml.automl: 09-18 23:35:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:35:23] {3072} INFO -  at 40.8s,	estimator xgboost's best error=8.0045,	best estimator xgboost's best error=8.0045
[flaml.automl: 09-18 23:35:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:35:25] {3072} INFO -  at 43.7s,	estimator xgboost's best error=7.3522,	best estimator xgboost's best error=7.3522
[flaml.automl: 09-18 23:35:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:35:28] {3072} INFO -  at 46.6s,	estimator xgboost's best error=6.6340,	best estimator xgboost's best error=6.6340
[flaml.automl: 09-18 23:35:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:35:31] {3072} INFO -  at 49.3s,	estimator xgboost's best error=6.6340,	best estimator xgboost's best error=6.6340
[flaml.automl: 09-18 23:35:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:35:33] {3072} INFO -  at 50.9s,	estimator xgboost's best error=6.6340,	best estimator xgboost's best error=6.6340
[flaml.automl: 09-18 23:35:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:35:36] {3072} INFO -  at 53.9s,	estimator xgboost's best error=6.0124,	best estimator xgboost's best error=6.0124
[flaml.automl: 09-18 23:35:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:35:37] {3072} INFO -  at 55.5s,	estimator xgboost's best error=6.0124,	best estimator xgboost's best error=6.0124
[flaml.automl: 09-18 23:35:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:35:38] {3072} INFO -  at 56.6s,	estimator xgboost's best error=6.0124,	best estimator xgboost's best error=6.0124
[flaml.automl: 09-18 23:35:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:35:41] {3072} INFO -  at 59.2s,	estimator xgboost's best error=6.0124,	best estimator xgboost's best error=6.0124
[flaml.automl: 09-18 23:35:44] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-18 23:35:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:35:44] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:35:44] {2637} INFO - Time taken to find the best model: 53.86606550216675
[flaml.automl: 09-18 23:35:44] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-5.012446165378578
PM2.5(0)最好结果：{'pred_time': 1.5874662928023132e-05, 'wall_clock_time': 53.86606550216675, 'metric_for_logging': {'pred_time': 1.5874662928023132e-05}, 'val_loss': 6.012446165378578, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 2.974358081817627}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7352191769171201
PM2.5(0)的mse=99.38529664137154
PM2.5(0)的mae=6.069518065329225
PM2.5(0)的mar=0.2895612828687057
总共花费的时间为：62.61
平凉市
2656A
2657A
[flaml.automl: 09-18 23:42:41] {2390} INFO - task = regression
[flaml.automl: 09-18 23:42:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:42:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:42:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:42:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:42:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:42:44] {3025} INFO - Estimated sufficient time budget=31840s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 23:42:44] {3072} INFO -  at 3.3s,	estimator xgboost's best error=12.7406,	best estimator xgboost's best error=12.7406
[flaml.automl: 09-18 23:42:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:42:49] {3072} INFO -  at 8.9s,	estimator xgboost's best error=6.0634,	best estimator xgboost's best error=6.0634
[flaml.automl: 09-18 23:42:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:42:53] {3072} INFO -  at 12.1s,	estimator xgboost's best error=6.0634,	best estimator xgboost's best error=6.0634
[flaml.automl: 09-18 23:42:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:43:18] {3072} INFO -  at 37.8s,	estimator xgboost's best error=6.0634,	best estimator xgboost's best error=6.0634
[flaml.automl: 09-18 23:43:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:43:21] {3072} INFO -  at 40.8s,	estimator xgboost's best error=4.8666,	best estimator xgboost's best error=4.8666
[flaml.automl: 09-18 23:43:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:43:26] {3072} INFO -  at 45.0s,	estimator xgboost's best error=4.8666,	best estimator xgboost's best error=4.8666
[flaml.automl: 09-18 23:43:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:43:30] {3072} INFO -  at 49.7s,	estimator xgboost's best error=3.8984,	best estimator xgboost's best error=3.8984
[flaml.automl: 09-18 23:43:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:43:36] {3072} INFO -  at 55.2s,	estimator xgboost's best error=3.8984,	best estimator xgboost's best error=3.8984
[flaml.automl: 09-18 23:43:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:43:38] {3072} INFO -  at 57.2s,	estimator xgboost's best error=3.8984,	best estimator xgboost's best error=3.8984
[flaml.automl: 09-18 23:43:39] {3335} INFO - retrain xgboost for 1.6s
[flaml.automl: 09-18 23:43:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:43:39] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:43:39] {2637} INFO - Time taken to find the best model: 49.74539089202881
[flaml.automl: 09-18 23:43:39] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-2.898447154097436
PM2.5(0)最好结果：{'pred_time': 4.074311861730227e-05, 'wall_clock_time': 49.74539089202881, 'metric_for_logging': {'pred_time': 4.074311861730227e-05}, 'val_loss': 3.898447154097436, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.717520713806152}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.738935048373144
PM2.5(0)的mse=35.45997183975614
PM2.5(0)的mae=3.9840741874632957
PM2.5(0)的mar=0.23790169983248347
总共花费的时间为：59.24
酒泉市
2658A
2659A
[flaml.automl: 09-18 23:51:07] {2390} INFO - task = regression
[flaml.automl: 09-18 23:51:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:51:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:51:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:51:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:51:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:51:11] {3025} INFO - Estimated sufficient time budget=33030s. Estimated necessary time budget=33s.
[flaml.automl: 09-18 23:51:11] {3072} INFO -  at 3.4s,	estimator xgboost's best error=17.7017,	best estimator xgboost's best error=17.7017
[flaml.automl: 09-18 23:51:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:51:17] {3072} INFO -  at 9.1s,	estimator xgboost's best error=8.4493,	best estimator xgboost's best error=8.4493
[flaml.automl: 09-18 23:51:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:51:20] {3072} INFO -  at 12.4s,	estimator xgboost's best error=8.4493,	best estimator xgboost's best error=8.4493
[flaml.automl: 09-18 23:51:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:51:45] {3072} INFO -  at 38.0s,	estimator xgboost's best error=8.4493,	best estimator xgboost's best error=8.4493
[flaml.automl: 09-18 23:51:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:51:48] {3072} INFO -  at 41.0s,	estimator xgboost's best error=6.4843,	best estimator xgboost's best error=6.4843
[flaml.automl: 09-18 23:51:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:51:53] {3072} INFO -  at 45.2s,	estimator xgboost's best error=6.4843,	best estimator xgboost's best error=6.4843
[flaml.automl: 09-18 23:51:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:51:57] {3072} INFO -  at 49.5s,	estimator xgboost's best error=5.0491,	best estimator xgboost's best error=5.0491
[flaml.automl: 09-18 23:51:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:52:01] {3072} INFO -  at 54.0s,	estimator xgboost's best error=5.0491,	best estimator xgboost's best error=5.0491
[flaml.automl: 09-18 23:52:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:52:03] {3072} INFO -  at 55.6s,	estimator xgboost's best error=5.0491,	best estimator xgboost's best error=5.0491
[flaml.automl: 09-18 23:52:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:52:06] {3072} INFO -  at 58.6s,	estimator xgboost's best error=5.0491,	best estimator xgboost's best error=5.0491
[flaml.automl: 09-18 23:52:08] {3335} INFO - retrain xgboost for 1.6s
[flaml.automl: 09-18 23:52:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:52:08] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:52:08] {2637} INFO - Time taken to find the best model: 49.47406721115112
[flaml.automl: 09-18 23:52:08] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-4.049091500442845
PM2.5(0)最好结果：{'pred_time': 3.135518660168135e-05, 'wall_clock_time': 49.47406721115112, 'metric_for_logging': {'pred_time': 3.135518660168135e-05}, 'val_loss': 5.049091500442845, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.302082538604736}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7209213254905183
PM2.5(0)的mse=76.94047026469482
PM2.5(0)的mae=5.041312945067513
PM2.5(0)的mar=0.19600227951579632
总共花费的时间为：60.64
庆阳市
2662A
[flaml.automl: 09-18 23:56:12] {2390} INFO - task = regression
[flaml.automl: 09-18 23:56:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:56:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:56:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:56:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:56:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:56:16] {3025} INFO - Estimated sufficient time budget=48294s. Estimated necessary time budget=48s.
[flaml.automl: 09-18 23:56:16] {3072} INFO -  at 5.1s,	estimator xgboost's best error=15.5496,	best estimator xgboost's best error=15.5496
[flaml.automl: 09-18 23:56:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:56:24] {3072} INFO -  at 12.6s,	estimator xgboost's best error=8.4547,	best estimator xgboost's best error=8.4547
[flaml.automl: 09-18 23:56:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:56:29] {3072} INFO -  at 17.5s,	estimator xgboost's best error=8.4547,	best estimator xgboost's best error=8.4547
[flaml.automl: 09-18 23:56:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:57:00] {3072} INFO -  at 48.6s,	estimator xgboost's best error=8.4547,	best estimator xgboost's best error=8.4547
[flaml.automl: 09-18 23:57:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:57:03] {3072} INFO -  at 51.6s,	estimator xgboost's best error=5.0870,	best estimator xgboost's best error=5.0870
[flaml.automl: 09-18 23:57:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:57:05] {3072} INFO -  at 53.6s,	estimator xgboost's best error=4.0601,	best estimator xgboost's best error=4.0601
[flaml.automl: 09-18 23:57:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:57:07] {3072} INFO -  at 55.9s,	estimator xgboost's best error=4.0601,	best estimator xgboost's best error=4.0601
[flaml.automl: 09-18 23:57:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:57:10] {3072} INFO -  at 58.9s,	estimator xgboost's best error=4.0601,	best estimator xgboost's best error=4.0601
[flaml.automl: 09-18 23:57:13] {3335} INFO - retrain xgboost for 2.3s
[flaml.automl: 09-18 23:57:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:57:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:57:13] {2637} INFO - Time taken to find the best model: 53.646798610687256
[flaml.automl: 09-18 23:57:13] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
PM2.5(0)最佳损失：-3.0601180050824137
PM2.5(0)最好结果：{'pred_time': 3.831900113142484e-05, 'wall_clock_time': 53.646798610687256, 'metric_for_logging': {'pred_time': 3.831900113142484e-05}, 'val_loss': 4.060118005082414, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 2.092042922973633}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8449522048553838
PM2.5(0)的mse=40.036323769665294
PM2.5(0)的mae=4.130756620108905
PM2.5(0)的mar=0.2222476618068771
总共花费的时间为：61.59
定西市
2663A
2664A
[flaml.automl: 09-19 00:04:36] {2390} INFO - task = regression
[flaml.automl: 09-19 00:04:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:04:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:04:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:04:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:04:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:04:40] {3025} INFO - Estimated sufficient time budget=32583s. Estimated necessary time budget=33s.
[flaml.automl: 09-19 00:04:40] {3072} INFO -  at 3.4s,	estimator xgboost's best error=16.2465,	best estimator xgboost's best error=16.2465
[flaml.automl: 09-19 00:04:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:04:45] {3072} INFO -  at 9.0s,	estimator xgboost's best error=7.9750,	best estimator xgboost's best error=7.9750
[flaml.automl: 09-19 00:04:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:04:48] {3072} INFO -  at 12.2s,	estimator xgboost's best error=7.9750,	best estimator xgboost's best error=7.9750
[flaml.automl: 09-19 00:04:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:05:13] {3072} INFO -  at 37.1s,	estimator xgboost's best error=7.9750,	best estimator xgboost's best error=7.9750
[flaml.automl: 09-19 00:05:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:05:15] {3072} INFO -  at 39.2s,	estimator xgboost's best error=6.0485,	best estimator xgboost's best error=6.0485
[flaml.automl: 09-19 00:05:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:05:18] {3072} INFO -  at 42.1s,	estimator xgboost's best error=5.4123,	best estimator xgboost's best error=5.4123
[flaml.automl: 09-19 00:05:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:05:21] {3072} INFO -  at 44.9s,	estimator xgboost's best error=5.3239,	best estimator xgboost's best error=5.3239
[flaml.automl: 09-19 00:05:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:05:26] {3072} INFO -  at 49.3s,	estimator xgboost's best error=5.3239,	best estimator xgboost's best error=5.3239
[flaml.automl: 09-19 00:05:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:05:28] {3072} INFO -  at 51.4s,	estimator xgboost's best error=5.3239,	best estimator xgboost's best error=5.3239
[flaml.automl: 09-19 00:05:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:05:32] {3072} INFO -  at 56.1s,	estimator xgboost's best error=4.9431,	best estimator xgboost's best error=4.9431
[flaml.automl: 09-19 00:05:37] {3335} INFO - retrain xgboost for 4.9s
[flaml.automl: 09-19 00:05:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:05:37] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:05:37] {2637} INFO - Time taken to find the best model: 56.115784883499146
[flaml.automl: 09-19 00:05:37] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-3.9431098440931187
PM2.5(0)最好结果：{'pred_time': 3.184934563477769e-05, 'wall_clock_time': 56.115784883499146, 'metric_for_logging': {'pred_time': 3.184934563477769e-05}, 'val_loss': 4.943109844093119, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 4.665156602859497}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7667387998275674
PM2.5(0)的mse=59.72257961366371
PM2.5(0)的mae=4.870674291068688
PM2.5(0)的mar=0.23425179336182803
总共花费的时间为：61.42
陇南市
2665A
3247A
[flaml.automl: 09-19 00:12:39] {2390} INFO - task = regression
[flaml.automl: 09-19 00:12:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:12:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:12:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:12:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:12:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:12:42] {3025} INFO - Estimated sufficient time budget=32490s. Estimated necessary time budget=32s.
[flaml.automl: 09-19 00:12:42] {3072} INFO -  at 3.3s,	estimator xgboost's best error=9.5208,	best estimator xgboost's best error=9.5208
[flaml.automl: 09-19 00:12:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:12:47] {3072} INFO -  at 8.4s,	estimator xgboost's best error=5.0928,	best estimator xgboost's best error=5.0928
[flaml.automl: 09-19 00:12:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:12:50] {3072} INFO -  at 11.5s,	estimator xgboost's best error=5.0928,	best estimator xgboost's best error=5.0928
[flaml.automl: 09-19 00:12:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:13:13] {3072} INFO -  at 34.7s,	estimator xgboost's best error=5.0928,	best estimator xgboost's best error=5.0928
[flaml.automl: 09-19 00:13:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:13:16] {3072} INFO -  at 37.8s,	estimator xgboost's best error=3.3479,	best estimator xgboost's best error=3.3479
[flaml.automl: 09-19 00:13:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:13:20] {3072} INFO -  at 41.9s,	estimator xgboost's best error=3.3479,	best estimator xgboost's best error=3.3479
[flaml.automl: 09-19 00:13:20] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:13:22] {3072} INFO -  at 43.9s,	estimator xgboost's best error=2.7056,	best estimator xgboost's best error=2.7056
[flaml.automl: 09-19 00:13:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:13:26] {3072} INFO -  at 47.2s,	estimator xgboost's best error=2.7056,	best estimator xgboost's best error=2.7056
[flaml.automl: 09-19 00:13:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:13:28] {3072} INFO -  at 49.4s,	estimator xgboost's best error=2.7056,	best estimator xgboost's best error=2.7056
[flaml.automl: 09-19 00:13:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:13:32] {3072} INFO -  at 53.9s,	estimator xgboost's best error=2.7056,	best estimator xgboost's best error=2.7056
[flaml.automl: 09-19 00:13:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:13:36] {3072} INFO -  at 57.4s,	estimator xgboost's best error=2.6658,	best estimator xgboost's best error=2.6658
[flaml.automl: 09-19 00:13:39] {3335} INFO - retrain xgboost for 3.6s
[flaml.automl: 09-19 00:13:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.923976097470924, colsample_bynode=1,
             colsample_bytree=0.9654640505640502, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4847187609776744,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=104.45542841808965, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3217956171961422, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:13:39] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:13:39] {2637} INFO - Time taken to find the best model: 57.41426992416382
[flaml.automl: 09-19 00:13:39] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 104.45542841808965, 'learning_rate': 0.4847187609776744, 'subsample': 1.0, 'colsample_bylevel': 0.923976097470924, 'colsample_bytree': 0.9654640505640502, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3217956171961422}
PM2.5(0)最佳损失：-1.6658017118598134
PM2.5(0)最好结果：{'pred_time': 5.211404232141899e-05, 'wall_clock_time': 57.41426992416382, 'metric_for_logging': {'pred_time': 5.211404232141899e-05}, 'val_loss': 2.6658017118598134, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 104.45542841808965, 'learning_rate': 0.4847187609776744, 'subsample': 1.0, 'colsample_bylevel': 0.923976097470924, 'colsample_bytree': 0.9654640505640502, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3217956171961422}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 104.45542841808965, 'config/learning_rate': 0.4847187609776744, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.923976097470924, 'config/colsample_bytree': 0.9654640505640502, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.3217956171961422, 'experiment_tag': 'exp', 'time_total_s': 3.4909279346466064}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.923976097470924, colsample_bynode=1,
             colsample_bytree=0.9654640505640502, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4847187609776744,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=104.45542841808965, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3217956171961422, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8575678987645073
PM2.5(0)的mse=13.044320294537133
PM2.5(0)的mae=2.609118320085548
PM2.5(0)的mar=0.2153381034327108
总共花费的时间为：61.46
临夏回族自治州
2667A
2668A
[flaml.automl: 09-19 00:20:39] {2390} INFO - task = regression
[flaml.automl: 09-19 00:20:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:20:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:20:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:20:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:20:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:20:43] {3025} INFO - Estimated sufficient time budget=32057s. Estimated necessary time budget=32s.
[flaml.automl: 09-19 00:20:43] {3072} INFO -  at 3.3s,	estimator xgboost's best error=18.1733,	best estimator xgboost's best error=18.1733
[flaml.automl: 09-19 00:20:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:20:48] {3072} INFO -  at 9.0s,	estimator xgboost's best error=8.9493,	best estimator xgboost's best error=8.9493
[flaml.automl: 09-19 00:20:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:20:51] {3072} INFO -  at 12.1s,	estimator xgboost's best error=8.9493,	best estimator xgboost's best error=8.9493
[flaml.automl: 09-19 00:20:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:21:21] {3072} INFO -  at 42.1s,	estimator xgboost's best error=8.9493,	best estimator xgboost's best error=8.9493
[flaml.automl: 09-19 00:21:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:21:25] {3072} INFO -  at 45.3s,	estimator xgboost's best error=6.7396,	best estimator xgboost's best error=6.7396
[flaml.automl: 09-19 00:21:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:21:29] {3072} INFO -  at 49.3s,	estimator xgboost's best error=5.9424,	best estimator xgboost's best error=5.9424
[flaml.automl: 09-19 00:21:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:21:32] {3072} INFO -  at 52.5s,	estimator xgboost's best error=5.8025,	best estimator xgboost's best error=5.8025
[flaml.automl: 09-19 00:21:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:21:37] {3072} INFO -  at 58.3s,	estimator xgboost's best error=5.8025,	best estimator xgboost's best error=5.8025
[flaml.automl: 09-19 00:21:41] {3335} INFO - retrain xgboost for 3.4s
[flaml.automl: 09-19 00:21:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:21:41] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:21:41] {2637} INFO - Time taken to find the best model: 52.457783699035645
[flaml.automl: 09-19 00:21:41] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-4.802458780249771
PM2.5(0)最好结果：{'pred_time': 3.136790528589365e-05, 'wall_clock_time': 52.457783699035645, 'metric_for_logging': {'pred_time': 3.136790528589365e-05}, 'val_loss': 5.802458780249771, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 3.138765811920166}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.7429410988024846
PM2.5(0)的mse=61.98449630475797
PM2.5(0)的mae=5.474460793382054
PM2.5(0)的mar=0.25160792768493967
总共花费的时间为：62.15
甘南州
2669A
[flaml.automl: 09-19 00:25:10] {2390} INFO - task = regression
[flaml.automl: 09-19 00:25:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:25:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:25:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:25:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:25:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:25:13] {3025} INFO - Estimated sufficient time budget=31416s. Estimated necessary time budget=31s.
[flaml.automl: 09-19 00:25:13] {3072} INFO -  at 3.2s,	estimator xgboost's best error=13.4148,	best estimator xgboost's best error=13.4148
[flaml.automl: 09-19 00:25:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:25:18] {3072} INFO -  at 8.2s,	estimator xgboost's best error=7.4783,	best estimator xgboost's best error=7.4783
[flaml.automl: 09-19 00:25:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:25:21] {3072} INFO -  at 11.5s,	estimator xgboost's best error=7.4783,	best estimator xgboost's best error=7.4783
[flaml.automl: 09-19 00:25:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:25:40] {3072} INFO -  at 30.6s,	estimator xgboost's best error=7.4783,	best estimator xgboost's best error=7.4783
[flaml.automl: 09-19 00:25:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:25:44] {3072} INFO -  at 33.7s,	estimator xgboost's best error=4.6417,	best estimator xgboost's best error=4.6417
[flaml.automl: 09-19 00:25:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:25:48] {3072} INFO -  at 38.4s,	estimator xgboost's best error=4.2100,	best estimator xgboost's best error=4.2100
[flaml.automl: 09-19 00:25:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:25:53] {3072} INFO -  at 42.8s,	estimator xgboost's best error=4.2100,	best estimator xgboost's best error=4.2100
[flaml.automl: 09-19 00:25:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:25:56] {3072} INFO -  at 46.5s,	estimator xgboost's best error=4.2100,	best estimator xgboost's best error=4.2100
[flaml.automl: 09-19 00:25:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:25:58] {3072} INFO -  at 48.0s,	estimator xgboost's best error=4.2100,	best estimator xgboost's best error=4.2100
[flaml.automl: 09-19 00:25:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:26:01] {3072} INFO -  at 51.5s,	estimator xgboost's best error=4.2100,	best estimator xgboost's best error=4.2100
[flaml.automl: 09-19 00:26:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:26:03] {3072} INFO -  at 52.9s,	estimator xgboost's best error=4.2100,	best estimator xgboost's best error=4.2100
[flaml.automl: 09-19 00:26:03] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:26:04] {3072} INFO -  at 54.4s,	estimator xgboost's best error=4.2100,	best estimator xgboost's best error=4.2100
[flaml.automl: 09-19 00:26:04] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:26:09] {3072} INFO -  at 59.5s,	estimator xgboost's best error=4.0307,	best estimator xgboost's best error=4.0307
[flaml.automl: 09-19 00:26:17] {3335} INFO - retrain xgboost for 8.1s
[flaml.automl: 09-19 00:26:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:26:17] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:26:17] {2637} INFO - Time taken to find the best model: 59.538320541381836
[flaml.automl: 09-19 00:26:17] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
PM2.5(0)最佳损失：-3.0306691323507104
PM2.5(0)最好结果：{'pred_time': 3.599829316527525e-05, 'wall_clock_time': 59.538320541381836, 'metric_for_logging': {'pred_time': 3.599829316527525e-05}, 'val_loss': 4.0306691323507104, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 12, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 5.1553990840911865}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7787201432974149
PM2.5(0)的mse=29.968402549793502
PM2.5(0)的mae=3.701410994686923
PM2.5(0)的mar=0.2204742750285778
总共花费的时间为：67.89
海东地区
3867A
[flaml.automl: 09-19 00:29:35] {2390} INFO - task = regression
[flaml.automl: 09-19 00:29:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:29:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:29:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:29:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:29:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:29:39] {3025} INFO - Estimated sufficient time budget=43667s. Estimated necessary time budget=44s.
[flaml.automl: 09-19 00:29:39] {3072} INFO -  at 4.5s,	estimator xgboost's best error=22.3184,	best estimator xgboost's best error=22.3184
[flaml.automl: 09-19 00:29:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:29:45] {3072} INFO -  at 10.8s,	estimator xgboost's best error=12.0573,	best estimator xgboost's best error=12.0573
[flaml.automl: 09-19 00:29:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:29:49] {3072} INFO -  at 14.6s,	estimator xgboost's best error=12.0573,	best estimator xgboost's best error=12.0573
[flaml.automl: 09-19 00:29:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:30:08] {3072} INFO -  at 33.4s,	estimator xgboost's best error=12.0573,	best estimator xgboost's best error=12.0573
[flaml.automl: 09-19 00:30:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:30:11] {3072} INFO -  at 36.5s,	estimator xgboost's best error=5.9504,	best estimator xgboost's best error=5.9504
[flaml.automl: 09-19 00:30:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:30:14] {3072} INFO -  at 39.1s,	estimator xgboost's best error=5.2891,	best estimator xgboost's best error=5.2891
[flaml.automl: 09-19 00:30:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:30:16] {3072} INFO -  at 41.0s,	estimator xgboost's best error=5.2756,	best estimator xgboost's best error=5.2756
[flaml.automl: 09-19 00:30:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:30:19] {3072} INFO -  at 44.0s,	estimator xgboost's best error=5.2756,	best estimator xgboost's best error=5.2756
[flaml.automl: 09-19 00:30:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:30:21] {3072} INFO -  at 45.9s,	estimator xgboost's best error=5.2756,	best estimator xgboost's best error=5.2756
[flaml.automl: 09-19 00:30:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:30:23] {3072} INFO -  at 48.5s,	estimator xgboost's best error=5.2756,	best estimator xgboost's best error=5.2756
[flaml.automl: 09-19 00:30:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:30:25] {3072} INFO -  at 50.5s,	estimator xgboost's best error=4.8397,	best estimator xgboost's best error=4.8397
[flaml.automl: 09-19 00:30:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:30:27] {3072} INFO -  at 52.2s,	estimator xgboost's best error=4.8397,	best estimator xgboost's best error=4.8397
[flaml.automl: 09-19 00:30:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:30:34] {3072} INFO -  at 59.2s,	estimator xgboost's best error=4.7541,	best estimator xgboost's best error=4.7541
[flaml.automl: 09-19 00:30:43] {3335} INFO - retrain xgboost for 9.3s
[flaml.automl: 09-19 00:30:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=8, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:30:43] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:30:43] {2637} INFO - Time taken to find the best model: 59.24125385284424
[flaml.automl: 09-19 00:30:43] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 9, 'max_leaves': 8, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}
PM2.5(0)最佳损失：-3.754121758942711
PM2.5(0)最好结果：{'pred_time': 0.00022970971885634346, 'wall_clock_time': 59.24125385284424, 'metric_for_logging': {'pred_time': 0.00022970971885634346}, 'val_loss': 4.754121758942711, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 8, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}, 'config/n_estimators': 9, 'config/max_leaves': 8, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'experiment_tag': 'exp', 'time_total_s': 7.030773878097534}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=8, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8039726832188298
PM2.5(0)的mse=47.716089972023944
PM2.5(0)的mae=4.961430001784773
PM2.5(0)的mar=0.16880833195240932
总共花费的时间为：68.81
海北藏族自治州
2671A
[flaml.automl: 09-19 00:34:06] {2390} INFO - task = regression
[flaml.automl: 09-19 00:34:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:34:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:34:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:34:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:34:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:34:10] {3025} INFO - Estimated sufficient time budget=32296s. Estimated necessary time budget=32s.
[flaml.automl: 09-19 00:34:10] {3072} INFO -  at 3.3s,	estimator xgboost's best error=13.0558,	best estimator xgboost's best error=13.0558
[flaml.automl: 09-19 00:34:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:34:15] {3072} INFO -  at 8.2s,	estimator xgboost's best error=6.9249,	best estimator xgboost's best error=6.9249
[flaml.automl: 09-19 00:34:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:34:18] {3072} INFO -  at 11.5s,	estimator xgboost's best error=6.9249,	best estimator xgboost's best error=6.9249
[flaml.automl: 09-19 00:34:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:34:37] {3072} INFO -  at 30.4s,	estimator xgboost's best error=6.9249,	best estimator xgboost's best error=6.9249
[flaml.automl: 09-19 00:34:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:34:40] {3072} INFO -  at 33.5s,	estimator xgboost's best error=3.5776,	best estimator xgboost's best error=3.5776
[flaml.automl: 09-19 00:34:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:34:44] {3072} INFO -  at 37.8s,	estimator xgboost's best error=3.2228,	best estimator xgboost's best error=3.2228
[flaml.automl: 09-19 00:34:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:34:49] {3072} INFO -  at 42.2s,	estimator xgboost's best error=3.1299,	best estimator xgboost's best error=3.1299
[flaml.automl: 09-19 00:34:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:34:54] {3072} INFO -  at 47.5s,	estimator xgboost's best error=3.1299,	best estimator xgboost's best error=3.1299
[flaml.automl: 09-19 00:34:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:34:57] {3072} INFO -  at 50.5s,	estimator xgboost's best error=3.1299,	best estimator xgboost's best error=3.1299
[flaml.automl: 09-19 00:34:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:35:00] {3072} INFO -  at 53.3s,	estimator xgboost's best error=3.1299,	best estimator xgboost's best error=3.1299
[flaml.automl: 09-19 00:35:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:35:01] {3072} INFO -  at 55.0s,	estimator xgboost's best error=2.9955,	best estimator xgboost's best error=2.9955
[flaml.automl: 09-19 00:35:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:35:02] {3072} INFO -  at 56.1s,	estimator xgboost's best error=2.9955,	best estimator xgboost's best error=2.9955
[flaml.automl: 09-19 00:35:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:35:06] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.8826,	best estimator xgboost's best error=2.8826
[flaml.automl: 09-19 00:35:11] {3335} INFO - retrain xgboost for 5.6s
[flaml.automl: 09-19 00:35:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=8, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:35:11] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:35:11] {2637} INFO - Time taken to find the best model: 59.50402212142944
[flaml.automl: 09-19 00:35:11] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 8, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}
PM2.5(0)最佳损失：-1.8825894441259536
PM2.5(0)最好结果：{'pred_time': 3.1837703366028635e-05, 'wall_clock_time': 59.50402212142944, 'metric_for_logging': {'pred_time': 3.1837703366028635e-05}, 'val_loss': 2.8825894441259536, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 8, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}, 'config/n_estimators': 12, 'config/max_leaves': 8, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'experiment_tag': 'exp', 'time_total_s': 3.407332181930542}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=8, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8171034972273832
PM2.5(0)的mse=13.846151485684507
PM2.5(0)的mae=2.820472403038183
PM2.5(0)的mar=0.16052016460059532
总共花费的时间为：65.32
黄南藏族自治州
2672A
[flaml.automl: 09-19 00:39:10] {2390} INFO - task = regression
[flaml.automl: 09-19 00:39:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:39:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:39:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:39:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:39:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:39:14] {3025} INFO - Estimated sufficient time budget=47066s. Estimated necessary time budget=47s.
[flaml.automl: 09-19 00:39:14] {3072} INFO -  at 4.8s,	estimator xgboost's best error=13.9970,	best estimator xgboost's best error=13.9970
[flaml.automl: 09-19 00:39:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:39:22] {3072} INFO -  at 12.4s,	estimator xgboost's best error=7.9503,	best estimator xgboost's best error=7.9503
[flaml.automl: 09-19 00:39:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:39:27] {3072} INFO -  at 17.1s,	estimator xgboost's best error=7.9503,	best estimator xgboost's best error=7.9503
[flaml.automl: 09-19 00:39:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:39:44] {3072} INFO -  at 34.3s,	estimator xgboost's best error=7.9503,	best estimator xgboost's best error=7.9503
[flaml.automl: 09-19 00:39:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:39:45] {3072} INFO -  at 35.9s,	estimator xgboost's best error=5.2646,	best estimator xgboost's best error=5.2646
[flaml.automl: 09-19 00:39:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:39:47] {3072} INFO -  at 37.4s,	estimator xgboost's best error=4.7788,	best estimator xgboost's best error=4.7788
[flaml.automl: 09-19 00:39:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:39:48] {3072} INFO -  at 39.0s,	estimator xgboost's best error=4.7788,	best estimator xgboost's best error=4.7788
[flaml.automl: 09-19 00:39:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:39:51] {3072} INFO -  at 41.2s,	estimator xgboost's best error=4.7788,	best estimator xgboost's best error=4.7788
[flaml.automl: 09-19 00:39:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:39:52] {3072} INFO -  at 42.3s,	estimator xgboost's best error=4.6240,	best estimator xgboost's best error=4.6240
[flaml.automl: 09-19 00:39:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:39:53] {3072} INFO -  at 44.0s,	estimator xgboost's best error=4.5343,	best estimator xgboost's best error=4.5343
[flaml.automl: 09-19 00:39:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:39:55] {3072} INFO -  at 45.1s,	estimator xgboost's best error=4.5343,	best estimator xgboost's best error=4.5343
[flaml.automl: 09-19 00:39:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:39:56] {3072} INFO -  at 46.2s,	estimator xgboost's best error=4.5343,	best estimator xgboost's best error=4.5343
[flaml.automl: 09-19 00:39:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:40:02] {3072} INFO -  at 52.3s,	estimator xgboost's best error=4.5343,	best estimator xgboost's best error=4.5343
[flaml.automl: 09-19 00:40:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:40:05] {3072} INFO -  at 55.3s,	estimator xgboost's best error=4.5343,	best estimator xgboost's best error=4.5343
[flaml.automl: 09-19 00:40:05] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 00:40:07] {3072} INFO -  at 57.2s,	estimator xgboost's best error=4.5343,	best estimator xgboost's best error=4.5343
[flaml.automl: 09-19 00:40:07] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 00:40:09] {3072} INFO -  at 59.5s,	estimator xgboost's best error=4.5343,	best estimator xgboost's best error=4.5343
[flaml.automl: 09-19 00:40:11] {3335} INFO - retrain xgboost for 1.7s
[flaml.automl: 09-19 00:40:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5973214930244725, colsample_bynode=1,
             colsample_bytree=0.6218174092693041, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.7539473290018726,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.029059317643545923, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.013168912770005396, reg_lambda=52.75874448611699,
             scale_pos_weight=1, subsample=0.7203332520349655,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:40:11] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:40:11] {2637} INFO - Time taken to find the best model: 44.03962755203247
[flaml.automl: 09-19 00:40:11] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 4, 'max_leaves': 7, 'min_child_weight': 0.029059317643545923, 'learning_rate': 0.7539473290018726, 'subsample': 0.7203332520349655, 'colsample_bylevel': 0.5973214930244725, 'colsample_bytree': 0.6218174092693041, 'reg_alpha': 0.013168912770005396, 'reg_lambda': 52.75874448611699}
PM2.5(0)最佳损失：-3.5342541327585444
PM2.5(0)最好结果：{'pred_time': 3.352378918961188e-05, 'wall_clock_time': 44.03962755203247, 'metric_for_logging': {'pred_time': 3.352378918961188e-05}, 'val_loss': 4.534254132758544, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 7, 'min_child_weight': 0.029059317643545923, 'learning_rate': 0.7539473290018726, 'subsample': 0.7203332520349655, 'colsample_bylevel': 0.5973214930244725, 'colsample_bytree': 0.6218174092693041, 'reg_alpha': 0.013168912770005396, 'reg_lambda': 52.75874448611699}, 'config/n_estimators': 4, 'config/max_leaves': 7, 'config/min_child_weight': 0.029059317643545923, 'config/learning_rate': 0.7539473290018726, 'config/subsample': 0.7203332520349655, 'config/colsample_bylevel': 0.5973214930244725, 'config/colsample_bytree': 0.6218174092693041, 'config/reg_alpha': 0.013168912770005396, 'config/reg_lambda': 52.75874448611699, 'experiment_tag': 'exp', 'time_total_s': 1.7519495487213135}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5973214930244725, colsample_bynode=1,
             colsample_bytree=0.6218174092693041, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.7539473290018726,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.029059317643545923, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.013168912770005396, reg_lambda=52.75874448611699,
             scale_pos_weight=1, subsample=0.7203332520349655,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7217660505069075
PM2.5(0)的mse=40.83057166761928
PM2.5(0)的mae=4.470059806787515
PM2.5(0)的mar=0.3055025833965456
总共花费的时间为：61.52
海南藏族自治州
2673A
[flaml.automl: 09-19 00:43:34] {2390} INFO - task = regression
[flaml.automl: 09-19 00:43:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:43:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:43:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:43:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:43:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:43:37] {3025} INFO - Estimated sufficient time budget=32118s. Estimated necessary time budget=32s.
[flaml.automl: 09-19 00:43:37] {3072} INFO -  at 3.3s,	estimator xgboost's best error=12.1871,	best estimator xgboost's best error=12.1871
[flaml.automl: 09-19 00:43:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:43:42] {3072} INFO -  at 8.3s,	estimator xgboost's best error=6.5428,	best estimator xgboost's best error=6.5428
[flaml.automl: 09-19 00:43:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:43:45] {3072} INFO -  at 11.6s,	estimator xgboost's best error=6.5428,	best estimator xgboost's best error=6.5428
[flaml.automl: 09-19 00:43:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:44:04] {3072} INFO -  at 30.6s,	estimator xgboost's best error=6.5428,	best estimator xgboost's best error=6.5428
[flaml.automl: 09-19 00:44:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:44:07] {3072} INFO -  at 33.7s,	estimator xgboost's best error=3.9218,	best estimator xgboost's best error=3.9218
[flaml.automl: 09-19 00:44:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:44:11] {3072} INFO -  at 37.9s,	estimator xgboost's best error=3.5099,	best estimator xgboost's best error=3.5099
[flaml.automl: 09-19 00:44:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:44:16] {3072} INFO -  at 42.4s,	estimator xgboost's best error=3.5099,	best estimator xgboost's best error=3.5099
[flaml.automl: 09-19 00:44:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:44:22] {3072} INFO -  at 48.3s,	estimator xgboost's best error=3.5099,	best estimator xgboost's best error=3.5099
[flaml.automl: 09-19 00:44:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:44:24] {3072} INFO -  at 50.3s,	estimator xgboost's best error=3.5099,	best estimator xgboost's best error=3.5099
[flaml.automl: 09-19 00:44:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:44:27] {3072} INFO -  at 53.0s,	estimator xgboost's best error=3.5099,	best estimator xgboost's best error=3.5099
[flaml.automl: 09-19 00:44:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:44:28] {3072} INFO -  at 54.1s,	estimator xgboost's best error=3.5099,	best estimator xgboost's best error=3.5099
[flaml.automl: 09-19 00:44:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:44:29] {3072} INFO -  at 55.3s,	estimator xgboost's best error=3.5099,	best estimator xgboost's best error=3.5099
[flaml.automl: 09-19 00:44:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:44:33] {3072} INFO -  at 59.4s,	estimator xgboost's best error=3.3799,	best estimator xgboost's best error=3.3799
[flaml.automl: 09-19 00:44:39] {3335} INFO - retrain xgboost for 5.5s
[flaml.automl: 09-19 00:44:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:44:39] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:44:39] {2637} INFO - Time taken to find the best model: 59.43571996688843
[flaml.automl: 09-19 00:44:39] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
PM2.5(0)最佳损失：-2.3799218884763356
PM2.5(0)最好结果：{'pred_time': 3.138529069016995e-05, 'wall_clock_time': 59.43571996688843, 'metric_for_logging': {'pred_time': 3.138529069016995e-05}, 'val_loss': 3.3799218884763356, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 12, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 4.1802659034729}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.6196012431115095
PM2.5(0)的mse=24.90309518750689
PM2.5(0)的mae=3.4709298017243633
PM2.5(0)的mar=0.21136394824751595
总共花费的时间为：65.15
果洛藏族自治州
2674A
[flaml.automl: 09-19 00:48:08] {2390} INFO - task = regression
[flaml.automl: 09-19 00:48:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:48:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:48:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:48:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:48:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:48:12] {3025} INFO - Estimated sufficient time budget=30917s. Estimated necessary time budget=31s.
[flaml.automl: 09-19 00:48:12] {3072} INFO -  at 3.1s,	estimator xgboost's best error=10.5186,	best estimator xgboost's best error=10.5186
[flaml.automl: 09-19 00:48:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:48:17] {3072} INFO -  at 8.2s,	estimator xgboost's best error=5.6199,	best estimator xgboost's best error=5.6199
[flaml.automl: 09-19 00:48:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:48:20] {3072} INFO -  at 11.4s,	estimator xgboost's best error=5.6199,	best estimator xgboost's best error=5.6199
[flaml.automl: 09-19 00:48:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:48:38] {3072} INFO -  at 30.0s,	estimator xgboost's best error=5.6199,	best estimator xgboost's best error=5.6199
[flaml.automl: 09-19 00:48:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:48:42] {3072} INFO -  at 33.1s,	estimator xgboost's best error=3.5645,	best estimator xgboost's best error=3.5645
[flaml.automl: 09-19 00:48:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:48:46] {3072} INFO -  at 37.3s,	estimator xgboost's best error=3.3528,	best estimator xgboost's best error=3.3528
[flaml.automl: 09-19 00:48:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:48:50] {3072} INFO -  at 41.7s,	estimator xgboost's best error=3.3292,	best estimator xgboost's best error=3.3292
[flaml.automl: 09-19 00:48:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:48:56] {3072} INFO -  at 47.8s,	estimator xgboost's best error=3.3292,	best estimator xgboost's best error=3.3292
[flaml.automl: 09-19 00:48:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:48:58] {3072} INFO -  at 49.9s,	estimator xgboost's best error=3.3292,	best estimator xgboost's best error=3.3292
[flaml.automl: 09-19 00:48:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:49:01] {3072} INFO -  at 52.5s,	estimator xgboost's best error=3.3292,	best estimator xgboost's best error=3.3292
[flaml.automl: 09-19 00:49:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:49:03] {3072} INFO -  at 54.2s,	estimator xgboost's best error=3.2705,	best estimator xgboost's best error=3.2705
[flaml.automl: 09-19 00:49:03] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:49:04] {3072} INFO -  at 55.3s,	estimator xgboost's best error=3.2705,	best estimator xgboost's best error=3.2705
[flaml.automl: 09-19 00:49:04] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:49:08] {3072} INFO -  at 59.6s,	estimator xgboost's best error=3.2705,	best estimator xgboost's best error=3.2705
[flaml.automl: 09-19 00:49:10] {3335} INFO - retrain xgboost for 1.6s
[flaml.automl: 09-19 00:49:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8660549326361876, colsample_bynode=1,
             colsample_bytree=0.9004061282219729, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.590319080116618,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=53.34352431682733, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.778332386065348, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:49:10] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:49:10] {2637} INFO - Time taken to find the best model: 54.15967774391174
[flaml.automl: 09-19 00:49:10] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 53.34352431682733, 'learning_rate': 0.590319080116618, 'subsample': 1.0, 'colsample_bylevel': 0.8660549326361876, 'colsample_bytree': 0.9004061282219729, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.778332386065348}
PM2.5(0)最佳损失：-2.2705086061399276
PM2.5(0)最好结果：{'pred_time': 3.336084215608362e-05, 'wall_clock_time': 54.15967774391174, 'metric_for_logging': {'pred_time': 3.336084215608362e-05}, 'val_loss': 3.2705086061399276, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 53.34352431682733, 'learning_rate': 0.590319080116618, 'subsample': 1.0, 'colsample_bylevel': 0.8660549326361876, 'colsample_bytree': 0.9004061282219729, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.778332386065348}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 53.34352431682733, 'config/learning_rate': 0.590319080116618, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8660549326361876, 'config/colsample_bytree': 0.9004061282219729, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.778332386065348, 'experiment_tag': 'exp', 'time_total_s': 1.6602981090545654}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8660549326361876, colsample_bynode=1,
             colsample_bytree=0.9004061282219729, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.590319080116618,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=53.34352431682733, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.778332386065348, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.4308155511452798
PM2.5(0)的mse=17.820611631421954
PM2.5(0)的mae=3.241228389666461
PM2.5(0)的mar=0.22344556164026044
总共花费的时间为：61.47
玉树藏族自治州
2675A
[flaml.automl: 09-19 00:52:57] {2390} INFO - task = regression
[flaml.automl: 09-19 00:52:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:52:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:52:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:52:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:52:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:53:00] {3025} INFO - Estimated sufficient time budget=31611s. Estimated necessary time budget=32s.
[flaml.automl: 09-19 00:53:00] {3072} INFO -  at 3.3s,	estimator xgboost's best error=4.6233,	best estimator xgboost's best error=4.6233
[flaml.automl: 09-19 00:53:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:53:06] {3072} INFO -  at 8.3s,	estimator xgboost's best error=2.9310,	best estimator xgboost's best error=2.9310
[flaml.automl: 09-19 00:53:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:53:09] {3072} INFO -  at 11.6s,	estimator xgboost's best error=2.9310,	best estimator xgboost's best error=2.9310
[flaml.automl: 09-19 00:53:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:53:28] {3072} INFO -  at 30.6s,	estimator xgboost's best error=2.9310,	best estimator xgboost's best error=2.9310
[flaml.automl: 09-19 00:53:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:53:31] {3072} INFO -  at 33.6s,	estimator xgboost's best error=2.2811,	best estimator xgboost's best error=2.2811
[flaml.automl: 09-19 00:53:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:53:35] {3072} INFO -  at 37.9s,	estimator xgboost's best error=2.2032,	best estimator xgboost's best error=2.2032
[flaml.automl: 09-19 00:53:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:53:39] {3072} INFO -  at 42.3s,	estimator xgboost's best error=2.1971,	best estimator xgboost's best error=2.1971
[flaml.automl: 09-19 00:53:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:53:45] {3072} INFO -  at 47.3s,	estimator xgboost's best error=2.1971,	best estimator xgboost's best error=2.1971
[flaml.automl: 09-19 00:53:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:53:46] {3072} INFO -  at 49.1s,	estimator xgboost's best error=2.1971,	best estimator xgboost's best error=2.1971
[flaml.automl: 09-19 00:53:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:53:49] {3072} INFO -  at 51.7s,	estimator xgboost's best error=2.1971,	best estimator xgboost's best error=2.1971
[flaml.automl: 09-19 00:53:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:53:51] {3072} INFO -  at 53.4s,	estimator xgboost's best error=2.1971,	best estimator xgboost's best error=2.1971
[flaml.automl: 09-19 00:53:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:53:52] {3072} INFO -  at 54.5s,	estimator xgboost's best error=2.1971,	best estimator xgboost's best error=2.1971
[flaml.automl: 09-19 00:53:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:53:57] {3072} INFO -  at 59.6s,	estimator xgboost's best error=2.1853,	best estimator xgboost's best error=2.1853
[flaml.automl: 09-19 00:54:02] {3335} INFO - retrain xgboost for 5.1s
[flaml.automl: 09-19 00:54:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=8, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:54:02] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:54:02] {2637} INFO - Time taken to find the best model: 59.628573417663574
[flaml.automl: 09-19 00:54:02] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 11, 'max_leaves': 8, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386}
PM2.5(0)最佳损失：-1.1853257690953627
PM2.5(0)最好结果：{'pred_time': 3.352156885854014e-05, 'wall_clock_time': 59.628573417663574, 'metric_for_logging': {'pred_time': 3.352156885854014e-05}, 'val_loss': 2.1853257690953627, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 8, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386}, 'config/n_estimators': 11, 'config/max_leaves': 8, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'experiment_tag': 'exp', 'time_total_s': 5.154150009155273}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=8, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.5390053228412985
PM2.5(0)的mse=9.811704878915734
PM2.5(0)的mae=2.126861422878846
PM2.5(0)的mar=0.4374577666343519
总共花费的时间为：64.97
海西蒙古族藏族自治州
2676A
[flaml.automl: 09-19 00:57:19] {2390} INFO - task = regression
[flaml.automl: 09-19 00:57:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:57:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:57:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:57:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:57:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:57:23] {3025} INFO - Estimated sufficient time budget=34779s. Estimated necessary time budget=35s.
[flaml.automl: 09-19 00:57:23] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.3843,	best estimator xgboost's best error=9.3843
[flaml.automl: 09-19 00:57:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:57:28] {3072} INFO -  at 8.9s,	estimator xgboost's best error=5.0958,	best estimator xgboost's best error=5.0958
[flaml.automl: 09-19 00:57:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:57:32] {3072} INFO -  at 12.3s,	estimator xgboost's best error=5.0958,	best estimator xgboost's best error=5.0958
[flaml.automl: 09-19 00:57:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:57:52] {3072} INFO -  at 32.6s,	estimator xgboost's best error=5.0958,	best estimator xgboost's best error=5.0958
[flaml.automl: 09-19 00:57:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:57:55] {3072} INFO -  at 35.9s,	estimator xgboost's best error=3.2965,	best estimator xgboost's best error=3.2965
[flaml.automl: 09-19 00:57:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:58:00] {3072} INFO -  at 40.5s,	estimator xgboost's best error=3.0617,	best estimator xgboost's best error=3.0617
[flaml.automl: 09-19 00:58:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:58:04] {3072} INFO -  at 45.1s,	estimator xgboost's best error=3.0471,	best estimator xgboost's best error=3.0471
[flaml.automl: 09-19 00:58:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:58:11] {3072} INFO -  at 51.8s,	estimator xgboost's best error=3.0471,	best estimator xgboost's best error=3.0471
[flaml.automl: 09-19 00:58:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:58:16] {3072} INFO -  at 56.4s,	estimator xgboost's best error=3.0471,	best estimator xgboost's best error=3.0471
[flaml.automl: 09-19 00:58:20] {3335} INFO - retrain xgboost for 4.4s
[flaml.automl: 09-19 00:58:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:58:20] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:58:20] {2637} INFO - Time taken to find the best model: 45.13139367103577
[flaml.automl: 09-19 00:58:20] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-2.047050760496383
PM2.5(0)最好结果：{'pred_time': 0.00010210895698342548, 'wall_clock_time': 45.13139367103577, 'metric_for_logging': {'pred_time': 0.00010210895698342548}, 'val_loss': 3.047050760496383, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.652060270309448}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.608477607077265
PM2.5(0)的mse=18.864460461375568
PM2.5(0)的mae=3.1623287975247893
PM2.5(0)的mar=0.2406587143843456
总共花费的时间为：61.07
吴忠市
2677A
3648A
[flaml.automl: 09-19 01:06:19] {2390} INFO - task = regression
[flaml.automl: 09-19 01:06:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:06:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:06:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:06:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:06:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:06:23] {3025} INFO - Estimated sufficient time budget=35604s. Estimated necessary time budget=36s.
[flaml.automl: 09-19 01:06:23] {3072} INFO -  at 3.8s,	estimator xgboost's best error=21.5709,	best estimator xgboost's best error=21.5709
[flaml.automl: 09-19 01:06:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:06:29] {3072} INFO -  at 9.9s,	estimator xgboost's best error=10.6926,	best estimator xgboost's best error=10.6926
[flaml.automl: 09-19 01:06:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:06:32] {3072} INFO -  at 13.3s,	estimator xgboost's best error=10.6926,	best estimator xgboost's best error=10.6926
[flaml.automl: 09-19 01:06:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:06:59] {3072} INFO -  at 39.7s,	estimator xgboost's best error=10.6926,	best estimator xgboost's best error=10.6926
[flaml.automl: 09-19 01:06:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:07:03] {3072} INFO -  at 44.1s,	estimator xgboost's best error=7.7167,	best estimator xgboost's best error=7.7167
[flaml.automl: 09-19 01:07:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:07:09] {3072} INFO -  at 50.1s,	estimator xgboost's best error=7.3950,	best estimator xgboost's best error=7.3950
[flaml.automl: 09-19 01:07:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:07:15] {3072} INFO -  at 55.6s,	estimator xgboost's best error=6.5539,	best estimator xgboost's best error=6.5539
[flaml.automl: 09-19 01:07:17] {3335} INFO - retrain xgboost for 2.8s
[flaml.automl: 09-19 01:07:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:07:17] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:07:17] {2637} INFO - Time taken to find the best model: 55.600109815597534
[flaml.automl: 09-19 01:07:17] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-5.553934176297368
PM2.5(0)最好结果：{'pred_time': 3.308411921417364e-05, 'wall_clock_time': 55.600109815597534, 'metric_for_logging': {'pred_time': 3.308411921417364e-05}, 'val_loss': 6.553934176297368, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 5.500433444976807}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8361220187313889
PM2.5(0)的mse=102.78448240729436
PM2.5(0)的mae=6.504396806280297
PM2.5(0)的mar=0.27331549305244934
总共花费的时间为：58.84
中卫市
2680A
3650A
[flaml.automl: 09-19 01:14:54] {2390} INFO - task = regression
[flaml.automl: 09-19 01:14:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:14:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:14:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:14:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:14:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:14:57] {3025} INFO - Estimated sufficient time budget=30017s. Estimated necessary time budget=30s.
[flaml.automl: 09-19 01:14:57] {3072} INFO -  at 3.1s,	estimator xgboost's best error=21.4926,	best estimator xgboost's best error=21.4926
[flaml.automl: 09-19 01:14:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:15:02] {3072} INFO -  at 8.5s,	estimator xgboost's best error=11.9596,	best estimator xgboost's best error=11.9596
[flaml.automl: 09-19 01:15:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:15:06] {3072} INFO -  at 11.9s,	estimator xgboost's best error=11.9596,	best estimator xgboost's best error=11.9596
[flaml.automl: 09-19 01:15:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:15:30] {3072} INFO -  at 35.8s,	estimator xgboost's best error=11.9596,	best estimator xgboost's best error=11.9596
[flaml.automl: 09-19 01:15:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:15:33] {3072} INFO -  at 39.1s,	estimator xgboost's best error=9.0492,	best estimator xgboost's best error=9.0492
[flaml.automl: 09-19 01:15:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:15:37] {3072} INFO -  at 43.6s,	estimator xgboost's best error=9.0492,	best estimator xgboost's best error=9.0492
[flaml.automl: 09-19 01:15:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:15:42] {3072} INFO -  at 48.5s,	estimator xgboost's best error=6.3515,	best estimator xgboost's best error=6.3515
[flaml.automl: 09-19 01:15:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:15:49] {3072} INFO -  at 55.1s,	estimator xgboost's best error=6.3515,	best estimator xgboost's best error=6.3515
[flaml.automl: 09-19 01:15:54] {3335} INFO - retrain xgboost for 4.6s
[flaml.automl: 09-19 01:15:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 01:15:54] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:15:54] {2637} INFO - Time taken to find the best model: 48.48261308670044
[flaml.automl: 09-19 01:15:54] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-5.351520522566866
PM2.5(0)最好结果：{'pred_time': 6.743841372683467e-05, 'wall_clock_time': 48.48261308670044, 'metric_for_logging': {'pred_time': 6.743841372683467e-05}, 'val_loss': 6.351520522566866, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.899601459503174}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7921999174027945
PM2.5(0)的mse=110.92600461139136
PM2.5(0)的mae=6.367961038636767
PM2.5(0)的mar=0.2760132008427229
总共花费的时间为：60.25
固原市
2683A
2684A
2685A
3522A
[flaml.automl: 09-19 01:30:44] {2390} INFO - task = regression
[flaml.automl: 09-19 01:30:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:30:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:30:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:30:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:30:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:30:48] {3025} INFO - Estimated sufficient time budget=142072s. Estimated necessary time budget=142s.
[flaml.automl: 09-19 01:30:48] {3072} INFO -  at 3.6s,	estimator xgboost's best error=15.3270,	best estimator xgboost's best error=15.3270
[flaml.automl: 09-19 01:30:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:30:54] {3072} INFO -  at 9.7s,	estimator xgboost's best error=7.4525,	best estimator xgboost's best error=7.4525
[flaml.automl: 09-19 01:30:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:30:57] {3072} INFO -  at 12.9s,	estimator xgboost's best error=7.4525,	best estimator xgboost's best error=7.4525
[flaml.automl: 09-19 01:30:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:31:02] {3072} INFO -  at 17.8s,	estimator xgboost's best error=7.4525,	best estimator xgboost's best error=7.4525
[flaml.automl: 09-19 01:31:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:31:05] {3072} INFO -  at 21.1s,	estimator xgboost's best error=5.7254,	best estimator xgboost's best error=5.7254
[flaml.automl: 09-19 01:31:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:31:08] {3072} INFO -  at 24.6s,	estimator xgboost's best error=5.7254,	best estimator xgboost's best error=5.7254
[flaml.automl: 09-19 01:31:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:31:12] {3072} INFO -  at 28.1s,	estimator xgboost's best error=5.3008,	best estimator xgboost's best error=5.3008
[flaml.automl: 09-19 01:31:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:31:15] {3072} INFO -  at 31.0s,	estimator xgboost's best error=5.3008,	best estimator xgboost's best error=5.3008
[flaml.automl: 09-19 01:31:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:31:18] {3072} INFO -  at 34.1s,	estimator xgboost's best error=5.3008,	best estimator xgboost's best error=5.3008
[flaml.automl: 09-19 01:31:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:31:21] {3072} INFO -  at 36.8s,	estimator xgboost's best error=5.3008,	best estimator xgboost's best error=5.3008
[flaml.automl: 09-19 01:31:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:31:23] {3072} INFO -  at 39.4s,	estimator xgboost's best error=4.2219,	best estimator xgboost's best error=4.2219
[flaml.automl: 09-19 01:31:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:31:25] {3072} INFO -  at 41.5s,	estimator xgboost's best error=4.2219,	best estimator xgboost's best error=4.2219
[flaml.automl: 09-19 01:31:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:31:27] {3072} INFO -  at 42.8s,	estimator xgboost's best error=4.1825,	best estimator xgboost's best error=4.1825
[flaml.automl: 09-19 01:31:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:31:29] {3072} INFO -  at 44.7s,	estimator xgboost's best error=4.1825,	best estimator xgboost's best error=4.1825
[flaml.automl: 09-19 01:31:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:31:30] {3072} INFO -  at 46.3s,	estimator xgboost's best error=4.1825,	best estimator xgboost's best error=4.1825
[flaml.automl: 09-19 01:31:30] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 01:31:32] {3072} INFO -  at 48.2s,	estimator xgboost's best error=4.1690,	best estimator xgboost's best error=4.1690
[flaml.automl: 09-19 01:31:32] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 01:31:33] {3072} INFO -  at 49.4s,	estimator xgboost's best error=4.1690,	best estimator xgboost's best error=4.1690
[flaml.automl: 09-19 01:31:33] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-19 01:31:34] {3072} INFO -  at 50.5s,	estimator xgboost's best error=4.1690,	best estimator xgboost's best error=4.1690
[flaml.automl: 09-19 01:31:34] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-19 01:31:37] {3072} INFO -  at 52.9s,	estimator xgboost's best error=4.0222,	best estimator xgboost's best error=4.0222
[flaml.automl: 09-19 01:31:37] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-19 01:31:38] {3072} INFO -  at 54.1s,	estimator xgboost's best error=4.0222,	best estimator xgboost's best error=4.0222
[flaml.automl: 09-19 01:31:38] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-19 01:31:42] {3072} INFO -  at 58.0s,	estimator xgboost's best error=3.8142,	best estimator xgboost's best error=3.8142
[flaml.automl: 09-19 01:32:43] {3335} INFO - retrain xgboost for 61.1s
[flaml.automl: 09-19 01:32:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.658072927772472, colsample_bynode=1,
             colsample_bytree=0.7726943154627821, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=69, min_child_weight=6.2767572074692355,
             missing=nan, monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.022643141207931615, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 01:32:43] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:32:43] {2637} INFO - Time taken to find the best model: 58.00463938713074
[flaml.automl: 09-19 01:32:43] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 21, 'max_leaves': 69, 'min_child_weight': 6.2767572074692355, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.658072927772472, 'colsample_bytree': 0.7726943154627821, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.022643141207931615, 'FLAML_sample_size': 10000}
PM2.5(0)最佳损失：-2.814169404748133
PM2.5(0)最好结果：{'pred_time': 9.278726389616114e-06, 'wall_clock_time': 58.00463938713074, 'metric_for_logging': {'pred_time': 9.278726389616114e-06}, 'val_loss': 3.814169404748133, 'training_iteration': 1, 'config': {'n_estimators': 21, 'max_leaves': 69, 'min_child_weight': 6.2767572074692355, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.658072927772472, 'colsample_bytree': 0.7726943154627821, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.022643141207931615, 'FLAML_sample_size': 10000}, 'config/n_estimators': 21, 'config/max_leaves': 69, 'config/min_child_weight': 6.2767572074692355, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.658072927772472, 'config/colsample_bytree': 0.7726943154627821, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.022643141207931615, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.9529411792755127}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.658072927772472, colsample_bynode=1,
             colsample_bytree=0.7726943154627821, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=69, min_child_weight=6.2767572074692355,
             missing=nan, monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.022643141207931615, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8554608289582268
PM2.5(0)的mse=40.25815433534344
PM2.5(0)的mae=3.742973959861622
PM2.5(0)的mar=0.1934801114890796
总共花费的时间为：120.41
吐鲁番地区
2686A
2687A
[flaml.automl: 09-19 01:39:12] {2390} INFO - task = regression
[flaml.automl: 09-19 01:39:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:39:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:39:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:39:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:39:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:39:17] {3025} INFO - Estimated sufficient time budget=48029s. Estimated necessary time budget=48s.
[flaml.automl: 09-19 01:39:17] {3072} INFO -  at 4.9s,	estimator xgboost's best error=29.4155,	best estimator xgboost's best error=29.4155
[flaml.automl: 09-19 01:39:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:39:25] {3072} INFO -  at 13.3s,	estimator xgboost's best error=14.4605,	best estimator xgboost's best error=14.4605
[flaml.automl: 09-19 01:39:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:39:29] {3072} INFO -  at 17.3s,	estimator xgboost's best error=14.4605,	best estimator xgboost's best error=14.4605
[flaml.automl: 09-19 01:39:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:39:42] {3072} INFO -  at 30.2s,	estimator xgboost's best error=14.4605,	best estimator xgboost's best error=14.4605
[flaml.automl: 09-19 01:39:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:39:43] {3072} INFO -  at 31.3s,	estimator xgboost's best error=9.8547,	best estimator xgboost's best error=9.8547
[flaml.automl: 09-19 01:39:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:39:45] {3072} INFO -  at 32.9s,	estimator xgboost's best error=9.8547,	best estimator xgboost's best error=9.8547
[flaml.automl: 09-19 01:39:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:39:46] {3072} INFO -  at 34.5s,	estimator xgboost's best error=7.9216,	best estimator xgboost's best error=7.9216
[flaml.automl: 09-19 01:39:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:39:49] {3072} INFO -  at 37.1s,	estimator xgboost's best error=7.9216,	best estimator xgboost's best error=7.9216
[flaml.automl: 09-19 01:39:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:39:51] {3072} INFO -  at 38.7s,	estimator xgboost's best error=7.9216,	best estimator xgboost's best error=7.9216
[flaml.automl: 09-19 01:39:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:39:54] {3072} INFO -  at 41.7s,	estimator xgboost's best error=7.9216,	best estimator xgboost's best error=7.9216
[flaml.automl: 09-19 01:39:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:39:55] {3072} INFO -  at 43.3s,	estimator xgboost's best error=7.6991,	best estimator xgboost's best error=7.6991
[flaml.automl: 09-19 01:39:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:39:56] {3072} INFO -  at 44.4s,	estimator xgboost's best error=7.6991,	best estimator xgboost's best error=7.6991
[flaml.automl: 09-19 01:39:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:40:03] {3072} INFO -  at 50.8s,	estimator xgboost's best error=7.1270,	best estimator xgboost's best error=7.1270
[flaml.automl: 09-19 01:40:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:40:12] {3072} INFO -  at 59.8s,	estimator xgboost's best error=7.0331,	best estimator xgboost's best error=7.0331
[flaml.automl: 09-19 01:40:23] {3335} INFO - retrain xgboost for 11.1s
[flaml.automl: 09-19 01:40:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:40:23] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:40:23] {2637} INFO - Time taken to find the best model: 59.779178619384766
[flaml.automl: 09-19 01:40:23] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
PM2.5(0)最佳损失：-6.033059040738299
PM2.5(0)最好结果：{'pred_time': 1.6311960896169108e-05, 'wall_clock_time': 59.779178619384766, 'metric_for_logging': {'pred_time': 1.6311960896169108e-05}, 'val_loss': 7.033059040738299, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 8.995548486709595}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8797908515689696
PM2.5(0)的mse=159.08185577170778
PM2.5(0)的mae=7.049063847074754
PM2.5(0)的mar=0.1940723242610673
总共花费的时间为：71.42
哈密地区
2688A
2689A
[flaml.automl: 09-19 01:46:38] {2390} INFO - task = regression
[flaml.automl: 09-19 01:46:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:46:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:46:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:46:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:46:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:46:42] {3025} INFO - Estimated sufficient time budget=41268s. Estimated necessary time budget=41s.
[flaml.automl: 09-19 01:46:42] {3072} INFO -  at 4.2s,	estimator xgboost's best error=15.6749,	best estimator xgboost's best error=15.6749
[flaml.automl: 09-19 01:46:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:46:49] {3072} INFO -  at 11.9s,	estimator xgboost's best error=7.5725,	best estimator xgboost's best error=7.5725
[flaml.automl: 09-19 01:46:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:46:54] {3072} INFO -  at 16.4s,	estimator xgboost's best error=7.5725,	best estimator xgboost's best error=7.5725
[flaml.automl: 09-19 01:46:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:47:19] {3072} INFO -  at 41.1s,	estimator xgboost's best error=7.5725,	best estimator xgboost's best error=7.5725
[flaml.automl: 09-19 01:47:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:47:20] {3072} INFO -  at 42.3s,	estimator xgboost's best error=5.0020,	best estimator xgboost's best error=5.0020
[flaml.automl: 09-19 01:47:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:47:21] {3072} INFO -  at 43.8s,	estimator xgboost's best error=5.0020,	best estimator xgboost's best error=5.0020
[flaml.automl: 09-19 01:47:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:47:23] {3072} INFO -  at 45.5s,	estimator xgboost's best error=4.2280,	best estimator xgboost's best error=4.2280
[flaml.automl: 09-19 01:47:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:47:26] {3072} INFO -  at 48.1s,	estimator xgboost's best error=4.2280,	best estimator xgboost's best error=4.2280
[flaml.automl: 09-19 01:47:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:47:27] {3072} INFO -  at 49.7s,	estimator xgboost's best error=4.2280,	best estimator xgboost's best error=4.2280
[flaml.automl: 09-19 01:47:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:47:30] {3072} INFO -  at 52.7s,	estimator xgboost's best error=4.2280,	best estimator xgboost's best error=4.2280
[flaml.automl: 09-19 01:47:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:47:32] {3072} INFO -  at 54.4s,	estimator xgboost's best error=4.1362,	best estimator xgboost's best error=4.1362
[flaml.automl: 09-19 01:47:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:47:33] {3072} INFO -  at 55.5s,	estimator xgboost's best error=4.1362,	best estimator xgboost's best error=4.1362
[flaml.automl: 09-19 01:47:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:47:37] {3072} INFO -  at 59.8s,	estimator xgboost's best error=3.8575,	best estimator xgboost's best error=3.8575
[flaml.automl: 09-19 01:47:43] {3335} INFO - retrain xgboost for 6.1s
[flaml.automl: 09-19 01:47:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:47:43] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:47:43] {2637} INFO - Time taken to find the best model: 59.80532741546631
[flaml.automl: 09-19 01:47:43] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
PM2.5(0)最佳损失：-2.8575463439538216
PM2.5(0)最好结果：{'pred_time': 1.4889782347282589e-05, 'wall_clock_time': 59.80532741546631, 'metric_for_logging': {'pred_time': 1.4889782347282589e-05}, 'val_loss': 3.8575463439538216, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 4.297910690307617}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8749280130909101
PM2.5(0)的mse=31.42992723518941
PM2.5(0)的mae=3.840521451076194
PM2.5(0)的mar=0.20161910381396436
总共花费的时间为：66.37
昌吉州
2690A
3613A
[flaml.automl: 09-19 01:54:34] {2390} INFO - task = regression
[flaml.automl: 09-19 01:54:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:54:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:54:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:54:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:54:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:54:38] {3025} INFO - Estimated sufficient time budget=33914s. Estimated necessary time budget=34s.
[flaml.automl: 09-19 01:54:38] {3072} INFO -  at 3.7s,	estimator xgboost's best error=18.5762,	best estimator xgboost's best error=18.5762
[flaml.automl: 09-19 01:54:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:54:43] {3072} INFO -  at 9.1s,	estimator xgboost's best error=11.2190,	best estimator xgboost's best error=11.2190
[flaml.automl: 09-19 01:54:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:54:47] {3072} INFO -  at 12.5s,	estimator xgboost's best error=11.2190,	best estimator xgboost's best error=11.2190
[flaml.automl: 09-19 01:54:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:55:11] {3072} INFO -  at 36.3s,	estimator xgboost's best error=11.2190,	best estimator xgboost's best error=11.2190
[flaml.automl: 09-19 01:55:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:55:14] {3072} INFO -  at 39.6s,	estimator xgboost's best error=8.0138,	best estimator xgboost's best error=8.0138
[flaml.automl: 09-19 01:55:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:55:18] {3072} INFO -  at 44.1s,	estimator xgboost's best error=8.0138,	best estimator xgboost's best error=8.0138
[flaml.automl: 09-19 01:55:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:55:21] {3072} INFO -  at 46.6s,	estimator xgboost's best error=5.6322,	best estimator xgboost's best error=5.6322
[flaml.automl: 09-19 01:55:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:55:25] {3072} INFO -  at 50.7s,	estimator xgboost's best error=5.6322,	best estimator xgboost's best error=5.6322
[flaml.automl: 09-19 01:55:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:55:27] {3072} INFO -  at 53.1s,	estimator xgboost's best error=5.6322,	best estimator xgboost's best error=5.6322
[flaml.automl: 09-19 01:55:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:55:32] {3072} INFO -  at 58.0s,	estimator xgboost's best error=5.6322,	best estimator xgboost's best error=5.6322
[flaml.automl: 09-19 01:55:34] {3335} INFO - retrain xgboost for 1.9s
[flaml.automl: 09-19 01:55:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 01:55:34] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:55:34] {2637} INFO - Time taken to find the best model: 46.61499357223511
[flaml.automl: 09-19 01:55:34] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-4.632209415398282
PM2.5(0)最好结果：{'pred_time': 4.0607466458570994e-05, 'wall_clock_time': 46.61499357223511, 'metric_for_logging': {'pred_time': 4.0607466458570994e-05}, 'val_loss': 5.632209415398282, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 2.517098903656006}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9411205141112896
PM2.5(0)的mse=86.24210246965156
PM2.5(0)的mae=5.443623876571655
PM2.5(0)的mar=0.5765907583717927
总共花费的时间为：60.32
博尔塔拉蒙古自治州
2693A
2694A
[flaml.automl: 09-19 02:02:23] {2390} INFO - task = regression
[flaml.automl: 09-19 02:02:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:02:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:02:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:02:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:02:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:02:27] {3025} INFO - Estimated sufficient time budget=33841s. Estimated necessary time budget=34s.
[flaml.automl: 09-19 02:02:27] {3072} INFO -  at 3.5s,	estimator xgboost's best error=13.8081,	best estimator xgboost's best error=13.8081
[flaml.automl: 09-19 02:02:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:02:32] {3072} INFO -  at 9.4s,	estimator xgboost's best error=6.6207,	best estimator xgboost's best error=6.6207
[flaml.automl: 09-19 02:02:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:02:35] {3072} INFO -  at 11.6s,	estimator xgboost's best error=6.6207,	best estimator xgboost's best error=6.6207
[flaml.automl: 09-19 02:02:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:02:50] {3072} INFO -  at 26.5s,	estimator xgboost's best error=6.6207,	best estimator xgboost's best error=6.6207
[flaml.automl: 09-19 02:02:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:02:51] {3072} INFO -  at 28.3s,	estimator xgboost's best error=4.5590,	best estimator xgboost's best error=4.5590
[flaml.automl: 09-19 02:02:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:02:54] {3072} INFO -  at 31.1s,	estimator xgboost's best error=3.9379,	best estimator xgboost's best error=3.9379
[flaml.automl: 09-19 02:02:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:02:57] {3072} INFO -  at 33.9s,	estimator xgboost's best error=3.5611,	best estimator xgboost's best error=3.5611
[flaml.automl: 09-19 02:02:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:03:04] {3072} INFO -  at 40.5s,	estimator xgboost's best error=3.5611,	best estimator xgboost's best error=3.5611
[flaml.automl: 09-19 02:03:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:03:07] {3072} INFO -  at 44.3s,	estimator xgboost's best error=3.5611,	best estimator xgboost's best error=3.5611
[flaml.automl: 09-19 02:03:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:03:14] {3072} INFO -  at 51.0s,	estimator xgboost's best error=3.1566,	best estimator xgboost's best error=3.1566
[flaml.automl: 09-19 02:03:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:03:18] {3072} INFO -  at 54.7s,	estimator xgboost's best error=3.1566,	best estimator xgboost's best error=3.1566
[flaml.automl: 09-19 02:03:23] {3335} INFO - retrain xgboost for 5.3s
[flaml.automl: 09-19 02:03:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:03:23] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:03:23] {2637} INFO - Time taken to find the best model: 50.98605442047119
[flaml.automl: 09-19 02:03:23] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-2.1566261707167222
PM2.5(0)最好结果：{'pred_time': 3.6753723425094025e-05, 'wall_clock_time': 50.98605442047119, 'metric_for_logging': {'pred_time': 3.6753723425094025e-05}, 'val_loss': 3.1566261707167222, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 6.665504693984985}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9277728465250259
PM2.5(0)的mse=18.331472579325283
PM2.5(0)的mae=3.0620671654608342
PM2.5(0)的mar=0.21308151426347374
总共花费的时间为：60.50
阿克苏地区
2695A
2696A
[flaml.automl: 09-19 02:09:51] {2390} INFO - task = regression
[flaml.automl: 09-19 02:09:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:09:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:09:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:09:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:09:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:09:55] {3025} INFO - Estimated sufficient time budget=33587s. Estimated necessary time budget=34s.
[flaml.automl: 09-19 02:09:55] {3072} INFO -  at 3.4s,	estimator xgboost's best error=30.1235,	best estimator xgboost's best error=30.1235
[flaml.automl: 09-19 02:09:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:10:01] {3072} INFO -  at 9.4s,	estimator xgboost's best error=14.2732,	best estimator xgboost's best error=14.2732
[flaml.automl: 09-19 02:10:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:10:04] {3072} INFO -  at 12.3s,	estimator xgboost's best error=14.2732,	best estimator xgboost's best error=14.2732
[flaml.automl: 09-19 02:10:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:10:19] {3072} INFO -  at 28.0s,	estimator xgboost's best error=14.2732,	best estimator xgboost's best error=14.2732
[flaml.automl: 09-19 02:10:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:10:21] {3072} INFO -  at 29.9s,	estimator xgboost's best error=9.7650,	best estimator xgboost's best error=9.7650
[flaml.automl: 09-19 02:10:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:10:24] {3072} INFO -  at 32.6s,	estimator xgboost's best error=9.7650,	best estimator xgboost's best error=9.7650
[flaml.automl: 09-19 02:10:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:10:27] {3072} INFO -  at 35.2s,	estimator xgboost's best error=7.7084,	best estimator xgboost's best error=7.7084
[flaml.automl: 09-19 02:10:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:10:31] {3072} INFO -  at 39.8s,	estimator xgboost's best error=7.7084,	best estimator xgboost's best error=7.7084
[flaml.automl: 09-19 02:10:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:10:34] {3072} INFO -  at 42.7s,	estimator xgboost's best error=7.7084,	best estimator xgboost's best error=7.7084
[flaml.automl: 09-19 02:10:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:10:39] {3072} INFO -  at 48.0s,	estimator xgboost's best error=7.7084,	best estimator xgboost's best error=7.7084
[flaml.automl: 09-19 02:10:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:10:42] {3072} INFO -  at 50.7s,	estimator xgboost's best error=7.6518,	best estimator xgboost's best error=7.6518
[flaml.automl: 09-19 02:10:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:10:44] {3072} INFO -  at 52.6s,	estimator xgboost's best error=7.6518,	best estimator xgboost's best error=7.6518
[flaml.automl: 09-19 02:10:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:10:51] {3072} INFO -  at 59.5s,	estimator xgboost's best error=7.5028,	best estimator xgboost's best error=7.5028
[flaml.automl: 09-19 02:11:02] {3335} INFO - retrain xgboost for 10.9s
[flaml.automl: 09-19 02:11:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:11:02] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:11:02] {2637} INFO - Time taken to find the best model: 59.49235486984253
[flaml.automl: 09-19 02:11:02] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
PM2.5(0)最佳损失：-6.502828424862623
PM2.5(0)最好结果：{'pred_time': 3.220145292875909e-05, 'wall_clock_time': 59.49235486984253, 'metric_for_logging': {'pred_time': 3.220145292875909e-05}, 'val_loss': 7.502828424862623, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 6.87846827507019}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8360427310315126
PM2.5(0)的mse=173.0390535899037
PM2.5(0)的mae=7.362093279889851
PM2.5(0)的mar=0.17915155998930601
总共花费的时间为：70.81
克孜勒苏柯尔克孜自治州
2697A
3665A
[flaml.automl: 09-19 02:17:39] {2390} INFO - task = regression
[flaml.automl: 09-19 02:17:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:17:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:17:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:17:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:17:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:17:40] {3025} INFO - Estimated sufficient time budget=12068s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 02:17:40] {3072} INFO -  at 1.3s,	estimator xgboost's best error=24.7922,	best estimator xgboost's best error=24.7922
[flaml.automl: 09-19 02:17:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:17:43] {3072} INFO -  at 4.2s,	estimator xgboost's best error=13.1007,	best estimator xgboost's best error=13.1007
[flaml.automl: 09-19 02:17:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:17:45] {3072} INFO -  at 6.2s,	estimator xgboost's best error=13.1007,	best estimator xgboost's best error=13.1007
[flaml.automl: 09-19 02:17:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:18:01] {3072} INFO -  at 21.8s,	estimator xgboost's best error=13.1007,	best estimator xgboost's best error=13.1007
[flaml.automl: 09-19 02:18:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:18:02] {3072} INFO -  at 23.7s,	estimator xgboost's best error=11.4619,	best estimator xgboost's best error=11.4619
[flaml.automl: 09-19 02:18:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:18:05] {3072} INFO -  at 26.3s,	estimator xgboost's best error=9.8007,	best estimator xgboost's best error=9.8007
[flaml.automl: 09-19 02:18:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:18:08] {3072} INFO -  at 29.0s,	estimator xgboost's best error=9.3452,	best estimator xgboost's best error=9.3452
[flaml.automl: 09-19 02:18:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:18:12] {3072} INFO -  at 33.7s,	estimator xgboost's best error=9.3452,	best estimator xgboost's best error=9.3452
[flaml.automl: 09-19 02:18:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:18:16] {3072} INFO -  at 36.8s,	estimator xgboost's best error=9.3452,	best estimator xgboost's best error=9.3452
[flaml.automl: 09-19 02:18:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:18:21] {3072} INFO -  at 42.0s,	estimator xgboost's best error=8.0544,	best estimator xgboost's best error=8.0544
[flaml.automl: 09-19 02:18:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:18:23] {3072} INFO -  at 44.6s,	estimator xgboost's best error=8.0544,	best estimator xgboost's best error=8.0544
[flaml.automl: 09-19 02:18:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:18:26] {3072} INFO -  at 46.9s,	estimator xgboost's best error=8.0544,	best estimator xgboost's best error=8.0544
[flaml.automl: 09-19 02:18:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:18:38] {3072} INFO -  at 59.3s,	estimator xgboost's best error=7.8952,	best estimator xgboost's best error=7.8952
[flaml.automl: 09-19 02:18:55] {3335} INFO - retrain xgboost for 16.7s
[flaml.automl: 09-19 02:18:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:18:55] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:18:55] {2637} INFO - Time taken to find the best model: 59.263853549957275
[flaml.automl: 09-19 02:18:55] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-6.895187263963628
PM2.5(0)最好结果：{'pred_time': 3.405161418360793e-05, 'wall_clock_time': 59.263853549957275, 'metric_for_logging': {'pred_time': 3.405161418360793e-05}, 'val_loss': 7.895187263963628, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 12.390870571136475}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8793302913056115
PM2.5(0)的mse=154.3514253371743
PM2.5(0)的mae=7.969174247915548
PM2.5(0)的mar=0.524572541082299
总共花费的时间为：76.35
喀什地区
2698A
2699A
2700A
[flaml.automl: 09-19 02:29:01] {2390} INFO - task = regression
[flaml.automl: 09-19 02:29:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:29:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:29:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:29:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:29:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:29:04] {3025} INFO - Estimated sufficient time budget=23350s. Estimated necessary time budget=23s.
[flaml.automl: 09-19 02:29:04] {3072} INFO -  at 2.5s,	estimator xgboost's best error=46.1144,	best estimator xgboost's best error=46.1144
[flaml.automl: 09-19 02:29:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:29:07] {3072} INFO -  at 5.9s,	estimator xgboost's best error=22.8652,	best estimator xgboost's best error=22.8652
[flaml.automl: 09-19 02:29:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:29:09] {3072} INFO -  at 8.1s,	estimator xgboost's best error=22.8652,	best estimator xgboost's best error=22.8652
[flaml.automl: 09-19 02:29:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:29:27] {3072} INFO -  at 25.5s,	estimator xgboost's best error=22.8652,	best estimator xgboost's best error=22.8652
[flaml.automl: 09-19 02:29:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:29:29] {3072} INFO -  at 27.5s,	estimator xgboost's best error=17.2292,	best estimator xgboost's best error=17.2292
[flaml.automl: 09-19 02:29:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:29:31] {3072} INFO -  at 30.2s,	estimator xgboost's best error=15.6973,	best estimator xgboost's best error=15.6973
[flaml.automl: 09-19 02:29:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:29:34] {3072} INFO -  at 33.3s,	estimator xgboost's best error=14.4937,	best estimator xgboost's best error=14.4937
[flaml.automl: 09-19 02:29:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:29:39] {3072} INFO -  at 38.1s,	estimator xgboost's best error=14.4937,	best estimator xgboost's best error=14.4937
[flaml.automl: 09-19 02:29:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:29:42] {3072} INFO -  at 41.0s,	estimator xgboost's best error=14.0188,	best estimator xgboost's best error=14.0188
[flaml.automl: 09-19 02:29:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:29:47] {3072} INFO -  at 46.1s,	estimator xgboost's best error=13.8537,	best estimator xgboost's best error=13.8537
[flaml.automl: 09-19 02:29:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:29:49] {3072} INFO -  at 48.1s,	estimator xgboost's best error=13.8537,	best estimator xgboost's best error=13.8537
[flaml.automl: 09-19 02:29:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:30:01] {3072} INFO -  at 59.5s,	estimator xgboost's best error=12.3600,	best estimator xgboost's best error=12.3600
[flaml.automl: 09-19 02:30:18] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-19 02:30:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:30:18] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:30:18] {2637} INFO - Time taken to find the best model: 59.53688716888428
[flaml.automl: 09-19 02:30:18] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
PM2.5(0)最佳损失：-11.36000837614054
PM2.5(0)最好结果：{'pred_time': 1.9043734949878138e-05, 'wall_clock_time': 59.53688716888428, 'metric_for_logging': {'pred_time': 1.9043734949878138e-05}, 'val_loss': 12.36000837614054, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 11.458973169326782}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9056543305424457
PM2.5(0)的mse=316.8772238857928
PM2.5(0)的mae=12.265320970492485
PM2.5(0)的mar=0.31530261656997866
总共花费的时间为：77.43
和田地区
3614A
3615A
[flaml.automl: 09-19 02:37:11] {2390} INFO - task = regression
[flaml.automl: 09-19 02:37:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:37:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:37:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:37:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:37:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:37:13] {3025} INFO - Estimated sufficient time budget=20986s. Estimated necessary time budget=21s.
[flaml.automl: 09-19 02:37:13] {3072} INFO -  at 2.2s,	estimator xgboost's best error=54.3409,	best estimator xgboost's best error=54.3409
[flaml.automl: 09-19 02:37:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:37:17] {3072} INFO -  at 5.6s,	estimator xgboost's best error=29.8607,	best estimator xgboost's best error=29.8607
[flaml.automl: 09-19 02:37:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:37:19] {3072} INFO -  at 7.5s,	estimator xgboost's best error=29.8607,	best estimator xgboost's best error=29.8607
[flaml.automl: 09-19 02:37:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:37:33] {3072} INFO -  at 22.0s,	estimator xgboost's best error=29.8607,	best estimator xgboost's best error=29.8607
[flaml.automl: 09-19 02:37:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:37:35] {3072} INFO -  at 23.8s,	estimator xgboost's best error=22.0412,	best estimator xgboost's best error=22.0412
[flaml.automl: 09-19 02:37:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:37:37] {3072} INFO -  at 26.1s,	estimator xgboost's best error=22.0412,	best estimator xgboost's best error=22.0412
[flaml.automl: 09-19 02:37:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:37:40] {3072} INFO -  at 29.0s,	estimator xgboost's best error=14.4678,	best estimator xgboost's best error=14.4678
[flaml.automl: 09-19 02:37:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:37:44] {3072} INFO -  at 33.3s,	estimator xgboost's best error=14.4678,	best estimator xgboost's best error=14.4678
[flaml.automl: 09-19 02:37:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:37:47] {3072} INFO -  at 36.0s,	estimator xgboost's best error=14.4678,	best estimator xgboost's best error=14.4678
[flaml.automl: 09-19 02:37:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:37:52] {3072} INFO -  at 41.3s,	estimator xgboost's best error=14.4678,	best estimator xgboost's best error=14.4678
[flaml.automl: 09-19 02:37:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:37:55] {3072} INFO -  at 44.1s,	estimator xgboost's best error=14.4678,	best estimator xgboost's best error=14.4678
[flaml.automl: 09-19 02:37:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:37:57] {3072} INFO -  at 45.8s,	estimator xgboost's best error=14.4678,	best estimator xgboost's best error=14.4678
[flaml.automl: 09-19 02:37:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:38:08] {3072} INFO -  at 57.2s,	estimator xgboost's best error=13.1708,	best estimator xgboost's best error=13.1708
[flaml.automl: 09-19 02:38:23] {3335} INFO - retrain xgboost for 14.6s
[flaml.automl: 09-19 02:38:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:38:23] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:38:23] {2637} INFO - Time taken to find the best model: 57.157947063446045
[flaml.automl: 09-19 02:38:23] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
PM2.5(0)最佳损失：-12.170769490716664
PM2.5(0)最好结果：{'pred_time': 5.1089182291945365e-05, 'wall_clock_time': 57.157947063446045, 'metric_for_logging': {'pred_time': 5.1089182291945365e-05}, 'val_loss': 13.170769490716664, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 11.334941625595093}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8937024178831715
PM2.5(0)的mse=561.2032521926023
PM2.5(0)的mae=13.191156526088127
PM2.5(0)的mar=0.20890843659965547
总共花费的时间为：72.28
伊犁哈萨克州
2703A
2704A
2705A
[flaml.automl: 09-19 02:48:22] {2390} INFO - task = regression
[flaml.automl: 09-19 02:48:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:48:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:48:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:48:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:48:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:48:24] {3025} INFO - Estimated sufficient time budget=21128s. Estimated necessary time budget=21s.
[flaml.automl: 09-19 02:48:24] {3072} INFO -  at 2.3s,	estimator xgboost's best error=19.0522,	best estimator xgboost's best error=19.0522
[flaml.automl: 09-19 02:48:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:48:27] {3072} INFO -  at 5.8s,	estimator xgboost's best error=10.0279,	best estimator xgboost's best error=10.0279
[flaml.automl: 09-19 02:48:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:48:29] {3072} INFO -  at 8.0s,	estimator xgboost's best error=10.0279,	best estimator xgboost's best error=10.0279
[flaml.automl: 09-19 02:48:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:48:47] {3072} INFO -  at 25.8s,	estimator xgboost's best error=10.0279,	best estimator xgboost's best error=10.0279
[flaml.automl: 09-19 02:48:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:48:49] {3072} INFO -  at 27.7s,	estimator xgboost's best error=8.2165,	best estimator xgboost's best error=8.2165
[flaml.automl: 09-19 02:48:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:48:52] {3072} INFO -  at 30.2s,	estimator xgboost's best error=7.5378,	best estimator xgboost's best error=7.5378
[flaml.automl: 09-19 02:48:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:48:54] {3072} INFO -  at 32.9s,	estimator xgboost's best error=7.2150,	best estimator xgboost's best error=7.2150
[flaml.automl: 09-19 02:48:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:48:59] {3072} INFO -  at 37.3s,	estimator xgboost's best error=7.2150,	best estimator xgboost's best error=7.2150
[flaml.automl: 09-19 02:48:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:49:02] {3072} INFO -  at 40.2s,	estimator xgboost's best error=7.2150,	best estimator xgboost's best error=7.2150
[flaml.automl: 09-19 02:49:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:49:07] {3072} INFO -  at 45.9s,	estimator xgboost's best error=6.7623,	best estimator xgboost's best error=6.7623
[flaml.automl: 09-19 02:49:07] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:49:10] {3072} INFO -  at 48.9s,	estimator xgboost's best error=6.7623,	best estimator xgboost's best error=6.7623
[flaml.automl: 09-19 02:49:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:49:13] {3072} INFO -  at 51.1s,	estimator xgboost's best error=6.7623,	best estimator xgboost's best error=6.7623
[flaml.automl: 09-19 02:49:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:49:20] {3072} INFO -  at 58.1s,	estimator xgboost's best error=6.5163,	best estimator xgboost's best error=6.5163
[flaml.automl: 09-19 02:49:33] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-19 02:49:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:49:33] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:49:33] {2637} INFO - Time taken to find the best model: 58.08022689819336
[flaml.automl: 09-19 02:49:33] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
PM2.5(0)最佳损失：-5.51631844675439
PM2.5(0)最好结果：{'pred_time': 1.1358369789232217e-05, 'wall_clock_time': 58.08022689819336, 'metric_for_logging': {'pred_time': 1.1358369789232217e-05}, 'val_loss': 6.51631844675439, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 7.002549171447754}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.8465948624038881
PM2.5(0)的mse=97.90901221574777
PM2.5(0)的mae=6.78679810826595
PM2.5(0)的mar=0.3813541733729205
总共花费的时间为：72.29
塔城地区
2706A
[flaml.automl: 09-19 02:52:33] {2390} INFO - task = regression
[flaml.automl: 09-19 02:52:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:52:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:52:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:52:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:52:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:52:35] {3025} INFO - Estimated sufficient time budget=28102s. Estimated necessary time budget=28s.
[flaml.automl: 09-19 02:52:35] {3072} INFO -  at 2.9s,	estimator xgboost's best error=7.6250,	best estimator xgboost's best error=7.6250
[flaml.automl: 09-19 02:52:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:52:40] {3072} INFO -  at 7.1s,	estimator xgboost's best error=4.4501,	best estimator xgboost's best error=4.4501
[flaml.automl: 09-19 02:52:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:52:42] {3072} INFO -  at 9.8s,	estimator xgboost's best error=4.4501,	best estimator xgboost's best error=4.4501
[flaml.automl: 09-19 02:52:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:52:58] {3072} INFO -  at 25.1s,	estimator xgboost's best error=4.4501,	best estimator xgboost's best error=4.4501
[flaml.automl: 09-19 02:52:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:52:59] {3072} INFO -  at 27.0s,	estimator xgboost's best error=3.5830,	best estimator xgboost's best error=3.5830
[flaml.automl: 09-19 02:52:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:53:02] {3072} INFO -  at 29.8s,	estimator xgboost's best error=3.5830,	best estimator xgboost's best error=3.5830
[flaml.automl: 09-19 02:53:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:53:05] {3072} INFO -  at 32.8s,	estimator xgboost's best error=3.4988,	best estimator xgboost's best error=3.4988
[flaml.automl: 09-19 02:53:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:53:09] {3072} INFO -  at 36.8s,	estimator xgboost's best error=3.4988,	best estimator xgboost's best error=3.4988
[flaml.automl: 09-19 02:53:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:53:12] {3072} INFO -  at 39.4s,	estimator xgboost's best error=3.4988,	best estimator xgboost's best error=3.4988
[flaml.automl: 09-19 02:53:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:53:16] {3072} INFO -  at 43.6s,	estimator xgboost's best error=3.4988,	best estimator xgboost's best error=3.4988
[flaml.automl: 09-19 02:53:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:53:19] {3072} INFO -  at 46.6s,	estimator xgboost's best error=3.4988,	best estimator xgboost's best error=3.4988
[flaml.automl: 09-19 02:53:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:53:21] {3072} INFO -  at 48.3s,	estimator xgboost's best error=3.4988,	best estimator xgboost's best error=3.4988
[flaml.automl: 09-19 02:53:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:53:31] {3072} INFO -  at 58.2s,	estimator xgboost's best error=3.4988,	best estimator xgboost's best error=3.4988
[flaml.automl: 09-19 02:53:33] {3335} INFO - retrain xgboost for 2.7s
[flaml.automl: 09-19 02:53:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:53:33] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:53:33] {2637} INFO - Time taken to find the best model: 32.8310170173645
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-2.4987608187225105
PM2.5(0)最好结果：{'pred_time': 5.193839818366464e-05, 'wall_clock_time': 32.8310170173645, 'metric_for_logging': {'pred_time': 5.193839818366464e-05}, 'val_loss': 3.4987608187225105, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 3.0594868659973145}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=-0.17925930604935014
PM2.5(0)的mse=19.650597563060916
PM2.5(0)的mae=3.382635460002527
PM2.5(0)的mar=0.3304737044948307
总共花费的时间为：61.22
阿勒泰地区
阿勒泰地区没有数据
石河子市
2709A
2710A
3442A
[flaml.automl: 09-19 03:03:17] {2390} INFO - task = regression
[flaml.automl: 09-19 03:03:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:03:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:03:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:03:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:03:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:03:21] {3025} INFO - Estimated sufficient time budget=32822s. Estimated necessary time budget=33s.
[flaml.automl: 09-19 03:03:21] {3072} INFO -  at 3.4s,	estimator xgboost's best error=31.5458,	best estimator xgboost's best error=31.5458
[flaml.automl: 09-19 03:03:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:03:26] {3072} INFO -  at 9.1s,	estimator xgboost's best error=15.4824,	best estimator xgboost's best error=15.4824
[flaml.automl: 09-19 03:03:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:03:30] {3072} INFO -  at 12.5s,	estimator xgboost's best error=15.4824,	best estimator xgboost's best error=15.4824
[flaml.automl: 09-19 03:03:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:03:40] {3072} INFO -  at 22.5s,	estimator xgboost's best error=15.4824,	best estimator xgboost's best error=15.4824
[flaml.automl: 09-19 03:03:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:03:41] {3072} INFO -  at 23.6s,	estimator xgboost's best error=11.5898,	best estimator xgboost's best error=11.5898
[flaml.automl: 09-19 03:03:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:03:43] {3072} INFO -  at 25.2s,	estimator xgboost's best error=11.3525,	best estimator xgboost's best error=11.3525
[flaml.automl: 09-19 03:03:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:03:44] {3072} INFO -  at 26.8s,	estimator xgboost's best error=9.2184,	best estimator xgboost's best error=9.2184
[flaml.automl: 09-19 03:03:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:03:47] {3072} INFO -  at 29.4s,	estimator xgboost's best error=9.2184,	best estimator xgboost's best error=9.2184
[flaml.automl: 09-19 03:03:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:03:48] {3072} INFO -  at 31.0s,	estimator xgboost's best error=8.1787,	best estimator xgboost's best error=8.1787
[flaml.automl: 09-19 03:03:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:03:51] {3072} INFO -  at 34.0s,	estimator xgboost's best error=7.7385,	best estimator xgboost's best error=7.7385
[flaml.automl: 09-19 03:03:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:03:52] {3072} INFO -  at 35.1s,	estimator xgboost's best error=7.7385,	best estimator xgboost's best error=7.7385
[flaml.automl: 09-19 03:03:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:04:10] {3072} INFO -  at 53.0s,	estimator xgboost's best error=6.2294,	best estimator xgboost's best error=6.2294
[flaml.automl: 09-19 03:04:33] {3335} INFO - retrain xgboost for 22.8s
[flaml.automl: 09-19 03:04:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:04:33] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:04:33] {2637} INFO - Time taken to find the best model: 52.973639726638794
[flaml.automl: 09-19 03:04:33] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
PM2.5(0)最佳损失：-5.229394802474724
PM2.5(0)最好结果：{'pred_time': 2.568401887737667e-05, 'wall_clock_time': 52.973639726638794, 'metric_for_logging': {'pred_time': 2.568401887737667e-05}, 'val_loss': 6.229394802474724, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 17.860220909118652}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.9662536638353236
PM2.5(0)的mse=115.11155204432453
PM2.5(0)的mae=6.5027186807572415
PM2.5(0)的mar=0.30551144311051587
总共花费的时间为：76.66
五家渠市
2711A
3441A
[flaml.automl: 09-19 03:11:01] {2390} INFO - task = regression
[flaml.automl: 09-19 03:11:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:11:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:11:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:11:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:11:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:11:03] {3025} INFO - Estimated sufficient time budget=23167s. Estimated necessary time budget=23s.
[flaml.automl: 09-19 03:11:03] {3072} INFO -  at 2.5s,	estimator xgboost's best error=31.4969,	best estimator xgboost's best error=31.4969
[flaml.automl: 09-19 03:11:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:11:07] {3072} INFO -  at 6.1s,	estimator xgboost's best error=17.2993,	best estimator xgboost's best error=17.2993
[flaml.automl: 09-19 03:11:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:11:09] {3072} INFO -  at 8.4s,	estimator xgboost's best error=17.2993,	best estimator xgboost's best error=17.2993
[flaml.automl: 09-19 03:11:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:11:31] {3072} INFO -  at 30.5s,	estimator xgboost's best error=17.2993,	best estimator xgboost's best error=17.2993
[flaml.automl: 09-19 03:11:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:11:34] {3072} INFO -  at 33.8s,	estimator xgboost's best error=10.8487,	best estimator xgboost's best error=10.8487
[flaml.automl: 09-19 03:11:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:11:39] {3072} INFO -  at 38.4s,	estimator xgboost's best error=10.8487,	best estimator xgboost's best error=10.8487
[flaml.automl: 09-19 03:11:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:11:44] {3072} INFO -  at 43.8s,	estimator xgboost's best error=6.8352,	best estimator xgboost's best error=6.8352
[flaml.automl: 09-19 03:11:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:11:52] {3072} INFO -  at 51.3s,	estimator xgboost's best error=6.8352,	best estimator xgboost's best error=6.8352
[flaml.automl: 09-19 03:11:52] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:11:57] {3072} INFO -  at 56.3s,	estimator xgboost's best error=6.8352,	best estimator xgboost's best error=6.8352
[flaml.automl: 09-19 03:12:02] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-19 03:12:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 03:12:02] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:12:02] {2637} INFO - Time taken to find the best model: 43.826682806015015
[flaml.automl: 09-19 03:12:02] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
PM2.5(0)最佳损失：-5.835228316881744
PM2.5(0)最好结果：{'pred_time': 6.074565533170432e-05, 'wall_clock_time': 43.826682806015015, 'metric_for_logging': {'pred_time': 6.074565533170432e-05}, 'val_loss': 6.835228316881744, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 5.4140214920043945}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.9586223563573174
PM2.5(0)的mse=125.08527926248581
PM2.5(0)的mae=6.639434134793318
PM2.5(0)的mar=0.24360570448960508
总共花费的时间为：62.01
三沙市
三沙市没有数据
兰州新区
3245A
3246A
[flaml.automl: 09-19 03:19:23] {2390} INFO - task = regression
[flaml.automl: 09-19 03:19:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:19:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:19:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:19:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:19:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:19:25] {3025} INFO - Estimated sufficient time budget=21637s. Estimated necessary time budget=22s.
[flaml.automl: 09-19 03:19:25] {3072} INFO -  at 2.3s,	estimator xgboost's best error=17.7406,	best estimator xgboost's best error=17.7406
[flaml.automl: 09-19 03:19:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:19:29] {3072} INFO -  at 6.2s,	estimator xgboost's best error=8.7165,	best estimator xgboost's best error=8.7165
[flaml.automl: 09-19 03:19:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:19:31] {3072} INFO -  at 8.4s,	estimator xgboost's best error=8.7165,	best estimator xgboost's best error=8.7165
[flaml.automl: 09-19 03:19:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:19:53] {3072} INFO -  at 30.7s,	estimator xgboost's best error=8.7165,	best estimator xgboost's best error=8.7165
[flaml.automl: 09-19 03:19:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:19:57] {3072} INFO -  at 33.9s,	estimator xgboost's best error=6.6822,	best estimator xgboost's best error=6.6822
[flaml.automl: 09-19 03:19:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:20:01] {3072} INFO -  at 38.0s,	estimator xgboost's best error=6.1349,	best estimator xgboost's best error=6.1349
[flaml.automl: 09-19 03:20:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:20:05] {3072} INFO -  at 42.5s,	estimator xgboost's best error=5.7511,	best estimator xgboost's best error=5.7511
[flaml.automl: 09-19 03:20:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:20:12] {3072} INFO -  at 49.3s,	estimator xgboost's best error=5.7511,	best estimator xgboost's best error=5.7511
[flaml.automl: 09-19 03:20:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:20:16] {3072} INFO -  at 53.6s,	estimator xgboost's best error=5.7511,	best estimator xgboost's best error=5.7511
[flaml.automl: 09-19 03:20:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:20:22] {3072} INFO -  at 59.3s,	estimator xgboost's best error=5.7511,	best estimator xgboost's best error=5.7511
[flaml.automl: 09-19 03:20:24] {3335} INFO - retrain xgboost for 2.4s
[flaml.automl: 09-19 03:20:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:20:24] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:20:24] {2637} INFO - Time taken to find the best model: 42.465352058410645
[flaml.automl: 09-19 03:20:24] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
PM2.5(0)最佳损失：-4.751111881299452
PM2.5(0)最好结果：{'pred_time': 5.26503502548515e-05, 'wall_clock_time': 42.465352058410645, 'metric_for_logging': {'pred_time': 5.26503502548515e-05}, 'val_loss': 5.751111881299452, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.441821336746216}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.7239624781801228
PM2.5(0)的mse=75.45646566437371
PM2.5(0)的mae=5.826214686896002
PM2.5(0)的mar=0.3427700249738267
总共花费的时间为：62.14
赣江新区
3414A
[flaml.automl: 09-19 03:23:52] {2390} INFO - task = regression
[flaml.automl: 09-19 03:23:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:23:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:23:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:23:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:23:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:23:54] {3025} INFO - Estimated sufficient time budget=21229s. Estimated necessary time budget=21s.
[flaml.automl: 09-19 03:23:54] {3072} INFO -  at 2.2s,	estimator xgboost's best error=19.9653,	best estimator xgboost's best error=19.9653
[flaml.automl: 09-19 03:23:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:23:58] {3072} INFO -  at 5.6s,	estimator xgboost's best error=11.0087,	best estimator xgboost's best error=11.0087
[flaml.automl: 09-19 03:23:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:24:00] {3072} INFO -  at 7.8s,	estimator xgboost's best error=11.0087,	best estimator xgboost's best error=11.0087
[flaml.automl: 09-19 03:24:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:24:19] {3072} INFO -  at 26.8s,	estimator xgboost's best error=11.0087,	best estimator xgboost's best error=11.0087
[flaml.automl: 09-19 03:24:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:24:22] {3072} INFO -  at 29.9s,	estimator xgboost's best error=6.3798,	best estimator xgboost's best error=6.3798
[flaml.automl: 09-19 03:24:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:24:26] {3072} INFO -  at 34.2s,	estimator xgboost's best error=5.5513,	best estimator xgboost's best error=5.5513
[flaml.automl: 09-19 03:24:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:24:31] {3072} INFO -  at 38.6s,	estimator xgboost's best error=5.5513,	best estimator xgboost's best error=5.5513
[flaml.automl: 09-19 03:24:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:24:37] {3072} INFO -  at 44.7s,	estimator xgboost's best error=5.5513,	best estimator xgboost's best error=5.5513
[flaml.automl: 09-19 03:24:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:24:40] {3072} INFO -  at 47.8s,	estimator xgboost's best error=5.4988,	best estimator xgboost's best error=5.4988
[flaml.automl: 09-19 03:24:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:24:45] {3072} INFO -  at 52.4s,	estimator xgboost's best error=5.4988,	best estimator xgboost's best error=5.4988
[flaml.automl: 09-19 03:24:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:24:48] {3072} INFO -  at 55.5s,	estimator xgboost's best error=5.4988,	best estimator xgboost's best error=5.4988
[flaml.automl: 09-19 03:24:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:24:51] {3072} INFO -  at 58.5s,	estimator xgboost's best error=5.4988,	best estimator xgboost's best error=5.4988
[flaml.automl: 09-19 03:24:54] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-19 03:24:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6653719127866307, colsample_bynode=1,
             colsample_bytree=0.5872814598333542, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=4, min_child_weight=0.6008315285021533,
             missing=nan, monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.008482962580746356, reg_lambda=83.39200099767129,
             scale_pos_weight=1, subsample=0.8098031956057953,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 03:24:54] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:24:54] {2637} INFO - Time taken to find the best model: 47.8201801776886
[flaml.automl: 09-19 03:24:54] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 0.6008315285021533, 'learning_rate': 1.0, 'subsample': 0.8098031956057953, 'colsample_bylevel': 0.6653719127866307, 'colsample_bytree': 0.5872814598333542, 'reg_alpha': 0.008482962580746356, 'reg_lambda': 83.39200099767129}
PM2.5(0)最佳损失：-4.498803174200137
PM2.5(0)最好结果：{'pred_time': 9.381120855158026e-05, 'wall_clock_time': 47.8201801776886, 'metric_for_logging': {'pred_time': 9.381120855158026e-05}, 'val_loss': 5.498803174200137, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 0.6008315285021533, 'learning_rate': 1.0, 'subsample': 0.8098031956057953, 'colsample_bylevel': 0.6653719127866307, 'colsample_bytree': 0.5872814598333542, 'reg_alpha': 0.008482962580746356, 'reg_lambda': 83.39200099767129}, 'config/n_estimators': 4, 'config/max_leaves': 4, 'config/min_child_weight': 0.6008315285021533, 'config/learning_rate': 1.0, 'config/subsample': 0.8098031956057953, 'config/colsample_bylevel': 0.6653719127866307, 'config/colsample_bytree': 0.5872814598333542, 'config/reg_alpha': 0.008482962580746356, 'config/reg_lambda': 83.39200099767129, 'experiment_tag': 'exp', 'time_total_s': 3.0864484310150146}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6653719127866307, colsample_bynode=1,
             colsample_bytree=0.5872814598333542, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=4, min_child_weight=0.6008315285021533,
             missing=nan, monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.008482962580746356, reg_lambda=83.39200099767129,
             scale_pos_weight=1, subsample=0.8098031956057953,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.7147846319169836
PM2.5(0)的mse=58.058160983825594
PM2.5(0)的mae=5.93873018735931
PM2.5(0)的mar=0.29147383710229896
总共花费的时间为：61.88
儋州市
3541A
3542A
[flaml.automl: 09-19 03:31:26] {2390} INFO - task = regression
[flaml.automl: 09-19 03:31:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:31:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:31:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:31:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:31:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:31:27] {3025} INFO - Estimated sufficient time budget=12084s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 03:31:27] {3072} INFO -  at 1.3s,	estimator xgboost's best error=8.0115,	best estimator xgboost's best error=8.0115
[flaml.automl: 09-19 03:31:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:31:29] {3072} INFO -  at 3.4s,	estimator xgboost's best error=3.7950,	best estimator xgboost's best error=3.7950
[flaml.automl: 09-19 03:31:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:31:30] {3072} INFO -  at 4.6s,	estimator xgboost's best error=3.7950,	best estimator xgboost's best error=3.7950
[flaml.automl: 09-19 03:31:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:31:47] {3072} INFO -  at 21.3s,	estimator xgboost's best error=3.7950,	best estimator xgboost's best error=3.7950
[flaml.automl: 09-19 03:31:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:31:50] {3072} INFO -  at 24.5s,	estimator xgboost's best error=2.7007,	best estimator xgboost's best error=2.7007
[flaml.automl: 09-19 03:31:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:31:55] {3072} INFO -  at 29.0s,	estimator xgboost's best error=2.3797,	best estimator xgboost's best error=2.3797
[flaml.automl: 09-19 03:31:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:31:59] {3072} INFO -  at 33.6s,	estimator xgboost's best error=2.1581,	best estimator xgboost's best error=2.1581
[flaml.automl: 09-19 03:31:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:32:07] {3072} INFO -  at 41.2s,	estimator xgboost's best error=2.1581,	best estimator xgboost's best error=2.1581
[flaml.automl: 09-19 03:32:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:32:12] {3072} INFO -  at 45.9s,	estimator xgboost's best error=2.1005,	best estimator xgboost's best error=2.1005
[flaml.automl: 09-19 03:32:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:32:20] {3072} INFO -  at 54.4s,	estimator xgboost's best error=2.1005,	best estimator xgboost's best error=2.1005
[flaml.automl: 09-19 03:32:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:32:25] {3072} INFO -  at 59.0s,	estimator xgboost's best error=2.1005,	best estimator xgboost's best error=2.1005
[flaml.automl: 09-19 03:32:29] {3335} INFO - retrain xgboost for 4.4s
[flaml.automl: 09-19 03:32:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:32:29] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:32:29] {2637} INFO - Time taken to find the best model: 45.9139666557312
[flaml.automl: 09-19 03:32:29] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}
PM2.5(0)最佳损失：-1.1004521552486874
PM2.5(0)最好结果：{'pred_time': 5.485576350213255e-05, 'wall_clock_time': 45.9139666557312, 'metric_for_logging': {'pred_time': 5.485576350213255e-05}, 'val_loss': 2.1004521552486874, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 1.1765273389893034, 'config/learning_rate': 0.8338182546732649, 'config/subsample': 0.9114052575858262, 'config/colsample_bylevel': 0.7232930776213671, 'config/colsample_bytree': 0.8046603913292237, 'config/reg_alpha': 0.00473280911734172, 'config/reg_lambda': 15.090081382165623, 'experiment_tag': 'exp', 'time_total_s': 4.705065965652466}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8627763359449855
PM2.5(0)的mse=8.063204090932981
PM2.5(0)的mae=2.0447074846708926
PM2.5(0)的mar=0.2038027394169182
总共花费的时间为：63.89
雄安新区
3584A
3585A
3586A
3587A
3588A
3589A
[flaml.automl: 09-19 03:54:09] {2390} INFO - task = regression
[flaml.automl: 09-19 03:54:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:54:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:54:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:54:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:54:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:54:12] {3025} INFO - Estimated sufficient time budget=221076s. Estimated necessary time budget=221s.
[flaml.automl: 09-19 03:54:12] {3072} INFO -  at 3.9s,	estimator xgboost's best error=25.5955,	best estimator xgboost's best error=25.5955
[flaml.automl: 09-19 03:54:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:54:16] {3072} INFO -  at 7.7s,	estimator xgboost's best error=18.7801,	best estimator xgboost's best error=18.7801
[flaml.automl: 09-19 03:54:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:54:20] {3072} INFO -  at 11.1s,	estimator xgboost's best error=18.7801,	best estimator xgboost's best error=18.7801
[flaml.automl: 09-19 03:54:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:54:23] {3072} INFO -  at 14.5s,	estimator xgboost's best error=18.7801,	best estimator xgboost's best error=18.7801
[flaml.automl: 09-19 03:54:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:54:26] {3072} INFO -  at 17.6s,	estimator xgboost's best error=9.7911,	best estimator xgboost's best error=9.7911
[flaml.automl: 09-19 03:54:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:54:29] {3072} INFO -  at 20.1s,	estimator xgboost's best error=9.7911,	best estimator xgboost's best error=9.7911
[flaml.automl: 09-19 03:54:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:54:31] {3072} INFO -  at 22.7s,	estimator xgboost's best error=9.7911,	best estimator xgboost's best error=9.7911
[flaml.automl: 09-19 03:54:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:54:33] {3072} INFO -  at 24.4s,	estimator xgboost's best error=9.7911,	best estimator xgboost's best error=9.7911
[flaml.automl: 09-19 03:54:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:54:35] {3072} INFO -  at 26.2s,	estimator xgboost's best error=9.7911,	best estimator xgboost's best error=9.7911
[flaml.automl: 09-19 03:54:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:54:38] {3072} INFO -  at 29.5s,	estimator xgboost's best error=9.7242,	best estimator xgboost's best error=9.7242
[flaml.automl: 09-19 03:54:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:54:44] {3072} INFO -  at 35.1s,	estimator xgboost's best error=9.7242,	best estimator xgboost's best error=9.7242
[flaml.automl: 09-19 03:54:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:54:48] {3072} INFO -  at 39.5s,	estimator xgboost's best error=9.0748,	best estimator xgboost's best error=9.0748
[flaml.automl: 09-19 03:54:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:54:52] {3072} INFO -  at 43.5s,	estimator xgboost's best error=9.0748,	best estimator xgboost's best error=9.0748
[flaml.automl: 09-19 03:54:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:55:07] {3072} INFO -  at 58.7s,	estimator xgboost's best error=6.8283,	best estimator xgboost's best error=6.8283
[flaml.automl: 09-19 03:55:15] {3335} INFO - retrain xgboost for 7.9s
[flaml.automl: 09-19 03:55:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253439, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699032,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:55:15] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:55:15] {2637} INFO - Time taken to find the best model: 58.703717947006226
[flaml.automl: 09-19 03:55:15] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253439, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699032, 'FLAML_sample_size': 64388}
PM2.5(0)最佳损失：-5.82828544111705
PM2.5(0)最好结果：{'pred_time': 1.2053716940783187e-05, 'wall_clock_time': 58.703717947006226, 'metric_for_logging': {'pred_time': 1.2053716940783187e-05}, 'val_loss': 6.82828544111705, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253439, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699032, 'FLAML_sample_size': 64388}, 'config/n_estimators': 8, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253439, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699032, 'config/FLAML_sample_size': 64388, 'experiment_tag': 'exp', 'time_total_s': 15.165580987930298}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253439, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699032,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
PM2.5(0)的R2=0.8978216757413096
PM2.5(0)的mse=88.17472585316241
PM2.5(0)的mae=6.8905277079756155
PM2.5(0)的mar=0.33375554462875623
总共花费的时间为：67.83
西咸新区
3606A
3607A
3608A
[flaml.automl: 09-19 04:06:08] {2390} INFO - task = regression
[flaml.automl: 09-19 04:06:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 04:06:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 04:06:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 04:06:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 04:06:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 04:06:11] {3025} INFO - Estimated sufficient time budget=33019s. Estimated necessary time budget=33s.
[flaml.automl: 09-19 04:06:11] {3072} INFO -  at 3.5s,	estimator xgboost's best error=27.5835,	best estimator xgboost's best error=27.5835
[flaml.automl: 09-19 04:06:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 04:06:17] {3072} INFO -  at 9.6s,	estimator xgboost's best error=12.9632,	best estimator xgboost's best error=12.9632
[flaml.automl: 09-19 04:06:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 04:06:21] {3072} INFO -  at 13.0s,	estimator xgboost's best error=12.9632,	best estimator xgboost's best error=12.9632
[flaml.automl: 09-19 04:06:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 04:06:49] {3072} INFO -  at 41.3s,	estimator xgboost's best error=12.9632,	best estimator xgboost's best error=12.9632
[flaml.automl: 09-19 04:06:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 04:06:52] {3072} INFO -  at 44.6s,	estimator xgboost's best error=9.1312,	best estimator xgboost's best error=9.1312
[flaml.automl: 09-19 04:06:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:06:57] {3072} INFO -  at 49.0s,	estimator xgboost's best error=7.8805,	best estimator xgboost's best error=7.8805
[flaml.automl: 09-19 04:06:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:06:59] {3072} INFO -  at 51.7s,	estimator xgboost's best error=6.7532,	best estimator xgboost's best error=6.7532
[flaml.automl: 09-19 04:06:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:07:02] {3072} INFO -  at 54.4s,	estimator xgboost's best error=6.7532,	best estimator xgboost's best error=6.7532
[flaml.automl: 09-19 04:07:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:07:04] {3072} INFO -  at 56.0s,	estimator xgboost's best error=6.7532,	best estimator xgboost's best error=6.7532
[flaml.automl: 09-19 04:07:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:07:07] {3072} INFO -  at 59.0s,	estimator xgboost's best error=5.5457,	best estimator xgboost's best error=5.5457
[flaml.automl: 09-19 04:07:10] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-19 04:07:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 04:07:10] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:07:10] {2637} INFO - Time taken to find the best model: 59.03731656074524
[flaml.automl: 09-19 04:07:10] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
PM2.5(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
PM2.5(0)最佳损失：-4.545660582930258
PM2.5(0)最好结果：{'pred_time': 1.0592401125552424e-05, 'wall_clock_time': 59.03731656074524, 'metric_for_logging': {'pred_time': 1.0592401125552424e-05}, 'val_loss': 5.545660582930258, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 2.9963796138763428}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
PM2.5(0)的R2=0.904168706057117
PM2.5(0)的mse=81.68269248467459
PM2.5(0)的mae=5.685167933919747
PM2.5(0)的mar=0.19015802557572367
总共花费的时间为：62.55
