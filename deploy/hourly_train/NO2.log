nohup: ignoring input
北京市
1005A
1008A
1009A
1010A
1012A
3417A
3418A
3671A
3672A
3673A
3674A
3675A
3695A
3696A
3697A
[flaml.automl: 09-16 04:29:47] {2390} INFO - task = regression
[flaml.automl: 09-16 04:29:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 04:29:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 04:29:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 04:29:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 04:29:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 04:29:48] {3025} INFO - Estimated sufficient time budget=214223s. Estimated necessary time budget=214s.
[flaml.automl: 09-16 04:29:48] {3072} INFO -  at 2.4s,	estimator xgboost's best error=19.0422,	best estimator xgboost's best error=19.0422
[flaml.automl: 09-16 04:29:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 04:29:49] {3072} INFO -  at 3.4s,	estimator xgboost's best error=18.1367,	best estimator xgboost's best error=18.1367
[flaml.automl: 09-16 04:29:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 04:29:51] {3072} INFO -  at 4.9s,	estimator xgboost's best error=18.1367,	best estimator xgboost's best error=18.1367
[flaml.automl: 09-16 04:29:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 04:29:53] {3072} INFO -  at 7.4s,	estimator xgboost's best error=18.1367,	best estimator xgboost's best error=18.1367
[flaml.automl: 09-16 04:29:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 04:29:55] {3072} INFO -  at 8.9s,	estimator xgboost's best error=12.1025,	best estimator xgboost's best error=12.1025
[flaml.automl: 09-16 04:29:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 04:29:57] {3072} INFO -  at 10.7s,	estimator xgboost's best error=10.0763,	best estimator xgboost's best error=10.0763
[flaml.automl: 09-16 04:29:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 04:29:58] {3072} INFO -  at 12.2s,	estimator xgboost's best error=10.0763,	best estimator xgboost's best error=10.0763
[flaml.automl: 09-16 04:29:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 04:30:01] {3072} INFO -  at 14.8s,	estimator xgboost's best error=10.0763,	best estimator xgboost's best error=10.0763
[flaml.automl: 09-16 04:30:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 04:30:02] {3072} INFO -  at 16.1s,	estimator xgboost's best error=5.6776,	best estimator xgboost's best error=5.6776
[flaml.automl: 09-16 04:30:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 04:30:04] {3072} INFO -  at 17.7s,	estimator xgboost's best error=5.6776,	best estimator xgboost's best error=5.6776
[flaml.automl: 09-16 04:30:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 04:30:05] {3072} INFO -  at 18.9s,	estimator xgboost's best error=5.6776,	best estimator xgboost's best error=5.6776
[flaml.automl: 09-16 04:30:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 04:30:06] {3072} INFO -  at 20.0s,	estimator xgboost's best error=5.6776,	best estimator xgboost's best error=5.6776
[flaml.automl: 09-16 04:30:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 04:30:08] {3072} INFO -  at 21.8s,	estimator xgboost's best error=4.6398,	best estimator xgboost's best error=4.6398
[flaml.automl: 09-16 04:30:08] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 04:30:09] {3072} INFO -  at 23.6s,	estimator xgboost's best error=4.6398,	best estimator xgboost's best error=4.6398
[flaml.automl: 09-16 04:30:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 04:30:11] {3072} INFO -  at 25.5s,	estimator xgboost's best error=4.6398,	best estimator xgboost's best error=4.6398
[flaml.automl: 09-16 04:30:11] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 04:30:13] {3072} INFO -  at 27.5s,	estimator xgboost's best error=4.6398,	best estimator xgboost's best error=4.6398
[flaml.automl: 09-16 04:30:13] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 04:30:15] {3072} INFO -  at 28.9s,	estimator xgboost's best error=4.6398,	best estimator xgboost's best error=4.6398
[flaml.automl: 09-16 04:30:15] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 04:30:17] {3072} INFO -  at 31.0s,	estimator xgboost's best error=4.4517,	best estimator xgboost's best error=4.4517
[flaml.automl: 09-16 04:30:17] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 04:30:20] {3072} INFO -  at 34.1s,	estimator xgboost's best error=4.4517,	best estimator xgboost's best error=4.4517
[flaml.automl: 09-16 04:30:20] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 04:30:24] {3072} INFO -  at 38.1s,	estimator xgboost's best error=4.3251,	best estimator xgboost's best error=4.3251
[flaml.automl: 09-16 04:30:24] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 04:30:26] {3072} INFO -  at 40.4s,	estimator xgboost's best error=4.3251,	best estimator xgboost's best error=4.3251
[flaml.automl: 09-16 04:30:26] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 04:30:34] {3072} INFO -  at 48.1s,	estimator xgboost's best error=4.0154,	best estimator xgboost's best error=4.0154
[flaml.automl: 09-16 04:30:34] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 04:30:48] {3072} INFO -  at 62.1s,	estimator xgboost's best error=4.0154,	best estimator xgboost's best error=4.0154
[flaml.automl: 09-16 04:31:39] {3335} INFO - retrain xgboost for 51.0s
[flaml.automl: 09-16 04:31:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7001538379998886, colsample_bynode=1,
             colsample_bytree=0.47444410128962616, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=39, min_child_weight=0.009044181493312308,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0044831876251910764, reg_lambda=3.0182563230698785,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 04:31:39] {2636} INFO - fit succeeded
[flaml.automl: 09-16 04:31:39] {2637} INFO - Time taken to find the best model: 48.10720157623291
[flaml.automl: 09-16 04:31:39] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 39, 'min_child_weight': 0.009044181493312308, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7001538379998886, 'colsample_bytree': 0.47444410128962616, 'reg_alpha': 0.0044831876251910764, 'reg_lambda': 3.0182563230698785, 'FLAML_sample_size': 40000}
NO2(0)最佳损失：-3.0153963719585706
NO2(0)最好结果：{'pred_time': 8.243056502454336e-06, 'wall_clock_time': 48.10720157623291, 'metric_for_logging': {'pred_time': 8.243056502454336e-06}, 'val_loss': 4.015396371958571, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 39, 'min_child_weight': 0.009044181493312308, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7001538379998886, 'colsample_bytree': 0.47444410128962616, 'reg_alpha': 0.0044831876251910764, 'reg_lambda': 3.0182563230698785, 'FLAML_sample_size': 40000}, 'config/n_estimators': 15, 'config/max_leaves': 39, 'config/min_child_weight': 0.009044181493312308, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7001538379998886, 'config/colsample_bytree': 0.47444410128962616, 'config/reg_alpha': 0.0044831876251910764, 'config/reg_lambda': 3.0182563230698785, 'config/FLAML_sample_size': 40000, 'experiment_tag': 'exp', 'time_total_s': 7.7311930656433105}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7001538379998886, colsample_bynode=1,
             colsample_bytree=0.47444410128962616, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=39, min_child_weight=0.009044181493312308,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0044831876251910764, reg_lambda=3.0182563230698785,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8635076322548944
NO2(0)的mse=36.074635791423226
NO2(0)的mae=3.87135966187494
NO2(0)的mar=0.2631925030216712
总共花费的时间为：116.30
天津市
1015A
1019A
2859A
2860A
2922A
3325A
3326A
3327A
3460A
3461A
3462A
[flaml.automl: 09-16 05:04:39] {2390} INFO - task = regression
[flaml.automl: 09-16 05:04:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:04:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:04:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:04:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:04:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:04:40] {3025} INFO - Estimated sufficient time budget=141963s. Estimated necessary time budget=142s.
[flaml.automl: 09-16 05:04:40] {3072} INFO -  at 1.8s,	estimator xgboost's best error=20.0610,	best estimator xgboost's best error=20.0610
[flaml.automl: 09-16 05:04:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:04:42] {3072} INFO -  at 3.8s,	estimator xgboost's best error=9.8743,	best estimator xgboost's best error=9.8743
[flaml.automl: 09-16 05:04:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:04:43] {3072} INFO -  at 5.0s,	estimator xgboost's best error=9.8743,	best estimator xgboost's best error=9.8743
[flaml.automl: 09-16 05:04:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:04:45] {3072} INFO -  at 6.8s,	estimator xgboost's best error=9.8743,	best estimator xgboost's best error=9.8743
[flaml.automl: 09-16 05:04:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:04:46] {3072} INFO -  at 8.4s,	estimator xgboost's best error=6.3340,	best estimator xgboost's best error=6.3340
[flaml.automl: 09-16 05:04:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:04:48] {3072} INFO -  at 10.2s,	estimator xgboost's best error=5.7939,	best estimator xgboost's best error=5.7939
[flaml.automl: 09-16 05:04:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:04:50] {3072} INFO -  at 11.7s,	estimator xgboost's best error=5.7939,	best estimator xgboost's best error=5.7939
[flaml.automl: 09-16 05:04:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:04:51] {3072} INFO -  at 13.0s,	estimator xgboost's best error=5.7939,	best estimator xgboost's best error=5.7939
[flaml.automl: 09-16 05:04:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:04:53] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.6450,	best estimator xgboost's best error=5.6450
[flaml.automl: 09-16 05:04:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:04:54] {3072} INFO -  at 16.2s,	estimator xgboost's best error=5.6450,	best estimator xgboost's best error=5.6450
[flaml.automl: 09-16 05:04:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:04:56] {3072} INFO -  at 17.6s,	estimator xgboost's best error=5.6450,	best estimator xgboost's best error=5.6450
[flaml.automl: 09-16 05:04:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:04:57] {3072} INFO -  at 19.1s,	estimator xgboost's best error=5.6450,	best estimator xgboost's best error=5.6450
[flaml.automl: 09-16 05:04:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:04:58] {3072} INFO -  at 20.2s,	estimator xgboost's best error=5.6450,	best estimator xgboost's best error=5.6450
[flaml.automl: 09-16 05:04:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:05:00] {3072} INFO -  at 22.2s,	estimator xgboost's best error=5.6450,	best estimator xgboost's best error=5.6450
[flaml.automl: 09-16 05:05:00] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 05:05:04] {3072} INFO -  at 26.4s,	estimator xgboost's best error=5.3704,	best estimator xgboost's best error=5.3704
[flaml.automl: 09-16 05:05:04] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 05:05:07] {3072} INFO -  at 29.1s,	estimator xgboost's best error=5.3704,	best estimator xgboost's best error=5.3704
[flaml.automl: 09-16 05:05:07] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 05:05:13] {3072} INFO -  at 34.9s,	estimator xgboost's best error=5.1949,	best estimator xgboost's best error=5.1949
[flaml.automl: 09-16 05:05:13] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 05:05:17] {3072} INFO -  at 38.9s,	estimator xgboost's best error=5.1949,	best estimator xgboost's best error=5.1949
[flaml.automl: 09-16 05:05:17] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 05:05:20] {3072} INFO -  at 42.4s,	estimator xgboost's best error=5.1949,	best estimator xgboost's best error=5.1949
[flaml.automl: 09-16 05:05:20] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 05:05:37] {3072} INFO -  at 58.7s,	estimator xgboost's best error=5.1949,	best estimator xgboost's best error=5.1949
[flaml.automl: 09-16 05:05:43] {3335} INFO - retrain xgboost for 6.6s
[flaml.automl: 09-16 05:05:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5819263921529537, colsample_bynode=1,
             colsample_bytree=0.5890518503132947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=1.381372596423323, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0021181412257918197, reg_lambda=31.98620812685581,
             scale_pos_weight=1, subsample=0.7551084056014155,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 05:05:43] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:05:43] {2637} INFO - Time taken to find the best model: 34.894415616989136
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 7, 'min_child_weight': 1.381372596423323, 'learning_rate': 0.5310029457853219, 'subsample': 0.7551084056014155, 'colsample_bylevel': 0.5819263921529537, 'colsample_bytree': 0.5890518503132947, 'reg_alpha': 0.0021181412257918197, 'reg_lambda': 31.98620812685581, 'FLAML_sample_size': 121524}
NO2(0)最佳损失：-4.194874246005861
NO2(0)最好结果：{'pred_time': 4.3520969452173246e-06, 'wall_clock_time': 34.894415616989136, 'metric_for_logging': {'pred_time': 4.3520969452173246e-06}, 'val_loss': 5.194874246005861, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 7, 'min_child_weight': 1.381372596423323, 'learning_rate': 0.5310029457853219, 'subsample': 0.7551084056014155, 'colsample_bylevel': 0.5819263921529537, 'colsample_bytree': 0.5890518503132947, 'reg_alpha': 0.0021181412257918197, 'reg_lambda': 31.98620812685581, 'FLAML_sample_size': 121524}, 'config/n_estimators': 9, 'config/max_leaves': 7, 'config/min_child_weight': 1.381372596423323, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.7551084056014155, 'config/colsample_bylevel': 0.5819263921529537, 'config/colsample_bytree': 0.5890518503132947, 'config/reg_alpha': 0.0021181412257918197, 'config/reg_lambda': 31.98620812685581, 'config/FLAML_sample_size': 121524, 'experiment_tag': 'exp', 'time_total_s': 5.799803733825684}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5819263921529537, colsample_bynode=1,
             colsample_bytree=0.5890518503132947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=1.381372596423323, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0021181412257918197, reg_lambda=31.98620812685581,
             scale_pos_weight=1, subsample=0.7551084056014155,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8642913824006837
NO2(0)的mse=55.54159302574674
NO2(0)的mae=5.104344475629258
NO2(0)的mar=0.22547942558154055
总共花费的时间为：67.09
石家庄市
1030A
1032A
1033A
1035A
2862A
3456A
3572A
3573A
3574A
[flaml.automl: 09-16 05:31:41] {2390} INFO - task = regression
[flaml.automl: 09-16 05:31:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:31:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:31:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:31:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:31:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:31:43] {3025} INFO - Estimated sufficient time budget=210956s. Estimated necessary time budget=211s.
[flaml.automl: 09-16 05:31:43] {3072} INFO -  at 2.7s,	estimator xgboost's best error=18.1484,	best estimator xgboost's best error=18.1484
[flaml.automl: 09-16 05:31:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:31:46] {3072} INFO -  at 5.3s,	estimator xgboost's best error=13.2077,	best estimator xgboost's best error=13.2077
[flaml.automl: 09-16 05:31:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:31:48] {3072} INFO -  at 7.5s,	estimator xgboost's best error=13.2077,	best estimator xgboost's best error=13.2077
[flaml.automl: 09-16 05:31:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:31:50] {3072} INFO -  at 9.7s,	estimator xgboost's best error=13.2077,	best estimator xgboost's best error=13.2077
[flaml.automl: 09-16 05:31:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:31:52] {3072} INFO -  at 11.8s,	estimator xgboost's best error=5.9349,	best estimator xgboost's best error=5.9349
[flaml.automl: 09-16 05:31:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:31:55] {3072} INFO -  at 14.1s,	estimator xgboost's best error=5.5664,	best estimator xgboost's best error=5.5664
[flaml.automl: 09-16 05:31:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:31:57] {3072} INFO -  at 16.2s,	estimator xgboost's best error=5.5664,	best estimator xgboost's best error=5.5664
[flaml.automl: 09-16 05:31:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:31:58] {3072} INFO -  at 17.6s,	estimator xgboost's best error=5.5664,	best estimator xgboost's best error=5.5664
[flaml.automl: 09-16 05:31:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:32:00] {3072} INFO -  at 19.7s,	estimator xgboost's best error=5.4409,	best estimator xgboost's best error=5.4409
[flaml.automl: 09-16 05:32:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:32:02] {3072} INFO -  at 21.6s,	estimator xgboost's best error=5.4409,	best estimator xgboost's best error=5.4409
[flaml.automl: 09-16 05:32:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:32:04] {3072} INFO -  at 23.2s,	estimator xgboost's best error=5.4409,	best estimator xgboost's best error=5.4409
[flaml.automl: 09-16 05:32:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:32:05] {3072} INFO -  at 24.8s,	estimator xgboost's best error=5.4409,	best estimator xgboost's best error=5.4409
[flaml.automl: 09-16 05:32:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:32:07] {3072} INFO -  at 26.8s,	estimator xgboost's best error=5.4409,	best estimator xgboost's best error=5.4409
[flaml.automl: 09-16 05:32:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:32:11] {3072} INFO -  at 30.2s,	estimator xgboost's best error=5.4409,	best estimator xgboost's best error=5.4409
[flaml.automl: 09-16 05:32:11] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 05:32:18] {3072} INFO -  at 37.1s,	estimator xgboost's best error=5.1944,	best estimator xgboost's best error=5.1944
[flaml.automl: 09-16 05:32:18] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 05:32:22] {3072} INFO -  at 41.2s,	estimator xgboost's best error=5.1944,	best estimator xgboost's best error=5.1944
[flaml.automl: 09-16 05:32:22] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 05:32:30] {3072} INFO -  at 49.3s,	estimator xgboost's best error=4.8688,	best estimator xgboost's best error=4.8688
[flaml.automl: 09-16 05:32:30] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 05:32:34] {3072} INFO -  at 53.2s,	estimator xgboost's best error=4.8688,	best estimator xgboost's best error=4.8688
[flaml.automl: 09-16 05:32:40] {3335} INFO - retrain xgboost for 6.3s
[flaml.automl: 09-16 05:32:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5819263921529537, colsample_bynode=1,
             colsample_bytree=0.5890518503132947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=1.381372596423323, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0021181412257918197, reg_lambda=31.98620812685581,
             scale_pos_weight=1, subsample=0.7551084056014155,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 05:32:40] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:32:40] {2637} INFO - Time taken to find the best model: 49.28300595283508
[flaml.automl: 09-16 05:32:40] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 7, 'min_child_weight': 1.381372596423323, 'learning_rate': 0.5310029457853219, 'subsample': 0.7551084056014155, 'colsample_bylevel': 0.5819263921529537, 'colsample_bytree': 0.5890518503132947, 'reg_alpha': 0.0021181412257918197, 'reg_lambda': 31.98620812685581, 'FLAML_sample_size': 95231}
NO2(0)最佳损失：-3.868785939872704
NO2(0)最好结果：{'pred_time': 8.412602492573806e-06, 'wall_clock_time': 49.28300595283508, 'metric_for_logging': {'pred_time': 8.412602492573806e-06}, 'val_loss': 4.868785939872704, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 7, 'min_child_weight': 1.381372596423323, 'learning_rate': 0.5310029457853219, 'subsample': 0.7551084056014155, 'colsample_bylevel': 0.5819263921529537, 'colsample_bytree': 0.5890518503132947, 'reg_alpha': 0.0021181412257918197, 'reg_lambda': 31.98620812685581, 'FLAML_sample_size': 95231}, 'config/n_estimators': 9, 'config/max_leaves': 7, 'config/min_child_weight': 1.381372596423323, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.7551084056014155, 'config/colsample_bylevel': 0.5819263921529537, 'config/colsample_bytree': 0.5890518503132947, 'config/reg_alpha': 0.0021181412257918197, 'config/reg_lambda': 31.98620812685581, 'config/FLAML_sample_size': 95231, 'experiment_tag': 'exp', 'time_total_s': 8.076712131500244}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5819263921529537, colsample_bynode=1,
             colsample_bytree=0.5890518503132947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=1.381372596423323, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0021181412257918197, reg_lambda=31.98620812685581,
             scale_pos_weight=1, subsample=0.7551084056014155,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8426932621508658
NO2(0)的mse=51.06302485429355
NO2(0)的mae=4.881982058076273
NO2(0)的mar=0.2640542213299828
总共花费的时间为：61.17
唐山市
1041A
3575A
3576A
3577A
3578A
3692A
[flaml.automl: 09-16 05:51:51] {2390} INFO - task = regression
[flaml.automl: 09-16 05:51:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:51:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:51:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:51:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:51:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:51:54] {3025} INFO - Estimated sufficient time budget=134664s. Estimated necessary time budget=135s.
[flaml.automl: 09-16 05:51:54] {3072} INFO -  at 2.7s,	estimator xgboost's best error=22.0460,	best estimator xgboost's best error=22.0460
[flaml.automl: 09-16 05:51:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:51:58] {3072} INFO -  at 6.6s,	estimator xgboost's best error=10.9628,	best estimator xgboost's best error=10.9628
[flaml.automl: 09-16 05:51:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:52:00] {3072} INFO -  at 9.3s,	estimator xgboost's best error=10.9628,	best estimator xgboost's best error=10.9628
[flaml.automl: 09-16 05:52:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:52:04] {3072} INFO -  at 12.8s,	estimator xgboost's best error=10.9628,	best estimator xgboost's best error=10.9628
[flaml.automl: 09-16 05:52:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:52:07] {3072} INFO -  at 16.0s,	estimator xgboost's best error=7.1684,	best estimator xgboost's best error=7.1684
[flaml.automl: 09-16 05:52:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:52:09] {3072} INFO -  at 18.5s,	estimator xgboost's best error=7.1684,	best estimator xgboost's best error=7.1684
[flaml.automl: 09-16 05:52:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:52:12] {3072} INFO -  at 21.2s,	estimator xgboost's best error=7.1684,	best estimator xgboost's best error=7.1684
[flaml.automl: 09-16 05:52:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:52:15] {3072} INFO -  at 24.1s,	estimator xgboost's best error=7.1684,	best estimator xgboost's best error=7.1684
[flaml.automl: 09-16 05:52:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:52:18] {3072} INFO -  at 26.7s,	estimator xgboost's best error=6.7473,	best estimator xgboost's best error=6.7473
[flaml.automl: 09-16 05:52:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:52:19] {3072} INFO -  at 28.4s,	estimator xgboost's best error=6.7473,	best estimator xgboost's best error=6.7473
[flaml.automl: 09-16 05:52:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:52:21] {3072} INFO -  at 30.4s,	estimator xgboost's best error=6.7473,	best estimator xgboost's best error=6.7473
[flaml.automl: 09-16 05:52:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:52:23] {3072} INFO -  at 32.2s,	estimator xgboost's best error=6.7473,	best estimator xgboost's best error=6.7473
[flaml.automl: 09-16 05:52:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:52:25] {3072} INFO -  at 34.1s,	estimator xgboost's best error=6.1452,	best estimator xgboost's best error=6.1452
[flaml.automl: 09-16 05:52:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:52:27] {3072} INFO -  at 35.8s,	estimator xgboost's best error=6.1452,	best estimator xgboost's best error=6.1452
[flaml.automl: 09-16 05:52:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 05:52:29] {3072} INFO -  at 38.3s,	estimator xgboost's best error=6.1452,	best estimator xgboost's best error=6.1452
[flaml.automl: 09-16 05:52:29] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 05:52:32] {3072} INFO -  at 41.3s,	estimator xgboost's best error=6.0110,	best estimator xgboost's best error=6.0110
[flaml.automl: 09-16 05:52:32] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 05:52:34] {3072} INFO -  at 43.2s,	estimator xgboost's best error=6.0110,	best estimator xgboost's best error=6.0110
[flaml.automl: 09-16 05:52:34] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 05:52:36] {3072} INFO -  at 44.8s,	estimator xgboost's best error=6.0110,	best estimator xgboost's best error=6.0110
[flaml.automl: 09-16 05:52:36] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 05:52:39] {3072} INFO -  at 47.8s,	estimator xgboost's best error=5.9297,	best estimator xgboost's best error=5.9297
[flaml.automl: 09-16 05:52:39] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 05:52:40] {3072} INFO -  at 49.4s,	estimator xgboost's best error=5.9297,	best estimator xgboost's best error=5.9297
[flaml.automl: 09-16 05:52:40] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 05:52:45] {3072} INFO -  at 54.5s,	estimator xgboost's best error=5.7107,	best estimator xgboost's best error=5.7107
[flaml.automl: 09-16 05:52:45] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 05:52:55] {3072} INFO -  at 63.9s,	estimator xgboost's best error=5.7107,	best estimator xgboost's best error=5.7107
[flaml.automl: 09-16 05:53:46] {3335} INFO - retrain xgboost for 51.3s
[flaml.automl: 09-16 05:53:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5161922612101613, colsample_bynode=1,
             colsample_bytree=0.6118906562279557, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=69, min_child_weight=0.07069787149048533,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.002426712663282027, reg_lambda=1.0618132296291931,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 05:53:46] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:53:46] {2637} INFO - Time taken to find the best model: 54.53584694862366
[flaml.automl: 09-16 05:53:46] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 69, 'min_child_weight': 0.07069787149048533, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.5161922612101613, 'colsample_bytree': 0.6118906562279557, 'reg_alpha': 0.002426712663282027, 'reg_lambda': 1.0618132296291931, 'FLAML_sample_size': 10000}
NO2(0)最佳损失：-4.710704709217456
NO2(0)最好结果：{'pred_time': 1.1611447089886317e-05, 'wall_clock_time': 54.53584694862366, 'metric_for_logging': {'pred_time': 1.1611447089886317e-05}, 'val_loss': 5.710704709217456, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 69, 'min_child_weight': 0.07069787149048533, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.5161922612101613, 'colsample_bytree': 0.6118906562279557, 'reg_alpha': 0.002426712663282027, 'reg_lambda': 1.0618132296291931, 'FLAML_sample_size': 10000}, 'config/n_estimators': 15, 'config/max_leaves': 69, 'config/min_child_weight': 0.07069787149048533, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.5161922612101613, 'config/colsample_bytree': 0.6118906562279557, 'config/reg_alpha': 0.002426712663282027, 'config/reg_lambda': 1.0618132296291931, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 5.136409044265747}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5161922612101613, colsample_bynode=1,
             colsample_bytree=0.6118906562279557, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=69, min_child_weight=0.07069787149048533,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.002426712663282027, reg_lambda=1.0618132296291931,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8627495830506128
NO2(0)的mse=63.64006080486273
NO2(0)的mae=5.601846953723943
NO2(0)的mar=0.24226699813461272
总共花费的时间为：116.32
秦皇岛市
1043A
1044A
3132A
[flaml.automl: 09-16 06:03:54] {2390} INFO - task = regression
[flaml.automl: 09-16 06:03:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:03:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:03:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:03:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:03:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:03:55] {3025} INFO - Estimated sufficient time budget=12188s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 06:03:55] {3072} INFO -  at 1.4s,	estimator xgboost's best error=20.8711,	best estimator xgboost's best error=20.8711
[flaml.automl: 09-16 06:03:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:03:57] {3072} INFO -  at 3.6s,	estimator xgboost's best error=10.7391,	best estimator xgboost's best error=10.7391
[flaml.automl: 09-16 06:03:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:03:59] {3072} INFO -  at 4.8s,	estimator xgboost's best error=10.7391,	best estimator xgboost's best error=10.7391
[flaml.automl: 09-16 06:03:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:04:09] {3072} INFO -  at 14.8s,	estimator xgboost's best error=10.7391,	best estimator xgboost's best error=10.7391
[flaml.automl: 09-16 06:04:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:04:10] {3072} INFO -  at 15.9s,	estimator xgboost's best error=7.5972,	best estimator xgboost's best error=7.5972
[flaml.automl: 09-16 06:04:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:04:11] {3072} INFO -  at 17.5s,	estimator xgboost's best error=6.6599,	best estimator xgboost's best error=6.6599
[flaml.automl: 09-16 06:04:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:04:13] {3072} INFO -  at 19.2s,	estimator xgboost's best error=6.6599,	best estimator xgboost's best error=6.6599
[flaml.automl: 09-16 06:04:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:04:15] {3072} INFO -  at 21.6s,	estimator xgboost's best error=6.6599,	best estimator xgboost's best error=6.6599
[flaml.automl: 09-16 06:04:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:04:17] {3072} INFO -  at 22.7s,	estimator xgboost's best error=6.6599,	best estimator xgboost's best error=6.6599
[flaml.automl: 09-16 06:04:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:04:19] {3072} INFO -  at 25.4s,	estimator xgboost's best error=6.6599,	best estimator xgboost's best error=6.6599
[flaml.automl: 09-16 06:04:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:04:20] {3072} INFO -  at 26.5s,	estimator xgboost's best error=6.6599,	best estimator xgboost's best error=6.6599
[flaml.automl: 09-16 06:04:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:04:22] {3072} INFO -  at 27.7s,	estimator xgboost's best error=6.6599,	best estimator xgboost's best error=6.6599
[flaml.automl: 09-16 06:04:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:04:28] {3072} INFO -  at 34.1s,	estimator xgboost's best error=6.3328,	best estimator xgboost's best error=6.3328
[flaml.automl: 09-16 06:04:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 06:04:40] {3072} INFO -  at 46.2s,	estimator xgboost's best error=6.1896,	best estimator xgboost's best error=6.1896
[flaml.automl: 09-16 06:04:40] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 06:04:47] {3072} INFO -  at 52.8s,	estimator xgboost's best error=6.1896,	best estimator xgboost's best error=6.1896
[flaml.automl: 09-16 06:04:59] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 06:04:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:04:59] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:04:59] {2637} INFO - Time taken to find the best model: 46.23751926422119
[flaml.automl: 09-16 06:04:59] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-5.189605175615424
NO2(0)最好结果：{'pred_time': 1.1532898717278601e-05, 'wall_clock_time': 46.23751926422119, 'metric_for_logging': {'pred_time': 1.1532898717278601e-05}, 'val_loss': 6.189605175615424, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.089046239852905}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7671547540436785
NO2(0)的mse=93.44110713611963
NO2(0)的mae=6.344837599217127
NO2(0)的mar=0.3038852899257287
总共花费的时间为：65.53
邯郸市
1050A
3579A
3580A
[flaml.automl: 09-16 06:14:42] {2390} INFO - task = regression
[flaml.automl: 09-16 06:14:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:14:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:14:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:14:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:14:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:14:44] {3025} INFO - Estimated sufficient time budget=12091s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 06:14:44] {3072} INFO -  at 1.4s,	estimator xgboost's best error=16.4659,	best estimator xgboost's best error=16.4659
[flaml.automl: 09-16 06:14:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:14:46] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.2837,	best estimator xgboost's best error=8.2837
[flaml.automl: 09-16 06:14:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:14:47] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.2837,	best estimator xgboost's best error=8.2837
[flaml.automl: 09-16 06:14:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:14:57] {3072} INFO -  at 14.7s,	estimator xgboost's best error=8.2837,	best estimator xgboost's best error=8.2837
[flaml.automl: 09-16 06:14:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:14:58] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.5299,	best estimator xgboost's best error=5.5299
[flaml.automl: 09-16 06:14:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:15:00] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.6238,	best estimator xgboost's best error=4.6238
[flaml.automl: 09-16 06:15:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:15:01] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.6238,	best estimator xgboost's best error=4.6238
[flaml.automl: 09-16 06:15:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:15:04] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.6238,	best estimator xgboost's best error=4.6238
[flaml.automl: 09-16 06:15:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:15:05] {3072} INFO -  at 22.8s,	estimator xgboost's best error=4.6238,	best estimator xgboost's best error=4.6238
[flaml.automl: 09-16 06:15:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:15:08] {3072} INFO -  at 25.4s,	estimator xgboost's best error=4.6238,	best estimator xgboost's best error=4.6238
[flaml.automl: 09-16 06:15:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:15:09] {3072} INFO -  at 26.6s,	estimator xgboost's best error=4.6238,	best estimator xgboost's best error=4.6238
[flaml.automl: 09-16 06:15:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:15:10] {3072} INFO -  at 27.7s,	estimator xgboost's best error=4.6238,	best estimator xgboost's best error=4.6238
[flaml.automl: 09-16 06:15:10] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:15:17] {3072} INFO -  at 34.2s,	estimator xgboost's best error=4.4423,	best estimator xgboost's best error=4.4423
[flaml.automl: 09-16 06:15:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 06:15:29] {3072} INFO -  at 46.3s,	estimator xgboost's best error=4.3139,	best estimator xgboost's best error=4.3139
[flaml.automl: 09-16 06:15:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 06:15:35] {3072} INFO -  at 52.8s,	estimator xgboost's best error=4.3139,	best estimator xgboost's best error=4.3139
[flaml.automl: 09-16 06:15:47] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 06:15:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:15:47] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:15:47] {2637} INFO - Time taken to find the best model: 46.307252407073975
[flaml.automl: 09-16 06:15:47] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.3138737049802653
NO2(0)最好结果：{'pred_time': 1.2004940527497122e-05, 'wall_clock_time': 46.307252407073975, 'metric_for_logging': {'pred_time': 1.2004940527497122e-05}, 'val_loss': 4.313873704980265, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.070518255233765}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8607985853172319
NO2(0)的mse=38.95742787575761
NO2(0)的mae=4.150469793387349
NO2(0)的mar=0.24594252134587316
总共花费的时间为：65.37
保定市
1051A
1053A
1054A
1055A
1056A
3581A
3582A
3583A
[flaml.automl: 09-16 06:40:02] {2390} INFO - task = regression
[flaml.automl: 09-16 06:40:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:40:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:40:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:40:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:40:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:40:04] {3025} INFO - Estimated sufficient time budget=103032s. Estimated necessary time budget=103s.
[flaml.automl: 09-16 06:40:04] {3072} INFO -  at 1.6s,	estimator xgboost's best error=21.5006,	best estimator xgboost's best error=21.5006
[flaml.automl: 09-16 06:40:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:40:06] {3072} INFO -  at 3.7s,	estimator xgboost's best error=10.3221,	best estimator xgboost's best error=10.3221
[flaml.automl: 09-16 06:40:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:40:07] {3072} INFO -  at 5.0s,	estimator xgboost's best error=10.3221,	best estimator xgboost's best error=10.3221
[flaml.automl: 09-16 06:40:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:40:10] {3072} INFO -  at 7.7s,	estimator xgboost's best error=10.3221,	best estimator xgboost's best error=10.3221
[flaml.automl: 09-16 06:40:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:40:11] {3072} INFO -  at 8.9s,	estimator xgboost's best error=6.4055,	best estimator xgboost's best error=6.4055
[flaml.automl: 09-16 06:40:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:40:13] {3072} INFO -  at 10.5s,	estimator xgboost's best error=5.4141,	best estimator xgboost's best error=5.4141
[flaml.automl: 09-16 06:40:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:40:14] {3072} INFO -  at 12.1s,	estimator xgboost's best error=5.4141,	best estimator xgboost's best error=5.4141
[flaml.automl: 09-16 06:40:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:40:17] {3072} INFO -  at 14.5s,	estimator xgboost's best error=5.4141,	best estimator xgboost's best error=5.4141
[flaml.automl: 09-16 06:40:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:40:18] {3072} INFO -  at 15.7s,	estimator xgboost's best error=5.4141,	best estimator xgboost's best error=5.4141
[flaml.automl: 09-16 06:40:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:40:20] {3072} INFO -  at 17.7s,	estimator xgboost's best error=5.4141,	best estimator xgboost's best error=5.4141
[flaml.automl: 09-16 06:40:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:40:21] {3072} INFO -  at 19.4s,	estimator xgboost's best error=5.2925,	best estimator xgboost's best error=5.2925
[flaml.automl: 09-16 06:40:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:40:23] {3072} INFO -  at 20.6s,	estimator xgboost's best error=5.2925,	best estimator xgboost's best error=5.2925
[flaml.automl: 09-16 06:40:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:40:29] {3072} INFO -  at 27.1s,	estimator xgboost's best error=4.9059,	best estimator xgboost's best error=4.9059
[flaml.automl: 09-16 06:40:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 06:40:41] {3072} INFO -  at 39.3s,	estimator xgboost's best error=4.7666,	best estimator xgboost's best error=4.7666
[flaml.automl: 09-16 06:40:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 06:40:48] {3072} INFO -  at 45.8s,	estimator xgboost's best error=4.7666,	best estimator xgboost's best error=4.7666
[flaml.automl: 09-16 06:40:48] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 06:41:02] {3072} INFO -  at 59.5s,	estimator xgboost's best error=4.7475,	best estimator xgboost's best error=4.7475
[flaml.automl: 09-16 06:41:23] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-16 06:41:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:41:23] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:41:23] {2637} INFO - Time taken to find the best model: 59.45762753486633
[flaml.automl: 09-16 06:41:23] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 86149}
NO2(0)最佳损失：-3.747503041623342
NO2(0)最好结果：{'pred_time': 4.312205835157311e-06, 'wall_clock_time': 59.45762753486633, 'metric_for_logging': {'pred_time': 4.312205835157311e-06}, 'val_loss': 4.747503041623342, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 86149}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 86149, 'experiment_tag': 'exp', 'time_total_s': 13.656104803085327}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8824154437494239
NO2(0)的mse=49.27311148298389
NO2(0)的mae=4.717499268810387
NO2(0)的mar=0.18955578702681383
总共花费的时间为：82.23
张家口市
1060A
1061A
3131A
3323A
[flaml.automl: 09-16 06:54:18] {2390} INFO - task = regression
[flaml.automl: 09-16 06:54:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:54:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:54:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:54:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:54:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:54:19] {3025} INFO - Estimated sufficient time budget=48364s. Estimated necessary time budget=48s.
[flaml.automl: 09-16 06:54:19] {3072} INFO -  at 1.4s,	estimator xgboost's best error=9.3507,	best estimator xgboost's best error=9.3507
[flaml.automl: 09-16 06:54:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:54:21] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.7535,	best estimator xgboost's best error=4.7535
[flaml.automl: 09-16 06:54:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:54:23] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.7535,	best estimator xgboost's best error=4.7535
[flaml.automl: 09-16 06:54:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:54:29] {3072} INFO -  at 11.1s,	estimator xgboost's best error=4.7535,	best estimator xgboost's best error=4.7535
[flaml.automl: 09-16 06:54:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:54:30] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.4846,	best estimator xgboost's best error=3.4846
[flaml.automl: 09-16 06:54:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:54:32] {3072} INFO -  at 13.9s,	estimator xgboost's best error=3.1679,	best estimator xgboost's best error=3.1679
[flaml.automl: 09-16 06:54:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:54:33] {3072} INFO -  at 15.5s,	estimator xgboost's best error=3.1679,	best estimator xgboost's best error=3.1679
[flaml.automl: 09-16 06:54:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:54:36] {3072} INFO -  at 18.0s,	estimator xgboost's best error=3.1679,	best estimator xgboost's best error=3.1679
[flaml.automl: 09-16 06:54:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:54:37] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.1679,	best estimator xgboost's best error=3.1679
[flaml.automl: 09-16 06:54:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:54:40] {3072} INFO -  at 21.8s,	estimator xgboost's best error=3.1679,	best estimator xgboost's best error=3.1679
[flaml.automl: 09-16 06:54:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:54:41] {3072} INFO -  at 23.4s,	estimator xgboost's best error=3.1679,	best estimator xgboost's best error=3.1679
[flaml.automl: 09-16 06:54:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:54:42] {3072} INFO -  at 24.5s,	estimator xgboost's best error=3.1679,	best estimator xgboost's best error=3.1679
[flaml.automl: 09-16 06:54:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:54:49] {3072} INFO -  at 31.1s,	estimator xgboost's best error=3.1097,	best estimator xgboost's best error=3.1097
[flaml.automl: 09-16 06:54:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 06:55:01] {3072} INFO -  at 43.1s,	estimator xgboost's best error=3.0408,	best estimator xgboost's best error=3.0408
[flaml.automl: 09-16 06:55:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 06:55:08] {3072} INFO -  at 49.6s,	estimator xgboost's best error=3.0408,	best estimator xgboost's best error=3.0408
[flaml.automl: 09-16 06:55:20] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 06:55:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:55:20] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:55:20] {2637} INFO - Time taken to find the best model: 43.136730670928955
[flaml.automl: 09-16 06:55:20] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40071}
NO2(0)最佳损失：-2.040767250905914
NO2(0)最好结果：{'pred_time': 9.808410774743239e-06, 'wall_clock_time': 43.136730670928955, 'metric_for_logging': {'pred_time': 9.808410774743239e-06}, 'val_loss': 3.040767250905914, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40071}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40071, 'experiment_tag': 'exp', 'time_total_s': 12.074891328811646}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7092493926341944
NO2(0)的mse=20.815248247748464
NO2(0)的mae=2.9735433618773737
NO2(0)的mar=0.24309448493644076
总共花费的时间为：62.51
承德市
1063A
1064A
1065A
[flaml.automl: 09-16 07:05:04] {2390} INFO - task = regression
[flaml.automl: 09-16 07:05:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:05:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:05:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:05:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:05:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:05:05] {3025} INFO - Estimated sufficient time budget=11892s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 07:05:05] {3072} INFO -  at 1.3s,	estimator xgboost's best error=16.7451,	best estimator xgboost's best error=16.7451
[flaml.automl: 09-16 07:05:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:05:07] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.1327,	best estimator xgboost's best error=8.1327
[flaml.automl: 09-16 07:05:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:05:08] {3072} INFO -  at 4.6s,	estimator xgboost's best error=8.1327,	best estimator xgboost's best error=8.1327
[flaml.automl: 09-16 07:05:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:05:18] {3072} INFO -  at 14.6s,	estimator xgboost's best error=8.1327,	best estimator xgboost's best error=8.1327
[flaml.automl: 09-16 07:05:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:05:20] {3072} INFO -  at 15.8s,	estimator xgboost's best error=4.9065,	best estimator xgboost's best error=4.9065
[flaml.automl: 09-16 07:05:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:05:21] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.0859,	best estimator xgboost's best error=4.0859
[flaml.automl: 09-16 07:05:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:05:23] {3072} INFO -  at 19.0s,	estimator xgboost's best error=4.0859,	best estimator xgboost's best error=4.0859
[flaml.automl: 09-16 07:05:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:05:25] {3072} INFO -  at 21.5s,	estimator xgboost's best error=4.0859,	best estimator xgboost's best error=4.0859
[flaml.automl: 09-16 07:05:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:05:27] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.0859,	best estimator xgboost's best error=4.0859
[flaml.automl: 09-16 07:05:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:05:29] {3072} INFO -  at 25.4s,	estimator xgboost's best error=4.0859,	best estimator xgboost's best error=4.0859
[flaml.automl: 09-16 07:05:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:05:30] {3072} INFO -  at 26.5s,	estimator xgboost's best error=4.0859,	best estimator xgboost's best error=4.0859
[flaml.automl: 09-16 07:05:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:05:32] {3072} INFO -  at 27.7s,	estimator xgboost's best error=4.0859,	best estimator xgboost's best error=4.0859
[flaml.automl: 09-16 07:05:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:05:38] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.7897,	best estimator xgboost's best error=3.7897
[flaml.automl: 09-16 07:05:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:05:50] {3072} INFO -  at 46.3s,	estimator xgboost's best error=3.7026,	best estimator xgboost's best error=3.7026
[flaml.automl: 09-16 07:05:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 07:05:57] {3072} INFO -  at 52.9s,	estimator xgboost's best error=3.7026,	best estimator xgboost's best error=3.7026
[flaml.automl: 09-16 07:06:09] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 07:06:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:06:09] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:06:09] {2637} INFO - Time taken to find the best model: 46.34881806373596
[flaml.automl: 09-16 07:06:09] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.7026174854208196
NO2(0)最好结果：{'pred_time': 1.1107732545961892e-05, 'wall_clock_time': 46.34881806373596, 'metric_for_logging': {'pred_time': 1.1107732545961892e-05}, 'val_loss': 3.7026174854208196, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.129638910293579}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.85829460725798
NO2(0)的mse=32.85849986504145
NO2(0)的mae=3.883665342992714
NO2(0)的mar=0.20467848728345725
总共花费的时间为：65.50
廊坊市
1070A
2919A
[flaml.automl: 09-16 07:12:32] {2390} INFO - task = regression
[flaml.automl: 09-16 07:12:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:12:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:12:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:12:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:12:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:12:33] {3025} INFO - Estimated sufficient time budget=12003s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 07:12:33] {3072} INFO -  at 1.3s,	estimator xgboost's best error=21.0769,	best estimator xgboost's best error=21.0769
[flaml.automl: 09-16 07:12:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:12:36] {3072} INFO -  at 3.4s,	estimator xgboost's best error=10.1008,	best estimator xgboost's best error=10.1008
[flaml.automl: 09-16 07:12:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:12:37] {3072} INFO -  at 4.6s,	estimator xgboost's best error=10.1008,	best estimator xgboost's best error=10.1008
[flaml.automl: 09-16 07:12:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:12:46] {3072} INFO -  at 14.0s,	estimator xgboost's best error=10.1008,	best estimator xgboost's best error=10.1008
[flaml.automl: 09-16 07:12:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:12:47] {3072} INFO -  at 15.2s,	estimator xgboost's best error=6.0906,	best estimator xgboost's best error=6.0906
[flaml.automl: 09-16 07:12:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:12:49] {3072} INFO -  at 16.8s,	estimator xgboost's best error=5.0219,	best estimator xgboost's best error=5.0219
[flaml.automl: 09-16 07:12:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:12:51] {3072} INFO -  at 18.4s,	estimator xgboost's best error=5.0219,	best estimator xgboost's best error=5.0219
[flaml.automl: 09-16 07:12:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:12:53] {3072} INFO -  at 20.8s,	estimator xgboost's best error=5.0219,	best estimator xgboost's best error=5.0219
[flaml.automl: 09-16 07:12:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:12:54] {3072} INFO -  at 22.0s,	estimator xgboost's best error=5.0219,	best estimator xgboost's best error=5.0219
[flaml.automl: 09-16 07:12:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:12:57] {3072} INFO -  at 24.5s,	estimator xgboost's best error=5.0219,	best estimator xgboost's best error=5.0219
[flaml.automl: 09-16 07:12:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:12:58] {3072} INFO -  at 25.6s,	estimator xgboost's best error=5.0219,	best estimator xgboost's best error=5.0219
[flaml.automl: 09-16 07:12:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:12:59] {3072} INFO -  at 26.7s,	estimator xgboost's best error=5.0219,	best estimator xgboost's best error=5.0219
[flaml.automl: 09-16 07:12:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:13:05] {3072} INFO -  at 32.8s,	estimator xgboost's best error=4.5759,	best estimator xgboost's best error=4.5759
[flaml.automl: 09-16 07:13:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:13:15] {3072} INFO -  at 43.2s,	estimator xgboost's best error=4.5150,	best estimator xgboost's best error=4.5150
[flaml.automl: 09-16 07:13:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 07:13:21] {3072} INFO -  at 49.3s,	estimator xgboost's best error=4.5150,	best estimator xgboost's best error=4.5150
[flaml.automl: 09-16 07:13:21] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 07:13:31] {3072} INFO -  at 59.0s,	estimator xgboost's best error=4.4811,	best estimator xgboost's best error=4.4811
[flaml.automl: 09-16 07:13:49] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-16 07:13:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:13:49] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:13:49] {2637} INFO - Time taken to find the best model: 59.02824902534485
[flaml.automl: 09-16 07:13:49] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-3.4810679159302644
NO2(0)最好结果：{'pred_time': 1.6338040369638003e-05, 'wall_clock_time': 59.02824902534485, 'metric_for_logging': {'pred_time': 1.6338040369638003e-05}, 'val_loss': 4.481067915930264, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.721725463867188}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8908824973249155
NO2(0)的mse=43.10629928868788
NO2(0)的mae=4.389823875199842
NO2(0)的mar=0.1711683098943058
总共花费的时间为：76.74
沧州市
1071A
1073A
3324A
[flaml.automl: 09-16 07:23:58] {2390} INFO - task = regression
[flaml.automl: 09-16 07:23:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:23:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:23:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:23:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:23:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:24:00] {3025} INFO - Estimated sufficient time budget=12120s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 07:24:00] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.4133,	best estimator xgboost's best error=18.4133
[flaml.automl: 09-16 07:24:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:24:02] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.8885,	best estimator xgboost's best error=8.8885
[flaml.automl: 09-16 07:24:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:24:03] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.8885,	best estimator xgboost's best error=8.8885
[flaml.automl: 09-16 07:24:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:24:13] {3072} INFO -  at 14.8s,	estimator xgboost's best error=8.8885,	best estimator xgboost's best error=8.8885
[flaml.automl: 09-16 07:24:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:24:14] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.5751,	best estimator xgboost's best error=5.5751
[flaml.automl: 09-16 07:24:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:24:16] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.7775,	best estimator xgboost's best error=4.7775
[flaml.automl: 09-16 07:24:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:24:17] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.7775,	best estimator xgboost's best error=4.7775
[flaml.automl: 09-16 07:24:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:24:20] {3072} INFO -  at 21.5s,	estimator xgboost's best error=4.7775,	best estimator xgboost's best error=4.7775
[flaml.automl: 09-16 07:24:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:24:21] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.7775,	best estimator xgboost's best error=4.7775
[flaml.automl: 09-16 07:24:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:24:23] {3072} INFO -  at 25.3s,	estimator xgboost's best error=4.7775,	best estimator xgboost's best error=4.7775
[flaml.automl: 09-16 07:24:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:24:25] {3072} INFO -  at 26.4s,	estimator xgboost's best error=4.7775,	best estimator xgboost's best error=4.7775
[flaml.automl: 09-16 07:24:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:24:26] {3072} INFO -  at 27.5s,	estimator xgboost's best error=4.7775,	best estimator xgboost's best error=4.7775
[flaml.automl: 09-16 07:24:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:24:32] {3072} INFO -  at 33.9s,	estimator xgboost's best error=4.4688,	best estimator xgboost's best error=4.4688
[flaml.automl: 09-16 07:24:32] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:24:44] {3072} INFO -  at 45.8s,	estimator xgboost's best error=4.3473,	best estimator xgboost's best error=4.3473
[flaml.automl: 09-16 07:24:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 07:24:50] {3072} INFO -  at 52.3s,	estimator xgboost's best error=4.3473,	best estimator xgboost's best error=4.3473
[flaml.automl: 09-16 07:25:02] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 07:25:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:25:02] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:25:02] {2637} INFO - Time taken to find the best model: 45.83898305892944
[flaml.automl: 09-16 07:25:02] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.3472555777086415
NO2(0)最好结果：{'pred_time': 1.0915376521922923e-05, 'wall_clock_time': 45.83898305892944, 'metric_for_logging': {'pred_time': 1.0915376521922923e-05}, 'val_loss': 4.3472555777086415, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.915828704833984}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8571764640772975
NO2(0)的mse=42.10079900732522
NO2(0)的mae=4.451687082882021
NO2(0)的mar=0.19886839811229456
总共花费的时间为：64.81
衡水市
1074A
1075A
3188A
3459A
[flaml.automl: 09-16 07:37:45] {2390} INFO - task = regression
[flaml.automl: 09-16 07:37:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:37:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:37:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:37:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:37:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:37:46] {3025} INFO - Estimated sufficient time budget=52020s. Estimated necessary time budget=52s.
[flaml.automl: 09-16 07:37:46] {3072} INFO -  at 1.4s,	estimator xgboost's best error=16.3411,	best estimator xgboost's best error=16.3411
[flaml.automl: 09-16 07:37:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:37:48] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.6955,	best estimator xgboost's best error=7.6955
[flaml.automl: 09-16 07:37:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:37:49] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.6955,	best estimator xgboost's best error=7.6955
[flaml.automl: 09-16 07:37:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:37:55] {3072} INFO -  at 10.5s,	estimator xgboost's best error=7.6955,	best estimator xgboost's best error=7.6955
[flaml.automl: 09-16 07:37:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:37:56] {3072} INFO -  at 11.7s,	estimator xgboost's best error=4.7411,	best estimator xgboost's best error=4.7411
[flaml.automl: 09-16 07:37:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:37:58] {3072} INFO -  at 13.2s,	estimator xgboost's best error=3.9169,	best estimator xgboost's best error=3.9169
[flaml.automl: 09-16 07:37:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:37:59] {3072} INFO -  at 14.8s,	estimator xgboost's best error=3.9169,	best estimator xgboost's best error=3.9169
[flaml.automl: 09-16 07:37:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:38:02] {3072} INFO -  at 17.3s,	estimator xgboost's best error=3.9169,	best estimator xgboost's best error=3.9169
[flaml.automl: 09-16 07:38:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:38:03] {3072} INFO -  at 18.4s,	estimator xgboost's best error=3.9169,	best estimator xgboost's best error=3.9169
[flaml.automl: 09-16 07:38:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:38:06] {3072} INFO -  at 21.1s,	estimator xgboost's best error=3.9169,	best estimator xgboost's best error=3.9169
[flaml.automl: 09-16 07:38:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:38:07] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.8751,	best estimator xgboost's best error=3.8751
[flaml.automl: 09-16 07:38:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:38:08] {3072} INFO -  at 23.8s,	estimator xgboost's best error=3.8751,	best estimator xgboost's best error=3.8751
[flaml.automl: 09-16 07:38:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:38:15] {3072} INFO -  at 30.3s,	estimator xgboost's best error=3.6237,	best estimator xgboost's best error=3.6237
[flaml.automl: 09-16 07:38:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:38:27] {3072} INFO -  at 42.3s,	estimator xgboost's best error=3.5360,	best estimator xgboost's best error=3.5360
[flaml.automl: 09-16 07:38:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 07:38:33] {3072} INFO -  at 48.7s,	estimator xgboost's best error=3.5360,	best estimator xgboost's best error=3.5360
[flaml.automl: 09-16 07:38:45] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 07:38:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:38:45] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:38:45] {2637} INFO - Time taken to find the best model: 42.26919865608215
[flaml.automl: 09-16 07:38:45] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43638}
NO2(0)最佳损失：-2.535967541542415
NO2(0)最好结果：{'pred_time': 8.57279260863125e-06, 'wall_clock_time': 42.26919865608215, 'metric_for_logging': {'pred_time': 8.57279260863125e-06}, 'val_loss': 3.535967541542415, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43638}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43638, 'experiment_tag': 'exp', 'time_total_s': 11.941104888916016}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8871485503476558
NO2(0)的mse=29.401431560969748
NO2(0)的mae=3.5466506569670497
NO2(0)的mar=0.1628438046005976
总共花费的时间为：61.40
邢台市
1078A
1079A
1080A
[flaml.automl: 09-16 07:48:53] {2390} INFO - task = regression
[flaml.automl: 09-16 07:48:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:48:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:48:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:48:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:48:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:48:54] {3025} INFO - Estimated sufficient time budget=12177s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 07:48:54] {3072} INFO -  at 1.4s,	estimator xgboost's best error=19.3344,	best estimator xgboost's best error=19.3344
[flaml.automl: 09-16 07:48:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:48:56] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.3183,	best estimator xgboost's best error=9.3183
[flaml.automl: 09-16 07:48:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:48:57] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.3183,	best estimator xgboost's best error=9.3183
[flaml.automl: 09-16 07:48:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:49:07] {3072} INFO -  at 14.7s,	estimator xgboost's best error=9.3183,	best estimator xgboost's best error=9.3183
[flaml.automl: 09-16 07:49:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:49:08] {3072} INFO -  at 15.8s,	estimator xgboost's best error=5.7616,	best estimator xgboost's best error=5.7616
[flaml.automl: 09-16 07:49:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:49:10] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.7754,	best estimator xgboost's best error=4.7754
[flaml.automl: 09-16 07:49:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:49:11] {3072} INFO -  at 19.0s,	estimator xgboost's best error=4.7754,	best estimator xgboost's best error=4.7754
[flaml.automl: 09-16 07:49:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:49:14] {3072} INFO -  at 21.4s,	estimator xgboost's best error=4.7754,	best estimator xgboost's best error=4.7754
[flaml.automl: 09-16 07:49:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:49:15] {3072} INFO -  at 22.6s,	estimator xgboost's best error=4.7754,	best estimator xgboost's best error=4.7754
[flaml.automl: 09-16 07:49:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:49:18] {3072} INFO -  at 25.2s,	estimator xgboost's best error=4.7754,	best estimator xgboost's best error=4.7754
[flaml.automl: 09-16 07:49:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:49:19] {3072} INFO -  at 26.3s,	estimator xgboost's best error=4.7754,	best estimator xgboost's best error=4.7754
[flaml.automl: 09-16 07:49:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:49:20] {3072} INFO -  at 27.4s,	estimator xgboost's best error=4.7754,	best estimator xgboost's best error=4.7754
[flaml.automl: 09-16 07:49:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:49:26] {3072} INFO -  at 33.8s,	estimator xgboost's best error=4.4459,	best estimator xgboost's best error=4.4459
[flaml.automl: 09-16 07:49:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:49:38] {3072} INFO -  at 45.7s,	estimator xgboost's best error=4.3558,	best estimator xgboost's best error=4.3558
[flaml.automl: 09-16 07:49:38] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 07:49:45] {3072} INFO -  at 52.2s,	estimator xgboost's best error=4.3558,	best estimator xgboost's best error=4.3558
[flaml.automl: 09-16 07:49:57] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 07:49:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:49:57] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:49:57] {2637} INFO - Time taken to find the best model: 45.74264407157898
[flaml.automl: 09-16 07:49:57] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.35577419534368
NO2(0)最好结果：{'pred_time': 1.2896407363761184e-05, 'wall_clock_time': 45.74264407157898, 'metric_for_logging': {'pred_time': 1.2896407363761184e-05}, 'val_loss': 4.35577419534368, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.916350603103638}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.869615377938865
NO2(0)的mse=39.736740039341036
NO2(0)的mae=4.406498722663293
NO2(0)的mar=0.19967276077815826
总共花费的时间为：64.67
太原市
1081A
1084A
1085A
1086A
1087A
3185A
[flaml.automl: 09-16 08:10:05] {2390} INFO - task = regression
[flaml.automl: 09-16 08:10:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:10:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:10:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:10:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:10:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:10:06] {3025} INFO - Estimated sufficient time budget=79545s. Estimated necessary time budget=80s.
[flaml.automl: 09-16 08:10:06] {3072} INFO -  at 1.5s,	estimator xgboost's best error=24.9350,	best estimator xgboost's best error=24.9350
[flaml.automl: 09-16 08:10:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:10:08] {3072} INFO -  at 3.7s,	estimator xgboost's best error=11.9918,	best estimator xgboost's best error=11.9918
[flaml.automl: 09-16 08:10:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:10:10] {3072} INFO -  at 4.8s,	estimator xgboost's best error=11.9918,	best estimator xgboost's best error=11.9918
[flaml.automl: 09-16 08:10:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:10:13] {3072} INFO -  at 8.6s,	estimator xgboost's best error=11.9918,	best estimator xgboost's best error=11.9918
[flaml.automl: 09-16 08:10:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:10:15] {3072} INFO -  at 9.8s,	estimator xgboost's best error=7.5515,	best estimator xgboost's best error=7.5515
[flaml.automl: 09-16 08:10:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:10:16] {3072} INFO -  at 11.3s,	estimator xgboost's best error=6.4053,	best estimator xgboost's best error=6.4053
[flaml.automl: 09-16 08:10:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:10:18] {3072} INFO -  at 13.0s,	estimator xgboost's best error=6.4053,	best estimator xgboost's best error=6.4053
[flaml.automl: 09-16 08:10:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:10:20] {3072} INFO -  at 15.4s,	estimator xgboost's best error=6.4053,	best estimator xgboost's best error=6.4053
[flaml.automl: 09-16 08:10:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:10:21] {3072} INFO -  at 16.5s,	estimator xgboost's best error=6.4053,	best estimator xgboost's best error=6.4053
[flaml.automl: 09-16 08:10:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:10:24] {3072} INFO -  at 19.2s,	estimator xgboost's best error=6.4053,	best estimator xgboost's best error=6.4053
[flaml.automl: 09-16 08:10:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:10:26] {3072} INFO -  at 20.8s,	estimator xgboost's best error=6.2670,	best estimator xgboost's best error=6.2670
[flaml.automl: 09-16 08:10:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:10:27] {3072} INFO -  at 22.0s,	estimator xgboost's best error=6.2670,	best estimator xgboost's best error=6.2670
[flaml.automl: 09-16 08:10:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:10:33] {3072} INFO -  at 28.5s,	estimator xgboost's best error=5.9921,	best estimator xgboost's best error=5.9921
[flaml.automl: 09-16 08:10:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:10:45] {3072} INFO -  at 40.5s,	estimator xgboost's best error=5.8308,	best estimator xgboost's best error=5.8308
[flaml.automl: 09-16 08:10:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:10:52] {3072} INFO -  at 46.9s,	estimator xgboost's best error=5.8308,	best estimator xgboost's best error=5.8308
[flaml.automl: 09-16 08:10:52] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 08:11:04] {3072} INFO -  at 59.3s,	estimator xgboost's best error=5.8308,	best estimator xgboost's best error=5.8308
[flaml.automl: 09-16 08:11:16] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 08:11:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:11:16] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:11:16] {2637} INFO - Time taken to find the best model: 40.50096249580383
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 66399}
NO2(0)最佳损失：-4.830825846219134
NO2(0)最好结果：{'pred_time': 5.375825153453204e-06, 'wall_clock_time': 40.50096249580383, 'metric_for_logging': {'pred_time': 5.375825153453204e-06}, 'val_loss': 5.830825846219134, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 66399}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 66399, 'experiment_tag': 'exp', 'time_total_s': 12.006891965866089}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8438603325598638
NO2(0)的mse=75.20729573176311
NO2(0)的mae=5.8275656337765955
NO2(0)的mar=0.1850876372005473
总共花费的时间为：72.40
呼和浩特市
1095A
1097A
3698A
3699A
[flaml.automl: 09-16 08:24:03] {2390} INFO - task = regression
[flaml.automl: 09-16 08:24:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:24:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:24:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:24:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:24:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:24:04] {3025} INFO - Estimated sufficient time budget=53605s. Estimated necessary time budget=54s.
[flaml.automl: 09-16 08:24:04] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.7991,	best estimator xgboost's best error=17.7991
[flaml.automl: 09-16 08:24:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:24:06] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.0762,	best estimator xgboost's best error=9.0762
[flaml.automl: 09-16 08:24:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:24:08] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.0762,	best estimator xgboost's best error=9.0762
[flaml.automl: 09-16 08:24:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:24:13] {3072} INFO -  at 10.6s,	estimator xgboost's best error=9.0762,	best estimator xgboost's best error=9.0762
[flaml.automl: 09-16 08:24:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:24:15] {3072} INFO -  at 11.8s,	estimator xgboost's best error=6.4481,	best estimator xgboost's best error=6.4481
[flaml.automl: 09-16 08:24:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:24:16] {3072} INFO -  at 13.3s,	estimator xgboost's best error=5.6149,	best estimator xgboost's best error=5.6149
[flaml.automl: 09-16 08:24:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:24:18] {3072} INFO -  at 15.0s,	estimator xgboost's best error=5.6149,	best estimator xgboost's best error=5.6149
[flaml.automl: 09-16 08:24:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:24:20] {3072} INFO -  at 17.4s,	estimator xgboost's best error=5.6149,	best estimator xgboost's best error=5.6149
[flaml.automl: 09-16 08:24:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:24:21] {3072} INFO -  at 18.6s,	estimator xgboost's best error=5.6149,	best estimator xgboost's best error=5.6149
[flaml.automl: 09-16 08:24:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:24:24] {3072} INFO -  at 21.2s,	estimator xgboost's best error=5.6149,	best estimator xgboost's best error=5.6149
[flaml.automl: 09-16 08:24:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:24:26] {3072} INFO -  at 22.8s,	estimator xgboost's best error=5.5747,	best estimator xgboost's best error=5.5747
[flaml.automl: 09-16 08:24:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:24:27] {3072} INFO -  at 24.0s,	estimator xgboost's best error=5.5747,	best estimator xgboost's best error=5.5747
[flaml.automl: 09-16 08:24:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:24:33] {3072} INFO -  at 30.6s,	estimator xgboost's best error=5.3341,	best estimator xgboost's best error=5.3341
[flaml.automl: 09-16 08:24:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:24:45] {3072} INFO -  at 42.7s,	estimator xgboost's best error=5.1680,	best estimator xgboost's best error=5.1680
[flaml.automl: 09-16 08:24:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:24:52] {3072} INFO -  at 49.2s,	estimator xgboost's best error=5.1680,	best estimator xgboost's best error=5.1680
[flaml.automl: 09-16 08:25:04] {3335} INFO - retrain xgboost for 11.8s
[flaml.automl: 09-16 08:25:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:25:04] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:25:04] {2637} INFO - Time taken to find the best model: 42.65165114402771
[flaml.automl: 09-16 08:25:04] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 44290}
NO2(0)最佳损失：-4.167989635215645
NO2(0)最好结果：{'pred_time': 9.739413488109826e-06, 'wall_clock_time': 42.65165114402771, 'metric_for_logging': {'pred_time': 9.739413488109826e-06}, 'val_loss': 5.167989635215645, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 44290}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 44290, 'experiment_tag': 'exp', 'time_total_s': 12.09179425239563}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8216581269472875
NO2(0)的mse=58.49286433839147
NO2(0)的mae=4.932430014482971
NO2(0)的mar=0.23259151386386606
总共花费的时间为：61.73
沈阳市
1098A
1099A
1100A
1104A
1105A
1106A
2900A
[flaml.automl: 09-16 08:49:20] {2390} INFO - task = regression
[flaml.automl: 09-16 08:49:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:49:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:49:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:49:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:49:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:49:21] {3025} INFO - Estimated sufficient time budget=90580s. Estimated necessary time budget=91s.
[flaml.automl: 09-16 08:49:21] {3072} INFO -  at 1.6s,	estimator xgboost's best error=18.7419,	best estimator xgboost's best error=18.7419
[flaml.automl: 09-16 08:49:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:49:24] {3072} INFO -  at 3.7s,	estimator xgboost's best error=9.1004,	best estimator xgboost's best error=9.1004
[flaml.automl: 09-16 08:49:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:49:25] {3072} INFO -  at 4.9s,	estimator xgboost's best error=9.1004,	best estimator xgboost's best error=9.1004
[flaml.automl: 09-16 08:49:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:49:28] {3072} INFO -  at 8.2s,	estimator xgboost's best error=9.1004,	best estimator xgboost's best error=9.1004
[flaml.automl: 09-16 08:49:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:49:29] {3072} INFO -  at 9.3s,	estimator xgboost's best error=5.8827,	best estimator xgboost's best error=5.8827
[flaml.automl: 09-16 08:49:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:49:31] {3072} INFO -  at 10.9s,	estimator xgboost's best error=5.0570,	best estimator xgboost's best error=5.0570
[flaml.automl: 09-16 08:49:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:49:32] {3072} INFO -  at 12.5s,	estimator xgboost's best error=5.0570,	best estimator xgboost's best error=5.0570
[flaml.automl: 09-16 08:49:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:49:35] {3072} INFO -  at 15.0s,	estimator xgboost's best error=5.0570,	best estimator xgboost's best error=5.0570
[flaml.automl: 09-16 08:49:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:49:36] {3072} INFO -  at 16.1s,	estimator xgboost's best error=5.0570,	best estimator xgboost's best error=5.0570
[flaml.automl: 09-16 08:49:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:49:39] {3072} INFO -  at 18.8s,	estimator xgboost's best error=5.0570,	best estimator xgboost's best error=5.0570
[flaml.automl: 09-16 08:49:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:49:40] {3072} INFO -  at 20.4s,	estimator xgboost's best error=5.0086,	best estimator xgboost's best error=5.0086
[flaml.automl: 09-16 08:49:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:49:42] {3072} INFO -  at 21.7s,	estimator xgboost's best error=5.0086,	best estimator xgboost's best error=5.0086
[flaml.automl: 09-16 08:49:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:49:48] {3072} INFO -  at 28.2s,	estimator xgboost's best error=4.7415,	best estimator xgboost's best error=4.7415
[flaml.automl: 09-16 08:49:48] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:50:00] {3072} INFO -  at 40.4s,	estimator xgboost's best error=4.6601,	best estimator xgboost's best error=4.6601
[flaml.automl: 09-16 08:50:00] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:50:07] {3072} INFO -  at 46.9s,	estimator xgboost's best error=4.6601,	best estimator xgboost's best error=4.6601
[flaml.automl: 09-16 08:50:07] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 08:50:19] {3072} INFO -  at 58.9s,	estimator xgboost's best error=4.6601,	best estimator xgboost's best error=4.6601
[flaml.automl: 09-16 08:50:39] {3335} INFO - retrain xgboost for 20.4s
[flaml.automl: 09-16 08:50:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:50:39] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:50:39] {2637} INFO - Time taken to find the best model: 40.37429976463318
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 75450}
NO2(0)最佳损失：-3.66006368284917
NO2(0)最好结果：{'pred_time': 5.737999240860684e-06, 'wall_clock_time': 40.37429976463318, 'metric_for_logging': {'pred_time': 5.737999240860684e-06}, 'val_loss': 4.66006368284917, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 75450}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 75450, 'experiment_tag': 'exp', 'time_total_s': 12.16405701637268}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8403975811004181
NO2(0)的mse=44.893409809742515
NO2(0)的mae=4.608019265678066
NO2(0)的mar=0.1933140571064888
总共花费的时间为：80.49
大连市
1110A
1117A
[flaml.automl: 09-16 08:57:23] {2390} INFO - task = regression
[flaml.automl: 09-16 08:57:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:57:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:57:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:57:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:57:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:57:24] {3025} INFO - Estimated sufficient time budget=12092s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 08:57:24] {3072} INFO -  at 1.3s,	estimator xgboost's best error=18.3918,	best estimator xgboost's best error=18.3918
[flaml.automl: 09-16 08:57:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:57:26] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.8910,	best estimator xgboost's best error=8.8910
[flaml.automl: 09-16 08:57:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:57:27] {3072} INFO -  at 4.6s,	estimator xgboost's best error=8.8910,	best estimator xgboost's best error=8.8910
[flaml.automl: 09-16 08:57:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:57:37] {3072} INFO -  at 14.1s,	estimator xgboost's best error=8.8910,	best estimator xgboost's best error=8.8910
[flaml.automl: 09-16 08:57:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:57:38] {3072} INFO -  at 15.3s,	estimator xgboost's best error=6.2086,	best estimator xgboost's best error=6.2086
[flaml.automl: 09-16 08:57:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:57:39] {3072} INFO -  at 16.9s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:57:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:57:41] {3072} INFO -  at 18.5s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:57:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:57:44] {3072} INFO -  at 21.0s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:57:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:57:45] {3072} INFO -  at 22.1s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:57:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:57:47] {3072} INFO -  at 24.6s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:57:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:57:48] {3072} INFO -  at 25.7s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:57:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:57:49] {3072} INFO -  at 26.9s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:57:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:57:55] {3072} INFO -  at 32.9s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:57:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:57:58] {3072} INFO -  at 35.8s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:57:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:58:00] {3072} INFO -  at 37.6s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:58:00] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 08:58:03] {3072} INFO -  at 40.1s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:58:03] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 08:58:04] {3072} INFO -  at 41.2s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:58:04] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 08:58:05] {3072} INFO -  at 42.4s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:58:05] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 08:58:09] {3072} INFO -  at 46.2s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:58:09] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 08:58:10] {3072} INFO -  at 47.4s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:58:10] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 08:58:14] {3072} INFO -  at 51.3s,	estimator xgboost's best error=5.5427,	best estimator xgboost's best error=5.5427
[flaml.automl: 09-16 08:58:14] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 08:58:20] {3072} INFO -  at 57.8s,	estimator xgboost's best error=5.4001,	best estimator xgboost's best error=5.4001
[flaml.automl: 09-16 08:58:27] {3335} INFO - retrain xgboost for 6.4s
[flaml.automl: 09-16 08:58:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8492588522931204, colsample_bynode=1,
             colsample_bytree=0.6537360078914272, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.32648040091234304,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=1.728347470404329, missing=nan,
             monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3980257413064903, scale_pos_weight=1,
             subsample=0.8883759987059676, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:58:27] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:58:27] {2637} INFO - Time taken to find the best model: 57.755674839019775
[flaml.automl: 09-16 08:58:27] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 10, 'max_leaves': 11, 'min_child_weight': 1.728347470404329, 'learning_rate': 0.32648040091234304, 'subsample': 0.8883759987059676, 'colsample_bylevel': 0.8492588522931204, 'colsample_bytree': 0.6537360078914272, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3980257413064903}
NO2(0)最佳损失：-4.400073872981764
NO2(0)最好结果：{'pred_time': 1.6462524267588316e-05, 'wall_clock_time': 57.755674839019775, 'metric_for_logging': {'pred_time': 1.6462524267588316e-05}, 'val_loss': 5.400073872981764, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 11, 'min_child_weight': 1.728347470404329, 'learning_rate': 0.32648040091234304, 'subsample': 0.8883759987059676, 'colsample_bylevel': 0.8492588522931204, 'colsample_bytree': 0.6537360078914272, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3980257413064903}, 'config/n_estimators': 10, 'config/max_leaves': 11, 'config/min_child_weight': 1.728347470404329, 'config/learning_rate': 0.32648040091234304, 'config/subsample': 0.8883759987059676, 'config/colsample_bylevel': 0.8492588522931204, 'config/colsample_bytree': 0.6537360078914272, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.3980257413064903, 'experiment_tag': 'exp', 'time_total_s': 6.455205917358398}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8492588522931204, colsample_bynode=1,
             colsample_bytree=0.6537360078914272, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.32648040091234304,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=1.728347470404329, missing=nan,
             monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3980257413064903, scale_pos_weight=1,
             subsample=0.8883759987059676, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6837662745497807
NO2(0)的mse=58.03821852436238
NO2(0)的mae=5.306128419314355
NO2(0)的mar=0.20683940968424505
总共花费的时间为：64.48
长春市
1119A
1120A
1121A
1122A
1124A
1125A
1126A
1128A
[flaml.automl: 09-16 09:23:37] {2390} INFO - task = regression
[flaml.automl: 09-16 09:23:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 09:23:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 09:23:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 09:23:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 09:23:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 09:23:39] {3025} INFO - Estimated sufficient time budget=98166s. Estimated necessary time budget=98s.
[flaml.automl: 09-16 09:23:39] {3072} INFO -  at 1.6s,	estimator xgboost's best error=17.0615,	best estimator xgboost's best error=17.0615
[flaml.automl: 09-16 09:23:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 09:23:41] {3072} INFO -  at 3.7s,	estimator xgboost's best error=8.3446,	best estimator xgboost's best error=8.3446
[flaml.automl: 09-16 09:23:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 09:23:42] {3072} INFO -  at 4.9s,	estimator xgboost's best error=8.3446,	best estimator xgboost's best error=8.3446
[flaml.automl: 09-16 09:23:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 09:23:45] {3072} INFO -  at 7.6s,	estimator xgboost's best error=8.3446,	best estimator xgboost's best error=8.3446
[flaml.automl: 09-16 09:23:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 09:23:46] {3072} INFO -  at 8.8s,	estimator xgboost's best error=5.6869,	best estimator xgboost's best error=5.6869
[flaml.automl: 09-16 09:23:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 09:23:47] {3072} INFO -  at 10.3s,	estimator xgboost's best error=4.8853,	best estimator xgboost's best error=4.8853
[flaml.automl: 09-16 09:23:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 09:23:49] {3072} INFO -  at 11.9s,	estimator xgboost's best error=4.8853,	best estimator xgboost's best error=4.8853
[flaml.automl: 09-16 09:23:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 09:23:51] {3072} INFO -  at 14.4s,	estimator xgboost's best error=4.8853,	best estimator xgboost's best error=4.8853
[flaml.automl: 09-16 09:23:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 09:23:53] {3072} INFO -  at 15.5s,	estimator xgboost's best error=4.8853,	best estimator xgboost's best error=4.8853
[flaml.automl: 09-16 09:23:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 09:23:55] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.8853,	best estimator xgboost's best error=4.8853
[flaml.automl: 09-16 09:23:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 09:23:56] {3072} INFO -  at 19.3s,	estimator xgboost's best error=4.8853,	best estimator xgboost's best error=4.8853
[flaml.automl: 09-16 09:23:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 09:23:58] {3072} INFO -  at 20.5s,	estimator xgboost's best error=4.8853,	best estimator xgboost's best error=4.8853
[flaml.automl: 09-16 09:23:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 09:24:04] {3072} INFO -  at 26.9s,	estimator xgboost's best error=4.5858,	best estimator xgboost's best error=4.5858
[flaml.automl: 09-16 09:24:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 09:24:16] {3072} INFO -  at 39.0s,	estimator xgboost's best error=4.4571,	best estimator xgboost's best error=4.4571
[flaml.automl: 09-16 09:24:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 09:24:23] {3072} INFO -  at 45.6s,	estimator xgboost's best error=4.4571,	best estimator xgboost's best error=4.4571
[flaml.automl: 09-16 09:24:23] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 09:24:36] {3072} INFO -  at 59.3s,	estimator xgboost's best error=4.4571,	best estimator xgboost's best error=4.4571
[flaml.automl: 09-16 09:24:49] {3335} INFO - retrain xgboost for 12.2s
[flaml.automl: 09-16 09:24:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 09:24:49] {2636} INFO - fit succeeded
[flaml.automl: 09-16 09:24:49] {2637} INFO - Time taken to find the best model: 39.04842209815979
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 84434}
NO2(0)最佳损失：-3.4570881255118735
NO2(0)最好结果：{'pred_time': 4.31021249413719e-06, 'wall_clock_time': 39.04842209815979, 'metric_for_logging': {'pred_time': 4.31021249413719e-06}, 'val_loss': 4.4570881255118735, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 84434}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 84434, 'experiment_tag': 'exp', 'time_total_s': 12.126605033874512}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8340681892362118
NO2(0)的mse=46.4413271169209
NO2(0)的mae=4.588023871229139
NO2(0)的mar=0.20597645185617783
总共花费的时间为：72.90
哈尔滨市
1129A
1130A
1139A
1140A
[flaml.automl: 09-16 09:38:19] {2390} INFO - task = regression
[flaml.automl: 09-16 09:38:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 09:38:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 09:38:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 09:38:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 09:38:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 09:38:20] {3025} INFO - Estimated sufficient time budget=48555s. Estimated necessary time budget=49s.
[flaml.automl: 09-16 09:38:20] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.7707,	best estimator xgboost's best error=15.7707
[flaml.automl: 09-16 09:38:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 09:38:22] {3072} INFO -  at 3.4s,	estimator xgboost's best error=7.7765,	best estimator xgboost's best error=7.7765
[flaml.automl: 09-16 09:38:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 09:38:23] {3072} INFO -  at 4.6s,	estimator xgboost's best error=7.7765,	best estimator xgboost's best error=7.7765
[flaml.automl: 09-16 09:38:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 09:38:30] {3072} INFO -  at 10.8s,	estimator xgboost's best error=7.7765,	best estimator xgboost's best error=7.7765
[flaml.automl: 09-16 09:38:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 09:38:31] {3072} INFO -  at 11.9s,	estimator xgboost's best error=5.4356,	best estimator xgboost's best error=5.4356
[flaml.automl: 09-16 09:38:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 09:38:32] {3072} INFO -  at 13.5s,	estimator xgboost's best error=4.7868,	best estimator xgboost's best error=4.7868
[flaml.automl: 09-16 09:38:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 09:38:34] {3072} INFO -  at 15.1s,	estimator xgboost's best error=4.7868,	best estimator xgboost's best error=4.7868
[flaml.automl: 09-16 09:38:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 09:38:36] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.7868,	best estimator xgboost's best error=4.7868
[flaml.automl: 09-16 09:38:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 09:38:37] {3072} INFO -  at 18.7s,	estimator xgboost's best error=4.7868,	best estimator xgboost's best error=4.7868
[flaml.automl: 09-16 09:38:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 09:38:40] {3072} INFO -  at 21.4s,	estimator xgboost's best error=4.7868,	best estimator xgboost's best error=4.7868
[flaml.automl: 09-16 09:38:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 09:38:42] {3072} INFO -  at 23.0s,	estimator xgboost's best error=4.7467,	best estimator xgboost's best error=4.7467
[flaml.automl: 09-16 09:38:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 09:38:43] {3072} INFO -  at 24.1s,	estimator xgboost's best error=4.7467,	best estimator xgboost's best error=4.7467
[flaml.automl: 09-16 09:38:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 09:38:49] {3072} INFO -  at 30.6s,	estimator xgboost's best error=4.5484,	best estimator xgboost's best error=4.5484
[flaml.automl: 09-16 09:38:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 09:39:01] {3072} INFO -  at 42.7s,	estimator xgboost's best error=4.4912,	best estimator xgboost's best error=4.4912
[flaml.automl: 09-16 09:39:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 09:39:08] {3072} INFO -  at 49.2s,	estimator xgboost's best error=4.4912,	best estimator xgboost's best error=4.4912
[flaml.automl: 09-16 09:39:20] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 09:39:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 09:39:20] {2636} INFO - fit succeeded
[flaml.automl: 09-16 09:39:20] {2637} INFO - Time taken to find the best model: 42.71977663040161
[flaml.automl: 09-16 09:39:20] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41879}
NO2(0)最佳损失：-3.4911619329278336
NO2(0)最好结果：{'pred_time': 8.767184957087065e-06, 'wall_clock_time': 42.71977663040161, 'metric_for_logging': {'pred_time': 8.767184957087065e-06}, 'val_loss': 4.491161932927834, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41879}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41879, 'experiment_tag': 'exp', 'time_total_s': 12.112669229507446}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8457047909545279
NO2(0)的mse=42.71928529249838
NO2(0)的mae=4.429006054147349
NO2(0)的mar=0.23298434966459433
总共花费的时间为：62.04
上海市
1143A
1144A
1145A
1148A
1150A
3265A
3266A
3269A
3270A
3271A
3272A
3273A
3274A
3544A
[flaml.automl: 09-16 10:20:46] {2390} INFO - task = regression
[flaml.automl: 09-16 10:20:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 10:20:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 10:20:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 10:20:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 10:20:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 10:20:47] {3025} INFO - Estimated sufficient time budget=182508s. Estimated necessary time budget=183s.
[flaml.automl: 09-16 10:20:47] {3072} INFO -  at 2.2s,	estimator xgboost's best error=18.2381,	best estimator xgboost's best error=18.2381
[flaml.automl: 09-16 10:20:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 10:20:49] {3072} INFO -  at 3.8s,	estimator xgboost's best error=11.2301,	best estimator xgboost's best error=11.2301
[flaml.automl: 09-16 10:20:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 10:20:50] {3072} INFO -  at 5.0s,	estimator xgboost's best error=11.2301,	best estimator xgboost's best error=11.2301
[flaml.automl: 09-16 10:20:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 10:20:52] {3072} INFO -  at 6.7s,	estimator xgboost's best error=11.2301,	best estimator xgboost's best error=11.2301
[flaml.automl: 09-16 10:20:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 10:20:53] {3072} INFO -  at 7.8s,	estimator xgboost's best error=5.3019,	best estimator xgboost's best error=5.3019
[flaml.automl: 09-16 10:20:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 10:20:54] {3072} INFO -  at 9.4s,	estimator xgboost's best error=4.4794,	best estimator xgboost's best error=4.4794
[flaml.automl: 09-16 10:20:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 10:20:56] {3072} INFO -  at 10.8s,	estimator xgboost's best error=4.4794,	best estimator xgboost's best error=4.4794
[flaml.automl: 09-16 10:20:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 10:20:57] {3072} INFO -  at 12.1s,	estimator xgboost's best error=4.4794,	best estimator xgboost's best error=4.4794
[flaml.automl: 09-16 10:20:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 10:20:58] {3072} INFO -  at 13.3s,	estimator xgboost's best error=4.4794,	best estimator xgboost's best error=4.4794
[flaml.automl: 09-16 10:20:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 10:21:00] {3072} INFO -  at 14.7s,	estimator xgboost's best error=4.4794,	best estimator xgboost's best error=4.4794
[flaml.automl: 09-16 10:21:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 10:21:01] {3072} INFO -  at 16.4s,	estimator xgboost's best error=4.4761,	best estimator xgboost's best error=4.4761
[flaml.automl: 09-16 10:21:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 10:21:03] {3072} INFO -  at 17.7s,	estimator xgboost's best error=4.4761,	best estimator xgboost's best error=4.4761
[flaml.automl: 09-16 10:21:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 10:21:09] {3072} INFO -  at 24.3s,	estimator xgboost's best error=4.1205,	best estimator xgboost's best error=4.1205
[flaml.automl: 09-16 10:21:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 10:21:21] {3072} INFO -  at 36.5s,	estimator xgboost's best error=4.0502,	best estimator xgboost's best error=4.0502
[flaml.automl: 09-16 10:21:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 10:21:28] {3072} INFO -  at 43.1s,	estimator xgboost's best error=4.0502,	best estimator xgboost's best error=4.0502
[flaml.automl: 09-16 10:21:28] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 10:21:44] {3072} INFO -  at 59.1s,	estimator xgboost's best error=4.0100,	best estimator xgboost's best error=4.0100
[flaml.automl: 09-16 10:22:06] {3335} INFO - retrain xgboost for 21.6s
[flaml.automl: 09-16 10:22:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 10:22:06] {2636} INFO - fit succeeded
[flaml.automl: 09-16 10:22:06] {2637} INFO - Time taken to find the best model: 59.103331089019775
[flaml.automl: 09-16 10:22:06] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 151291}
NO2(0)最佳损失：-3.0100153831615586
NO2(0)最好结果：{'pred_time': 2.8817858306538037e-06, 'wall_clock_time': 59.103331089019775, 'metric_for_logging': {'pred_time': 2.8817858306538037e-06}, 'val_loss': 4.010015383161559, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 151291}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 151291, 'experiment_tag': 'exp', 'time_total_s': 15.991665124893188}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8882289397845881
NO2(0)的mse=35.865399157450966
NO2(0)的mae=3.9529733411606354
NO2(0)的mar=0.15414005091115807
总共花费的时间为：83.01
南京市
1151A
1152A
1153A
1154A
3423A
3424A
3427A
[flaml.automl: 09-16 10:44:03] {2390} INFO - task = regression
[flaml.automl: 09-16 10:44:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 10:44:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 10:44:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 10:44:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 10:44:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 10:44:04] {3025} INFO - Estimated sufficient time budget=89734s. Estimated necessary time budget=90s.
[flaml.automl: 09-16 10:44:04] {3072} INFO -  at 1.6s,	estimator xgboost's best error=17.7532,	best estimator xgboost's best error=17.7532
[flaml.automl: 09-16 10:44:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 10:44:06] {3072} INFO -  at 3.7s,	estimator xgboost's best error=8.6005,	best estimator xgboost's best error=8.6005
[flaml.automl: 09-16 10:44:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 10:44:07] {3072} INFO -  at 4.9s,	estimator xgboost's best error=8.6005,	best estimator xgboost's best error=8.6005
[flaml.automl: 09-16 10:44:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 10:44:11] {3072} INFO -  at 8.2s,	estimator xgboost's best error=8.6005,	best estimator xgboost's best error=8.6005
[flaml.automl: 09-16 10:44:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 10:44:12] {3072} INFO -  at 9.3s,	estimator xgboost's best error=5.5083,	best estimator xgboost's best error=5.5083
[flaml.automl: 09-16 10:44:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 10:44:13] {3072} INFO -  at 10.9s,	estimator xgboost's best error=4.7122,	best estimator xgboost's best error=4.7122
[flaml.automl: 09-16 10:44:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 10:44:15] {3072} INFO -  at 12.5s,	estimator xgboost's best error=4.7122,	best estimator xgboost's best error=4.7122
[flaml.automl: 09-16 10:44:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 10:44:17] {3072} INFO -  at 15.0s,	estimator xgboost's best error=4.7122,	best estimator xgboost's best error=4.7122
[flaml.automl: 09-16 10:44:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 10:44:18] {3072} INFO -  at 16.1s,	estimator xgboost's best error=4.7122,	best estimator xgboost's best error=4.7122
[flaml.automl: 09-16 10:44:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 10:44:21] {3072} INFO -  at 18.8s,	estimator xgboost's best error=4.7122,	best estimator xgboost's best error=4.7122
[flaml.automl: 09-16 10:44:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 10:44:23] {3072} INFO -  at 20.4s,	estimator xgboost's best error=4.6757,	best estimator xgboost's best error=4.6757
[flaml.automl: 09-16 10:44:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 10:44:24] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.6757,	best estimator xgboost's best error=4.6757
[flaml.automl: 09-16 10:44:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 10:44:30] {3072} INFO -  at 28.1s,	estimator xgboost's best error=4.4356,	best estimator xgboost's best error=4.4356
[flaml.automl: 09-16 10:44:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 10:44:43] {3072} INFO -  at 40.3s,	estimator xgboost's best error=4.3616,	best estimator xgboost's best error=4.3616
[flaml.automl: 09-16 10:44:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 10:44:49] {3072} INFO -  at 46.8s,	estimator xgboost's best error=4.3616,	best estimator xgboost's best error=4.3616
[flaml.automl: 09-16 10:44:49] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 10:45:02] {3072} INFO -  at 59.3s,	estimator xgboost's best error=4.3380,	best estimator xgboost's best error=4.3380
[flaml.automl: 09-16 10:45:23] {3335} INFO - retrain xgboost for 21.4s
[flaml.automl: 09-16 10:45:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 10:45:23] {2636} INFO - fit succeeded
[flaml.automl: 09-16 10:45:23] {2637} INFO - Time taken to find the best model: 59.3044011592865
[flaml.automl: 09-16 10:45:23] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 75059}
NO2(0)最佳损失：-3.33801840202128
NO2(0)最好结果：{'pred_time': 5.002530644551742e-06, 'wall_clock_time': 59.3044011592865, 'metric_for_logging': {'pred_time': 5.002530644551742e-06}, 'val_loss': 4.33801840202128, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 75059}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 75059, 'experiment_tag': 'exp', 'time_total_s': 12.521543502807617}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8592220583055984
NO2(0)的mse=41.63587957307862
NO2(0)的mae=4.296870059413639
NO2(0)的mar=0.19365658711652034
总共花费的时间为：81.98
苏州市
1160A
1164A
1165A
1166A
1167A
3289A
3290A
3425A
3431A
[flaml.automl: 09-16 11:13:12] {2390} INFO - task = regression
[flaml.automl: 09-16 11:13:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:13:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:13:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:13:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:13:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:13:13] {3025} INFO - Estimated sufficient time budget=112874s. Estimated necessary time budget=113s.
[flaml.automl: 09-16 11:13:13] {3072} INFO -  at 1.7s,	estimator xgboost's best error=17.4496,	best estimator xgboost's best error=17.4496
[flaml.automl: 09-16 11:13:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:13:15] {3072} INFO -  at 3.8s,	estimator xgboost's best error=8.4580,	best estimator xgboost's best error=8.4580
[flaml.automl: 09-16 11:13:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:13:16] {3072} INFO -  at 5.0s,	estimator xgboost's best error=8.4580,	best estimator xgboost's best error=8.4580
[flaml.automl: 09-16 11:13:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:13:19] {3072} INFO -  at 7.7s,	estimator xgboost's best error=8.4580,	best estimator xgboost's best error=8.4580
[flaml.automl: 09-16 11:13:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:13:20] {3072} INFO -  at 8.9s,	estimator xgboost's best error=5.4711,	best estimator xgboost's best error=5.4711
[flaml.automl: 09-16 11:13:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:13:22] {3072} INFO -  at 10.4s,	estimator xgboost's best error=4.5436,	best estimator xgboost's best error=4.5436
[flaml.automl: 09-16 11:13:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:13:23] {3072} INFO -  at 12.1s,	estimator xgboost's best error=4.5436,	best estimator xgboost's best error=4.5436
[flaml.automl: 09-16 11:13:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:13:25] {3072} INFO -  at 13.9s,	estimator xgboost's best error=4.5436,	best estimator xgboost's best error=4.5436
[flaml.automl: 09-16 11:13:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:13:26] {3072} INFO -  at 15.1s,	estimator xgboost's best error=4.5436,	best estimator xgboost's best error=4.5436
[flaml.automl: 09-16 11:13:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:13:29] {3072} INFO -  at 17.1s,	estimator xgboost's best error=4.5436,	best estimator xgboost's best error=4.5436
[flaml.automl: 09-16 11:13:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:13:30] {3072} INFO -  at 18.8s,	estimator xgboost's best error=4.5436,	best estimator xgboost's best error=4.5436
[flaml.automl: 09-16 11:13:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:13:31] {3072} INFO -  at 19.9s,	estimator xgboost's best error=4.5436,	best estimator xgboost's best error=4.5436
[flaml.automl: 09-16 11:13:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:13:38] {3072} INFO -  at 26.5s,	estimator xgboost's best error=4.2143,	best estimator xgboost's best error=4.2143
[flaml.automl: 09-16 11:13:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:13:50] {3072} INFO -  at 38.5s,	estimator xgboost's best error=4.0960,	best estimator xgboost's best error=4.0960
[flaml.automl: 09-16 11:13:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:13:56] {3072} INFO -  at 45.1s,	estimator xgboost's best error=4.0960,	best estimator xgboost's best error=4.0960
[flaml.automl: 09-16 11:13:56] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 11:14:11] {3072} INFO -  at 59.8s,	estimator xgboost's best error=4.0460,	best estimator xgboost's best error=4.0460
[flaml.automl: 09-16 11:14:33] {3335} INFO - retrain xgboost for 21.6s
[flaml.automl: 09-16 11:14:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:14:33] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:14:33] {2637} INFO - Time taken to find the best model: 59.798765659332275
[flaml.automl: 09-16 11:14:33] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 95267}
NO2(0)最佳损失：-3.0460321767645926
NO2(0)最好结果：{'pred_time': 4.104533718908394e-06, 'wall_clock_time': 59.798765659332275, 'metric_for_logging': {'pred_time': 4.104533718908394e-06}, 'val_loss': 4.046032176764593, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 95267}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 95267, 'experiment_tag': 'exp', 'time_total_s': 14.736100912094116}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8829007868877268
NO2(0)的mse=36.70588220546079
NO2(0)的mae=4.060076511417029
NO2(0)的mar=0.191664150286208
总共花费的时间为：82.99
南通市
1168A
1169A
1171A
1172A
3291A
3432A
[flaml.automl: 09-16 11:32:33] {2390} INFO - task = regression
[flaml.automl: 09-16 11:32:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:32:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:32:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:32:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:32:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:32:35] {3025} INFO - Estimated sufficient time budget=75231s. Estimated necessary time budget=75s.
[flaml.automl: 09-16 11:32:35] {3072} INFO -  at 1.5s,	estimator xgboost's best error=15.3689,	best estimator xgboost's best error=15.3689
[flaml.automl: 09-16 11:32:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:32:37] {3072} INFO -  at 3.6s,	estimator xgboost's best error=7.7219,	best estimator xgboost's best error=7.7219
[flaml.automl: 09-16 11:32:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:32:38] {3072} INFO -  at 4.8s,	estimator xgboost's best error=7.7219,	best estimator xgboost's best error=7.7219
[flaml.automl: 09-16 11:32:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:32:42] {3072} INFO -  at 9.0s,	estimator xgboost's best error=7.7219,	best estimator xgboost's best error=7.7219
[flaml.automl: 09-16 11:32:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:32:43] {3072} INFO -  at 10.1s,	estimator xgboost's best error=5.4372,	best estimator xgboost's best error=5.4372
[flaml.automl: 09-16 11:32:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:32:45] {3072} INFO -  at 11.7s,	estimator xgboost's best error=4.6200,	best estimator xgboost's best error=4.6200
[flaml.automl: 09-16 11:32:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:32:46] {3072} INFO -  at 13.3s,	estimator xgboost's best error=4.6200,	best estimator xgboost's best error=4.6200
[flaml.automl: 09-16 11:32:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:32:49] {3072} INFO -  at 15.7s,	estimator xgboost's best error=4.6200,	best estimator xgboost's best error=4.6200
[flaml.automl: 09-16 11:32:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:32:50] {3072} INFO -  at 16.9s,	estimator xgboost's best error=4.6200,	best estimator xgboost's best error=4.6200
[flaml.automl: 09-16 11:32:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:32:53] {3072} INFO -  at 19.6s,	estimator xgboost's best error=4.6200,	best estimator xgboost's best error=4.6200
[flaml.automl: 09-16 11:32:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:32:54] {3072} INFO -  at 21.2s,	estimator xgboost's best error=4.5792,	best estimator xgboost's best error=4.5792
[flaml.automl: 09-16 11:32:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:32:56] {3072} INFO -  at 22.4s,	estimator xgboost's best error=4.5792,	best estimator xgboost's best error=4.5792
[flaml.automl: 09-16 11:32:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:33:02] {3072} INFO -  at 29.0s,	estimator xgboost's best error=4.3559,	best estimator xgboost's best error=4.3559
[flaml.automl: 09-16 11:33:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:33:14] {3072} INFO -  at 41.1s,	estimator xgboost's best error=4.2383,	best estimator xgboost's best error=4.2383
[flaml.automl: 09-16 11:33:14] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:33:21] {3072} INFO -  at 47.6s,	estimator xgboost's best error=4.2383,	best estimator xgboost's best error=4.2383
[flaml.automl: 09-16 11:33:21] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 11:33:32] {3072} INFO -  at 58.9s,	estimator xgboost's best error=4.2383,	best estimator xgboost's best error=4.2383
[flaml.automl: 09-16 11:33:44] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 11:33:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:33:44] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:33:44] {2637} INFO - Time taken to find the best model: 41.05632042884827
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 63444}
NO2(0)最佳损失：-3.2382970026875215
NO2(0)最好结果：{'pred_time': 5.783426000716839e-06, 'wall_clock_time': 41.05632042884827, 'metric_for_logging': {'pred_time': 5.783426000716839e-06}, 'val_loss': 4.2382970026875215, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 63444}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 63444, 'experiment_tag': 'exp', 'time_total_s': 12.08755898475647}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8360650407568523
NO2(0)的mse=45.84195799793244
NO2(0)的mae=4.198258007088643
NO2(0)的mar=0.22253316580211846
总共花费的时间为：72.08
连云港市
1173A
3000A
3008A
3009A
3434A
3664A
[flaml.automl: 09-16 11:52:23] {2390} INFO - task = regression
[flaml.automl: 09-16 11:52:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:52:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:52:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:52:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:52:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:52:24] {3025} INFO - Estimated sufficient time budget=78029s. Estimated necessary time budget=78s.
[flaml.automl: 09-16 11:52:24] {3072} INFO -  at 1.6s,	estimator xgboost's best error=13.6287,	best estimator xgboost's best error=13.6287
[flaml.automl: 09-16 11:52:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:52:26] {3072} INFO -  at 3.8s,	estimator xgboost's best error=6.9093,	best estimator xgboost's best error=6.9093
[flaml.automl: 09-16 11:52:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:52:27] {3072} INFO -  at 4.9s,	estimator xgboost's best error=6.9093,	best estimator xgboost's best error=6.9093
[flaml.automl: 09-16 11:52:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:52:31] {3072} INFO -  at 8.7s,	estimator xgboost's best error=6.9093,	best estimator xgboost's best error=6.9093
[flaml.automl: 09-16 11:52:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:52:32] {3072} INFO -  at 9.9s,	estimator xgboost's best error=4.9828,	best estimator xgboost's best error=4.9828
[flaml.automl: 09-16 11:52:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:52:34] {3072} INFO -  at 11.5s,	estimator xgboost's best error=4.4718,	best estimator xgboost's best error=4.4718
[flaml.automl: 09-16 11:52:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:52:35] {3072} INFO -  at 13.0s,	estimator xgboost's best error=4.4718,	best estimator xgboost's best error=4.4718
[flaml.automl: 09-16 11:52:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:52:38] {3072} INFO -  at 15.5s,	estimator xgboost's best error=4.4718,	best estimator xgboost's best error=4.4718
[flaml.automl: 09-16 11:52:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:52:39] {3072} INFO -  at 16.6s,	estimator xgboost's best error=4.4718,	best estimator xgboost's best error=4.4718
[flaml.automl: 09-16 11:52:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:52:41] {3072} INFO -  at 19.2s,	estimator xgboost's best error=4.4718,	best estimator xgboost's best error=4.4718
[flaml.automl: 09-16 11:52:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:52:43] {3072} INFO -  at 20.8s,	estimator xgboost's best error=4.4718,	best estimator xgboost's best error=4.4718
[flaml.automl: 09-16 11:52:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:52:44] {3072} INFO -  at 22.0s,	estimator xgboost's best error=4.4718,	best estimator xgboost's best error=4.4718
[flaml.automl: 09-16 11:52:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:52:51] {3072} INFO -  at 28.5s,	estimator xgboost's best error=4.2807,	best estimator xgboost's best error=4.2807
[flaml.automl: 09-16 11:52:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:53:03] {3072} INFO -  at 40.6s,	estimator xgboost's best error=4.1990,	best estimator xgboost's best error=4.1990
[flaml.automl: 09-16 11:53:03] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:53:09] {3072} INFO -  at 47.2s,	estimator xgboost's best error=4.1990,	best estimator xgboost's best error=4.1990
[flaml.automl: 09-16 11:53:09] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 11:53:22] {3072} INFO -  at 59.7s,	estimator xgboost's best error=4.1990,	best estimator xgboost's best error=4.1990
[flaml.automl: 09-16 11:53:42] {3335} INFO - retrain xgboost for 20.1s
[flaml.automl: 09-16 11:53:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:53:42] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:53:42] {2637} INFO - Time taken to find the best model: 40.63160943984985
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64656}
NO2(0)最佳损失：-3.199007120012988
NO2(0)最好结果：{'pred_time': 6.462586946427698e-06, 'wall_clock_time': 40.63160943984985, 'metric_for_logging': {'pred_time': 6.462586946427698e-06}, 'val_loss': 4.199007120012988, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64656}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 64656, 'experiment_tag': 'exp', 'time_total_s': 12.121145009994507}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7850259690232338
NO2(0)的mse=43.019975718749855
NO2(0)的mae=4.088907428482459
NO2(0)的mar=0.2461794957729794
总共花费的时间为：80.87
徐州市
1177A
3006A
3288A
[flaml.automl: 09-16 12:02:50] {2390} INFO - task = regression
[flaml.automl: 09-16 12:02:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:02:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:02:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:02:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:02:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:02:51] {3025} INFO - Estimated sufficient time budget=12170s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 12:02:51] {3072} INFO -  at 1.4s,	estimator xgboost's best error=16.4444,	best estimator xgboost's best error=16.4444
[flaml.automl: 09-16 12:02:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:02:53] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.7998,	best estimator xgboost's best error=7.7998
[flaml.automl: 09-16 12:02:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:02:55] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.7998,	best estimator xgboost's best error=7.7998
[flaml.automl: 09-16 12:02:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:03:05] {3072} INFO -  at 14.7s,	estimator xgboost's best error=7.7998,	best estimator xgboost's best error=7.7998
[flaml.automl: 09-16 12:03:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:03:06] {3072} INFO -  at 15.9s,	estimator xgboost's best error=4.8890,	best estimator xgboost's best error=4.8890
[flaml.automl: 09-16 12:03:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:03:07] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.1710,	best estimator xgboost's best error=4.1710
[flaml.automl: 09-16 12:03:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:03:09] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.1710,	best estimator xgboost's best error=4.1710
[flaml.automl: 09-16 12:03:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:03:11] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.1710,	best estimator xgboost's best error=4.1710
[flaml.automl: 09-16 12:03:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:03:13] {3072} INFO -  at 22.8s,	estimator xgboost's best error=4.1710,	best estimator xgboost's best error=4.1710
[flaml.automl: 09-16 12:03:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:03:15] {3072} INFO -  at 25.4s,	estimator xgboost's best error=4.1710,	best estimator xgboost's best error=4.1710
[flaml.automl: 09-16 12:03:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:03:16] {3072} INFO -  at 26.6s,	estimator xgboost's best error=4.1710,	best estimator xgboost's best error=4.1710
[flaml.automl: 09-16 12:03:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:03:18] {3072} INFO -  at 27.8s,	estimator xgboost's best error=4.1710,	best estimator xgboost's best error=4.1710
[flaml.automl: 09-16 12:03:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:03:24] {3072} INFO -  at 34.3s,	estimator xgboost's best error=3.8750,	best estimator xgboost's best error=3.8750
[flaml.automl: 09-16 12:03:24] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:03:36] {3072} INFO -  at 46.3s,	estimator xgboost's best error=3.8308,	best estimator xgboost's best error=3.8308
[flaml.automl: 09-16 12:03:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 12:03:43] {3072} INFO -  at 52.8s,	estimator xgboost's best error=3.8308,	best estimator xgboost's best error=3.8308
[flaml.automl: 09-16 12:03:55] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-16 12:03:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 12:03:55] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:03:55] {2637} INFO - Time taken to find the best model: 46.33622860908508
[flaml.automl: 09-16 12:03:55] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.830839734307759
NO2(0)最好结果：{'pred_time': 1.1221964717827304e-05, 'wall_clock_time': 46.33622860908508, 'metric_for_logging': {'pred_time': 1.1221964717827304e-05}, 'val_loss': 3.830839734307759, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.056442737579346}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.883206171927521
NO2(0)的mse=31.591662256976207
NO2(0)的mae=3.7425917040480305
NO2(0)的mar=0.16958240754736473
总共花费的时间为：65.30
扬州市
1186A
3164A
3195A
3294A
[flaml.automl: 09-16 12:16:45] {2390} INFO - task = regression
[flaml.automl: 09-16 12:16:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:16:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:16:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:16:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:16:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:16:48] {3025} INFO - Estimated sufficient time budget=97506s. Estimated necessary time budget=98s.
[flaml.automl: 09-16 12:16:48] {3072} INFO -  at 2.6s,	estimator xgboost's best error=16.9743,	best estimator xgboost's best error=16.9743
[flaml.automl: 09-16 12:16:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:16:52] {3072} INFO -  at 6.4s,	estimator xgboost's best error=8.0799,	best estimator xgboost's best error=8.0799
[flaml.automl: 09-16 12:16:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:16:54] {3072} INFO -  at 8.6s,	estimator xgboost's best error=8.0799,	best estimator xgboost's best error=8.0799
[flaml.automl: 09-16 12:16:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:16:59] {3072} INFO -  at 13.8s,	estimator xgboost's best error=8.0799,	best estimator xgboost's best error=8.0799
[flaml.automl: 09-16 12:16:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:17:01] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.1353,	best estimator xgboost's best error=5.1353
[flaml.automl: 09-16 12:17:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:17:04] {3072} INFO -  at 18.9s,	estimator xgboost's best error=4.3511,	best estimator xgboost's best error=4.3511
[flaml.automl: 09-16 12:17:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:17:07] {3072} INFO -  at 21.9s,	estimator xgboost's best error=4.3511,	best estimator xgboost's best error=4.3511
[flaml.automl: 09-16 12:17:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:17:11] {3072} INFO -  at 25.4s,	estimator xgboost's best error=4.3511,	best estimator xgboost's best error=4.3511
[flaml.automl: 09-16 12:17:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:17:13] {3072} INFO -  at 27.6s,	estimator xgboost's best error=4.3511,	best estimator xgboost's best error=4.3511
[flaml.automl: 09-16 12:17:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:17:15] {3072} INFO -  at 30.2s,	estimator xgboost's best error=4.3511,	best estimator xgboost's best error=4.3511
[flaml.automl: 09-16 12:17:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:17:18] {3072} INFO -  at 33.0s,	estimator xgboost's best error=4.3511,	best estimator xgboost's best error=4.3511
[flaml.automl: 09-16 12:17:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:17:20] {3072} INFO -  at 35.2s,	estimator xgboost's best error=4.3511,	best estimator xgboost's best error=4.3511
[flaml.automl: 09-16 12:17:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:17:32] {3072} INFO -  at 47.0s,	estimator xgboost's best error=4.1189,	best estimator xgboost's best error=4.1189
[flaml.automl: 09-16 12:17:32] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:17:44] {3072} INFO -  at 58.7s,	estimator xgboost's best error=4.0457,	best estimator xgboost's best error=4.0457
[flaml.automl: 09-16 12:17:56] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-16 12:17:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 12:17:56] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:17:56] {2637} INFO - Time taken to find the best model: 58.72542858123779
[flaml.automl: 09-16 12:17:56] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43040}
NO2(0)最佳损失：-3.045734825379525
NO2(0)最好结果：{'pred_time': 9.746599366867764e-06, 'wall_clock_time': 58.72542858123779, 'metric_for_logging': {'pred_time': 9.746599366867764e-06}, 'val_loss': 4.045734825379525, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43040}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43040, 'experiment_tag': 'exp', 'time_total_s': 11.756661415100098}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8706328662355178
NO2(0)的mse=41.581072208704086
NO2(0)的mae=4.225747352576049
NO2(0)的mar=0.1794375085794262
总共花费的时间为：71.70
无锡市
1189A
1190A
1191A
1192A
1193A
1194A
1195A
3428A
[flaml.automl: 09-16 12:43:18] {2390} INFO - task = regression
[flaml.automl: 09-16 12:43:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:43:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:43:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:43:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:43:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:43:19] {3025} INFO - Estimated sufficient time budget=101813s. Estimated necessary time budget=102s.
[flaml.automl: 09-16 12:43:19] {3072} INFO -  at 1.6s,	estimator xgboost's best error=18.9702,	best estimator xgboost's best error=18.9702
[flaml.automl: 09-16 12:43:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:43:21] {3072} INFO -  at 3.7s,	estimator xgboost's best error=9.1884,	best estimator xgboost's best error=9.1884
[flaml.automl: 09-16 12:43:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:43:22] {3072} INFO -  at 4.9s,	estimator xgboost's best error=9.1884,	best estimator xgboost's best error=9.1884
[flaml.automl: 09-16 12:43:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:43:25] {3072} INFO -  at 7.7s,	estimator xgboost's best error=9.1884,	best estimator xgboost's best error=9.1884
[flaml.automl: 09-16 12:43:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:43:26] {3072} INFO -  at 8.8s,	estimator xgboost's best error=5.9164,	best estimator xgboost's best error=5.9164
[flaml.automl: 09-16 12:43:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:43:28] {3072} INFO -  at 10.4s,	estimator xgboost's best error=5.1089,	best estimator xgboost's best error=5.1089
[flaml.automl: 09-16 12:43:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:43:30] {3072} INFO -  at 12.0s,	estimator xgboost's best error=5.1089,	best estimator xgboost's best error=5.1089
[flaml.automl: 09-16 12:43:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:43:32] {3072} INFO -  at 14.5s,	estimator xgboost's best error=5.1089,	best estimator xgboost's best error=5.1089
[flaml.automl: 09-16 12:43:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:43:33] {3072} INFO -  at 15.6s,	estimator xgboost's best error=5.1089,	best estimator xgboost's best error=5.1089
[flaml.automl: 09-16 12:43:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:43:35] {3072} INFO -  at 17.7s,	estimator xgboost's best error=5.1089,	best estimator xgboost's best error=5.1089
[flaml.automl: 09-16 12:43:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:43:37] {3072} INFO -  at 19.3s,	estimator xgboost's best error=5.1034,	best estimator xgboost's best error=5.1034
[flaml.automl: 09-16 12:43:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:43:38] {3072} INFO -  at 20.5s,	estimator xgboost's best error=5.1034,	best estimator xgboost's best error=5.1034
[flaml.automl: 09-16 12:43:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:43:45] {3072} INFO -  at 27.0s,	estimator xgboost's best error=4.8848,	best estimator xgboost's best error=4.8848
[flaml.automl: 09-16 12:43:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:43:57] {3072} INFO -  at 39.1s,	estimator xgboost's best error=4.6692,	best estimator xgboost's best error=4.6692
[flaml.automl: 09-16 12:43:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 12:44:03] {3072} INFO -  at 45.7s,	estimator xgboost's best error=4.6692,	best estimator xgboost's best error=4.6692
[flaml.automl: 09-16 12:44:03] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 12:44:17] {3072} INFO -  at 59.4s,	estimator xgboost's best error=4.6215,	best estimator xgboost's best error=4.6215
[flaml.automl: 09-16 12:44:38] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-16 12:44:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 12:44:38] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:44:38] {2637} INFO - Time taken to find the best model: 59.38330554962158
[flaml.automl: 09-16 12:44:38] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 85125}
NO2(0)最佳损失：-3.6214771068352
NO2(0)最好结果：{'pred_time': 4.298642367512091e-06, 'wall_clock_time': 59.38330554962158, 'metric_for_logging': {'pred_time': 4.298642367512091e-06}, 'val_loss': 4.6214771068352, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 85125}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 85125, 'experiment_tag': 'exp', 'time_total_s': 13.67636489868164}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8516797280365298
NO2(0)的mse=48.61769678464832
NO2(0)的mae=4.622961090554746
NO2(0)的mar=0.20674513466887823
总共花费的时间为：82.27
常州市
1196A
3003A
3010A
3429A
3430A
[flaml.automl: 09-16 13:01:50] {2390} INFO - task = regression
[flaml.automl: 09-16 13:01:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:01:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:01:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:01:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:01:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:01:52] {3025} INFO - Estimated sufficient time budget=66215s. Estimated necessary time budget=66s.
[flaml.automl: 09-16 13:01:52] {3072} INFO -  at 1.5s,	estimator xgboost's best error=20.3962,	best estimator xgboost's best error=20.3962
[flaml.automl: 09-16 13:01:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:01:54] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.7980,	best estimator xgboost's best error=9.7980
[flaml.automl: 09-16 13:01:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:01:55] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.7980,	best estimator xgboost's best error=9.7980
[flaml.automl: 09-16 13:01:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:02:00] {3072} INFO -  at 9.5s,	estimator xgboost's best error=9.7980,	best estimator xgboost's best error=9.7980
[flaml.automl: 09-16 13:02:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:02:01] {3072} INFO -  at 10.7s,	estimator xgboost's best error=6.1804,	best estimator xgboost's best error=6.1804
[flaml.automl: 09-16 13:02:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:02:02] {3072} INFO -  at 12.3s,	estimator xgboost's best error=5.2131,	best estimator xgboost's best error=5.2131
[flaml.automl: 09-16 13:02:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:02:04] {3072} INFO -  at 13.9s,	estimator xgboost's best error=5.2131,	best estimator xgboost's best error=5.2131
[flaml.automl: 09-16 13:02:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:02:06] {3072} INFO -  at 16.3s,	estimator xgboost's best error=5.2131,	best estimator xgboost's best error=5.2131
[flaml.automl: 09-16 13:02:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:02:08] {3072} INFO -  at 17.5s,	estimator xgboost's best error=5.2131,	best estimator xgboost's best error=5.2131
[flaml.automl: 09-16 13:02:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:02:10] {3072} INFO -  at 20.1s,	estimator xgboost's best error=5.2131,	best estimator xgboost's best error=5.2131
[flaml.automl: 09-16 13:02:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:02:12] {3072} INFO -  at 21.7s,	estimator xgboost's best error=5.1562,	best estimator xgboost's best error=5.1562
[flaml.automl: 09-16 13:02:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:02:13] {3072} INFO -  at 22.9s,	estimator xgboost's best error=5.1562,	best estimator xgboost's best error=5.1562
[flaml.automl: 09-16 13:02:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:02:19] {3072} INFO -  at 29.4s,	estimator xgboost's best error=4.9060,	best estimator xgboost's best error=4.9060
[flaml.automl: 09-16 13:02:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:02:32] {3072} INFO -  at 41.5s,	estimator xgboost's best error=4.7085,	best estimator xgboost's best error=4.7085
[flaml.automl: 09-16 13:02:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:02:38] {3072} INFO -  at 48.0s,	estimator xgboost's best error=4.7085,	best estimator xgboost's best error=4.7085
[flaml.automl: 09-16 13:02:50] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 13:02:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:02:50] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:02:50] {2637} INFO - Time taken to find the best model: 41.486960887908936
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53972}
NO2(0)最佳损失：-3.708516690340244
NO2(0)最好结果：{'pred_time': 6.804649921701574e-06, 'wall_clock_time': 41.486960887908936, 'metric_for_logging': {'pred_time': 6.804649921701574e-06}, 'val_loss': 4.708516690340244, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53972}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53972, 'experiment_tag': 'exp', 'time_total_s': 12.098994016647339}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.856636333117072
NO2(0)的mse=49.31977556471983
NO2(0)的mae=4.702583960732635
NO2(0)的mar=0.1799176708127277
总共花费的时间为：60.92
镇江市
3287A
[flaml.automl: 09-16 13:06:04] {2390} INFO - task = regression
[flaml.automl: 09-16 13:06:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:06:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:06:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:06:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:06:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:06:07] {3025} INFO - Estimated sufficient time budget=23254s. Estimated necessary time budget=23s.
[flaml.automl: 09-16 13:06:07] {3072} INFO -  at 2.4s,	estimator xgboost's best error=17.8628,	best estimator xgboost's best error=17.8628
[flaml.automl: 09-16 13:06:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:06:10] {3072} INFO -  at 6.0s,	estimator xgboost's best error=9.8650,	best estimator xgboost's best error=9.8650
[flaml.automl: 09-16 13:06:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:06:12] {3072} INFO -  at 8.1s,	estimator xgboost's best error=9.8650,	best estimator xgboost's best error=9.8650
[flaml.automl: 09-16 13:06:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:06:24] {3072} INFO -  at 19.8s,	estimator xgboost's best error=9.8650,	best estimator xgboost's best error=9.8650
[flaml.automl: 09-16 13:06:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:06:26] {3072} INFO -  at 21.7s,	estimator xgboost's best error=5.8211,	best estimator xgboost's best error=5.8211
[flaml.automl: 09-16 13:06:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:06:29] {3072} INFO -  at 24.4s,	estimator xgboost's best error=5.0975,	best estimator xgboost's best error=5.0975
[flaml.automl: 09-16 13:06:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:06:32] {3072} INFO -  at 27.5s,	estimator xgboost's best error=5.0975,	best estimator xgboost's best error=5.0975
[flaml.automl: 09-16 13:06:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:06:36] {3072} INFO -  at 31.5s,	estimator xgboost's best error=5.0975,	best estimator xgboost's best error=5.0975
[flaml.automl: 09-16 13:06:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:06:38] {3072} INFO -  at 33.5s,	estimator xgboost's best error=5.0975,	best estimator xgboost's best error=5.0975
[flaml.automl: 09-16 13:06:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:06:42] {3072} INFO -  at 38.0s,	estimator xgboost's best error=5.0975,	best estimator xgboost's best error=5.0975
[flaml.automl: 09-16 13:06:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:06:44] {3072} INFO -  at 40.2s,	estimator xgboost's best error=5.0975,	best estimator xgboost's best error=5.0975
[flaml.automl: 09-16 13:06:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:06:46] {3072} INFO -  at 41.8s,	estimator xgboost's best error=5.0975,	best estimator xgboost's best error=5.0975
[flaml.automl: 09-16 13:06:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:06:52] {3072} INFO -  at 47.5s,	estimator xgboost's best error=5.0975,	best estimator xgboost's best error=5.0975
[flaml.automl: 09-16 13:06:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:06:54] {3072} INFO -  at 50.0s,	estimator xgboost's best error=5.0243,	best estimator xgboost's best error=5.0243
[flaml.automl: 09-16 13:06:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:06:56] {3072} INFO -  at 51.6s,	estimator xgboost's best error=5.0243,	best estimator xgboost's best error=5.0243
[flaml.automl: 09-16 13:06:56] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 13:07:00] {3072} INFO -  at 55.4s,	estimator xgboost's best error=5.0243,	best estimator xgboost's best error=5.0243
[flaml.automl: 09-16 13:07:00] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 13:07:02] {3072} INFO -  at 57.5s,	estimator xgboost's best error=5.0243,	best estimator xgboost's best error=5.0243
[flaml.automl: 09-16 13:07:04] {3335} INFO - retrain xgboost for 2.5s
[flaml.automl: 09-16 13:07:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:07:04] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:07:04] {2637} INFO - Time taken to find the best model: 50.008713722229004
[flaml.automl: 09-16 13:07:04] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}
NO2(0)最佳损失：-4.024348060158004
NO2(0)最好结果：{'pred_time': 3.330691708282775e-05, 'wall_clock_time': 50.008713722229004, 'metric_for_logging': {'pred_time': 3.330691708282775e-05}, 'val_loss': 5.024348060158004, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}, 'config/n_estimators': 8, 'config/max_leaves': 5, 'config/min_child_weight': 0.758814211387459, 'config/learning_rate': 0.29313806474778853, 'config/subsample': 0.7520389032316431, 'config/colsample_bylevel': 0.8985205690300798, 'config/colsample_bytree': 0.7992659131995336, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.2266745236797676, 'experiment_tag': 'exp', 'time_total_s': 2.5412840843200684}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7031655413215827
NO2(0)的mse=53.24035398490035
NO2(0)的mae=4.835295850385999
NO2(0)的mar=0.21208651822714247
总共花费的时间为：60.27
泰州市
1206A
1207A
3295A
3435A
[flaml.automl: 09-16 13:19:34] {2390} INFO - task = regression
[flaml.automl: 09-16 13:19:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:19:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:19:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:19:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:19:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:19:36] {3025} INFO - Estimated sufficient time budget=87168s. Estimated necessary time budget=87s.
[flaml.automl: 09-16 13:19:36] {3072} INFO -  at 2.3s,	estimator xgboost's best error=13.0760,	best estimator xgboost's best error=13.0760
[flaml.automl: 09-16 13:19:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:19:39] {3072} INFO -  at 6.1s,	estimator xgboost's best error=6.2606,	best estimator xgboost's best error=6.2606
[flaml.automl: 09-16 13:19:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:19:42] {3072} INFO -  at 8.4s,	estimator xgboost's best error=6.2606,	best estimator xgboost's best error=6.2606
[flaml.automl: 09-16 13:19:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:19:47] {3072} INFO -  at 13.4s,	estimator xgboost's best error=6.2606,	best estimator xgboost's best error=6.2606
[flaml.automl: 09-16 13:19:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:19:49] {3072} INFO -  at 15.5s,	estimator xgboost's best error=3.8373,	best estimator xgboost's best error=3.8373
[flaml.automl: 09-16 13:19:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:19:52] {3072} INFO -  at 18.4s,	estimator xgboost's best error=3.2330,	best estimator xgboost's best error=3.2330
[flaml.automl: 09-16 13:19:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:19:55] {3072} INFO -  at 21.4s,	estimator xgboost's best error=3.2330,	best estimator xgboost's best error=3.2330
[flaml.automl: 09-16 13:19:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:19:58] {3072} INFO -  at 24.9s,	estimator xgboost's best error=3.2330,	best estimator xgboost's best error=3.2330
[flaml.automl: 09-16 13:19:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:20:00] {3072} INFO -  at 26.8s,	estimator xgboost's best error=3.2330,	best estimator xgboost's best error=3.2330
[flaml.automl: 09-16 13:20:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:20:03] {3072} INFO -  at 29.5s,	estimator xgboost's best error=3.2330,	best estimator xgboost's best error=3.2330
[flaml.automl: 09-16 13:20:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:20:04] {3072} INFO -  at 31.0s,	estimator xgboost's best error=3.1940,	best estimator xgboost's best error=3.1940
[flaml.automl: 09-16 13:20:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:20:06] {3072} INFO -  at 32.2s,	estimator xgboost's best error=3.1940,	best estimator xgboost's best error=3.1940
[flaml.automl: 09-16 13:20:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:20:12] {3072} INFO -  at 38.6s,	estimator xgboost's best error=2.9445,	best estimator xgboost's best error=2.9445
[flaml.automl: 09-16 13:20:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:20:24] {3072} INFO -  at 50.5s,	estimator xgboost's best error=2.8752,	best estimator xgboost's best error=2.8752
[flaml.automl: 09-16 13:20:36] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-16 13:20:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:20:36] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:20:36] {2637} INFO - Time taken to find the best model: 50.472662925720215
[flaml.automl: 09-16 13:20:36] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42368}
NO2(0)最佳损失：-1.8752338396560972
NO2(0)最好结果：{'pred_time': 8.27192956345914e-06, 'wall_clock_time': 50.472662925720215, 'metric_for_logging': {'pred_time': 8.27192956345914e-06}, 'val_loss': 2.8752338396560972, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42368}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42368, 'experiment_tag': 'exp', 'time_total_s': 11.882404088973999}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.871450372053289
NO2(0)的mse=19.47977055897999
NO2(0)的mae=2.8790597634433084
NO2(0)的mar=0.16049858716296564
总共花费的时间为：63.06
淮安市
1210A
1211A
1213A
1214A
3426A
[flaml.automl: 09-16 13:36:56] {2390} INFO - task = regression
[flaml.automl: 09-16 13:36:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:36:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:36:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:36:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:36:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:36:58] {3025} INFO - Estimated sufficient time budget=93834s. Estimated necessary time budget=94s.
[flaml.automl: 09-16 13:36:58] {3072} INFO -  at 2.1s,	estimator xgboost's best error=14.1130,	best estimator xgboost's best error=14.1130
[flaml.automl: 09-16 13:36:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:37:02] {3072} INFO -  at 5.9s,	estimator xgboost's best error=6.7465,	best estimator xgboost's best error=6.7465
[flaml.automl: 09-16 13:37:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:37:03] {3072} INFO -  at 7.5s,	estimator xgboost's best error=6.7465,	best estimator xgboost's best error=6.7465
[flaml.automl: 09-16 13:37:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:37:08] {3072} INFO -  at 12.4s,	estimator xgboost's best error=6.7465,	best estimator xgboost's best error=6.7465
[flaml.automl: 09-16 13:37:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:37:10] {3072} INFO -  at 14.2s,	estimator xgboost's best error=4.3028,	best estimator xgboost's best error=4.3028
[flaml.automl: 09-16 13:37:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:37:13] {3072} INFO -  at 17.2s,	estimator xgboost's best error=3.7630,	best estimator xgboost's best error=3.7630
[flaml.automl: 09-16 13:37:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:37:16] {3072} INFO -  at 20.2s,	estimator xgboost's best error=3.7630,	best estimator xgboost's best error=3.7630
[flaml.automl: 09-16 13:37:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:37:19] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.7630,	best estimator xgboost's best error=3.7630
[flaml.automl: 09-16 13:37:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:37:21] {3072} INFO -  at 24.8s,	estimator xgboost's best error=3.7630,	best estimator xgboost's best error=3.7630
[flaml.automl: 09-16 13:37:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:37:23] {3072} INFO -  at 27.5s,	estimator xgboost's best error=3.7630,	best estimator xgboost's best error=3.7630
[flaml.automl: 09-16 13:37:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:37:26] {3072} INFO -  at 30.3s,	estimator xgboost's best error=3.7426,	best estimator xgboost's best error=3.7426
[flaml.automl: 09-16 13:37:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:37:27] {3072} INFO -  at 31.5s,	estimator xgboost's best error=3.7426,	best estimator xgboost's best error=3.7426
[flaml.automl: 09-16 13:37:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:37:34] {3072} INFO -  at 38.0s,	estimator xgboost's best error=3.5510,	best estimator xgboost's best error=3.5510
[flaml.automl: 09-16 13:37:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:37:46] {3072} INFO -  at 50.1s,	estimator xgboost's best error=3.4840,	best estimator xgboost's best error=3.4840
[flaml.automl: 09-16 13:37:58] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 13:37:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:37:58] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:37:58] {2637} INFO - Time taken to find the best model: 50.08680319786072
[flaml.automl: 09-16 13:37:58] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53259}
NO2(0)最佳损失：-2.484031650253791
NO2(0)最好结果：{'pred_time': 7.001440764359825e-06, 'wall_clock_time': 50.08680319786072, 'metric_for_logging': {'pred_time': 7.001440764359825e-06}, 'val_loss': 3.484031650253791, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53259}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53259, 'experiment_tag': 'exp', 'time_total_s': 12.134530544281006}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8497546655674036
NO2(0)的mse=26.32143131291429
NO2(0)的mae=3.3714414571396047
NO2(0)的mar=0.17337195888674856
总共花费的时间为：63.27
盐城市
1215A
1216A
3293A
3436A
[flaml.automl: 09-16 13:51:43] {2390} INFO - task = regression
[flaml.automl: 09-16 13:51:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:51:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:51:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:51:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:51:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:51:44] {3025} INFO - Estimated sufficient time budget=50619s. Estimated necessary time budget=51s.
[flaml.automl: 09-16 13:51:44] {3072} INFO -  at 1.4s,	estimator xgboost's best error=11.1867,	best estimator xgboost's best error=11.1867
[flaml.automl: 09-16 13:51:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:51:46] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.3579,	best estimator xgboost's best error=5.3579
[flaml.automl: 09-16 13:51:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:51:48] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.3579,	best estimator xgboost's best error=5.3579
[flaml.automl: 09-16 13:51:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:51:54] {3072} INFO -  at 11.1s,	estimator xgboost's best error=5.3579,	best estimator xgboost's best error=5.3579
[flaml.automl: 09-16 13:51:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:51:55] {3072} INFO -  at 12.2s,	estimator xgboost's best error=3.3598,	best estimator xgboost's best error=3.3598
[flaml.automl: 09-16 13:51:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:51:57] {3072} INFO -  at 13.8s,	estimator xgboost's best error=2.8731,	best estimator xgboost's best error=2.8731
[flaml.automl: 09-16 13:51:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:51:58] {3072} INFO -  at 15.5s,	estimator xgboost's best error=2.8731,	best estimator xgboost's best error=2.8731
[flaml.automl: 09-16 13:51:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:52:01] {3072} INFO -  at 17.9s,	estimator xgboost's best error=2.8731,	best estimator xgboost's best error=2.8731
[flaml.automl: 09-16 13:52:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:52:02] {3072} INFO -  at 19.0s,	estimator xgboost's best error=2.8731,	best estimator xgboost's best error=2.8731
[flaml.automl: 09-16 13:52:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:52:05] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.8731,	best estimator xgboost's best error=2.8731
[flaml.automl: 09-16 13:52:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:52:06] {3072} INFO -  at 23.3s,	estimator xgboost's best error=2.8573,	best estimator xgboost's best error=2.8573
[flaml.automl: 09-16 13:52:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:52:07] {3072} INFO -  at 24.5s,	estimator xgboost's best error=2.8573,	best estimator xgboost's best error=2.8573
[flaml.automl: 09-16 13:52:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:52:14] {3072} INFO -  at 31.0s,	estimator xgboost's best error=2.7424,	best estimator xgboost's best error=2.7424
[flaml.automl: 09-16 13:52:14] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:52:26] {3072} INFO -  at 43.1s,	estimator xgboost's best error=2.6359,	best estimator xgboost's best error=2.6359
[flaml.automl: 09-16 13:52:26] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:52:33] {3072} INFO -  at 49.6s,	estimator xgboost's best error=2.6359,	best estimator xgboost's best error=2.6359
[flaml.automl: 09-16 13:52:45] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 13:52:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:52:45] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:52:45] {2637} INFO - Time taken to find the best model: 43.10387825965881
[flaml.automl: 09-16 13:52:45] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42014}
NO2(0)最佳损失：-1.6359078944891112
NO2(0)最好结果：{'pred_time': 8.99582048074212e-06, 'wall_clock_time': 43.10387825965881, 'metric_for_logging': {'pred_time': 8.99582048074212e-06}, 'val_loss': 2.6359078944891112, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42014}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42014, 'experiment_tag': 'exp', 'time_total_s': 12.13144826889038}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8742860001003541
NO2(0)的mse=17.15828401750928
NO2(0)的mae=2.6750538263214305
NO2(0)的mar=0.17573037996460084
总共花费的时间为：62.33
宿迁市
3191A
[flaml.automl: 09-16 13:56:09] {2390} INFO - task = regression
[flaml.automl: 09-16 13:56:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:56:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:56:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:56:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:56:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:56:10] {3025} INFO - Estimated sufficient time budget=11945s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 13:56:10] {3072} INFO -  at 1.3s,	estimator xgboost's best error=13.6029,	best estimator xgboost's best error=13.6029
[flaml.automl: 09-16 13:56:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:56:12] {3072} INFO -  at 3.1s,	estimator xgboost's best error=7.5132,	best estimator xgboost's best error=7.5132
[flaml.automl: 09-16 13:56:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:56:13] {3072} INFO -  at 4.3s,	estimator xgboost's best error=7.5132,	best estimator xgboost's best error=7.5132
[flaml.automl: 09-16 13:56:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:56:20] {3072} INFO -  at 11.4s,	estimator xgboost's best error=7.5132,	best estimator xgboost's best error=7.5132
[flaml.automl: 09-16 13:56:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:56:21] {3072} INFO -  at 12.5s,	estimator xgboost's best error=4.5234,	best estimator xgboost's best error=4.5234
[flaml.automl: 09-16 13:56:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:56:23] {3072} INFO -  at 14.1s,	estimator xgboost's best error=4.0928,	best estimator xgboost's best error=4.0928
[flaml.automl: 09-16 13:56:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:56:24] {3072} INFO -  at 15.7s,	estimator xgboost's best error=3.9974,	best estimator xgboost's best error=3.9974
[flaml.automl: 09-16 13:56:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:56:27] {3072} INFO -  at 18.1s,	estimator xgboost's best error=3.9974,	best estimator xgboost's best error=3.9974
[flaml.automl: 09-16 13:56:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:56:28] {3072} INFO -  at 19.7s,	estimator xgboost's best error=3.9335,	best estimator xgboost's best error=3.9335
[flaml.automl: 09-16 13:56:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:56:31] {3072} INFO -  at 22.3s,	estimator xgboost's best error=3.9335,	best estimator xgboost's best error=3.9335
[flaml.automl: 09-16 13:56:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:56:33] {3072} INFO -  at 24.0s,	estimator xgboost's best error=3.9335,	best estimator xgboost's best error=3.9335
[flaml.automl: 09-16 13:56:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:56:34] {3072} INFO -  at 25.1s,	estimator xgboost's best error=3.9335,	best estimator xgboost's best error=3.9335
[flaml.automl: 09-16 13:56:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:56:39] {3072} INFO -  at 30.3s,	estimator xgboost's best error=3.7106,	best estimator xgboost's best error=3.7106
[flaml.automl: 09-16 13:56:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:56:47] {3072} INFO -  at 38.5s,	estimator xgboost's best error=3.5478,	best estimator xgboost's best error=3.5478
[flaml.automl: 09-16 13:56:47] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:56:52] {3072} INFO -  at 43.1s,	estimator xgboost's best error=3.5478,	best estimator xgboost's best error=3.5478
[flaml.automl: 09-16 13:56:52] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 13:57:05] {3072} INFO -  at 56.4s,	estimator xgboost's best error=3.5478,	best estimator xgboost's best error=3.5478
[flaml.automl: 09-16 13:57:13] {3335} INFO - retrain xgboost for 8.2s
[flaml.automl: 09-16 13:57:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 13:57:13] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:57:13] {2637} INFO - Time taken to find the best model: 38.50849962234497
NO2(0)最佳参数：{'n_estimators': 23, 'max_leaves': 6, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}
NO2(0)最佳损失：-2.5477950771308886
NO2(0)最好结果：{'pred_time': 3.428892655806108e-05, 'wall_clock_time': 38.50849962234497, 'metric_for_logging': {'pred_time': 3.428892655806108e-05}, 'val_loss': 3.5477950771308886, 'training_iteration': 1, 'config': {'n_estimators': 23, 'max_leaves': 6, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}, 'config/n_estimators': 23, 'config/max_leaves': 6, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'experiment_tag': 'exp', 'time_total_s': 8.209101676940918}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8407195897718839
NO2(0)的mse=35.857517284604114
NO2(0)的mae=3.8624362458049517
NO2(0)的mar=0.21806362273664323
总共花费的时间为：64.86
杭州市
1227A
1231A
1232A
3557A
3558A
3656A
[flaml.automl: 09-16 14:16:53] {2390} INFO - task = regression
[flaml.automl: 09-16 14:16:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:16:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:16:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:16:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:16:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:16:54] {3025} INFO - Estimated sufficient time budget=79996s. Estimated necessary time budget=80s.
[flaml.automl: 09-16 14:16:54] {3072} INFO -  at 1.5s,	estimator xgboost's best error=19.7161,	best estimator xgboost's best error=19.7161
[flaml.automl: 09-16 14:16:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:16:56] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.1664,	best estimator xgboost's best error=9.1664
[flaml.automl: 09-16 14:16:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:16:58] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.1664,	best estimator xgboost's best error=9.1664
[flaml.automl: 09-16 14:16:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:17:01] {3072} INFO -  at 8.6s,	estimator xgboost's best error=9.1664,	best estimator xgboost's best error=9.1664
[flaml.automl: 09-16 14:17:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:17:02] {3072} INFO -  at 9.8s,	estimator xgboost's best error=5.2557,	best estimator xgboost's best error=5.2557
[flaml.automl: 09-16 14:17:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:17:04] {3072} INFO -  at 11.3s,	estimator xgboost's best error=4.4088,	best estimator xgboost's best error=4.4088
[flaml.automl: 09-16 14:17:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:17:06] {3072} INFO -  at 13.0s,	estimator xgboost's best error=4.4088,	best estimator xgboost's best error=4.4088
[flaml.automl: 09-16 14:17:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:17:08] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.4088,	best estimator xgboost's best error=4.4088
[flaml.automl: 09-16 14:17:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:17:09] {3072} INFO -  at 16.6s,	estimator xgboost's best error=4.4088,	best estimator xgboost's best error=4.4088
[flaml.automl: 09-16 14:17:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:17:12] {3072} INFO -  at 19.2s,	estimator xgboost's best error=4.4088,	best estimator xgboost's best error=4.4088
[flaml.automl: 09-16 14:17:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:17:14] {3072} INFO -  at 20.9s,	estimator xgboost's best error=4.4088,	best estimator xgboost's best error=4.4088
[flaml.automl: 09-16 14:17:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:17:15] {3072} INFO -  at 22.0s,	estimator xgboost's best error=4.4088,	best estimator xgboost's best error=4.4088
[flaml.automl: 09-16 14:17:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:17:21] {3072} INFO -  at 28.6s,	estimator xgboost's best error=4.1145,	best estimator xgboost's best error=4.1145
[flaml.automl: 09-16 14:17:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:17:33] {3072} INFO -  at 40.7s,	estimator xgboost's best error=4.0033,	best estimator xgboost's best error=4.0033
[flaml.automl: 09-16 14:17:33] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 14:17:40] {3072} INFO -  at 47.3s,	estimator xgboost's best error=4.0033,	best estimator xgboost's best error=4.0033
[flaml.automl: 09-16 14:17:40] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 14:17:52] {3072} INFO -  at 59.8s,	estimator xgboost's best error=4.0033,	best estimator xgboost's best error=4.0033
[flaml.automl: 09-16 14:18:05] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 14:18:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:18:05] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:18:05] {2637} INFO - Time taken to find the best model: 40.721813678741455
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 66134}
NO2(0)最佳损失：-3.003303149230206
NO2(0)最好结果：{'pred_time': 5.587540056968582e-06, 'wall_clock_time': 40.721813678741455, 'metric_for_logging': {'pred_time': 5.587540056968582e-06}, 'val_loss': 4.003303149230206, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 66134}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 66134, 'experiment_tag': 'exp', 'time_total_s': 12.13202691078186}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8795668978624066
NO2(0)的mse=34.09324896586737
NO2(0)的mae=3.979838762236605
NO2(0)的mar=0.1501750732422942
总共花费的时间为：72.89
宁波市
1235A
1236A
1239A
1240A
2871A
3710A
[flaml.automl: 09-16 14:37:06] {2390} INFO - task = regression
[flaml.automl: 09-16 14:37:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:37:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:37:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:37:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:37:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:37:08] {3025} INFO - Estimated sufficient time budget=78936s. Estimated necessary time budget=79s.
[flaml.automl: 09-16 14:37:08] {3072} INFO -  at 1.5s,	estimator xgboost's best error=18.0468,	best estimator xgboost's best error=18.0468
[flaml.automl: 09-16 14:37:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:37:12] {3072} INFO -  at 5.4s,	estimator xgboost's best error=8.6991,	best estimator xgboost's best error=8.6991
[flaml.automl: 09-16 14:37:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:37:14] {3072} INFO -  at 7.7s,	estimator xgboost's best error=8.6991,	best estimator xgboost's best error=8.6991
[flaml.automl: 09-16 14:37:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:37:17] {3072} INFO -  at 10.8s,	estimator xgboost's best error=8.6991,	best estimator xgboost's best error=8.6991
[flaml.automl: 09-16 14:37:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:37:19] {3072} INFO -  at 12.9s,	estimator xgboost's best error=5.6127,	best estimator xgboost's best error=5.6127
[flaml.automl: 09-16 14:37:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:37:22] {3072} INFO -  at 15.9s,	estimator xgboost's best error=4.7593,	best estimator xgboost's best error=4.7593
[flaml.automl: 09-16 14:37:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:37:25] {3072} INFO -  at 18.8s,	estimator xgboost's best error=4.7593,	best estimator xgboost's best error=4.7593
[flaml.automl: 09-16 14:37:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:37:27] {3072} INFO -  at 21.2s,	estimator xgboost's best error=4.7593,	best estimator xgboost's best error=4.7593
[flaml.automl: 09-16 14:37:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:37:29] {3072} INFO -  at 23.4s,	estimator xgboost's best error=4.7593,	best estimator xgboost's best error=4.7593
[flaml.automl: 09-16 14:37:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:37:31] {3072} INFO -  at 24.9s,	estimator xgboost's best error=4.7593,	best estimator xgboost's best error=4.7593
[flaml.automl: 09-16 14:37:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:37:34] {3072} INFO -  at 27.9s,	estimator xgboost's best error=4.7593,	best estimator xgboost's best error=4.7593
[flaml.automl: 09-16 14:37:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:37:36] {3072} INFO -  at 30.0s,	estimator xgboost's best error=4.7593,	best estimator xgboost's best error=4.7593
[flaml.automl: 09-16 14:37:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:37:48] {3072} INFO -  at 41.9s,	estimator xgboost's best error=4.4855,	best estimator xgboost's best error=4.4855
[flaml.automl: 09-16 14:37:48] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:38:06] {3072} INFO -  at 59.7s,	estimator xgboost's best error=4.3883,	best estimator xgboost's best error=4.3883
[flaml.automl: 09-16 14:38:24] {3335} INFO - retrain xgboost for 17.9s
[flaml.automl: 09-16 14:38:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:38:24] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:38:24] {2637} INFO - Time taken to find the best model: 59.734864711761475
[flaml.automl: 09-16 14:38:24] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64970}
NO2(0)最佳损失：-3.3883024009084215
NO2(0)最好结果：{'pred_time': 1.0497320026947796e-05, 'wall_clock_time': 59.734864711761475, 'metric_for_logging': {'pred_time': 1.0497320026947796e-05}, 'val_loss': 4.3883024009084215, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64970}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 64970, 'experiment_tag': 'exp', 'time_total_s': 17.841702222824097}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8489501851287822
NO2(0)的mse=44.282340899178465
NO2(0)的mae=4.373244870379298
NO2(0)的mar=0.18674600324457147
总共花费的时间为：78.96
温州市
1242A
1243A
1244A
[flaml.automl: 09-16 14:47:27] {2390} INFO - task = regression
[flaml.automl: 09-16 14:47:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:47:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:47:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:47:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:47:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:47:28] {3025} INFO - Estimated sufficient time budget=12241s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 14:47:28] {3072} INFO -  at 1.4s,	estimator xgboost's best error=20.6891,	best estimator xgboost's best error=20.6891
[flaml.automl: 09-16 14:47:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:47:30] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.8173,	best estimator xgboost's best error=9.8173
[flaml.automl: 09-16 14:47:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:47:32] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.8173,	best estimator xgboost's best error=9.8173
[flaml.automl: 09-16 14:47:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:47:42] {3072} INFO -  at 14.7s,	estimator xgboost's best error=9.8173,	best estimator xgboost's best error=9.8173
[flaml.automl: 09-16 14:47:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:47:43] {3072} INFO -  at 15.9s,	estimator xgboost's best error=6.0648,	best estimator xgboost's best error=6.0648
[flaml.automl: 09-16 14:47:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:47:44] {3072} INFO -  at 17.5s,	estimator xgboost's best error=5.1378,	best estimator xgboost's best error=5.1378
[flaml.automl: 09-16 14:47:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:47:46] {3072} INFO -  at 19.2s,	estimator xgboost's best error=5.1378,	best estimator xgboost's best error=5.1378
[flaml.automl: 09-16 14:47:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:47:49] {3072} INFO -  at 21.7s,	estimator xgboost's best error=5.1378,	best estimator xgboost's best error=5.1378
[flaml.automl: 09-16 14:47:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:47:50] {3072} INFO -  at 22.8s,	estimator xgboost's best error=5.1378,	best estimator xgboost's best error=5.1378
[flaml.automl: 09-16 14:47:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:47:52] {3072} INFO -  at 25.5s,	estimator xgboost's best error=5.1378,	best estimator xgboost's best error=5.1378
[flaml.automl: 09-16 14:47:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:47:54] {3072} INFO -  at 26.7s,	estimator xgboost's best error=5.1378,	best estimator xgboost's best error=5.1378
[flaml.automl: 09-16 14:47:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:47:55] {3072} INFO -  at 27.8s,	estimator xgboost's best error=5.1378,	best estimator xgboost's best error=5.1378
[flaml.automl: 09-16 14:47:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:48:01] {3072} INFO -  at 34.3s,	estimator xgboost's best error=4.9319,	best estimator xgboost's best error=4.9319
[flaml.automl: 09-16 14:48:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:48:13] {3072} INFO -  at 46.4s,	estimator xgboost's best error=4.7924,	best estimator xgboost's best error=4.7924
[flaml.automl: 09-16 14:48:13] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 14:48:20] {3072} INFO -  at 52.9s,	estimator xgboost's best error=4.7924,	best estimator xgboost's best error=4.7924
[flaml.automl: 09-16 14:48:32] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 14:48:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:48:32] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:48:32] {2637} INFO - Time taken to find the best model: 46.428303480148315
[flaml.automl: 09-16 14:48:32] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.792366235583743
NO2(0)最好结果：{'pred_time': 1.0999030732821426e-05, 'wall_clock_time': 46.428303480148315, 'metric_for_logging': {'pred_time': 1.0999030732821426e-05}, 'val_loss': 4.792366235583743, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.078522205352783}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8266284429030383
NO2(0)的mse=51.75881805236046
NO2(0)的mae=4.890502913068668
NO2(0)的mar=0.18693173967209897
总共花费的时间为：65.57
绍兴市
2921A
3408A
3409A
3410A
3411A
3560A
3658A
[flaml.automl: 09-16 15:12:02] {2390} INFO - task = regression
[flaml.automl: 09-16 15:12:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:12:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:12:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:12:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:12:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:12:03] {3025} INFO - Estimated sufficient time budget=98827s. Estimated necessary time budget=99s.
[flaml.automl: 09-16 15:12:03] {3072} INFO -  at 1.7s,	estimator xgboost's best error=17.3237,	best estimator xgboost's best error=17.3237
[flaml.automl: 09-16 15:12:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:12:05] {3072} INFO -  at 3.8s,	estimator xgboost's best error=8.3603,	best estimator xgboost's best error=8.3603
[flaml.automl: 09-16 15:12:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:12:06] {3072} INFO -  at 5.0s,	estimator xgboost's best error=8.3603,	best estimator xgboost's best error=8.3603
[flaml.automl: 09-16 15:12:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:12:09] {3072} INFO -  at 8.3s,	estimator xgboost's best error=8.3603,	best estimator xgboost's best error=8.3603
[flaml.automl: 09-16 15:12:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:12:11] {3072} INFO -  at 9.4s,	estimator xgboost's best error=5.2276,	best estimator xgboost's best error=5.2276
[flaml.automl: 09-16 15:12:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:12:12] {3072} INFO -  at 11.0s,	estimator xgboost's best error=4.5221,	best estimator xgboost's best error=4.5221
[flaml.automl: 09-16 15:12:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:12:14] {3072} INFO -  at 12.7s,	estimator xgboost's best error=4.5221,	best estimator xgboost's best error=4.5221
[flaml.automl: 09-16 15:12:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:12:16] {3072} INFO -  at 15.1s,	estimator xgboost's best error=4.5221,	best estimator xgboost's best error=4.5221
[flaml.automl: 09-16 15:12:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:12:17] {3072} INFO -  at 16.2s,	estimator xgboost's best error=4.5221,	best estimator xgboost's best error=4.5221
[flaml.automl: 09-16 15:12:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:12:20] {3072} INFO -  at 18.9s,	estimator xgboost's best error=4.5221,	best estimator xgboost's best error=4.5221
[flaml.automl: 09-16 15:12:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:12:22] {3072} INFO -  at 20.5s,	estimator xgboost's best error=4.4605,	best estimator xgboost's best error=4.4605
[flaml.automl: 09-16 15:12:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:12:23] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.4605,	best estimator xgboost's best error=4.4605
[flaml.automl: 09-16 15:12:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:12:29] {3072} INFO -  at 28.3s,	estimator xgboost's best error=4.1887,	best estimator xgboost's best error=4.1887
[flaml.automl: 09-16 15:12:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:12:41] {3072} INFO -  at 40.2s,	estimator xgboost's best error=4.1250,	best estimator xgboost's best error=4.1250
[flaml.automl: 09-16 15:12:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:12:48] {3072} INFO -  at 46.6s,	estimator xgboost's best error=4.1250,	best estimator xgboost's best error=4.1250
[flaml.automl: 09-16 15:12:48] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 15:13:00] {3072} INFO -  at 59.0s,	estimator xgboost's best error=4.1026,	best estimator xgboost's best error=4.1026
[flaml.automl: 09-16 15:13:34] {3335} INFO - retrain xgboost for 34.1s
[flaml.automl: 09-16 15:13:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:13:34] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:13:34] {2637} INFO - Time taken to find the best model: 59.039008378982544
[flaml.automl: 09-16 15:13:34] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 76470}
NO2(0)最佳损失：-3.1026424366094734
NO2(0)最好结果：{'pred_time': 8.408905717082706e-06, 'wall_clock_time': 59.039008378982544, 'metric_for_logging': {'pred_time': 8.408905717082706e-06}, 'val_loss': 4.102642436609473, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 76470}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 76470, 'experiment_tag': 'exp', 'time_total_s': 12.390849590301514}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8458380046763383
NO2(0)的mse=35.82064761535832
NO2(0)的mae=4.056077669465104
NO2(0)的mar=0.1830571154555921
总共花费的时间为：94.60
湖州市
1250A
3562A
[flaml.automl: 09-16 15:19:49] {2390} INFO - task = regression
[flaml.automl: 09-16 15:19:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:19:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:19:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:19:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:19:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:19:50] {3025} INFO - Estimated sufficient time budget=12190s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:19:50] {3072} INFO -  at 1.3s,	estimator xgboost's best error=20.0509,	best estimator xgboost's best error=20.0509
[flaml.automl: 09-16 15:19:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:19:52] {3072} INFO -  at 3.4s,	estimator xgboost's best error=9.4247,	best estimator xgboost's best error=9.4247
[flaml.automl: 09-16 15:19:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:19:54] {3072} INFO -  at 4.6s,	estimator xgboost's best error=9.4247,	best estimator xgboost's best error=9.4247
[flaml.automl: 09-16 15:19:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:20:03] {3072} INFO -  at 14.1s,	estimator xgboost's best error=9.4247,	best estimator xgboost's best error=9.4247
[flaml.automl: 09-16 15:20:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:20:04] {3072} INFO -  at 15.3s,	estimator xgboost's best error=5.8708,	best estimator xgboost's best error=5.8708
[flaml.automl: 09-16 15:20:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:20:06] {3072} INFO -  at 16.8s,	estimator xgboost's best error=4.8952,	best estimator xgboost's best error=4.8952
[flaml.automl: 09-16 15:20:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:20:07] {3072} INFO -  at 18.5s,	estimator xgboost's best error=4.8952,	best estimator xgboost's best error=4.8952
[flaml.automl: 09-16 15:20:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:20:10] {3072} INFO -  at 20.9s,	estimator xgboost's best error=4.8952,	best estimator xgboost's best error=4.8952
[flaml.automl: 09-16 15:20:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:20:11] {3072} INFO -  at 22.1s,	estimator xgboost's best error=4.8952,	best estimator xgboost's best error=4.8952
[flaml.automl: 09-16 15:20:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:20:14] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.8952,	best estimator xgboost's best error=4.8952
[flaml.automl: 09-16 15:20:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:20:15] {3072} INFO -  at 25.7s,	estimator xgboost's best error=4.8952,	best estimator xgboost's best error=4.8952
[flaml.automl: 09-16 15:20:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:20:16] {3072} INFO -  at 26.8s,	estimator xgboost's best error=4.8952,	best estimator xgboost's best error=4.8952
[flaml.automl: 09-16 15:20:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:20:22] {3072} INFO -  at 32.9s,	estimator xgboost's best error=4.8102,	best estimator xgboost's best error=4.8102
[flaml.automl: 09-16 15:20:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:20:32] {3072} INFO -  at 43.3s,	estimator xgboost's best error=4.6409,	best estimator xgboost's best error=4.6409
[flaml.automl: 09-16 15:20:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:20:38] {3072} INFO -  at 49.3s,	estimator xgboost's best error=4.6409,	best estimator xgboost's best error=4.6409
[flaml.automl: 09-16 15:20:38] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 15:20:48] {3072} INFO -  at 59.0s,	estimator xgboost's best error=4.6294,	best estimator xgboost's best error=4.6294
[flaml.automl: 09-16 15:21:11] {3335} INFO - retrain xgboost for 22.7s
[flaml.automl: 09-16 15:21:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:21:11] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:21:11] {2637} INFO - Time taken to find the best model: 59.034332275390625
[flaml.automl: 09-16 15:21:11] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-3.629418594546241
NO2(0)最好结果：{'pred_time': 1.6903887188270756e-05, 'wall_clock_time': 59.034332275390625, 'metric_for_logging': {'pred_time': 1.6903887188270756e-05}, 'val_loss': 4.629418594546241, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.703010082244873}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8529637763097975
NO2(0)的mse=48.52548976223389
NO2(0)的mae=4.60452101773944
NO2(0)的mar=0.16923808156279027
总共花费的时间为：82.19
嘉兴市
1253A
3407A
[flaml.automl: 09-16 15:27:31] {2390} INFO - task = regression
[flaml.automl: 09-16 15:27:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:27:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:27:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:27:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:27:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:27:33] {3025} INFO - Estimated sufficient time budget=22450s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 15:27:33] {3072} INFO -  at 2.4s,	estimator xgboost's best error=18.6342,	best estimator xgboost's best error=18.6342
[flaml.automl: 09-16 15:27:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:27:37] {3072} INFO -  at 6.3s,	estimator xgboost's best error=8.7709,	best estimator xgboost's best error=8.7709
[flaml.automl: 09-16 15:27:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:27:39] {3072} INFO -  at 8.7s,	estimator xgboost's best error=8.7709,	best estimator xgboost's best error=8.7709
[flaml.automl: 09-16 15:27:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:27:57] {3072} INFO -  at 26.3s,	estimator xgboost's best error=8.7709,	best estimator xgboost's best error=8.7709
[flaml.automl: 09-16 15:27:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:27:59] {3072} INFO -  at 28.5s,	estimator xgboost's best error=5.3018,	best estimator xgboost's best error=5.3018
[flaml.automl: 09-16 15:27:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:28:02] {3072} INFO -  at 31.5s,	estimator xgboost's best error=4.4636,	best estimator xgboost's best error=4.4636
[flaml.automl: 09-16 15:28:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:28:04] {3072} INFO -  at 33.1s,	estimator xgboost's best error=4.4636,	best estimator xgboost's best error=4.4636
[flaml.automl: 09-16 15:28:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:28:06] {3072} INFO -  at 35.6s,	estimator xgboost's best error=4.4636,	best estimator xgboost's best error=4.4636
[flaml.automl: 09-16 15:28:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:28:07] {3072} INFO -  at 36.7s,	estimator xgboost's best error=4.4636,	best estimator xgboost's best error=4.4636
[flaml.automl: 09-16 15:28:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:28:10] {3072} INFO -  at 40.0s,	estimator xgboost's best error=4.4636,	best estimator xgboost's best error=4.4636
[flaml.automl: 09-16 15:28:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:28:12] {3072} INFO -  at 42.1s,	estimator xgboost's best error=4.4636,	best estimator xgboost's best error=4.4636
[flaml.automl: 09-16 15:28:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:28:14] {3072} INFO -  at 44.0s,	estimator xgboost's best error=4.4636,	best estimator xgboost's best error=4.4636
[flaml.automl: 09-16 15:28:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:28:25] {3072} INFO -  at 54.1s,	estimator xgboost's best error=4.2838,	best estimator xgboost's best error=4.2838
[flaml.automl: 09-16 15:28:35] {3335} INFO - retrain xgboost for 10.2s
[flaml.automl: 09-16 15:28:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 15:28:35] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:28:35] {2637} INFO - Time taken to find the best model: 54.09294390678406
[flaml.automl: 09-16 15:28:35] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-3.2837709105451953
NO2(0)最好结果：{'pred_time': 3.7614828674794226e-05, 'wall_clock_time': 54.09294390678406, 'metric_for_logging': {'pred_time': 3.7614828674794226e-05}, 'val_loss': 4.283770910545195, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 10.130423784255981}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8666387936546001
NO2(0)的mse=33.81761784233812
NO2(0)的mae=4.061221211706496
NO2(0)的mar=0.15948326077951555
总共花费的时间为：64.76
台州市
1256A
1257A
3564A
[flaml.automl: 09-16 15:38:31] {2390} INFO - task = regression
[flaml.automl: 09-16 15:38:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:38:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:38:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:38:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:38:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:38:32] {3025} INFO - Estimated sufficient time budget=12182s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:38:32] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.2309,	best estimator xgboost's best error=13.2309
[flaml.automl: 09-16 15:38:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:38:34] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.4321,	best estimator xgboost's best error=6.4321
[flaml.automl: 09-16 15:38:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:38:35] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.4321,	best estimator xgboost's best error=6.4321
[flaml.automl: 09-16 15:38:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:38:45] {3072} INFO -  at 14.7s,	estimator xgboost's best error=6.4321,	best estimator xgboost's best error=6.4321
[flaml.automl: 09-16 15:38:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:38:46] {3072} INFO -  at 15.8s,	estimator xgboost's best error=4.3096,	best estimator xgboost's best error=4.3096
[flaml.automl: 09-16 15:38:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:38:48] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.7560,	best estimator xgboost's best error=3.7560
[flaml.automl: 09-16 15:38:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:38:49] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.7560,	best estimator xgboost's best error=3.7560
[flaml.automl: 09-16 15:38:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:38:52] {3072} INFO -  at 21.5s,	estimator xgboost's best error=3.7560,	best estimator xgboost's best error=3.7560
[flaml.automl: 09-16 15:38:52] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:38:53] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.7560,	best estimator xgboost's best error=3.7560
[flaml.automl: 09-16 15:38:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:38:56] {3072} INFO -  at 25.3s,	estimator xgboost's best error=3.7560,	best estimator xgboost's best error=3.7560
[flaml.automl: 09-16 15:38:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:38:57] {3072} INFO -  at 26.5s,	estimator xgboost's best error=3.7560,	best estimator xgboost's best error=3.7560
[flaml.automl: 09-16 15:38:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:38:58] {3072} INFO -  at 27.6s,	estimator xgboost's best error=3.7560,	best estimator xgboost's best error=3.7560
[flaml.automl: 09-16 15:38:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:39:04] {3072} INFO -  at 34.1s,	estimator xgboost's best error=3.6310,	best estimator xgboost's best error=3.6310
[flaml.automl: 09-16 15:39:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:39:17] {3072} INFO -  at 46.2s,	estimator xgboost's best error=3.5425,	best estimator xgboost's best error=3.5425
[flaml.automl: 09-16 15:39:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:39:23] {3072} INFO -  at 52.7s,	estimator xgboost's best error=3.5425,	best estimator xgboost's best error=3.5425
[flaml.automl: 09-16 15:39:35] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-16 15:39:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:39:35] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:39:35] {2637} INFO - Time taken to find the best model: 46.22508978843689
[flaml.automl: 09-16 15:39:35] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.5424519471901994
NO2(0)最好结果：{'pred_time': 1.1113661209785919e-05, 'wall_clock_time': 46.22508978843689, 'metric_for_logging': {'pred_time': 1.1113661209785919e-05}, 'val_loss': 3.5424519471901994, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.112427949905396}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8107641125739993
NO2(0)的mse=29.70748478319623
NO2(0)的mae=3.5772740413268447
NO2(0)的mar=0.22831759602146823
总共花费的时间为：65.19
舟山市
1258A
1259A
[flaml.automl: 09-16 15:46:48] {2390} INFO - task = regression
[flaml.automl: 09-16 15:46:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:46:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:46:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:46:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:46:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:46:50] {3025} INFO - Estimated sufficient time budget=22278s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 15:46:50] {3072} INFO -  at 2.3s,	estimator xgboost's best error=10.6451,	best estimator xgboost's best error=10.6451
[flaml.automl: 09-16 15:46:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:46:54] {3072} INFO -  at 6.3s,	estimator xgboost's best error=5.3011,	best estimator xgboost's best error=5.3011
[flaml.automl: 09-16 15:46:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:46:56] {3072} INFO -  at 8.5s,	estimator xgboost's best error=5.3011,	best estimator xgboost's best error=5.3011
[flaml.automl: 09-16 15:46:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:47:09] {3072} INFO -  at 21.6s,	estimator xgboost's best error=5.3011,	best estimator xgboost's best error=5.3011
[flaml.automl: 09-16 15:47:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:47:11] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.7016,	best estimator xgboost's best error=3.7016
[flaml.automl: 09-16 15:47:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:47:12] {3072} INFO -  at 24.4s,	estimator xgboost's best error=3.3541,	best estimator xgboost's best error=3.3541
[flaml.automl: 09-16 15:47:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:47:14] {3072} INFO -  at 26.0s,	estimator xgboost's best error=3.3541,	best estimator xgboost's best error=3.3541
[flaml.automl: 09-16 15:47:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:47:16] {3072} INFO -  at 28.5s,	estimator xgboost's best error=3.3541,	best estimator xgboost's best error=3.3541
[flaml.automl: 09-16 15:47:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:47:17] {3072} INFO -  at 29.6s,	estimator xgboost's best error=3.3541,	best estimator xgboost's best error=3.3541
[flaml.automl: 09-16 15:47:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:47:20] {3072} INFO -  at 32.1s,	estimator xgboost's best error=3.3541,	best estimator xgboost's best error=3.3541
[flaml.automl: 09-16 15:47:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:47:21] {3072} INFO -  at 33.2s,	estimator xgboost's best error=3.3541,	best estimator xgboost's best error=3.3541
[flaml.automl: 09-16 15:47:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:47:22] {3072} INFO -  at 34.4s,	estimator xgboost's best error=3.3541,	best estimator xgboost's best error=3.3541
[flaml.automl: 09-16 15:47:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:47:28] {3072} INFO -  at 40.4s,	estimator xgboost's best error=3.2619,	best estimator xgboost's best error=3.2619
[flaml.automl: 09-16 15:47:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:47:44] {3072} INFO -  at 55.8s,	estimator xgboost's best error=3.2000,	best estimator xgboost's best error=3.2000
[flaml.automl: 09-16 15:48:03] {3335} INFO - retrain xgboost for 19.2s
[flaml.automl: 09-16 15:48:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:48:03] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:48:03] {2637} INFO - Time taken to find the best model: 55.774789333343506
[flaml.automl: 09-16 15:48:03] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.2000144431813236
NO2(0)最好结果：{'pred_time': 3.2405108588768794e-05, 'wall_clock_time': 55.774789333343506, 'metric_for_logging': {'pred_time': 3.2405108588768794e-05}, 'val_loss': 3.2000144431813236, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 15.367210865020752}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7140765576081477
NO2(0)的mse=20.955588260002223
NO2(0)的mae=3.042969205347048
NO2(0)的mar=0.20883400231382154
总共花费的时间为：75.44
金华市
金华市没有数据
衢州市
1264A
1265A
[flaml.automl: 09-16 15:53:58] {2390} INFO - task = regression
[flaml.automl: 09-16 15:53:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:53:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:53:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:53:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:53:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:54:00] {3025} INFO - Estimated sufficient time budget=12242s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:54:00] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.5120,	best estimator xgboost's best error=15.5120
[flaml.automl: 09-16 15:54:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:54:02] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.2708,	best estimator xgboost's best error=7.2708
[flaml.automl: 09-16 15:54:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:54:03] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.2708,	best estimator xgboost's best error=7.2708
[flaml.automl: 09-16 15:54:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:54:12] {3072} INFO -  at 14.2s,	estimator xgboost's best error=7.2708,	best estimator xgboost's best error=7.2708
[flaml.automl: 09-16 15:54:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:54:14] {3072} INFO -  at 15.3s,	estimator xgboost's best error=4.4356,	best estimator xgboost's best error=4.4356
[flaml.automl: 09-16 15:54:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:54:15] {3072} INFO -  at 16.9s,	estimator xgboost's best error=3.8260,	best estimator xgboost's best error=3.8260
[flaml.automl: 09-16 15:54:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:54:17] {3072} INFO -  at 18.6s,	estimator xgboost's best error=3.8260,	best estimator xgboost's best error=3.8260
[flaml.automl: 09-16 15:54:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:54:19] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.8260,	best estimator xgboost's best error=3.8260
[flaml.automl: 09-16 15:54:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:54:20] {3072} INFO -  at 22.2s,	estimator xgboost's best error=3.8260,	best estimator xgboost's best error=3.8260
[flaml.automl: 09-16 15:54:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:54:23] {3072} INFO -  at 24.6s,	estimator xgboost's best error=3.8260,	best estimator xgboost's best error=3.8260
[flaml.automl: 09-16 15:54:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:54:24] {3072} INFO -  at 25.8s,	estimator xgboost's best error=3.8260,	best estimator xgboost's best error=3.8260
[flaml.automl: 09-16 15:54:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:54:25] {3072} INFO -  at 26.9s,	estimator xgboost's best error=3.8260,	best estimator xgboost's best error=3.8260
[flaml.automl: 09-16 15:54:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:54:31] {3072} INFO -  at 33.0s,	estimator xgboost's best error=3.7154,	best estimator xgboost's best error=3.7154
[flaml.automl: 09-16 15:54:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:54:42] {3072} INFO -  at 43.4s,	estimator xgboost's best error=3.6254,	best estimator xgboost's best error=3.6254
[flaml.automl: 09-16 15:54:42] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:54:48] {3072} INFO -  at 49.5s,	estimator xgboost's best error=3.6254,	best estimator xgboost's best error=3.6254
[flaml.automl: 09-16 15:54:48] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 15:54:57] {3072} INFO -  at 59.1s,	estimator xgboost's best error=3.5953,	best estimator xgboost's best error=3.5953
[flaml.automl: 09-16 15:55:15] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-16 15:55:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:55:15] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:55:15] {2637} INFO - Time taken to find the best model: 59.13986253738403
[flaml.automl: 09-16 15:55:15] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-2.595319592997191
NO2(0)最好结果：{'pred_time': 1.6315814325338174e-05, 'wall_clock_time': 59.13986253738403, 'metric_for_logging': {'pred_time': 1.6315814325338174e-05}, 'val_loss': 3.595319592997191, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.667579650878906}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.823160892890924
NO2(0)的mse=25.789938043071043
NO2(0)的mae=3.5750109220075768
NO2(0)的mar=0.17222323858203475
总共花费的时间为：77.01
丽水市
1267A
[flaml.automl: 09-16 15:58:52] {2390} INFO - task = regression
[flaml.automl: 09-16 15:58:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:58:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:58:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:58:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:58:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:58:53] {3025} INFO - Estimated sufficient time budget=12057s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:58:53] {3072} INFO -  at 1.3s,	estimator xgboost's best error=10.6097,	best estimator xgboost's best error=10.6097
[flaml.automl: 09-16 15:58:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:58:55] {3072} INFO -  at 3.1s,	estimator xgboost's best error=5.7167,	best estimator xgboost's best error=5.7167
[flaml.automl: 09-16 15:58:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:58:56] {3072} INFO -  at 4.3s,	estimator xgboost's best error=5.7167,	best estimator xgboost's best error=5.7167
[flaml.automl: 09-16 15:58:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:59:03] {3072} INFO -  at 11.4s,	estimator xgboost's best error=5.7167,	best estimator xgboost's best error=5.7167
[flaml.automl: 09-16 15:59:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:59:04] {3072} INFO -  at 12.6s,	estimator xgboost's best error=2.9852,	best estimator xgboost's best error=2.9852
[flaml.automl: 09-16 15:59:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:59:06] {3072} INFO -  at 14.2s,	estimator xgboost's best error=2.6255,	best estimator xgboost's best error=2.6255
[flaml.automl: 09-16 15:59:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:59:08] {3072} INFO -  at 15.8s,	estimator xgboost's best error=2.6255,	best estimator xgboost's best error=2.6255
[flaml.automl: 09-16 15:59:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:59:10] {3072} INFO -  at 18.0s,	estimator xgboost's best error=2.6255,	best estimator xgboost's best error=2.6255
[flaml.automl: 09-16 15:59:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:59:11] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.6255,	best estimator xgboost's best error=2.6255
[flaml.automl: 09-16 15:59:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:59:13] {3072} INFO -  at 21.5s,	estimator xgboost's best error=2.6255,	best estimator xgboost's best error=2.6255
[flaml.automl: 09-16 15:59:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:59:14] {3072} INFO -  at 22.7s,	estimator xgboost's best error=2.6255,	best estimator xgboost's best error=2.6255
[flaml.automl: 09-16 15:59:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:59:16] {3072} INFO -  at 23.8s,	estimator xgboost's best error=2.6255,	best estimator xgboost's best error=2.6255
[flaml.automl: 09-16 15:59:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:59:21] {3072} INFO -  at 29.4s,	estimator xgboost's best error=2.6255,	best estimator xgboost's best error=2.6255
[flaml.automl: 09-16 15:59:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:59:24] {3072} INFO -  at 32.3s,	estimator xgboost's best error=2.4618,	best estimator xgboost's best error=2.4618
[flaml.automl: 09-16 15:59:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:59:26] {3072} INFO -  at 33.9s,	estimator xgboost's best error=2.4618,	best estimator xgboost's best error=2.4618
[flaml.automl: 09-16 15:59:26] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 15:59:30] {3072} INFO -  at 38.2s,	estimator xgboost's best error=2.4618,	best estimator xgboost's best error=2.4618
[flaml.automl: 09-16 15:59:30] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 15:59:32] {3072} INFO -  at 40.6s,	estimator xgboost's best error=2.4618,	best estimator xgboost's best error=2.4618
[flaml.automl: 09-16 15:59:32] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 15:59:34] {3072} INFO -  at 42.7s,	estimator xgboost's best error=2.4618,	best estimator xgboost's best error=2.4618
[flaml.automl: 09-16 15:59:34] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 15:59:40] {3072} INFO -  at 48.6s,	estimator xgboost's best error=2.4581,	best estimator xgboost's best error=2.4581
[flaml.automl: 09-16 15:59:40] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 15:59:43] {3072} INFO -  at 51.2s,	estimator xgboost's best error=2.4581,	best estimator xgboost's best error=2.4581
[flaml.automl: 09-16 15:59:43] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 15:59:51] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.4581,	best estimator xgboost's best error=2.4581
[flaml.automl: 09-16 15:59:57] {3335} INFO - retrain xgboost for 5.8s
[flaml.automl: 09-16 15:59:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7178642069579442, colsample_bynode=1,
             colsample_bytree=0.7808640703646168, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9481937935550555,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.2628045260160708, missing=nan,
             monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9333220038816206, scale_pos_weight=1,
             subsample=0.8151775216506888, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:59:57] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:59:57] {2637} INFO - Time taken to find the best model: 48.573994874954224
[flaml.automl: 09-16 15:59:57] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 10, 'max_leaves': 10, 'min_child_weight': 0.2628045260160708, 'learning_rate': 0.9481937935550555, 'subsample': 0.8151775216506888, 'colsample_bylevel': 0.7178642069579442, 'colsample_bytree': 0.7808640703646168, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9333220038816206}
NO2(0)最佳损失：-1.4581442253339594
NO2(0)最好结果：{'pred_time': 3.26485795289598e-05, 'wall_clock_time': 48.573994874954224, 'metric_for_logging': {'pred_time': 3.26485795289598e-05}, 'val_loss': 2.4581442253339594, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 10, 'min_child_weight': 0.2628045260160708, 'learning_rate': 0.9481937935550555, 'subsample': 0.8151775216506888, 'colsample_bylevel': 0.7178642069579442, 'colsample_bytree': 0.7808640703646168, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9333220038816206}, 'config/n_estimators': 10, 'config/max_leaves': 10, 'config/min_child_weight': 0.2628045260160708, 'config/learning_rate': 0.9481937935550555, 'config/subsample': 0.8151775216506888, 'config/colsample_bylevel': 0.7178642069579442, 'config/colsample_bytree': 0.7808640703646168, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.9333220038816206, 'experiment_tag': 'exp', 'time_total_s': 5.894534587860107}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7178642069579442, colsample_bynode=1,
             colsample_bytree=0.7808640703646168, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9481937935550555,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.2628045260160708, missing=nan,
             monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9333220038816206, scale_pos_weight=1,
             subsample=0.8151775216506888, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8443529969789003
NO2(0)的mse=11.66136412844596
NO2(0)的mae=2.4347315707851482
NO2(0)的mar=0.16423372311770817
总共花费的时间为：65.52
合肥市
1273A
1274A
1275A
1277A
1278A
1279A
3464A
[flaml.automl: 09-16 16:22:22] {2390} INFO - task = regression
[flaml.automl: 09-16 16:22:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:22:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:22:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:22:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:22:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:22:25] {3025} INFO - Estimated sufficient time budget=220891s. Estimated necessary time budget=221s.
[flaml.automl: 09-16 16:22:25] {3072} INFO -  at 3.7s,	estimator xgboost's best error=18.9364,	best estimator xgboost's best error=18.9364
[flaml.automl: 09-16 16:22:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:22:29] {3072} INFO -  at 7.3s,	estimator xgboost's best error=11.7246,	best estimator xgboost's best error=11.7246
[flaml.automl: 09-16 16:22:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:22:32] {3072} INFO -  at 10.1s,	estimator xgboost's best error=11.7246,	best estimator xgboost's best error=11.7246
[flaml.automl: 09-16 16:22:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:22:35] {3072} INFO -  at 13.0s,	estimator xgboost's best error=11.7246,	best estimator xgboost's best error=11.7246
[flaml.automl: 09-16 16:22:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:22:37] {3072} INFO -  at 15.6s,	estimator xgboost's best error=5.7293,	best estimator xgboost's best error=5.7293
[flaml.automl: 09-16 16:22:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:22:40] {3072} INFO -  at 18.2s,	estimator xgboost's best error=5.2881,	best estimator xgboost's best error=5.2881
[flaml.automl: 09-16 16:22:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:22:42] {3072} INFO -  at 20.7s,	estimator xgboost's best error=5.2881,	best estimator xgboost's best error=5.2881
[flaml.automl: 09-16 16:22:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:22:45] {3072} INFO -  at 23.1s,	estimator xgboost's best error=5.2881,	best estimator xgboost's best error=5.2881
[flaml.automl: 09-16 16:22:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:22:47] {3072} INFO -  at 24.9s,	estimator xgboost's best error=5.2881,	best estimator xgboost's best error=5.2881
[flaml.automl: 09-16 16:22:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:22:48] {3072} INFO -  at 26.4s,	estimator xgboost's best error=5.2881,	best estimator xgboost's best error=5.2881
[flaml.automl: 09-16 16:22:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:22:51] {3072} INFO -  at 29.4s,	estimator xgboost's best error=4.8253,	best estimator xgboost's best error=4.8253
[flaml.automl: 09-16 16:22:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:22:53] {3072} INFO -  at 31.6s,	estimator xgboost's best error=4.8253,	best estimator xgboost's best error=4.8253
[flaml.automl: 09-16 16:22:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:23:05] {3072} INFO -  at 43.0s,	estimator xgboost's best error=4.5020,	best estimator xgboost's best error=4.5020
[flaml.automl: 09-16 16:23:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:23:17] {3072} INFO -  at 55.1s,	estimator xgboost's best error=4.3862,	best estimator xgboost's best error=4.3862
[flaml.automl: 09-16 16:23:29] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 16:23:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 16:23:29] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:23:29] {2637} INFO - Time taken to find the best model: 55.13677525520325
[flaml.automl: 09-16 16:23:29] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 73403}
NO2(0)最佳损失：-3.3862026249912223
NO2(0)最好结果：{'pred_time': 5.103958302002552e-06, 'wall_clock_time': 55.13677525520325, 'metric_for_logging': {'pred_time': 5.103958302002552e-06}, 'val_loss': 4.386202624991222, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 73403}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 73403, 'experiment_tag': 'exp', 'time_total_s': 12.119302988052368}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.865334293453907
NO2(0)的mse=46.8582753125403
NO2(0)的mae=4.43460983683123
NO2(0)的mar=0.18046854350473582
总共花费的时间为：68.78
福州市
1280A
1285A
3048A
3526A
[flaml.automl: 09-16 16:35:44] {2390} INFO - task = regression
[flaml.automl: 09-16 16:35:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:35:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:35:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:35:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:35:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:35:45] {3025} INFO - Estimated sufficient time budget=48833s. Estimated necessary time budget=49s.
[flaml.automl: 09-16 16:35:45] {3072} INFO -  at 1.4s,	estimator xgboost's best error=9.2279,	best estimator xgboost's best error=9.2279
[flaml.automl: 09-16 16:35:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:35:47] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.6671,	best estimator xgboost's best error=4.6671
[flaml.automl: 09-16 16:35:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:35:48] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.6671,	best estimator xgboost's best error=4.6671
[flaml.automl: 09-16 16:35:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:35:55] {3072} INFO -  at 11.1s,	estimator xgboost's best error=4.6671,	best estimator xgboost's best error=4.6671
[flaml.automl: 09-16 16:35:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:35:56] {3072} INFO -  at 12.2s,	estimator xgboost's best error=3.3080,	best estimator xgboost's best error=3.3080
[flaml.automl: 09-16 16:35:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:35:57] {3072} INFO -  at 13.8s,	estimator xgboost's best error=2.8472,	best estimator xgboost's best error=2.8472
[flaml.automl: 09-16 16:35:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:35:59] {3072} INFO -  at 15.4s,	estimator xgboost's best error=2.8472,	best estimator xgboost's best error=2.8472
[flaml.automl: 09-16 16:35:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:36:01] {3072} INFO -  at 17.9s,	estimator xgboost's best error=2.8472,	best estimator xgboost's best error=2.8472
[flaml.automl: 09-16 16:36:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:36:03] {3072} INFO -  at 19.0s,	estimator xgboost's best error=2.8472,	best estimator xgboost's best error=2.8472
[flaml.automl: 09-16 16:36:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:36:05] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.8472,	best estimator xgboost's best error=2.8472
[flaml.automl: 09-16 16:36:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:36:07] {3072} INFO -  at 23.3s,	estimator xgboost's best error=2.8472,	best estimator xgboost's best error=2.8472
[flaml.automl: 09-16 16:36:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:36:08] {3072} INFO -  at 24.4s,	estimator xgboost's best error=2.8472,	best estimator xgboost's best error=2.8472
[flaml.automl: 09-16 16:36:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:36:15] {3072} INFO -  at 30.9s,	estimator xgboost's best error=2.7597,	best estimator xgboost's best error=2.7597
[flaml.automl: 09-16 16:36:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:36:27] {3072} INFO -  at 43.0s,	estimator xgboost's best error=2.6617,	best estimator xgboost's best error=2.6617
[flaml.automl: 09-16 16:36:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 16:36:33] {3072} INFO -  at 49.6s,	estimator xgboost's best error=2.6617,	best estimator xgboost's best error=2.6617
[flaml.automl: 09-16 16:36:45] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 16:36:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 16:36:45] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:36:45] {2637} INFO - Time taken to find the best model: 43.01847958564758
[flaml.automl: 09-16 16:36:45] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40567}
NO2(0)最佳损失：-1.661653540281458
NO2(0)最好结果：{'pred_time': 8.960371008993993e-06, 'wall_clock_time': 43.01847958564758, 'metric_for_logging': {'pred_time': 8.960371008993993e-06}, 'val_loss': 2.661653540281458, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40567}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40567, 'experiment_tag': 'exp', 'time_total_s': 12.08971357345581}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8126438009993189
NO2(0)的mse=17.617411700530692
NO2(0)的mae=2.6586307077427827
NO2(0)的mar=0.23293405973806744
总共花费的时间为：62.29
厦门市
1286A
3527A
3528A
[flaml.automl: 09-16 16:45:48] {2390} INFO - task = regression
[flaml.automl: 09-16 16:45:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:45:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:45:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:45:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:45:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:45:51] {3025} INFO - Estimated sufficient time budget=22480s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 16:45:51] {3072} INFO -  at 2.4s,	estimator xgboost's best error=9.5085,	best estimator xgboost's best error=9.5085
[flaml.automl: 09-16 16:45:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:45:54] {3072} INFO -  at 6.2s,	estimator xgboost's best error=4.7357,	best estimator xgboost's best error=4.7357
[flaml.automl: 09-16 16:45:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:45:57] {3072} INFO -  at 8.4s,	estimator xgboost's best error=4.7357,	best estimator xgboost's best error=4.7357
[flaml.automl: 09-16 16:45:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:46:17] {3072} INFO -  at 29.1s,	estimator xgboost's best error=4.7357,	best estimator xgboost's best error=4.7357
[flaml.automl: 09-16 16:46:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:46:20] {3072} INFO -  at 31.4s,	estimator xgboost's best error=3.4307,	best estimator xgboost's best error=3.4307
[flaml.automl: 09-16 16:46:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:46:22] {3072} INFO -  at 34.3s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 16:46:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:46:26] {3072} INFO -  at 37.4s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 16:46:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:46:30] {3072} INFO -  at 41.7s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 16:46:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:46:32] {3072} INFO -  at 43.8s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 16:46:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:46:37] {3072} INFO -  at 48.7s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 16:46:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:46:39] {3072} INFO -  at 50.8s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 16:46:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:46:41] {3072} INFO -  at 52.9s,	estimator xgboost's best error=3.0458,	best estimator xgboost's best error=3.0458
[flaml.automl: 09-16 16:46:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:46:47] {3072} INFO -  at 59.0s,	estimator xgboost's best error=2.9263,	best estimator xgboost's best error=2.9263
[flaml.automl: 09-16 16:46:59] {3335} INFO - retrain xgboost for 11.7s
[flaml.automl: 09-16 16:46:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 16:46:59] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:46:59] {2637} INFO - Time taken to find the best model: 58.9727828502655
[flaml.automl: 09-16 16:46:59] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-1.9263145810624827
NO2(0)最好结果：{'pred_time': 2.0618162293365038e-05, 'wall_clock_time': 58.9727828502655, 'metric_for_logging': {'pred_time': 2.0618162293365038e-05}, 'val_loss': 2.9263145810624827, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 6.036901235580444}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7840540905073861
NO2(0)的mse=23.02795275866129
NO2(0)的mae=3.0787909398429645
NO2(0)的mar=0.24273156943603777
总共花费的时间为：71.48
南昌市
1295A
1296A
1298A
3690A
[flaml.automl: 09-16 16:59:23] {2390} INFO - task = regression
[flaml.automl: 09-16 16:59:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:59:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:59:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:59:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:59:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:59:24] {3025} INFO - Estimated sufficient time budget=48804s. Estimated necessary time budget=49s.
[flaml.automl: 09-16 16:59:24] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.5229,	best estimator xgboost's best error=13.5229
[flaml.automl: 09-16 16:59:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:59:26] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.6444,	best estimator xgboost's best error=6.6444
[flaml.automl: 09-16 16:59:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:59:28] {3072} INFO -  at 4.8s,	estimator xgboost's best error=6.6444,	best estimator xgboost's best error=6.6444
[flaml.automl: 09-16 16:59:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:59:34] {3072} INFO -  at 11.1s,	estimator xgboost's best error=6.6444,	best estimator xgboost's best error=6.6444
[flaml.automl: 09-16 16:59:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:59:35] {3072} INFO -  at 12.3s,	estimator xgboost's best error=4.4750,	best estimator xgboost's best error=4.4750
[flaml.automl: 09-16 16:59:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:59:37] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-16 16:59:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:59:38] {3072} INFO -  at 15.5s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-16 16:59:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:59:41] {3072} INFO -  at 17.9s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-16 16:59:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:59:42] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-16 16:59:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:59:45] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-16 16:59:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:59:46] {3072} INFO -  at 23.3s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-16 16:59:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:59:47] {3072} INFO -  at 24.5s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-16 16:59:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:59:54] {3072} INFO -  at 31.0s,	estimator xgboost's best error=3.5033,	best estimator xgboost's best error=3.5033
[flaml.automl: 09-16 16:59:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:00:06] {3072} INFO -  at 43.1s,	estimator xgboost's best error=3.4120,	best estimator xgboost's best error=3.4120
[flaml.automl: 09-16 17:00:06] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 17:00:12] {3072} INFO -  at 49.6s,	estimator xgboost's best error=3.4120,	best estimator xgboost's best error=3.4120
[flaml.automl: 09-16 17:00:25] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 17:00:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 17:00:25] {2636} INFO - fit succeeded
[flaml.automl: 09-16 17:00:25] {2637} INFO - Time taken to find the best model: 43.12012076377869
[flaml.automl: 09-16 17:00:25] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40817}
NO2(0)最佳损失：-2.411964565541799
NO2(0)最好结果：{'pred_time': 8.886348213047788e-06, 'wall_clock_time': 43.12012076377869, 'metric_for_logging': {'pred_time': 8.886348213047788e-06}, 'val_loss': 3.411964565541799, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40817}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40817, 'experiment_tag': 'exp', 'time_total_s': 12.0753014087677}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8753514381582393
NO2(0)的mse=31.929789545723352
NO2(0)的mae=3.54559344668237
NO2(0)的mar=0.22686973485363754
总共花费的时间为：62.38
济南市
1300A
1301A
1305A
1306A
1961A
3064A
3494A
3495A
3682A
[flaml.automl: 09-16 17:28:30] {2390} INFO - task = regression
[flaml.automl: 09-16 17:28:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 17:28:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 17:28:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 17:28:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 17:28:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 17:28:31] {3025} INFO - Estimated sufficient time budget=114858s. Estimated necessary time budget=115s.
[flaml.automl: 09-16 17:28:31] {3072} INFO -  at 1.6s,	estimator xgboost's best error=18.6340,	best estimator xgboost's best error=18.6340
[flaml.automl: 09-16 17:28:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 17:28:33] {3072} INFO -  at 3.7s,	estimator xgboost's best error=9.1056,	best estimator xgboost's best error=9.1056
[flaml.automl: 09-16 17:28:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 17:28:34] {3072} INFO -  at 4.9s,	estimator xgboost's best error=9.1056,	best estimator xgboost's best error=9.1056
[flaml.automl: 09-16 17:28:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 17:28:36] {3072} INFO -  at 7.1s,	estimator xgboost's best error=9.1056,	best estimator xgboost's best error=9.1056
[flaml.automl: 09-16 17:28:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 17:28:38] {3072} INFO -  at 8.3s,	estimator xgboost's best error=6.0521,	best estimator xgboost's best error=6.0521
[flaml.automl: 09-16 17:28:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 17:28:39] {3072} INFO -  at 9.8s,	estimator xgboost's best error=5.2043,	best estimator xgboost's best error=5.2043
[flaml.automl: 09-16 17:28:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 17:28:41] {3072} INFO -  at 11.5s,	estimator xgboost's best error=5.2043,	best estimator xgboost's best error=5.2043
[flaml.automl: 09-16 17:28:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 17:28:43] {3072} INFO -  at 13.4s,	estimator xgboost's best error=5.2043,	best estimator xgboost's best error=5.2043
[flaml.automl: 09-16 17:28:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 17:28:44] {3072} INFO -  at 14.5s,	estimator xgboost's best error=5.2043,	best estimator xgboost's best error=5.2043
[flaml.automl: 09-16 17:28:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 17:28:46] {3072} INFO -  at 16.6s,	estimator xgboost's best error=5.2043,	best estimator xgboost's best error=5.2043
[flaml.automl: 09-16 17:28:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 17:28:47] {3072} INFO -  at 18.3s,	estimator xgboost's best error=5.1485,	best estimator xgboost's best error=5.1485
[flaml.automl: 09-16 17:28:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 17:28:49] {3072} INFO -  at 19.5s,	estimator xgboost's best error=5.1485,	best estimator xgboost's best error=5.1485
[flaml.automl: 09-16 17:28:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 17:28:55] {3072} INFO -  at 26.1s,	estimator xgboost's best error=4.8349,	best estimator xgboost's best error=4.8349
[flaml.automl: 09-16 17:28:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:29:07] {3072} INFO -  at 38.3s,	estimator xgboost's best error=4.7470,	best estimator xgboost's best error=4.7470
[flaml.automl: 09-16 17:29:07] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 17:29:14] {3072} INFO -  at 44.8s,	estimator xgboost's best error=4.7470,	best estimator xgboost's best error=4.7470
[flaml.automl: 09-16 17:29:14] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 17:29:29] {3072} INFO -  at 59.7s,	estimator xgboost's best error=4.7235,	best estimator xgboost's best error=4.7235
[flaml.automl: 09-16 17:29:50] {3335} INFO - retrain xgboost for 21.6s
[flaml.automl: 09-16 17:29:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 17:29:50] {2636} INFO - fit succeeded
[flaml.automl: 09-16 17:29:50] {2637} INFO - Time taken to find the best model: 59.687917709350586
[flaml.automl: 09-16 17:29:50] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 98226}
NO2(0)最佳损失：-3.7235154098417595
NO2(0)最好结果：{'pred_time': 3.8022454981623015e-06, 'wall_clock_time': 59.687917709350586, 'metric_for_logging': {'pred_time': 3.8022454981623015e-06}, 'val_loss': 4.7235154098417595, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 98226}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 98226, 'experiment_tag': 'exp', 'time_total_s': 14.837718725204468}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8531429572625561
NO2(0)的mse=53.34745197778485
NO2(0)的mae=4.794080847003862
NO2(0)的mar=0.21206953263772607
总共花费的时间为：82.76
青岛市
1307A
1311A
3362A
3642A
3643A
[flaml.automl: 09-16 17:46:46] {2390} INFO - task = regression
[flaml.automl: 09-16 17:46:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 17:46:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 17:46:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 17:46:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 17:46:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 17:46:48] {3025} INFO - Estimated sufficient time budget=63033s. Estimated necessary time budget=63s.
[flaml.automl: 09-16 17:46:48] {3072} INFO -  at 1.5s,	estimator xgboost's best error=16.6332,	best estimator xgboost's best error=16.6332
[flaml.automl: 09-16 17:46:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 17:46:50] {3072} INFO -  at 3.6s,	estimator xgboost's best error=8.1078,	best estimator xgboost's best error=8.1078
[flaml.automl: 09-16 17:46:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 17:46:51] {3072} INFO -  at 4.8s,	estimator xgboost's best error=8.1078,	best estimator xgboost's best error=8.1078
[flaml.automl: 09-16 17:46:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 17:46:56] {3072} INFO -  at 9.6s,	estimator xgboost's best error=8.1078,	best estimator xgboost's best error=8.1078
[flaml.automl: 09-16 17:46:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 17:46:57] {3072} INFO -  at 10.8s,	estimator xgboost's best error=5.6183,	best estimator xgboost's best error=5.6183
[flaml.automl: 09-16 17:46:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 17:46:58] {3072} INFO -  at 12.3s,	estimator xgboost's best error=4.9029,	best estimator xgboost's best error=4.9029
[flaml.automl: 09-16 17:46:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 17:47:00] {3072} INFO -  at 13.9s,	estimator xgboost's best error=4.9029,	best estimator xgboost's best error=4.9029
[flaml.automl: 09-16 17:47:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 17:47:02] {3072} INFO -  at 16.4s,	estimator xgboost's best error=4.9029,	best estimator xgboost's best error=4.9029
[flaml.automl: 09-16 17:47:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 17:47:04] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.9029,	best estimator xgboost's best error=4.9029
[flaml.automl: 09-16 17:47:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 17:47:06] {3072} INFO -  at 20.1s,	estimator xgboost's best error=4.9029,	best estimator xgboost's best error=4.9029
[flaml.automl: 09-16 17:47:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 17:47:08] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.8715,	best estimator xgboost's best error=4.8715
[flaml.automl: 09-16 17:47:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 17:47:09] {3072} INFO -  at 22.9s,	estimator xgboost's best error=4.8715,	best estimator xgboost's best error=4.8715
[flaml.automl: 09-16 17:47:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 17:47:15] {3072} INFO -  at 29.3s,	estimator xgboost's best error=4.5921,	best estimator xgboost's best error=4.5921
[flaml.automl: 09-16 17:47:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:47:27] {3072} INFO -  at 41.4s,	estimator xgboost's best error=4.5374,	best estimator xgboost's best error=4.5374
[flaml.automl: 09-16 17:47:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 17:47:34] {3072} INFO -  at 47.9s,	estimator xgboost's best error=4.5374,	best estimator xgboost's best error=4.5374
[flaml.automl: 09-16 17:47:46] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 17:47:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 17:47:46] {2636} INFO - fit succeeded
[flaml.automl: 09-16 17:47:46] {2637} INFO - Time taken to find the best model: 41.39688038825989
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52595}
NO2(0)最佳损失：-3.537427355376602
NO2(0)最好结果：{'pred_time': 6.729164489077673e-06, 'wall_clock_time': 41.39688038825989, 'metric_for_logging': {'pred_time': 6.729164489077673e-06}, 'val_loss': 4.537427355376602, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52595}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52595, 'experiment_tag': 'exp', 'time_total_s': 12.076991558074951}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8492201371387337
NO2(0)的mse=47.5105848144153
NO2(0)的mae=4.512177651134828
NO2(0)的mar=0.21066951566113096
总共花费的时间为：60.85
郑州市
1318A
1320A
1321A
1323A
1324A
3471A
3590A
3591A
[flaml.automl: 09-16 18:13:58] {2390} INFO - task = regression
[flaml.automl: 09-16 18:13:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 18:13:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 18:13:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 18:13:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 18:13:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 18:13:59] {3025} INFO - Estimated sufficient time budget=100540s. Estimated necessary time budget=101s.
[flaml.automl: 09-16 18:13:59] {3072} INFO -  at 1.6s,	estimator xgboost's best error=16.8211,	best estimator xgboost's best error=16.8211
[flaml.automl: 09-16 18:13:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 18:14:01] {3072} INFO -  at 3.7s,	estimator xgboost's best error=8.1486,	best estimator xgboost's best error=8.1486
[flaml.automl: 09-16 18:14:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 18:14:02] {3072} INFO -  at 5.0s,	estimator xgboost's best error=8.1486,	best estimator xgboost's best error=8.1486
[flaml.automl: 09-16 18:14:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 18:14:05] {3072} INFO -  at 7.7s,	estimator xgboost's best error=8.1486,	best estimator xgboost's best error=8.1486
[flaml.automl: 09-16 18:14:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 18:14:06] {3072} INFO -  at 8.9s,	estimator xgboost's best error=5.2526,	best estimator xgboost's best error=5.2526
[flaml.automl: 09-16 18:14:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 18:14:08] {3072} INFO -  at 10.5s,	estimator xgboost's best error=4.4757,	best estimator xgboost's best error=4.4757
[flaml.automl: 09-16 18:14:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 18:14:10] {3072} INFO -  at 12.1s,	estimator xgboost's best error=4.4757,	best estimator xgboost's best error=4.4757
[flaml.automl: 09-16 18:14:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 18:14:12] {3072} INFO -  at 14.6s,	estimator xgboost's best error=4.4757,	best estimator xgboost's best error=4.4757
[flaml.automl: 09-16 18:14:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 18:14:13] {3072} INFO -  at 15.7s,	estimator xgboost's best error=4.4757,	best estimator xgboost's best error=4.4757
[flaml.automl: 09-16 18:14:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 18:14:15] {3072} INFO -  at 17.7s,	estimator xgboost's best error=4.4757,	best estimator xgboost's best error=4.4757
[flaml.automl: 09-16 18:14:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 18:14:17] {3072} INFO -  at 19.4s,	estimator xgboost's best error=4.4390,	best estimator xgboost's best error=4.4390
[flaml.automl: 09-16 18:14:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 18:14:18] {3072} INFO -  at 20.6s,	estimator xgboost's best error=4.4390,	best estimator xgboost's best error=4.4390
[flaml.automl: 09-16 18:14:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 18:14:25] {3072} INFO -  at 27.1s,	estimator xgboost's best error=4.1451,	best estimator xgboost's best error=4.1451
[flaml.automl: 09-16 18:14:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 18:14:42] {3072} INFO -  at 44.2s,	estimator xgboost's best error=4.0664,	best estimator xgboost's best error=4.0664
[flaml.automl: 09-16 18:15:02] {3335} INFO - retrain xgboost for 20.5s
[flaml.automl: 09-16 18:15:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 18:15:02] {2636} INFO - fit succeeded
[flaml.automl: 09-16 18:15:02] {2637} INFO - Time taken to find the best model: 44.198073625564575
[flaml.automl: 09-16 18:15:02] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 83520}
NO2(0)最佳损失：-3.0663857335944638
NO2(0)最好结果：{'pred_time': 9.571988319813748e-06, 'wall_clock_time': 44.198073625564575, 'metric_for_logging': {'pred_time': 9.571988319813748e-06}, 'val_loss': 4.066385733594464, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 83520}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 83520, 'experiment_tag': 'exp', 'time_total_s': 17.090585947036743}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8565467404847309
NO2(0)的mse=37.33303867985909
NO2(0)的mae=4.04733159294232
NO2(0)的mar=0.1878081328520318
总共花费的时间为：66.05
武汉市
1325A
1326A
1327A
1328A
1329A
1331A
3153A
[flaml.automl: 09-16 18:37:05] {2390} INFO - task = regression
[flaml.automl: 09-16 18:37:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 18:37:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 18:37:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 18:37:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 18:37:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 18:37:06] {3025} INFO - Estimated sufficient time budget=92204s. Estimated necessary time budget=92s.
[flaml.automl: 09-16 18:37:06] {3072} INFO -  at 1.6s,	estimator xgboost's best error=22.4536,	best estimator xgboost's best error=22.4536
[flaml.automl: 09-16 18:37:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 18:37:08] {3072} INFO -  at 3.7s,	estimator xgboost's best error=10.7808,	best estimator xgboost's best error=10.7808
[flaml.automl: 09-16 18:37:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 18:37:10] {3072} INFO -  at 4.9s,	estimator xgboost's best error=10.7808,	best estimator xgboost's best error=10.7808
[flaml.automl: 09-16 18:37:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 18:37:13] {3072} INFO -  at 8.1s,	estimator xgboost's best error=10.7808,	best estimator xgboost's best error=10.7808
[flaml.automl: 09-16 18:37:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 18:37:14] {3072} INFO -  at 9.2s,	estimator xgboost's best error=7.0094,	best estimator xgboost's best error=7.0094
[flaml.automl: 09-16 18:37:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 18:37:15] {3072} INFO -  at 10.8s,	estimator xgboost's best error=5.9753,	best estimator xgboost's best error=5.9753
[flaml.automl: 09-16 18:37:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 18:37:17] {3072} INFO -  at 12.4s,	estimator xgboost's best error=5.9753,	best estimator xgboost's best error=5.9753
[flaml.automl: 09-16 18:37:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 18:37:20] {3072} INFO -  at 14.9s,	estimator xgboost's best error=5.9753,	best estimator xgboost's best error=5.9753
[flaml.automl: 09-16 18:37:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 18:37:21] {3072} INFO -  at 16.0s,	estimator xgboost's best error=5.9753,	best estimator xgboost's best error=5.9753
[flaml.automl: 09-16 18:37:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 18:37:23] {3072} INFO -  at 18.7s,	estimator xgboost's best error=5.9753,	best estimator xgboost's best error=5.9753
[flaml.automl: 09-16 18:37:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 18:37:25] {3072} INFO -  at 20.3s,	estimator xgboost's best error=5.9272,	best estimator xgboost's best error=5.9272
[flaml.automl: 09-16 18:37:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 18:37:26] {3072} INFO -  at 21.5s,	estimator xgboost's best error=5.9272,	best estimator xgboost's best error=5.9272
[flaml.automl: 09-16 18:37:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 18:37:33] {3072} INFO -  at 28.1s,	estimator xgboost's best error=5.6466,	best estimator xgboost's best error=5.6466
[flaml.automl: 09-16 18:37:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 18:37:45] {3072} INFO -  at 40.1s,	estimator xgboost's best error=5.4678,	best estimator xgboost's best error=5.4678
[flaml.automl: 09-16 18:37:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 18:37:51] {3072} INFO -  at 46.7s,	estimator xgboost's best error=5.4678,	best estimator xgboost's best error=5.4678
[flaml.automl: 09-16 18:37:51] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 18:38:04] {3072} INFO -  at 59.2s,	estimator xgboost's best error=5.4332,	best estimator xgboost's best error=5.4332
[flaml.automl: 09-16 18:38:25] {3335} INFO - retrain xgboost for 21.3s
[flaml.automl: 09-16 18:38:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 18:38:25] {2636} INFO - fit succeeded
[flaml.automl: 09-16 18:38:25] {2637} INFO - Time taken to find the best model: 59.20715117454529
[flaml.automl: 09-16 18:38:25] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 76182}
NO2(0)最佳损失：-4.433167353425587
NO2(0)最好结果：{'pred_time': 4.848698786709059e-06, 'wall_clock_time': 59.20715117454529, 'metric_for_logging': {'pred_time': 4.848698786709059e-06}, 'val_loss': 5.433167353425587, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 76182}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 76182, 'experiment_tag': 'exp', 'time_total_s': 12.525164604187012}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8476450069152157
NO2(0)的mse=70.89514983348755
NO2(0)的mae=5.40395369368514
NO2(0)的mar=0.18183068469522265
总共花费的时间为：81.59
长沙市
1340A
1342A
1343A
1344A
[flaml.automl: 09-16 18:49:51] {2390} INFO - task = regression
[flaml.automl: 09-16 18:49:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 18:49:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 18:49:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 18:49:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 18:49:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 18:49:52] {3025} INFO - Estimated sufficient time budget=48407s. Estimated necessary time budget=48s.
[flaml.automl: 09-16 18:49:52] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.4533,	best estimator xgboost's best error=15.4533
[flaml.automl: 09-16 18:49:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 18:49:54] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.4665,	best estimator xgboost's best error=7.4665
[flaml.automl: 09-16 18:49:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 18:49:56] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.4665,	best estimator xgboost's best error=7.4665
[flaml.automl: 09-16 18:49:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 18:50:02] {3072} INFO -  at 11.1s,	estimator xgboost's best error=7.4665,	best estimator xgboost's best error=7.4665
[flaml.automl: 09-16 18:50:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 18:50:03] {3072} INFO -  at 12.2s,	estimator xgboost's best error=4.6415,	best estimator xgboost's best error=4.6415
[flaml.automl: 09-16 18:50:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 18:50:05] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.7660,	best estimator xgboost's best error=3.7660
[flaml.automl: 09-16 18:50:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 18:50:06] {3072} INFO -  at 15.4s,	estimator xgboost's best error=3.7660,	best estimator xgboost's best error=3.7660
[flaml.automl: 09-16 18:50:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 18:50:09] {3072} INFO -  at 17.8s,	estimator xgboost's best error=3.7660,	best estimator xgboost's best error=3.7660
[flaml.automl: 09-16 18:50:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 18:50:10] {3072} INFO -  at 19.0s,	estimator xgboost's best error=3.7660,	best estimator xgboost's best error=3.7660
[flaml.automl: 09-16 18:50:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 18:50:12] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.7660,	best estimator xgboost's best error=3.7660
[flaml.automl: 09-16 18:50:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 18:50:14] {3072} INFO -  at 23.2s,	estimator xgboost's best error=3.7416,	best estimator xgboost's best error=3.7416
[flaml.automl: 09-16 18:50:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 18:50:15] {3072} INFO -  at 24.3s,	estimator xgboost's best error=3.7416,	best estimator xgboost's best error=3.7416
[flaml.automl: 09-16 18:50:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 18:50:22] {3072} INFO -  at 30.8s,	estimator xgboost's best error=3.4574,	best estimator xgboost's best error=3.4574
[flaml.automl: 09-16 18:50:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 18:50:34] {3072} INFO -  at 42.9s,	estimator xgboost's best error=3.4149,	best estimator xgboost's best error=3.4149
[flaml.automl: 09-16 18:50:34] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 18:50:40] {3072} INFO -  at 49.5s,	estimator xgboost's best error=3.4149,	best estimator xgboost's best error=3.4149
[flaml.automl: 09-16 18:50:52] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 18:50:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 18:50:52] {2636} INFO - fit succeeded
[flaml.automl: 09-16 18:50:52] {2637} INFO - Time taken to find the best model: 42.921128034591675
[flaml.automl: 09-16 18:50:52] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40550}
NO2(0)最佳损失：-2.414897478505129
NO2(0)最好结果：{'pred_time': 8.69016143622739e-06, 'wall_clock_time': 42.921128034591675, 'metric_for_logging': {'pred_time': 8.69016143622739e-06}, 'val_loss': 3.414897478505129, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40550}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40550, 'experiment_tag': 'exp', 'time_total_s': 12.070603847503662}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8709664265526869
NO2(0)的mse=29.48302255185588
NO2(0)的mae=3.4655135093583826
NO2(0)的mar=0.19215450254851707
总共花费的时间为：62.30
广州市
1345A
1346A
1349A
1351A
1352A
1354A
1355A
2846A
3299A
3300A
3301A
3302A
3303A
3304A
3443A
3445A
3446A
[flaml.automl: 09-16 19:41:33] {2390} INFO - task = regression
[flaml.automl: 09-16 19:41:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 19:41:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 19:41:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 19:41:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 19:41:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 19:41:34] {3025} INFO - Estimated sufficient time budget=216123s. Estimated necessary time budget=216s.
[flaml.automl: 09-16 19:41:34] {3072} INFO -  at 2.2s,	estimator xgboost's best error=19.1645,	best estimator xgboost's best error=19.1645
[flaml.automl: 09-16 19:41:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 19:41:36] {3072} INFO -  at 3.6s,	estimator xgboost's best error=13.6132,	best estimator xgboost's best error=13.6132
[flaml.automl: 09-16 19:41:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 19:41:37] {3072} INFO -  at 4.8s,	estimator xgboost's best error=13.6132,	best estimator xgboost's best error=13.6132
[flaml.automl: 09-16 19:41:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 19:41:38] {3072} INFO -  at 6.0s,	estimator xgboost's best error=13.6132,	best estimator xgboost's best error=13.6132
[flaml.automl: 09-16 19:41:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 19:41:39] {3072} INFO -  at 7.1s,	estimator xgboost's best error=5.4552,	best estimator xgboost's best error=5.4552
[flaml.automl: 09-16 19:41:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 19:41:40] {3072} INFO -  at 8.4s,	estimator xgboost's best error=5.0399,	best estimator xgboost's best error=5.0399
[flaml.automl: 09-16 19:41:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 19:41:42] {3072} INFO -  at 9.5s,	estimator xgboost's best error=5.0399,	best estimator xgboost's best error=5.0399
[flaml.automl: 09-16 19:41:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 19:41:43] {3072} INFO -  at 10.8s,	estimator xgboost's best error=5.0399,	best estimator xgboost's best error=5.0399
[flaml.automl: 09-16 19:41:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 19:41:44] {3072} INFO -  at 12.0s,	estimator xgboost's best error=5.0399,	best estimator xgboost's best error=5.0399
[flaml.automl: 09-16 19:41:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 19:41:45] {3072} INFO -  at 12.8s,	estimator xgboost's best error=5.0399,	best estimator xgboost's best error=5.0399
[flaml.automl: 09-16 19:41:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 19:41:46] {3072} INFO -  at 14.4s,	estimator xgboost's best error=4.5149,	best estimator xgboost's best error=4.5149
[flaml.automl: 09-16 19:41:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 19:41:48] {3072} INFO -  at 15.5s,	estimator xgboost's best error=4.5149,	best estimator xgboost's best error=4.5149
[flaml.automl: 09-16 19:41:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 19:41:52] {3072} INFO -  at 19.7s,	estimator xgboost's best error=4.3171,	best estimator xgboost's best error=4.3171
[flaml.automl: 09-16 19:41:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 19:41:56] {3072} INFO -  at 23.6s,	estimator xgboost's best error=4.1973,	best estimator xgboost's best error=4.1973
[flaml.automl: 09-16 19:41:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 19:41:59] {3072} INFO -  at 27.0s,	estimator xgboost's best error=4.1973,	best estimator xgboost's best error=4.1973
[flaml.automl: 09-16 19:41:59] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 19:42:03] {3072} INFO -  at 30.6s,	estimator xgboost's best error=4.1973,	best estimator xgboost's best error=4.1973
[flaml.automl: 09-16 19:42:03] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 19:42:06] {3072} INFO -  at 33.6s,	estimator xgboost's best error=4.1973,	best estimator xgboost's best error=4.1973
[flaml.automl: 09-16 19:42:06] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 19:42:08] {3072} INFO -  at 36.1s,	estimator xgboost's best error=4.1973,	best estimator xgboost's best error=4.1973
[flaml.automl: 09-16 19:42:08] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 19:42:21] {3072} INFO -  at 48.4s,	estimator xgboost's best error=4.1152,	best estimator xgboost's best error=4.1152
[flaml.automl: 09-16 19:42:33] {3335} INFO - retrain xgboost for 12.3s
[flaml.automl: 09-16 19:42:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 19:42:33] {2636} INFO - fit succeeded
[flaml.automl: 09-16 19:42:33] {2637} INFO - Time taken to find the best model: 48.4208607673645
[flaml.automl: 09-16 19:42:33] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 180000}
NO2(0)最佳损失：-3.115189030146599
NO2(0)最好结果：{'pred_time': 2.3693680763244627e-06, 'wall_clock_time': 48.4208607673645, 'metric_for_logging': {'pred_time': 2.3693680763244627e-06}, 'val_loss': 4.115189030146599, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 180000}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 180000, 'experiment_tag': 'exp', 'time_total_s': 12.278614044189453}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8636980002072581
NO2(0)的mse=40.20127931222319
NO2(0)的mae=4.173217681033269
NO2(0)的mar=0.15730029531217068
总共花费的时间为：63.80
深圳市
1356A
1363A
1364A
1365A
1366A
3305A
3306A
3307A
3447A
3623A
[flaml.automl: 09-16 20:13:47] {2390} INFO - task = regression
[flaml.automl: 09-16 20:13:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:13:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:13:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:13:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:13:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:13:49] {3025} INFO - Estimated sufficient time budget=225484s. Estimated necessary time budget=225s.
[flaml.automl: 09-16 20:13:49] {3072} INFO -  at 3.0s,	estimator xgboost's best error=12.0794,	best estimator xgboost's best error=12.0794
[flaml.automl: 09-16 20:13:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:13:52] {3072} INFO -  at 5.2s,	estimator xgboost's best error=10.0340,	best estimator xgboost's best error=10.0340
[flaml.automl: 09-16 20:13:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:13:54] {3072} INFO -  at 7.3s,	estimator xgboost's best error=10.0340,	best estimator xgboost's best error=10.0340
[flaml.automl: 09-16 20:13:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:13:56] {3072} INFO -  at 9.5s,	estimator xgboost's best error=10.0340,	best estimator xgboost's best error=10.0340
[flaml.automl: 09-16 20:13:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:13:58] {3072} INFO -  at 11.2s,	estimator xgboost's best error=4.1786,	best estimator xgboost's best error=4.1786
[flaml.automl: 09-16 20:13:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:13:59] {3072} INFO -  at 12.8s,	estimator xgboost's best error=3.5905,	best estimator xgboost's best error=3.5905
[flaml.automl: 09-16 20:13:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:14:01] {3072} INFO -  at 14.4s,	estimator xgboost's best error=3.5905,	best estimator xgboost's best error=3.5905
[flaml.automl: 09-16 20:14:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:14:03] {3072} INFO -  at 16.3s,	estimator xgboost's best error=3.5905,	best estimator xgboost's best error=3.5905
[flaml.automl: 09-16 20:14:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:14:04] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.5905,	best estimator xgboost's best error=3.5905
[flaml.automl: 09-16 20:14:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:14:05] {3072} INFO -  at 18.8s,	estimator xgboost's best error=3.5905,	best estimator xgboost's best error=3.5905
[flaml.automl: 09-16 20:14:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:14:07] {3072} INFO -  at 20.5s,	estimator xgboost's best error=3.5745,	best estimator xgboost's best error=3.5745
[flaml.automl: 09-16 20:14:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:14:08] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.5745,	best estimator xgboost's best error=3.5745
[flaml.automl: 09-16 20:14:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:14:15] {3072} INFO -  at 28.2s,	estimator xgboost's best error=3.4218,	best estimator xgboost's best error=3.4218
[flaml.automl: 09-16 20:14:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:14:32] {3072} INFO -  at 45.7s,	estimator xgboost's best error=3.3537,	best estimator xgboost's best error=3.3537
[flaml.automl: 09-16 20:14:53] {3335} INFO - retrain xgboost for 20.5s
[flaml.automl: 09-16 20:14:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 20:14:53] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:14:53] {2637} INFO - Time taken to find the best model: 45.72633957862854
[flaml.automl: 09-16 20:14:53] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 106421}
NO2(0)最佳损失：-2.353712911686484
NO2(0)最好结果：{'pred_time': 8.081613585006359e-06, 'wall_clock_time': 45.72633957862854, 'metric_for_logging': {'pred_time': 8.081613585006359e-06}, 'val_loss': 3.353712911686484, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 106421}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 106421, 'experiment_tag': 'exp', 'time_total_s': 17.475744009017944}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8186283551704108
NO2(0)的mse=26.47313958725808
NO2(0)的mae=3.2706765687051034
NO2(0)的mar=0.20409424917627497
总共花费的时间为：68.58
珠海市
1368A
1369A
1370A
3308A
3448A
[flaml.automl: 09-16 20:30:27] {2390} INFO - task = regression
[flaml.automl: 09-16 20:30:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:30:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:30:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:30:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:30:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:30:28] {3025} INFO - Estimated sufficient time budget=65056s. Estimated necessary time budget=65s.
[flaml.automl: 09-16 20:30:28] {3072} INFO -  at 1.5s,	estimator xgboost's best error=11.6654,	best estimator xgboost's best error=11.6654
[flaml.automl: 09-16 20:30:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:30:31] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.9425,	best estimator xgboost's best error=5.9425
[flaml.automl: 09-16 20:30:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:30:32] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.9425,	best estimator xgboost's best error=5.9425
[flaml.automl: 09-16 20:30:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:30:37] {3072} INFO -  at 9.6s,	estimator xgboost's best error=5.9425,	best estimator xgboost's best error=5.9425
[flaml.automl: 09-16 20:30:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:30:38] {3072} INFO -  at 10.7s,	estimator xgboost's best error=4.0193,	best estimator xgboost's best error=4.0193
[flaml.automl: 09-16 20:30:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:30:39] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.4811,	best estimator xgboost's best error=3.4811
[flaml.automl: 09-16 20:30:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:30:41] {3072} INFO -  at 14.0s,	estimator xgboost's best error=3.4811,	best estimator xgboost's best error=3.4811
[flaml.automl: 09-16 20:30:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:30:43] {3072} INFO -  at 16.4s,	estimator xgboost's best error=3.4811,	best estimator xgboost's best error=3.4811
[flaml.automl: 09-16 20:30:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:30:45] {3072} INFO -  at 17.6s,	estimator xgboost's best error=3.4811,	best estimator xgboost's best error=3.4811
[flaml.automl: 09-16 20:30:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:30:47] {3072} INFO -  at 20.2s,	estimator xgboost's best error=3.4811,	best estimator xgboost's best error=3.4811
[flaml.automl: 09-16 20:30:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:30:49] {3072} INFO -  at 21.9s,	estimator xgboost's best error=3.4475,	best estimator xgboost's best error=3.4475
[flaml.automl: 09-16 20:30:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:30:50] {3072} INFO -  at 23.0s,	estimator xgboost's best error=3.4475,	best estimator xgboost's best error=3.4475
[flaml.automl: 09-16 20:30:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:30:56] {3072} INFO -  at 29.5s,	estimator xgboost's best error=3.2441,	best estimator xgboost's best error=3.2441
[flaml.automl: 09-16 20:30:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:31:09] {3072} INFO -  at 41.7s,	estimator xgboost's best error=3.2032,	best estimator xgboost's best error=3.2032
[flaml.automl: 09-16 20:31:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:31:15] {3072} INFO -  at 48.2s,	estimator xgboost's best error=3.2032,	best estimator xgboost's best error=3.2032
[flaml.automl: 09-16 20:31:27] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 20:31:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 20:31:27] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:31:27] {2637} INFO - Time taken to find the best model: 41.665910482406616
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53859}
NO2(0)最佳损失：-2.2032329426871406
NO2(0)最好结果：{'pred_time': 7.196894863195587e-06, 'wall_clock_time': 41.665910482406616, 'metric_for_logging': {'pred_time': 7.196894863195587e-06}, 'val_loss': 3.2032329426871406, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53859}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53859, 'experiment_tag': 'exp', 'time_total_s': 12.12167477607727}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8681245314887336
NO2(0)的mse=26.179612559135794
NO2(0)的mae=3.2181080682474867
NO2(0)的mar=0.24291327761285017
总共花费的时间为：61.16
佛山市
1371A
1372A
1373A
1377A
1378A
3625A
[flaml.automl: 09-16 20:52:05] {2390} INFO - task = regression
[flaml.automl: 09-16 20:52:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:52:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:52:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:52:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:52:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:52:07] {3025} INFO - Estimated sufficient time budget=136419s. Estimated necessary time budget=136s.
[flaml.automl: 09-16 20:52:07] {3072} INFO -  at 2.5s,	estimator xgboost's best error=19.0055,	best estimator xgboost's best error=19.0055
[flaml.automl: 09-16 20:52:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:52:11] {3072} INFO -  at 6.2s,	estimator xgboost's best error=9.0117,	best estimator xgboost's best error=9.0117
[flaml.automl: 09-16 20:52:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:52:13] {3072} INFO -  at 8.2s,	estimator xgboost's best error=9.0117,	best estimator xgboost's best error=9.0117
[flaml.automl: 09-16 20:52:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:52:17] {3072} INFO -  at 12.0s,	estimator xgboost's best error=9.0117,	best estimator xgboost's best error=9.0117
[flaml.automl: 09-16 20:52:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:52:18] {3072} INFO -  at 13.9s,	estimator xgboost's best error=5.5663,	best estimator xgboost's best error=5.5663
[flaml.automl: 09-16 20:52:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:52:21] {3072} INFO -  at 16.1s,	estimator xgboost's best error=4.7492,	best estimator xgboost's best error=4.7492
[flaml.automl: 09-16 20:52:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:52:22] {3072} INFO -  at 17.7s,	estimator xgboost's best error=4.7492,	best estimator xgboost's best error=4.7492
[flaml.automl: 09-16 20:52:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:52:25] {3072} INFO -  at 20.1s,	estimator xgboost's best error=4.7492,	best estimator xgboost's best error=4.7492
[flaml.automl: 09-16 20:52:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:52:26] {3072} INFO -  at 21.3s,	estimator xgboost's best error=4.7492,	best estimator xgboost's best error=4.7492
[flaml.automl: 09-16 20:52:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:52:29] {3072} INFO -  at 23.9s,	estimator xgboost's best error=4.7492,	best estimator xgboost's best error=4.7492
[flaml.automl: 09-16 20:52:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:52:30] {3072} INFO -  at 25.6s,	estimator xgboost's best error=4.7001,	best estimator xgboost's best error=4.7001
[flaml.automl: 09-16 20:52:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:52:31] {3072} INFO -  at 26.8s,	estimator xgboost's best error=4.7001,	best estimator xgboost's best error=4.7001
[flaml.automl: 09-16 20:52:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:52:38] {3072} INFO -  at 33.3s,	estimator xgboost's best error=4.3927,	best estimator xgboost's best error=4.3927
[flaml.automl: 09-16 20:52:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:52:50] {3072} INFO -  at 45.3s,	estimator xgboost's best error=4.2538,	best estimator xgboost's best error=4.2538
[flaml.automl: 09-16 20:52:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:52:56] {3072} INFO -  at 51.8s,	estimator xgboost's best error=4.2538,	best estimator xgboost's best error=4.2538
[flaml.automl: 09-16 20:53:08] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 20:53:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 20:53:08] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:53:08] {2637} INFO - Time taken to find the best model: 45.26237654685974
[flaml.automl: 09-16 20:53:08] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64632}
NO2(0)最佳损失：-3.2537904962527984
NO2(0)最好结果：{'pred_time': 5.585441546863714e-06, 'wall_clock_time': 45.26237654685974, 'metric_for_logging': {'pred_time': 5.585441546863714e-06}, 'val_loss': 4.253790496252798, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64632}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 64632, 'experiment_tag': 'exp', 'time_total_s': 11.980134963989258}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8561317418016061
NO2(0)的mse=42.56436045395834
NO2(0)的mae=4.369730517290589
NO2(0)的mar=0.17347175200203757
总共花费的时间为：65.11
中山市
1379A
3454A
[flaml.automl: 09-16 20:59:27] {2390} INFO - task = regression
[flaml.automl: 09-16 20:59:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:59:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:59:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:59:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:59:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:59:28] {3025} INFO - Estimated sufficient time budget=12084s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 20:59:28] {3072} INFO -  at 1.3s,	estimator xgboost's best error=13.5018,	best estimator xgboost's best error=13.5018
[flaml.automl: 09-16 20:59:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:59:30] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.5224,	best estimator xgboost's best error=6.5224
[flaml.automl: 09-16 20:59:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:59:32] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.5224,	best estimator xgboost's best error=6.5224
[flaml.automl: 09-16 20:59:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:59:41] {3072} INFO -  at 14.2s,	estimator xgboost's best error=6.5224,	best estimator xgboost's best error=6.5224
[flaml.automl: 09-16 20:59:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:59:42] {3072} INFO -  at 15.3s,	estimator xgboost's best error=3.9907,	best estimator xgboost's best error=3.9907
[flaml.automl: 09-16 20:59:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:59:44] {3072} INFO -  at 16.9s,	estimator xgboost's best error=3.2129,	best estimator xgboost's best error=3.2129
[flaml.automl: 09-16 20:59:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:59:46] {3072} INFO -  at 18.6s,	estimator xgboost's best error=3.2129,	best estimator xgboost's best error=3.2129
[flaml.automl: 09-16 20:59:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:59:48] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.2129,	best estimator xgboost's best error=3.2129
[flaml.automl: 09-16 20:59:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:59:49] {3072} INFO -  at 22.2s,	estimator xgboost's best error=3.2129,	best estimator xgboost's best error=3.2129
[flaml.automl: 09-16 20:59:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:59:52] {3072} INFO -  at 24.6s,	estimator xgboost's best error=3.2129,	best estimator xgboost's best error=3.2129
[flaml.automl: 09-16 20:59:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:59:53] {3072} INFO -  at 25.8s,	estimator xgboost's best error=3.2129,	best estimator xgboost's best error=3.2129
[flaml.automl: 09-16 20:59:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:59:54] {3072} INFO -  at 27.0s,	estimator xgboost's best error=3.2129,	best estimator xgboost's best error=3.2129
[flaml.automl: 09-16 20:59:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:00:00] {3072} INFO -  at 33.1s,	estimator xgboost's best error=2.9090,	best estimator xgboost's best error=2.9090
[flaml.automl: 09-16 21:00:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:00:10] {3072} INFO -  at 43.5s,	estimator xgboost's best error=2.8369,	best estimator xgboost's best error=2.8369
[flaml.automl: 09-16 21:00:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:00:16] {3072} INFO -  at 49.4s,	estimator xgboost's best error=2.8369,	best estimator xgboost's best error=2.8369
[flaml.automl: 09-16 21:00:16] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 21:00:26] {3072} INFO -  at 59.0s,	estimator xgboost's best error=2.8083,	best estimator xgboost's best error=2.8083
[flaml.automl: 09-16 21:00:43] {3335} INFO - retrain xgboost for 17.2s
[flaml.automl: 09-16 21:00:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:00:43] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:00:43] {2637} INFO - Time taken to find the best model: 58.999603033065796
[flaml.automl: 09-16 21:00:43] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-1.80825610820941
NO2(0)最好结果：{'pred_time': 1.6193018952756186e-05, 'wall_clock_time': 58.999603033065796, 'metric_for_logging': {'pred_time': 1.6193018952756186e-05}, 'val_loss': 2.80825610820941, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.555790185928345}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.9119898468285736
NO2(0)的mse=19.10939908137643
NO2(0)的mae=2.8057721908911164
NO2(0)的mar=0.16692359978695248
总共花费的时间为：76.59
江门市
1386A
3449A
[flaml.automl: 09-16 21:07:21] {2390} INFO - task = regression
[flaml.automl: 09-16 21:07:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:07:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:07:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:07:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:07:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:07:23] {3025} INFO - Estimated sufficient time budget=12382s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 21:07:23] {3072} INFO -  at 1.3s,	estimator xgboost's best error=16.6091,	best estimator xgboost's best error=16.6091
[flaml.automl: 09-16 21:07:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:07:25] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.8108,	best estimator xgboost's best error=7.8108
[flaml.automl: 09-16 21:07:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:07:26] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.8108,	best estimator xgboost's best error=7.8108
[flaml.automl: 09-16 21:07:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:07:35] {3072} INFO -  at 14.1s,	estimator xgboost's best error=7.8108,	best estimator xgboost's best error=7.8108
[flaml.automl: 09-16 21:07:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:07:37] {3072} INFO -  at 15.3s,	estimator xgboost's best error=4.5455,	best estimator xgboost's best error=4.5455
[flaml.automl: 09-16 21:07:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:07:38] {3072} INFO -  at 16.9s,	estimator xgboost's best error=3.7506,	best estimator xgboost's best error=3.7506
[flaml.automl: 09-16 21:07:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:07:40] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.7506,	best estimator xgboost's best error=3.7506
[flaml.automl: 09-16 21:07:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:07:42] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.7506,	best estimator xgboost's best error=3.7506
[flaml.automl: 09-16 21:07:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:07:43] {3072} INFO -  at 22.1s,	estimator xgboost's best error=3.7506,	best estimator xgboost's best error=3.7506
[flaml.automl: 09-16 21:07:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:07:46] {3072} INFO -  at 24.6s,	estimator xgboost's best error=3.7506,	best estimator xgboost's best error=3.7506
[flaml.automl: 09-16 21:07:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:07:47] {3072} INFO -  at 25.8s,	estimator xgboost's best error=3.7506,	best estimator xgboost's best error=3.7506
[flaml.automl: 09-16 21:07:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:07:48] {3072} INFO -  at 26.9s,	estimator xgboost's best error=3.7506,	best estimator xgboost's best error=3.7506
[flaml.automl: 09-16 21:07:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:07:54] {3072} INFO -  at 33.0s,	estimator xgboost's best error=3.4803,	best estimator xgboost's best error=3.4803
[flaml.automl: 09-16 21:07:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:08:05] {3072} INFO -  at 43.4s,	estimator xgboost's best error=3.3664,	best estimator xgboost's best error=3.3664
[flaml.automl: 09-16 21:08:05] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:08:11] {3072} INFO -  at 49.4s,	estimator xgboost's best error=3.3664,	best estimator xgboost's best error=3.3664
[flaml.automl: 09-16 21:08:11] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 21:08:20] {3072} INFO -  at 59.1s,	estimator xgboost's best error=3.3368,	best estimator xgboost's best error=3.3368
[flaml.automl: 09-16 21:08:38] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-16 21:08:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:08:38] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:08:38] {2637} INFO - Time taken to find the best model: 59.083898305892944
[flaml.automl: 09-16 21:08:38] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-2.33684202720379
NO2(0)最好结果：{'pred_time': 1.739356148327145e-05, 'wall_clock_time': 59.083898305892944, 'metric_for_logging': {'pred_time': 1.739356148327145e-05}, 'val_loss': 3.33684202720379, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.667344093322754}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.9015519883844251
NO2(0)的mse=26.476566506422536
NO2(0)的mae=3.4217443639160363
NO2(0)的mar=0.1620884630606702
总共花费的时间为：76.78
东莞市
1389A
1391A
3319A
3626A
3627A
[flaml.automl: 09-16 21:24:01] {2390} INFO - task = regression
[flaml.automl: 09-16 21:24:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:24:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:24:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:24:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:24:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:24:03] {3025} INFO - Estimated sufficient time budget=118455s. Estimated necessary time budget=118s.
[flaml.automl: 09-16 21:24:03] {3072} INFO -  at 2.7s,	estimator xgboost's best error=16.5765,	best estimator xgboost's best error=16.5765
[flaml.automl: 09-16 21:24:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:24:07] {3072} INFO -  at 6.5s,	estimator xgboost's best error=7.6765,	best estimator xgboost's best error=7.6765
[flaml.automl: 09-16 21:24:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:24:09] {3072} INFO -  at 8.7s,	estimator xgboost's best error=7.6765,	best estimator xgboost's best error=7.6765
[flaml.automl: 09-16 21:24:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:24:13] {3072} INFO -  at 12.9s,	estimator xgboost's best error=7.6765,	best estimator xgboost's best error=7.6765
[flaml.automl: 09-16 21:24:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:24:15] {3072} INFO -  at 15.0s,	estimator xgboost's best error=4.6861,	best estimator xgboost's best error=4.6861
[flaml.automl: 09-16 21:24:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:24:17] {3072} INFO -  at 17.2s,	estimator xgboost's best error=4.0730,	best estimator xgboost's best error=4.0730
[flaml.automl: 09-16 21:24:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:24:19] {3072} INFO -  at 18.8s,	estimator xgboost's best error=4.0730,	best estimator xgboost's best error=4.0730
[flaml.automl: 09-16 21:24:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:24:22] {3072} INFO -  at 21.3s,	estimator xgboost's best error=4.0730,	best estimator xgboost's best error=4.0730
[flaml.automl: 09-16 21:24:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:24:23] {3072} INFO -  at 22.4s,	estimator xgboost's best error=4.0730,	best estimator xgboost's best error=4.0730
[flaml.automl: 09-16 21:24:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:24:25] {3072} INFO -  at 25.1s,	estimator xgboost's best error=4.0730,	best estimator xgboost's best error=4.0730
[flaml.automl: 09-16 21:24:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:24:27] {3072} INFO -  at 26.7s,	estimator xgboost's best error=4.0730,	best estimator xgboost's best error=4.0730
[flaml.automl: 09-16 21:24:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:24:28] {3072} INFO -  at 27.8s,	estimator xgboost's best error=4.0730,	best estimator xgboost's best error=4.0730
[flaml.automl: 09-16 21:24:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:24:35] {3072} INFO -  at 34.3s,	estimator xgboost's best error=3.8542,	best estimator xgboost's best error=3.8542
[flaml.automl: 09-16 21:24:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:24:46] {3072} INFO -  at 46.2s,	estimator xgboost's best error=3.7910,	best estimator xgboost's best error=3.7910
[flaml.automl: 09-16 21:24:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:24:53] {3072} INFO -  at 52.6s,	estimator xgboost's best error=3.7910,	best estimator xgboost's best error=3.7910
[flaml.automl: 09-16 21:25:05] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 21:25:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:25:05] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:25:05] {2637} INFO - Time taken to find the best model: 46.1715521812439
[flaml.automl: 09-16 21:25:05] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52949}
NO2(0)最佳损失：-2.7909864966031406
NO2(0)最好结果：{'pred_time': 6.918352854486876e-06, 'wall_clock_time': 46.1715521812439, 'metric_for_logging': {'pred_time': 6.918352854486876e-06}, 'val_loss': 3.7909864966031406, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52949}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52949, 'experiment_tag': 'exp', 'time_total_s': 11.916834354400635}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8373270636402321
NO2(0)的mse=31.691845232072417
NO2(0)的mae=3.8361365942825194
NO2(0)的mar=0.1629414879995137
总共花费的时间为：65.70
惠州市
1392A
1393A
1395A
1396A
3314A
3452A
[flaml.automl: 09-16 21:43:40] {2390} INFO - task = regression
[flaml.automl: 09-16 21:43:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:43:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:43:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:43:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:43:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:43:43] {3025} INFO - Estimated sufficient time budget=177447s. Estimated necessary time budget=177s.
[flaml.automl: 09-16 21:43:43] {3072} INFO -  at 3.2s,	estimator xgboost's best error=10.6110,	best estimator xgboost's best error=10.6110
[flaml.automl: 09-16 21:43:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:43:47] {3072} INFO -  at 7.0s,	estimator xgboost's best error=6.5578,	best estimator xgboost's best error=6.5578
[flaml.automl: 09-16 21:43:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:43:50] {3072} INFO -  at 10.1s,	estimator xgboost's best error=6.5578,	best estimator xgboost's best error=6.5578
[flaml.automl: 09-16 21:43:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:43:54] {3072} INFO -  at 13.9s,	estimator xgboost's best error=6.5578,	best estimator xgboost's best error=6.5578
[flaml.automl: 09-16 21:43:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:43:56] {3072} INFO -  at 16.3s,	estimator xgboost's best error=3.3028,	best estimator xgboost's best error=3.3028
[flaml.automl: 09-16 21:43:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:43:59] {3072} INFO -  at 19.3s,	estimator xgboost's best error=2.8776,	best estimator xgboost's best error=2.8776
[flaml.automl: 09-16 21:43:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:44:02] {3072} INFO -  at 22.3s,	estimator xgboost's best error=2.8776,	best estimator xgboost's best error=2.8776
[flaml.automl: 09-16 21:44:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:44:04] {3072} INFO -  at 24.6s,	estimator xgboost's best error=2.8776,	best estimator xgboost's best error=2.8776
[flaml.automl: 09-16 21:44:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:44:07] {3072} INFO -  at 26.7s,	estimator xgboost's best error=2.8776,	best estimator xgboost's best error=2.8776
[flaml.automl: 09-16 21:44:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:44:08] {3072} INFO -  at 28.3s,	estimator xgboost's best error=2.8776,	best estimator xgboost's best error=2.8776
[flaml.automl: 09-16 21:44:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:44:11] {3072} INFO -  at 31.1s,	estimator xgboost's best error=2.8776,	best estimator xgboost's best error=2.8776
[flaml.automl: 09-16 21:44:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:44:13] {3072} INFO -  at 33.1s,	estimator xgboost's best error=2.8776,	best estimator xgboost's best error=2.8776
[flaml.automl: 09-16 21:44:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:44:24] {3072} INFO -  at 44.3s,	estimator xgboost's best error=2.8218,	best estimator xgboost's best error=2.8218
[flaml.automl: 09-16 21:44:24] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:44:39] {3072} INFO -  at 59.3s,	estimator xgboost's best error=2.7426,	best estimator xgboost's best error=2.7426
[flaml.automl: 09-16 21:44:51] {3335} INFO - retrain xgboost for 12.3s
[flaml.automl: 09-16 21:44:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:44:51] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:44:51] {2637} INFO - Time taken to find the best model: 59.33271026611328
[flaml.automl: 09-16 21:44:51] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 63424}
NO2(0)最佳损失：-1.7425505758414337
NO2(0)最好结果：{'pred_time': 9.435936216599012e-06, 'wall_clock_time': 59.33271026611328, 'metric_for_logging': {'pred_time': 9.435936216599012e-06}, 'val_loss': 2.7425505758414337, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 63424}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 63424, 'experiment_tag': 'exp', 'time_total_s': 15.078742980957031}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.802245289854544
NO2(0)的mse=15.398448203542296
NO2(0)的mae=2.651237725359074
NO2(0)的mar=0.17945416724482408
总共花费的时间为：73.03
肇庆市
1397A
1398A
1400A
3451A
[flaml.automl: 09-16 21:57:49] {2390} INFO - task = regression
[flaml.automl: 09-16 21:57:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:57:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:57:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:57:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:57:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:57:51] {3025} INFO - Estimated sufficient time budget=20887s. Estimated necessary time budget=21s.
[flaml.automl: 09-16 21:57:51] {3072} INFO -  at 2.4s,	estimator xgboost's best error=14.7670,	best estimator xgboost's best error=14.7670
[flaml.automl: 09-16 21:57:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:57:55] {3072} INFO -  at 6.2s,	estimator xgboost's best error=6.9975,	best estimator xgboost's best error=6.9975
[flaml.automl: 09-16 21:57:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:57:57] {3072} INFO -  at 8.2s,	estimator xgboost's best error=6.9975,	best estimator xgboost's best error=6.9975
[flaml.automl: 09-16 21:57:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:58:13] {3072} INFO -  at 24.8s,	estimator xgboost's best error=6.9975,	best estimator xgboost's best error=6.9975
[flaml.automl: 09-16 21:58:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:58:15] {3072} INFO -  at 26.7s,	estimator xgboost's best error=4.4705,	best estimator xgboost's best error=4.4705
[flaml.automl: 09-16 21:58:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:58:18] {3072} INFO -  at 29.3s,	estimator xgboost's best error=3.7673,	best estimator xgboost's best error=3.7673
[flaml.automl: 09-16 21:58:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:58:20] {3072} INFO -  at 31.3s,	estimator xgboost's best error=3.7673,	best estimator xgboost's best error=3.7673
[flaml.automl: 09-16 21:58:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:58:22] {3072} INFO -  at 33.8s,	estimator xgboost's best error=3.7673,	best estimator xgboost's best error=3.7673
[flaml.automl: 09-16 21:58:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:58:23] {3072} INFO -  at 34.9s,	estimator xgboost's best error=3.7673,	best estimator xgboost's best error=3.7673
[flaml.automl: 09-16 21:58:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:58:26] {3072} INFO -  at 37.6s,	estimator xgboost's best error=3.7673,	best estimator xgboost's best error=3.7673
[flaml.automl: 09-16 21:58:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:58:27] {3072} INFO -  at 38.8s,	estimator xgboost's best error=3.7673,	best estimator xgboost's best error=3.7673
[flaml.automl: 09-16 21:58:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:58:28] {3072} INFO -  at 39.9s,	estimator xgboost's best error=3.7673,	best estimator xgboost's best error=3.7673
[flaml.automl: 09-16 21:58:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:58:35] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.5745,	best estimator xgboost's best error=3.5745
[flaml.automl: 09-16 21:58:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:58:47] {3072} INFO -  at 58.6s,	estimator xgboost's best error=3.5212,	best estimator xgboost's best error=3.5212
[flaml.automl: 09-16 21:59:04] {3335} INFO - retrain xgboost for 17.0s
[flaml.automl: 09-16 21:59:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:59:04] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:59:04] {2637} INFO - Time taken to find the best model: 58.55155277252197
[flaml.automl: 09-16 21:59:04] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.521173532885225
NO2(0)最好结果：{'pred_time': 9.178328703966021e-06, 'wall_clock_time': 58.55155277252197, 'metric_for_logging': {'pred_time': 9.178328703966021e-06}, 'val_loss': 3.521173532885225, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.100915908813477}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8463162532177074
NO2(0)的mse=31.64208366853253
NO2(0)的mae=3.435467817830717
NO2(0)的mar=0.16613849452863985
总共花费的时间为：76.34
南宁市
1401A
1408A
[flaml.automl: 09-16 22:05:35] {2390} INFO - task = regression
[flaml.automl: 09-16 22:05:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 22:05:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 22:05:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 22:05:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 22:05:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 22:05:37] {3025} INFO - Estimated sufficient time budget=12176s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 22:05:37] {3072} INFO -  at 1.3s,	estimator xgboost's best error=14.8501,	best estimator xgboost's best error=14.8501
[flaml.automl: 09-16 22:05:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 22:05:39] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.0553,	best estimator xgboost's best error=7.0553
[flaml.automl: 09-16 22:05:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 22:05:40] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.0553,	best estimator xgboost's best error=7.0553
[flaml.automl: 09-16 22:05:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 22:05:49] {3072} INFO -  at 14.2s,	estimator xgboost's best error=7.0553,	best estimator xgboost's best error=7.0553
[flaml.automl: 09-16 22:05:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 22:05:51] {3072} INFO -  at 15.3s,	estimator xgboost's best error=4.5200,	best estimator xgboost's best error=4.5200
[flaml.automl: 09-16 22:05:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 22:05:52] {3072} INFO -  at 16.9s,	estimator xgboost's best error=3.9819,	best estimator xgboost's best error=3.9819
[flaml.automl: 09-16 22:05:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 22:05:54] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.9819,	best estimator xgboost's best error=3.9819
[flaml.automl: 09-16 22:05:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 22:05:56] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.9819,	best estimator xgboost's best error=3.9819
[flaml.automl: 09-16 22:05:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 22:05:57] {3072} INFO -  at 22.2s,	estimator xgboost's best error=3.9819,	best estimator xgboost's best error=3.9819
[flaml.automl: 09-16 22:05:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 22:06:00] {3072} INFO -  at 24.6s,	estimator xgboost's best error=3.9819,	best estimator xgboost's best error=3.9819
[flaml.automl: 09-16 22:06:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 22:06:01] {3072} INFO -  at 25.8s,	estimator xgboost's best error=3.9819,	best estimator xgboost's best error=3.9819
[flaml.automl: 09-16 22:06:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 22:06:02] {3072} INFO -  at 26.9s,	estimator xgboost's best error=3.9819,	best estimator xgboost's best error=3.9819
[flaml.automl: 09-16 22:06:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 22:06:08] {3072} INFO -  at 33.0s,	estimator xgboost's best error=3.8603,	best estimator xgboost's best error=3.8603
[flaml.automl: 09-16 22:06:08] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 22:06:19] {3072} INFO -  at 43.4s,	estimator xgboost's best error=3.7363,	best estimator xgboost's best error=3.7363
[flaml.automl: 09-16 22:06:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 22:06:25] {3072} INFO -  at 49.5s,	estimator xgboost's best error=3.7363,	best estimator xgboost's best error=3.7363
[flaml.automl: 09-16 22:06:25] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 22:06:35] {3072} INFO -  at 59.3s,	estimator xgboost's best error=3.7363,	best estimator xgboost's best error=3.7363
[flaml.automl: 09-16 22:06:45] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-16 22:06:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 22:06:45] {2636} INFO - fit succeeded
[flaml.automl: 09-16 22:06:45] {2637} INFO - Time taken to find the best model: 43.43265891075134
[flaml.automl: 09-16 22:06:45] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.736261301964344
NO2(0)最好结果：{'pred_time': 2.0444545081437597e-05, 'wall_clock_time': 43.43265891075134, 'metric_for_logging': {'pred_time': 2.0444545081437597e-05}, 'val_loss': 3.736261301964344, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.451040506362915}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8197096537461869
NO2(0)的mse=26.830905190989398
NO2(0)的mae=3.5697209441333735
NO2(0)的mar=0.1769922932844006
总共花费的时间为：70.25
海口市
1409A
1411A
1413A
3539A
[flaml.automl: 09-16 22:18:36] {2390} INFO - task = regression
[flaml.automl: 09-16 22:18:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 22:18:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 22:18:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 22:18:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 22:18:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 22:18:37] {3025} INFO - Estimated sufficient time budget=48979s. Estimated necessary time budget=49s.
[flaml.automl: 09-16 22:18:37] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.3316,	best estimator xgboost's best error=5.3316
[flaml.automl: 09-16 22:18:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 22:18:39] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.8633,	best estimator xgboost's best error=2.8633
[flaml.automl: 09-16 22:18:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 22:18:40] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.8633,	best estimator xgboost's best error=2.8633
[flaml.automl: 09-16 22:18:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 22:18:47] {3072} INFO -  at 11.1s,	estimator xgboost's best error=2.8633,	best estimator xgboost's best error=2.8633
[flaml.automl: 09-16 22:18:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 22:18:48] {3072} INFO -  at 12.2s,	estimator xgboost's best error=2.2280,	best estimator xgboost's best error=2.2280
[flaml.automl: 09-16 22:18:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 22:18:49] {3072} INFO -  at 13.8s,	estimator xgboost's best error=2.0865,	best estimator xgboost's best error=2.0865
[flaml.automl: 09-16 22:18:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 22:18:51] {3072} INFO -  at 15.4s,	estimator xgboost's best error=2.0865,	best estimator xgboost's best error=2.0865
[flaml.automl: 09-16 22:18:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 22:18:54] {3072} INFO -  at 17.9s,	estimator xgboost's best error=2.0865,	best estimator xgboost's best error=2.0865
[flaml.automl: 09-16 22:18:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 22:18:55] {3072} INFO -  at 19.0s,	estimator xgboost's best error=2.0865,	best estimator xgboost's best error=2.0865
[flaml.automl: 09-16 22:18:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 22:18:57] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.0865,	best estimator xgboost's best error=2.0865
[flaml.automl: 09-16 22:18:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 22:18:59] {3072} INFO -  at 23.3s,	estimator xgboost's best error=2.0795,	best estimator xgboost's best error=2.0795
[flaml.automl: 09-16 22:18:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 22:19:00] {3072} INFO -  at 24.4s,	estimator xgboost's best error=2.0795,	best estimator xgboost's best error=2.0795
[flaml.automl: 09-16 22:19:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 22:19:07] {3072} INFO -  at 30.9s,	estimator xgboost's best error=2.0449,	best estimator xgboost's best error=2.0449
[flaml.automl: 09-16 22:19:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 22:19:19] {3072} INFO -  at 43.0s,	estimator xgboost's best error=2.0240,	best estimator xgboost's best error=2.0240
[flaml.automl: 09-16 22:19:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 22:19:25] {3072} INFO -  at 49.5s,	estimator xgboost's best error=2.0240,	best estimator xgboost's best error=2.0240
[flaml.automl: 09-16 22:19:46] {3335} INFO - retrain xgboost for 20.8s
[flaml.automl: 09-16 22:19:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 22:19:46] {2636} INFO - fit succeeded
[flaml.automl: 09-16 22:19:46] {2637} INFO - Time taken to find the best model: 42.99968481063843
[flaml.automl: 09-16 22:19:46] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40718}
NO2(0)最佳损失：-1.0240227288962727
NO2(0)最好结果：{'pred_time': 8.811529170083736e-06, 'wall_clock_time': 42.99968481063843, 'metric_for_logging': {'pred_time': 8.811529170083736e-06}, 'val_loss': 2.0240227288962727, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40718}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40718, 'experiment_tag': 'exp', 'time_total_s': 12.052772998809814}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6340781221932783
NO2(0)的mse=10.531993061577879
NO2(0)的mae=2.040363092936601
NO2(0)的mar=0.3179145541520888
总共花费的时间为：71.06
重庆市
1414A
1418A
1419A
1420A
1422A
1428A
1429A
3015A
3016A
3346A
3347A
3348A
3349A
3350A
3351A
3352A
3353A
3354A
3355A
3356A
3482A
3483A
3484A
3485A
3599A
3600A
3601A
3610A
[flaml.automl: 09-16 23:43:36] {2390} INFO - task = regression
[flaml.automl: 09-16 23:43:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 23:43:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 23:43:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 23:43:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 23:43:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 23:43:37] {3025} INFO - Estimated sufficient time budget=209555s. Estimated necessary time budget=210s.
[flaml.automl: 09-16 23:43:37] {3072} INFO -  at 2.9s,	estimator xgboost's best error=19.8408,	best estimator xgboost's best error=19.8408
[flaml.automl: 09-16 23:43:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 23:43:38] {3072} INFO -  at 3.5s,	estimator xgboost's best error=17.9900,	best estimator xgboost's best error=17.9900
[flaml.automl: 09-16 23:43:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 23:43:38] {3072} INFO -  at 4.2s,	estimator xgboost's best error=17.9900,	best estimator xgboost's best error=17.9900
[flaml.automl: 09-16 23:43:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 23:43:39] {3072} INFO -  at 4.9s,	estimator xgboost's best error=17.9900,	best estimator xgboost's best error=17.9900
[flaml.automl: 09-16 23:43:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 23:43:40] {3072} INFO -  at 5.6s,	estimator xgboost's best error=8.2222,	best estimator xgboost's best error=8.2222
[flaml.automl: 09-16 23:43:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 23:43:40] {3072} INFO -  at 6.1s,	estimator xgboost's best error=8.2222,	best estimator xgboost's best error=8.2222
[flaml.automl: 09-16 23:43:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 23:43:41] {3072} INFO -  at 6.8s,	estimator xgboost's best error=8.2222,	best estimator xgboost's best error=8.2222
[flaml.automl: 09-16 23:43:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 23:43:42] {3072} INFO -  at 7.4s,	estimator xgboost's best error=8.2222,	best estimator xgboost's best error=8.2222
[flaml.automl: 09-16 23:43:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 23:43:42] {3072} INFO -  at 8.1s,	estimator xgboost's best error=5.1329,	best estimator xgboost's best error=5.1329
[flaml.automl: 09-16 23:43:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 23:43:43] {3072} INFO -  at 8.7s,	estimator xgboost's best error=5.1329,	best estimator xgboost's best error=5.1329
[flaml.automl: 09-16 23:43:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 23:43:44] {3072} INFO -  at 9.4s,	estimator xgboost's best error=5.1005,	best estimator xgboost's best error=5.1005
[flaml.automl: 09-16 23:43:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 23:43:44] {3072} INFO -  at 10.1s,	estimator xgboost's best error=5.1005,	best estimator xgboost's best error=5.1005
[flaml.automl: 09-16 23:43:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 23:43:45] {3072} INFO -  at 10.8s,	estimator xgboost's best error=4.3454,	best estimator xgboost's best error=4.3454
[flaml.automl: 09-16 23:43:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 23:43:46] {3072} INFO -  at 11.4s,	estimator xgboost's best error=4.3454,	best estimator xgboost's best error=4.3454
[flaml.automl: 09-16 23:43:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 23:43:46] {3072} INFO -  at 12.1s,	estimator xgboost's best error=4.3454,	best estimator xgboost's best error=4.3454
[flaml.automl: 09-16 23:43:46] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 23:43:47] {3072} INFO -  at 12.8s,	estimator xgboost's best error=4.3454,	best estimator xgboost's best error=4.3454
[flaml.automl: 09-16 23:43:47] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 23:43:48] {3072} INFO -  at 13.3s,	estimator xgboost's best error=4.3454,	best estimator xgboost's best error=4.3454
[flaml.automl: 09-16 23:43:48] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 23:43:50] {3072} INFO -  at 16.2s,	estimator xgboost's best error=4.0427,	best estimator xgboost's best error=4.0427
[flaml.automl: 09-16 23:43:50] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 23:43:52] {3072} INFO -  at 18.1s,	estimator xgboost's best error=4.0427,	best estimator xgboost's best error=4.0427
[flaml.automl: 09-16 23:43:52] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 23:43:55] {3072} INFO -  at 20.5s,	estimator xgboost's best error=4.0427,	best estimator xgboost's best error=4.0427
[flaml.automl: 09-16 23:43:55] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 23:43:56] {3072} INFO -  at 22.2s,	estimator xgboost's best error=4.0427,	best estimator xgboost's best error=4.0427
[flaml.automl: 09-16 23:43:56] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 23:43:59] {3072} INFO -  at 24.4s,	estimator xgboost's best error=4.0267,	best estimator xgboost's best error=4.0267
[flaml.automl: 09-16 23:43:59] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 23:44:01] {3072} INFO -  at 26.4s,	estimator xgboost's best error=4.0267,	best estimator xgboost's best error=4.0267
[flaml.automl: 09-16 23:44:01] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-16 23:44:03] {3072} INFO -  at 28.3s,	estimator xgboost's best error=4.0267,	best estimator xgboost's best error=4.0267
[flaml.automl: 09-16 23:44:03] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-16 23:44:05] {3072} INFO -  at 30.9s,	estimator xgboost's best error=3.9648,	best estimator xgboost's best error=3.9648
[flaml.automl: 09-16 23:44:05] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-16 23:44:06] {3072} INFO -  at 32.1s,	estimator xgboost's best error=3.9648,	best estimator xgboost's best error=3.9648
[flaml.automl: 09-16 23:44:06] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-16 23:44:08] {3072} INFO -  at 34.0s,	estimator xgboost's best error=3.9648,	best estimator xgboost's best error=3.9648
[flaml.automl: 09-16 23:44:08] {2897} INFO - iteration 27, current learner xgboost
[flaml.automl: 09-16 23:44:13] {3072} INFO -  at 39.0s,	estimator xgboost's best error=3.9648,	best estimator xgboost's best error=3.9648
[flaml.automl: 09-16 23:44:13] {2897} INFO - iteration 28, current learner xgboost
[flaml.automl: 09-16 23:44:20] {3072} INFO -  at 45.7s,	estimator xgboost's best error=3.9648,	best estimator xgboost's best error=3.9648
[flaml.automl: 09-16 23:44:20] {2897} INFO - iteration 29, current learner xgboost
[flaml.automl: 09-16 23:44:33] {3072} INFO -  at 58.4s,	estimator xgboost's best error=3.9041,	best estimator xgboost's best error=3.9041
[flaml.automl: 09-16 23:45:31] {3335} INFO - retrain xgboost for 58.1s
[flaml.automl: 09-16 23:45:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.5557016263743871, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=43, min_child_weight=0.4933009733534718,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9818621502948582, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 23:45:31] {2636} INFO - fit succeeded
[flaml.automl: 09-16 23:45:31] {2637} INFO - Time taken to find the best model: 58.361714601516724
[flaml.automl: 09-16 23:45:31] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 43, 'min_child_weight': 0.4933009733534718, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.5557016263743871, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9818621502948582, 'FLAML_sample_size': 300240}
NO2(0)最佳损失：-2.9040900213791314
NO2(0)最好结果：{'pred_time': 2.795100497970764e-06, 'wall_clock_time': 58.361714601516724, 'metric_for_logging': {'pred_time': 2.795100497970764e-06}, 'val_loss': 3.9040900213791314, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 43, 'min_child_weight': 0.4933009733534718, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.5557016263743871, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9818621502948582, 'FLAML_sample_size': 300240}, 'config/n_estimators': 19, 'config/max_leaves': 43, 'config/min_child_weight': 0.4933009733534718, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.5557016263743871, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.9818621502948582, 'config/FLAML_sample_size': 300240, 'experiment_tag': 'exp', 'time_total_s': 12.706445932388306}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.5557016263743871, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=43, min_child_weight=0.4933009733534718,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9818621502948582, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8338421173486239
NO2(0)的mse=32.7348782565067
NO2(0)的mae=3.870683874817375
NO2(0)的mar=0.1891928951771394
总共花费的时间为：121.52
贵阳市
1440A
1442A
1443A
1444A
1445A
1446A
[flaml.automl: 09-17 00:03:39] {2390} INFO - task = regression
[flaml.automl: 09-17 00:03:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 00:03:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 00:03:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 00:03:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 00:03:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 00:03:40] {3025} INFO - Estimated sufficient time budget=77231s. Estimated necessary time budget=77s.
[flaml.automl: 09-17 00:03:40] {3072} INFO -  at 1.5s,	estimator xgboost's best error=11.5987,	best estimator xgboost's best error=11.5987
[flaml.automl: 09-17 00:03:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 00:03:42] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.5408,	best estimator xgboost's best error=5.5408
[flaml.automl: 09-17 00:03:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 00:03:44] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.5408,	best estimator xgboost's best error=5.5408
[flaml.automl: 09-17 00:03:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 00:03:47] {3072} INFO -  at 8.6s,	estimator xgboost's best error=5.5408,	best estimator xgboost's best error=5.5408
[flaml.automl: 09-17 00:03:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 00:03:49] {3072} INFO -  at 9.8s,	estimator xgboost's best error=3.7441,	best estimator xgboost's best error=3.7441
[flaml.automl: 09-17 00:03:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 00:03:50] {3072} INFO -  at 11.3s,	estimator xgboost's best error=3.2808,	best estimator xgboost's best error=3.2808
[flaml.automl: 09-17 00:03:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 00:03:52] {3072} INFO -  at 13.0s,	estimator xgboost's best error=3.2808,	best estimator xgboost's best error=3.2808
[flaml.automl: 09-17 00:03:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 00:03:54] {3072} INFO -  at 15.4s,	estimator xgboost's best error=3.2808,	best estimator xgboost's best error=3.2808
[flaml.automl: 09-17 00:03:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 00:03:55] {3072} INFO -  at 16.6s,	estimator xgboost's best error=3.2808,	best estimator xgboost's best error=3.2808
[flaml.automl: 09-17 00:03:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 00:03:58] {3072} INFO -  at 19.2s,	estimator xgboost's best error=3.2808,	best estimator xgboost's best error=3.2808
[flaml.automl: 09-17 00:03:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 00:04:00] {3072} INFO -  at 20.8s,	estimator xgboost's best error=3.2808,	best estimator xgboost's best error=3.2808
[flaml.automl: 09-17 00:04:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 00:04:01] {3072} INFO -  at 22.0s,	estimator xgboost's best error=3.2808,	best estimator xgboost's best error=3.2808
[flaml.automl: 09-17 00:04:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 00:04:07] {3072} INFO -  at 28.5s,	estimator xgboost's best error=3.1851,	best estimator xgboost's best error=3.1851
[flaml.automl: 09-17 00:04:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 00:04:20] {3072} INFO -  at 40.7s,	estimator xgboost's best error=3.1132,	best estimator xgboost's best error=3.1132
[flaml.automl: 09-17 00:04:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 00:04:26] {3072} INFO -  at 47.3s,	estimator xgboost's best error=3.1132,	best estimator xgboost's best error=3.1132
[flaml.automl: 09-17 00:04:26] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 00:04:39] {3072} INFO -  at 59.8s,	estimator xgboost's best error=3.0946,	best estimator xgboost's best error=3.0946
[flaml.automl: 09-17 00:05:00] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-17 00:05:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 00:05:00] {2636} INFO - fit succeeded
[flaml.automl: 09-17 00:05:00] {2637} INFO - Time taken to find the best model: 59.785943269729614
[flaml.automl: 09-17 00:05:00] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 64177}
NO2(0)最佳损失：-2.094637981823226
NO2(0)最好结果：{'pred_time': 5.725054967588971e-06, 'wall_clock_time': 59.785943269729614, 'metric_for_logging': {'pred_time': 5.725054967588971e-06}, 'val_loss': 3.094637981823226, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 64177}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 64177, 'experiment_tag': 'exp', 'time_total_s': 12.521219491958618}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.793821587248245
NO2(0)的mse=20.130694150031307
NO2(0)的mae=3.1078007352556742
NO2(0)的mar=0.19894575214066765
总共花费的时间为：82.29
昆明市
1452A
1453A
1455A
3179A
3375A
3550A
3551A
3552A
[flaml.automl: 09-17 00:30:09] {2390} INFO - task = regression
[flaml.automl: 09-17 00:30:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 00:30:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 00:30:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 00:30:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 00:30:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 00:30:10] {3025} INFO - Estimated sufficient time budget=104107s. Estimated necessary time budget=104s.
[flaml.automl: 09-17 00:30:10] {3072} INFO -  at 1.7s,	estimator xgboost's best error=13.3748,	best estimator xgboost's best error=13.3748
[flaml.automl: 09-17 00:30:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 00:30:13] {3072} INFO -  at 3.9s,	estimator xgboost's best error=6.7112,	best estimator xgboost's best error=6.7112
[flaml.automl: 09-17 00:30:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 00:30:14] {3072} INFO -  at 5.1s,	estimator xgboost's best error=6.7112,	best estimator xgboost's best error=6.7112
[flaml.automl: 09-17 00:30:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 00:30:16] {3072} INFO -  at 7.9s,	estimator xgboost's best error=6.7112,	best estimator xgboost's best error=6.7112
[flaml.automl: 09-17 00:30:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 00:30:18] {3072} INFO -  at 9.0s,	estimator xgboost's best error=4.7338,	best estimator xgboost's best error=4.7338
[flaml.automl: 09-17 00:30:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 00:30:19] {3072} INFO -  at 10.6s,	estimator xgboost's best error=4.2825,	best estimator xgboost's best error=4.2825
[flaml.automl: 09-17 00:30:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 00:30:21] {3072} INFO -  at 12.2s,	estimator xgboost's best error=4.2825,	best estimator xgboost's best error=4.2825
[flaml.automl: 09-17 00:30:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 00:30:23] {3072} INFO -  at 14.7s,	estimator xgboost's best error=4.2825,	best estimator xgboost's best error=4.2825
[flaml.automl: 09-17 00:30:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 00:30:24] {3072} INFO -  at 15.8s,	estimator xgboost's best error=4.2825,	best estimator xgboost's best error=4.2825
[flaml.automl: 09-17 00:30:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 00:30:26] {3072} INFO -  at 17.8s,	estimator xgboost's best error=4.2825,	best estimator xgboost's best error=4.2825
[flaml.automl: 09-17 00:30:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 00:30:28] {3072} INFO -  at 19.5s,	estimator xgboost's best error=4.2731,	best estimator xgboost's best error=4.2731
[flaml.automl: 09-17 00:30:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 00:30:29] {3072} INFO -  at 20.7s,	estimator xgboost's best error=4.2731,	best estimator xgboost's best error=4.2731
[flaml.automl: 09-17 00:30:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 00:30:36] {3072} INFO -  at 27.3s,	estimator xgboost's best error=4.0785,	best estimator xgboost's best error=4.0785
[flaml.automl: 09-17 00:30:36] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 00:30:48] {3072} INFO -  at 39.4s,	estimator xgboost's best error=3.9991,	best estimator xgboost's best error=3.9991
[flaml.automl: 09-17 00:30:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 00:30:55] {3072} INFO -  at 45.9s,	estimator xgboost's best error=3.9991,	best estimator xgboost's best error=3.9991
[flaml.automl: 09-17 00:30:55] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 00:31:08] {3072} INFO -  at 59.6s,	estimator xgboost's best error=3.9927,	best estimator xgboost's best error=3.9927
[flaml.automl: 09-17 00:31:30] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-17 00:31:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 00:31:30] {2636} INFO - fit succeeded
[flaml.automl: 09-17 00:31:30] {2637} INFO - Time taken to find the best model: 59.59468698501587
[flaml.automl: 09-17 00:31:30] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 86812}
NO2(0)最佳损失：-2.9926905964707626
NO2(0)最好结果：{'pred_time': 4.142269464342915e-06, 'wall_clock_time': 59.59468698501587, 'metric_for_logging': {'pred_time': 4.142269464342915e-06}, 'val_loss': 3.9926905964707626, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 86812}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 86812, 'experiment_tag': 'exp', 'time_total_s': 13.658757448196411}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7425578185158517
NO2(0)的mse=38.28692922068389
NO2(0)的mae=3.9876439661026715
NO2(0)的mar=0.2158784974252978
总共花费的时间为：82.50
拉萨市
1456A
1457A
1458A
1461A
[flaml.automl: 09-17 00:44:28] {2390} INFO - task = regression
[flaml.automl: 09-17 00:44:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 00:44:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 00:44:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 00:44:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 00:44:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 00:44:29] {3025} INFO - Estimated sufficient time budget=51437s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 00:44:29] {3072} INFO -  at 1.5s,	estimator xgboost's best error=7.8266,	best estimator xgboost's best error=7.8266
[flaml.automl: 09-17 00:44:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 00:44:31] {3072} INFO -  at 3.6s,	estimator xgboost's best error=4.3328,	best estimator xgboost's best error=4.3328
[flaml.automl: 09-17 00:44:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 00:44:32] {3072} INFO -  at 4.8s,	estimator xgboost's best error=4.3328,	best estimator xgboost's best error=4.3328
[flaml.automl: 09-17 00:44:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 00:44:39] {3072} INFO -  at 11.1s,	estimator xgboost's best error=4.3328,	best estimator xgboost's best error=4.3328
[flaml.automl: 09-17 00:44:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 00:44:40] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.5148,	best estimator xgboost's best error=3.5148
[flaml.automl: 09-17 00:44:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 00:44:41] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.4153,	best estimator xgboost's best error=3.4153
[flaml.automl: 09-17 00:44:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 00:44:43] {3072} INFO -  at 15.5s,	estimator xgboost's best error=3.4153,	best estimator xgboost's best error=3.4153
[flaml.automl: 09-17 00:44:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 00:44:45] {3072} INFO -  at 17.9s,	estimator xgboost's best error=3.4153,	best estimator xgboost's best error=3.4153
[flaml.automl: 09-17 00:44:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 00:44:46] {3072} INFO -  at 19.0s,	estimator xgboost's best error=3.4153,	best estimator xgboost's best error=3.4153
[flaml.automl: 09-17 00:44:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 00:44:49] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.3588,	best estimator xgboost's best error=3.3588
[flaml.automl: 09-17 00:44:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 00:44:51] {3072} INFO -  at 23.2s,	estimator xgboost's best error=3.3588,	best estimator xgboost's best error=3.3588
[flaml.automl: 09-17 00:44:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 00:44:52] {3072} INFO -  at 24.6s,	estimator xgboost's best error=3.3588,	best estimator xgboost's best error=3.3588
[flaml.automl: 09-17 00:44:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 00:44:55] {3072} INFO -  at 27.5s,	estimator xgboost's best error=3.3588,	best estimator xgboost's best error=3.3588
[flaml.automl: 09-17 00:44:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 00:44:59] {3072} INFO -  at 31.3s,	estimator xgboost's best error=3.3588,	best estimator xgboost's best error=3.3588
[flaml.automl: 09-17 00:44:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 00:45:01] {3072} INFO -  at 34.0s,	estimator xgboost's best error=3.3517,	best estimator xgboost's best error=3.3517
[flaml.automl: 09-17 00:45:01] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 00:45:06] {3072} INFO -  at 38.6s,	estimator xgboost's best error=3.3517,	best estimator xgboost's best error=3.3517
[flaml.automl: 09-17 00:45:06] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 00:45:08] {3072} INFO -  at 40.2s,	estimator xgboost's best error=3.3517,	best estimator xgboost's best error=3.3517
[flaml.automl: 09-17 00:45:08] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 00:45:09] {3072} INFO -  at 41.6s,	estimator xgboost's best error=3.3517,	best estimator xgboost's best error=3.3517
[flaml.automl: 09-17 00:45:09] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 00:45:16] {3072} INFO -  at 48.5s,	estimator xgboost's best error=3.3517,	best estimator xgboost's best error=3.3517
[flaml.automl: 09-17 00:45:16] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 00:45:17] {3072} INFO -  at 50.1s,	estimator xgboost's best error=3.3517,	best estimator xgboost's best error=3.3517
[flaml.automl: 09-17 00:45:17] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 00:45:25] {3072} INFO -  at 57.6s,	estimator xgboost's best error=3.3409,	best estimator xgboost's best error=3.3409
[flaml.automl: 09-17 00:45:33] {3335} INFO - retrain xgboost for 7.5s
[flaml.automl: 09-17 00:45:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7526449597462058, colsample_bynode=1,
             colsample_bytree=0.7562459575858208, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=22, min_child_weight=0.013026267655675718,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.002103225785486898, reg_lambda=0.6522497901406781,
             scale_pos_weight=1, subsample=0.8620707939384471,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 00:45:33] {2636} INFO - fit succeeded
[flaml.automl: 09-17 00:45:33] {2637} INFO - Time taken to find the best model: 57.637702226638794
[flaml.automl: 09-17 00:45:33] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 22, 'min_child_weight': 0.013026267655675718, 'learning_rate': 1.0, 'subsample': 0.8620707939384471, 'colsample_bylevel': 0.7526449597462058, 'colsample_bytree': 0.7562459575858208, 'reg_alpha': 0.002103225785486898, 'reg_lambda': 0.6522497901406781, 'FLAML_sample_size': 41498}
NO2(0)最佳损失：-2.340915933577778
NO2(0)最好结果：{'pred_time': 8.762645452561851e-06, 'wall_clock_time': 57.637702226638794, 'metric_for_logging': {'pred_time': 8.762645452561851e-06}, 'val_loss': 3.340915933577778, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 22, 'min_child_weight': 0.013026267655675718, 'learning_rate': 1.0, 'subsample': 0.8620707939384471, 'colsample_bylevel': 0.7526449597462058, 'colsample_bytree': 0.7562459575858208, 'reg_alpha': 0.002103225785486898, 'reg_lambda': 0.6522497901406781, 'FLAML_sample_size': 41498}, 'config/n_estimators': 6, 'config/max_leaves': 22, 'config/min_child_weight': 0.013026267655675718, 'config/learning_rate': 1.0, 'config/subsample': 0.8620707939384471, 'config/colsample_bylevel': 0.7526449597462058, 'config/colsample_bytree': 0.7562459575858208, 'config/reg_alpha': 0.002103225785486898, 'config/reg_lambda': 0.6522497901406781, 'config/FLAML_sample_size': 41498, 'experiment_tag': 'exp', 'time_total_s': 7.567946434020996}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7526449597462058, colsample_bynode=1,
             colsample_bytree=0.7562459575858208, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=22, min_child_weight=0.013026267655675718,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.002103225785486898, reg_lambda=0.6522497901406781,
             scale_pos_weight=1, subsample=0.8620707939384471,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.5473816431862404
NO2(0)的mse=26.47045418005945
NO2(0)的mae=3.2710844808077084
NO2(0)的mar=0.2945824112021592
总共花费的时间为：65.92
西安市
1462A
1463A
1464A
1465A
1466A
1468A
1474A
3524A
3605A
[flaml.automl: 09-17 01:13:10] {2390} INFO - task = regression
[flaml.automl: 09-17 01:13:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:13:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:13:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:13:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:13:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:13:12] {3025} INFO - Estimated sufficient time budget=211687s. Estimated necessary time budget=212s.
[flaml.automl: 09-17 01:13:12] {3072} INFO -  at 2.7s,	estimator xgboost's best error=24.2757,	best estimator xgboost's best error=24.2757
[flaml.automl: 09-17 01:13:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:13:15] {3072} INFO -  at 5.3s,	estimator xgboost's best error=13.3359,	best estimator xgboost's best error=13.3359
[flaml.automl: 09-17 01:13:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:13:17] {3072} INFO -  at 7.5s,	estimator xgboost's best error=13.3359,	best estimator xgboost's best error=13.3359
[flaml.automl: 09-17 01:13:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:13:19] {3072} INFO -  at 9.7s,	estimator xgboost's best error=13.3359,	best estimator xgboost's best error=13.3359
[flaml.automl: 09-17 01:13:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:13:21] {3072} INFO -  at 11.6s,	estimator xgboost's best error=7.3266,	best estimator xgboost's best error=7.3266
[flaml.automl: 09-17 01:13:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:13:23] {3072} INFO -  at 13.6s,	estimator xgboost's best error=6.9269,	best estimator xgboost's best error=6.9269
[flaml.automl: 09-17 01:13:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:13:25] {3072} INFO -  at 15.4s,	estimator xgboost's best error=6.9269,	best estimator xgboost's best error=6.9269
[flaml.automl: 09-17 01:13:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:13:27] {3072} INFO -  at 17.4s,	estimator xgboost's best error=6.9269,	best estimator xgboost's best error=6.9269
[flaml.automl: 09-17 01:13:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:13:29] {3072} INFO -  at 19.1s,	estimator xgboost's best error=6.9269,	best estimator xgboost's best error=6.9269
[flaml.automl: 09-17 01:13:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:13:30] {3072} INFO -  at 20.4s,	estimator xgboost's best error=6.9269,	best estimator xgboost's best error=6.9269
[flaml.automl: 09-17 01:13:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:13:33] {3072} INFO -  at 23.3s,	estimator xgboost's best error=6.1560,	best estimator xgboost's best error=6.1560
[flaml.automl: 09-17 01:13:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:13:35] {3072} INFO -  at 25.3s,	estimator xgboost's best error=6.1560,	best estimator xgboost's best error=6.1560
[flaml.automl: 09-17 01:13:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:13:50] {3072} INFO -  at 40.3s,	estimator xgboost's best error=5.7582,	best estimator xgboost's best error=5.7582
[flaml.automl: 09-17 01:13:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:14:09] {3072} INFO -  at 59.3s,	estimator xgboost's best error=5.6216,	best estimator xgboost's best error=5.6216
[flaml.automl: 09-17 01:14:30] {3335} INFO - retrain xgboost for 21.2s
[flaml.automl: 09-17 01:14:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:14:30] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:14:30] {2637} INFO - Time taken to find the best model: 59.26727223396301
[flaml.automl: 09-17 01:14:30] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 99435}
NO2(0)最佳损失：-4.621580021145528
NO2(0)最好结果：{'pred_time': 8.144338958046967e-06, 'wall_clock_time': 59.26727223396301, 'metric_for_logging': {'pred_time': 8.144338958046967e-06}, 'val_loss': 5.621580021145528, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 99435}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 99435, 'experiment_tag': 'exp', 'time_total_s': 18.917480945587158}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8629889166900404
NO2(0)的mse=69.32331898573193
NO2(0)的mae=5.598543724791763
NO2(0)的mar=0.18528834600892038
总共花费的时间为：82.30
兰州市
1478A
3186A
3241A
3242A
[flaml.automl: 09-17 01:27:29] {2390} INFO - task = regression
[flaml.automl: 09-17 01:27:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:27:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:27:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:27:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:27:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:27:30] {3025} INFO - Estimated sufficient time budget=53074s. Estimated necessary time budget=53s.
[flaml.automl: 09-17 01:27:30] {3072} INFO -  at 1.4s,	estimator xgboost's best error=24.6068,	best estimator xgboost's best error=24.6068
[flaml.automl: 09-17 01:27:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:27:32] {3072} INFO -  at 3.5s,	estimator xgboost's best error=11.8984,	best estimator xgboost's best error=11.8984
[flaml.automl: 09-17 01:27:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:27:34] {3072} INFO -  at 4.7s,	estimator xgboost's best error=11.8984,	best estimator xgboost's best error=11.8984
[flaml.automl: 09-17 01:27:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:27:39] {3072} INFO -  at 10.6s,	estimator xgboost's best error=11.8984,	best estimator xgboost's best error=11.8984
[flaml.automl: 09-17 01:27:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:27:41] {3072} INFO -  at 11.7s,	estimator xgboost's best error=7.5929,	best estimator xgboost's best error=7.5929
[flaml.automl: 09-17 01:27:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:27:42] {3072} INFO -  at 13.3s,	estimator xgboost's best error=6.6139,	best estimator xgboost's best error=6.6139
[flaml.automl: 09-17 01:27:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:27:44] {3072} INFO -  at 14.9s,	estimator xgboost's best error=6.6089,	best estimator xgboost's best error=6.6089
[flaml.automl: 09-17 01:27:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:27:46] {3072} INFO -  at 17.6s,	estimator xgboost's best error=6.6089,	best estimator xgboost's best error=6.6089
[flaml.automl: 09-17 01:27:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:27:48] {3072} INFO -  at 19.2s,	estimator xgboost's best error=6.6089,	best estimator xgboost's best error=6.6089
[flaml.automl: 09-17 01:27:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:27:51] {3072} INFO -  at 22.2s,	estimator xgboost's best error=6.4096,	best estimator xgboost's best error=6.4096
[flaml.automl: 09-17 01:27:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:27:53] {3072} INFO -  at 23.9s,	estimator xgboost's best error=6.4096,	best estimator xgboost's best error=6.4096
[flaml.automl: 09-17 01:27:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:27:54] {3072} INFO -  at 25.0s,	estimator xgboost's best error=6.4096,	best estimator xgboost's best error=6.4096
[flaml.automl: 09-17 01:27:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:27:58] {3072} INFO -  at 28.8s,	estimator xgboost's best error=6.3882,	best estimator xgboost's best error=6.3882
[flaml.automl: 09-17 01:27:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:28:01] {3072} INFO -  at 31.7s,	estimator xgboost's best error=6.3882,	best estimator xgboost's best error=6.3882
[flaml.automl: 09-17 01:28:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:28:03] {3072} INFO -  at 34.2s,	estimator xgboost's best error=6.3882,	best estimator xgboost's best error=6.3882
[flaml.automl: 09-17 01:28:03] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 01:28:05] {3072} INFO -  at 36.0s,	estimator xgboost's best error=6.3882,	best estimator xgboost's best error=6.3882
[flaml.automl: 09-17 01:28:05] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 01:28:07] {3072} INFO -  at 38.2s,	estimator xgboost's best error=6.3882,	best estimator xgboost's best error=6.3882
[flaml.automl: 09-17 01:28:07] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 01:28:21] {3072} INFO -  at 51.8s,	estimator xgboost's best error=6.0654,	best estimator xgboost's best error=6.0654
[flaml.automl: 09-17 01:28:34] {3335} INFO - retrain xgboost for 13.7s
[flaml.automl: 09-17 01:28:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 01:28:34] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:28:34] {2637} INFO - Time taken to find the best model: 51.77848410606384
[flaml.automl: 09-17 01:28:34] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007, 'FLAML_sample_size': 43995}
NO2(0)最佳损失：-5.065393746155224
NO2(0)最好结果：{'pred_time': 8.141050847865585e-06, 'wall_clock_time': 51.77848410606384, 'metric_for_logging': {'pred_time': 8.141050847865585e-06}, 'val_loss': 6.065393746155224, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007, 'FLAML_sample_size': 43995}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'config/FLAML_sample_size': 43995, 'experiment_tag': 'exp', 'time_total_s': 13.550611019134521}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8611156564674457
NO2(0)的mse=88.48428326810834
NO2(0)的mae=6.247428805195618
NO2(0)的mar=0.236841127553567
总共花费的时间为：66.20
西宁市
3629A
3630A
[flaml.automl: 09-17 01:35:17] {2390} INFO - task = regression
[flaml.automl: 09-17 01:35:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:35:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:35:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:35:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:35:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:35:18] {3025} INFO - Estimated sufficient time budget=11910s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 01:35:18] {3072} INFO -  at 1.3s,	estimator xgboost's best error=20.1819,	best estimator xgboost's best error=20.1819
[flaml.automl: 09-17 01:35:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:35:20] {3072} INFO -  at 3.4s,	estimator xgboost's best error=10.1472,	best estimator xgboost's best error=10.1472
[flaml.automl: 09-17 01:35:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:35:22] {3072} INFO -  at 4.6s,	estimator xgboost's best error=10.1472,	best estimator xgboost's best error=10.1472
[flaml.automl: 09-17 01:35:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:35:31] {3072} INFO -  at 14.0s,	estimator xgboost's best error=10.1472,	best estimator xgboost's best error=10.1472
[flaml.automl: 09-17 01:35:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:35:32] {3072} INFO -  at 15.2s,	estimator xgboost's best error=7.1840,	best estimator xgboost's best error=7.1840
[flaml.automl: 09-17 01:35:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:35:34] {3072} INFO -  at 16.8s,	estimator xgboost's best error=6.5150,	best estimator xgboost's best error=6.5150
[flaml.automl: 09-17 01:35:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:35:35] {3072} INFO -  at 18.4s,	estimator xgboost's best error=6.5150,	best estimator xgboost's best error=6.5150
[flaml.automl: 09-17 01:35:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:35:38] {3072} INFO -  at 20.9s,	estimator xgboost's best error=6.5150,	best estimator xgboost's best error=6.5150
[flaml.automl: 09-17 01:35:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:35:39] {3072} INFO -  at 22.1s,	estimator xgboost's best error=6.5150,	best estimator xgboost's best error=6.5150
[flaml.automl: 09-17 01:35:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:35:42] {3072} INFO -  at 24.6s,	estimator xgboost's best error=6.5150,	best estimator xgboost's best error=6.5150
[flaml.automl: 09-17 01:35:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:35:43] {3072} INFO -  at 25.7s,	estimator xgboost's best error=6.5150,	best estimator xgboost's best error=6.5150
[flaml.automl: 09-17 01:35:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:35:44] {3072} INFO -  at 26.9s,	estimator xgboost's best error=6.5150,	best estimator xgboost's best error=6.5150
[flaml.automl: 09-17 01:35:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:35:50] {3072} INFO -  at 32.9s,	estimator xgboost's best error=6.1300,	best estimator xgboost's best error=6.1300
[flaml.automl: 09-17 01:35:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:36:00] {3072} INFO -  at 43.4s,	estimator xgboost's best error=6.0287,	best estimator xgboost's best error=6.0287
[flaml.automl: 09-17 01:36:00] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:36:06] {3072} INFO -  at 49.5s,	estimator xgboost's best error=6.0287,	best estimator xgboost's best error=6.0287
[flaml.automl: 09-17 01:36:06] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 01:36:16] {3072} INFO -  at 59.2s,	estimator xgboost's best error=6.0131,	best estimator xgboost's best error=6.0131
[flaml.automl: 09-17 01:36:34] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-17 01:36:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:36:34] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:36:34] {2637} INFO - Time taken to find the best model: 59.2327618598938
[flaml.automl: 09-17 01:36:34] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-5.013066391600776
NO2(0)最好结果：{'pred_time': 1.794117013203729e-05, 'wall_clock_time': 59.2327618598938, 'metric_for_logging': {'pred_time': 1.794117013203729e-05}, 'val_loss': 6.013066391600776, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.740458250045776}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7621252521655896
NO2(0)的mse=79.07807252063179
NO2(0)的mae=6.280105393332911
NO2(0)的mar=0.2846714704036364
总共花费的时间为：76.99
银川市
1484A
1488A
2925A
2926A
3523A
[flaml.automl: 09-17 01:52:16] {2390} INFO - task = regression
[flaml.automl: 09-17 01:52:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:52:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:52:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:52:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:52:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:52:17] {3025} INFO - Estimated sufficient time budget=61482s. Estimated necessary time budget=61s.
[flaml.automl: 09-17 01:52:17] {3072} INFO -  at 1.4s,	estimator xgboost's best error=16.4138,	best estimator xgboost's best error=16.4138
[flaml.automl: 09-17 01:52:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:52:20] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.4757,	best estimator xgboost's best error=8.4757
[flaml.automl: 09-17 01:52:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:52:21] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.4757,	best estimator xgboost's best error=8.4757
[flaml.automl: 09-17 01:52:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:52:26] {3072} INFO -  at 9.5s,	estimator xgboost's best error=8.4757,	best estimator xgboost's best error=8.4757
[flaml.automl: 09-17 01:52:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:52:27] {3072} INFO -  at 10.6s,	estimator xgboost's best error=6.0998,	best estimator xgboost's best error=6.0998
[flaml.automl: 09-17 01:52:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:52:28] {3072} INFO -  at 12.2s,	estimator xgboost's best error=5.3042,	best estimator xgboost's best error=5.3042
[flaml.automl: 09-17 01:52:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:52:30] {3072} INFO -  at 13.8s,	estimator xgboost's best error=5.3042,	best estimator xgboost's best error=5.3042
[flaml.automl: 09-17 01:52:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:52:32] {3072} INFO -  at 16.2s,	estimator xgboost's best error=5.3042,	best estimator xgboost's best error=5.3042
[flaml.automl: 09-17 01:52:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:52:33] {3072} INFO -  at 17.3s,	estimator xgboost's best error=5.3042,	best estimator xgboost's best error=5.3042
[flaml.automl: 09-17 01:52:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:52:36] {3072} INFO -  at 20.0s,	estimator xgboost's best error=5.3042,	best estimator xgboost's best error=5.3042
[flaml.automl: 09-17 01:52:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:52:38] {3072} INFO -  at 21.6s,	estimator xgboost's best error=5.2843,	best estimator xgboost's best error=5.2843
[flaml.automl: 09-17 01:52:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:52:39] {3072} INFO -  at 22.8s,	estimator xgboost's best error=5.2843,	best estimator xgboost's best error=5.2843
[flaml.automl: 09-17 01:52:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:52:45] {3072} INFO -  at 29.3s,	estimator xgboost's best error=5.0717,	best estimator xgboost's best error=5.0717
[flaml.automl: 09-17 01:52:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:52:57] {3072} INFO -  at 41.4s,	estimator xgboost's best error=4.9129,	best estimator xgboost's best error=4.9129
[flaml.automl: 09-17 01:52:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:53:04] {3072} INFO -  at 47.8s,	estimator xgboost's best error=4.9129,	best estimator xgboost's best error=4.9129
[flaml.automl: 09-17 01:53:04] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 01:53:15] {3072} INFO -  at 59.2s,	estimator xgboost's best error=4.8754,	best estimator xgboost's best error=4.8754
[flaml.automl: 09-17 01:53:37] {3335} INFO - retrain xgboost for 21.4s
[flaml.automl: 09-17 01:53:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:53:37] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:53:37] {2637} INFO - Time taken to find the best model: 59.229182958602905
[flaml.automl: 09-17 01:53:37] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 51670}
NO2(0)最佳损失：-3.8753833027095252
NO2(0)最好结果：{'pred_time': 7.994579380291839e-06, 'wall_clock_time': 59.229182958602905, 'metric_for_logging': {'pred_time': 7.994579380291839e-06}, 'val_loss': 4.875383302709525, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 51670}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 51670, 'experiment_tag': 'exp', 'time_total_s': 11.390132665634155}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.830859727613112
NO2(0)的mse=55.302192538356
NO2(0)的mae=4.686552853597369
NO2(0)的mar=0.24326888693813936
总共花费的时间为：81.54
乌鲁木齐市
1491A
3033A
3437A
3438A
3439A
3440A
[flaml.automl: 09-17 02:12:33] {2390} INFO - task = regression
[flaml.automl: 09-17 02:12:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:12:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:12:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:12:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:12:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:12:35] {3025} INFO - Estimated sufficient time budget=83354s. Estimated necessary time budget=83s.
[flaml.automl: 09-17 02:12:35] {3072} INFO -  at 1.6s,	estimator xgboost's best error=20.7982,	best estimator xgboost's best error=20.7982
[flaml.automl: 09-17 02:12:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:12:37] {3072} INFO -  at 3.8s,	estimator xgboost's best error=10.3901,	best estimator xgboost's best error=10.3901
[flaml.automl: 09-17 02:12:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:12:38] {3072} INFO -  at 4.9s,	estimator xgboost's best error=10.3901,	best estimator xgboost's best error=10.3901
[flaml.automl: 09-17 02:12:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:12:42] {3072} INFO -  at 8.7s,	estimator xgboost's best error=10.3901,	best estimator xgboost's best error=10.3901
[flaml.automl: 09-17 02:12:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:12:43] {3072} INFO -  at 9.9s,	estimator xgboost's best error=7.1851,	best estimator xgboost's best error=7.1851
[flaml.automl: 09-17 02:12:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:12:44] {3072} INFO -  at 11.5s,	estimator xgboost's best error=6.1205,	best estimator xgboost's best error=6.1205
[flaml.automl: 09-17 02:12:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:12:46] {3072} INFO -  at 13.1s,	estimator xgboost's best error=6.1205,	best estimator xgboost's best error=6.1205
[flaml.automl: 09-17 02:12:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:12:48] {3072} INFO -  at 15.5s,	estimator xgboost's best error=6.1205,	best estimator xgboost's best error=6.1205
[flaml.automl: 09-17 02:12:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:12:50] {3072} INFO -  at 16.7s,	estimator xgboost's best error=6.1205,	best estimator xgboost's best error=6.1205
[flaml.automl: 09-17 02:12:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:12:52] {3072} INFO -  at 19.3s,	estimator xgboost's best error=6.1205,	best estimator xgboost's best error=6.1205
[flaml.automl: 09-17 02:12:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:12:54] {3072} INFO -  at 21.0s,	estimator xgboost's best error=6.1205,	best estimator xgboost's best error=6.1205
[flaml.automl: 09-17 02:12:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:12:55] {3072} INFO -  at 22.2s,	estimator xgboost's best error=6.1205,	best estimator xgboost's best error=6.1205
[flaml.automl: 09-17 02:12:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:13:06] {3072} INFO -  at 33.1s,	estimator xgboost's best error=5.6079,	best estimator xgboost's best error=5.6079
[flaml.automl: 09-17 02:13:06] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:13:29] {3072} INFO -  at 55.7s,	estimator xgboost's best error=5.5081,	best estimator xgboost's best error=5.5081
[flaml.automl: 09-17 02:13:51] {3335} INFO - retrain xgboost for 22.4s
[flaml.automl: 09-17 02:13:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:13:51] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:13:51] {2637} INFO - Time taken to find the best model: 55.673532247543335
[flaml.automl: 09-17 02:13:51] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65349}
NO2(0)最佳损失：-4.508078510293092
NO2(0)最好结果：{'pred_time': 1.0275079080533863e-05, 'wall_clock_time': 55.673532247543335, 'metric_for_logging': {'pred_time': 1.0275079080533863e-05}, 'val_loss': 5.508078510293092, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65349}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 65349, 'experiment_tag': 'exp', 'time_total_s': 22.563416242599487}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.892106648218421
NO2(0)的mse=78.73510411940073
NO2(0)的mae=5.405487302866204
NO2(0)的mar=0.24856878784041908
总共花费的时间为：79.34
湘潭市
1508A
1511A
1512A
1513A
1514A
1564A
[flaml.automl: 09-17 02:32:10] {2390} INFO - task = regression
[flaml.automl: 09-17 02:32:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:32:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:32:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:32:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:32:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:32:11] {3025} INFO - Estimated sufficient time budget=73847s. Estimated necessary time budget=74s.
[flaml.automl: 09-17 02:32:11] {3072} INFO -  at 1.5s,	estimator xgboost's best error=14.5374,	best estimator xgboost's best error=14.5374
[flaml.automl: 09-17 02:32:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:32:13] {3072} INFO -  at 3.6s,	estimator xgboost's best error=6.9570,	best estimator xgboost's best error=6.9570
[flaml.automl: 09-17 02:32:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:32:14] {3072} INFO -  at 4.8s,	estimator xgboost's best error=6.9570,	best estimator xgboost's best error=6.9570
[flaml.automl: 09-17 02:32:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:32:19] {3072} INFO -  at 9.1s,	estimator xgboost's best error=6.9570,	best estimator xgboost's best error=6.9570
[flaml.automl: 09-17 02:32:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:32:20] {3072} INFO -  at 10.3s,	estimator xgboost's best error=4.3186,	best estimator xgboost's best error=4.3186
[flaml.automl: 09-17 02:32:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:32:21] {3072} INFO -  at 11.9s,	estimator xgboost's best error=3.6363,	best estimator xgboost's best error=3.6363
[flaml.automl: 09-17 02:32:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:32:23] {3072} INFO -  at 13.5s,	estimator xgboost's best error=3.6363,	best estimator xgboost's best error=3.6363
[flaml.automl: 09-17 02:32:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:32:25] {3072} INFO -  at 15.9s,	estimator xgboost's best error=3.6363,	best estimator xgboost's best error=3.6363
[flaml.automl: 09-17 02:32:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:32:27] {3072} INFO -  at 17.1s,	estimator xgboost's best error=3.6363,	best estimator xgboost's best error=3.6363
[flaml.automl: 09-17 02:32:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:32:29] {3072} INFO -  at 19.8s,	estimator xgboost's best error=3.6363,	best estimator xgboost's best error=3.6363
[flaml.automl: 09-17 02:32:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:32:31] {3072} INFO -  at 21.4s,	estimator xgboost's best error=3.6333,	best estimator xgboost's best error=3.6333
[flaml.automl: 09-17 02:32:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:32:32] {3072} INFO -  at 22.6s,	estimator xgboost's best error=3.6333,	best estimator xgboost's best error=3.6333
[flaml.automl: 09-17 02:32:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:32:39] {3072} INFO -  at 29.1s,	estimator xgboost's best error=3.4105,	best estimator xgboost's best error=3.4105
[flaml.automl: 09-17 02:32:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:32:55] {3072} INFO -  at 45.8s,	estimator xgboost's best error=3.3155,	best estimator xgboost's best error=3.3155
[flaml.automl: 09-17 02:33:17] {3335} INFO - retrain xgboost for 22.2s
[flaml.automl: 09-17 02:33:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:33:17] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:33:17] {2637} INFO - Time taken to find the best model: 45.80543375015259
[flaml.automl: 09-17 02:33:17] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 61860}
NO2(0)最佳损失：-2.315463401808585
NO2(0)最好结果：{'pred_time': 1.0405372040265516e-05, 'wall_clock_time': 45.80543375015259, 'metric_for_logging': {'pred_time': 1.0405372040265516e-05}, 'val_loss': 3.315463401808585, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 61860}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 61860, 'experiment_tag': 'exp', 'time_total_s': 16.677420616149902}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8640634024613176
NO2(0)的mse=26.027503191113095
NO2(0)的mae=3.3318772468619335
NO2(0)的mar=0.17506877655494657
总共花费的时间为：68.98
株洲市
1515A
1518A
1520A
1524A
1559A
2031A
[flaml.automl: 09-17 02:50:58] {2390} INFO - task = regression
[flaml.automl: 09-17 02:50:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:50:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:50:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:50:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:50:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:50:59] {3025} INFO - Estimated sufficient time budget=74746s. Estimated necessary time budget=75s.
[flaml.automl: 09-17 02:50:59] {3072} INFO -  at 1.6s,	estimator xgboost's best error=15.4950,	best estimator xgboost's best error=15.4950
[flaml.automl: 09-17 02:50:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:51:01] {3072} INFO -  at 3.7s,	estimator xgboost's best error=7.4567,	best estimator xgboost's best error=7.4567
[flaml.automl: 09-17 02:51:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:51:02] {3072} INFO -  at 4.9s,	estimator xgboost's best error=7.4567,	best estimator xgboost's best error=7.4567
[flaml.automl: 09-17 02:51:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:51:07] {3072} INFO -  at 9.2s,	estimator xgboost's best error=7.4567,	best estimator xgboost's best error=7.4567
[flaml.automl: 09-17 02:51:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:51:08] {3072} INFO -  at 10.3s,	estimator xgboost's best error=4.4648,	best estimator xgboost's best error=4.4648
[flaml.automl: 09-17 02:51:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:51:09] {3072} INFO -  at 11.9s,	estimator xgboost's best error=3.7951,	best estimator xgboost's best error=3.7951
[flaml.automl: 09-17 02:51:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:51:11] {3072} INFO -  at 13.5s,	estimator xgboost's best error=3.7951,	best estimator xgboost's best error=3.7951
[flaml.automl: 09-17 02:51:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:51:13] {3072} INFO -  at 16.0s,	estimator xgboost's best error=3.7951,	best estimator xgboost's best error=3.7951
[flaml.automl: 09-17 02:51:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:51:15] {3072} INFO -  at 17.1s,	estimator xgboost's best error=3.7951,	best estimator xgboost's best error=3.7951
[flaml.automl: 09-17 02:51:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:51:17] {3072} INFO -  at 19.8s,	estimator xgboost's best error=3.7951,	best estimator xgboost's best error=3.7951
[flaml.automl: 09-17 02:51:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:51:19] {3072} INFO -  at 21.4s,	estimator xgboost's best error=3.7576,	best estimator xgboost's best error=3.7576
[flaml.automl: 09-17 02:51:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:51:20] {3072} INFO -  at 22.6s,	estimator xgboost's best error=3.7576,	best estimator xgboost's best error=3.7576
[flaml.automl: 09-17 02:51:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:51:27] {3072} INFO -  at 29.2s,	estimator xgboost's best error=3.4122,	best estimator xgboost's best error=3.4122
[flaml.automl: 09-17 02:51:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:51:39] {3072} INFO -  at 41.4s,	estimator xgboost's best error=3.3611,	best estimator xgboost's best error=3.3611
[flaml.automl: 09-17 02:51:39] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 02:51:45] {3072} INFO -  at 47.9s,	estimator xgboost's best error=3.3611,	best estimator xgboost's best error=3.3611
[flaml.automl: 09-17 02:51:57] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 02:51:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:51:57] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:51:58] {2637} INFO - Time taken to find the best model: 41.35201930999756
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 61981}
NO2(0)最佳损失：-2.3610522849164783
NO2(0)最好结果：{'pred_time': 6.23042063839778e-06, 'wall_clock_time': 41.35201930999756, 'metric_for_logging': {'pred_time': 6.23042063839778e-06}, 'val_loss': 3.3610522849164783, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 61981}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 61981, 'experiment_tag': 'exp', 'time_total_s': 12.165026426315308}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.88650894300606
NO2(0)的mse=26.226759611202755
NO2(0)的mae=3.3072056305763677
NO2(0)的mar=0.1675916552873606
总共花费的时间为：60.97
包头市
1585A
3283A
3419A
3683A
[flaml.automl: 09-17 03:04:33] {2390} INFO - task = regression
[flaml.automl: 09-17 03:04:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:04:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:04:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:04:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:04:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:04:34] {3025} INFO - Estimated sufficient time budget=54455s. Estimated necessary time budget=54s.
[flaml.automl: 09-17 03:04:34] {3072} INFO -  at 1.4s,	estimator xgboost's best error=19.7352,	best estimator xgboost's best error=19.7352
[flaml.automl: 09-17 03:04:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:04:36] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.2203,	best estimator xgboost's best error=10.2203
[flaml.automl: 09-17 03:04:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:04:37] {3072} INFO -  at 4.6s,	estimator xgboost's best error=10.2203,	best estimator xgboost's best error=10.2203
[flaml.automl: 09-17 03:04:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:04:43] {3072} INFO -  at 10.4s,	estimator xgboost's best error=10.2203,	best estimator xgboost's best error=10.2203
[flaml.automl: 09-17 03:04:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:04:44] {3072} INFO -  at 11.5s,	estimator xgboost's best error=7.4199,	best estimator xgboost's best error=7.4199
[flaml.automl: 09-17 03:04:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:04:46] {3072} INFO -  at 13.1s,	estimator xgboost's best error=6.5756,	best estimator xgboost's best error=6.5756
[flaml.automl: 09-17 03:04:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:04:47] {3072} INFO -  at 14.7s,	estimator xgboost's best error=6.5756,	best estimator xgboost's best error=6.5756
[flaml.automl: 09-17 03:04:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:04:50] {3072} INFO -  at 17.1s,	estimator xgboost's best error=6.5756,	best estimator xgboost's best error=6.5756
[flaml.automl: 09-17 03:04:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:04:51] {3072} INFO -  at 18.2s,	estimator xgboost's best error=6.5756,	best estimator xgboost's best error=6.5756
[flaml.automl: 09-17 03:04:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:04:54] {3072} INFO -  at 20.9s,	estimator xgboost's best error=6.5756,	best estimator xgboost's best error=6.5756
[flaml.automl: 09-17 03:04:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:04:55] {3072} INFO -  at 22.5s,	estimator xgboost's best error=6.5644,	best estimator xgboost's best error=6.5644
[flaml.automl: 09-17 03:04:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:04:56] {3072} INFO -  at 23.7s,	estimator xgboost's best error=6.5644,	best estimator xgboost's best error=6.5644
[flaml.automl: 09-17 03:04:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:05:03] {3072} INFO -  at 30.2s,	estimator xgboost's best error=6.2356,	best estimator xgboost's best error=6.2356
[flaml.automl: 09-17 03:05:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:05:15] {3072} INFO -  at 42.3s,	estimator xgboost's best error=6.0695,	best estimator xgboost's best error=6.0695
[flaml.automl: 09-17 03:05:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 03:05:22] {3072} INFO -  at 48.8s,	estimator xgboost's best error=6.0695,	best estimator xgboost's best error=6.0695
[flaml.automl: 09-17 03:05:34] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 03:05:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:05:34] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:05:34] {2637} INFO - Time taken to find the best model: 42.296313524246216
[flaml.automl: 09-17 03:05:34] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 45207}
NO2(0)最佳损失：-5.069527668430111
NO2(0)最好结果：{'pred_time': 7.975749373317311e-06, 'wall_clock_time': 42.296313524246216, 'metric_for_logging': {'pred_time': 7.975749373317311e-06}, 'val_loss': 6.069527668430111, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 45207}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 45207, 'experiment_tag': 'exp', 'time_total_s': 12.107189893722534}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7945069013630582
NO2(0)的mse=80.97672634296674
NO2(0)的mae=6.03367834202665
NO2(0)的mar=0.26179067969678044
总共花费的时间为：61.73
鄂尔多斯市
1591A
1592A
1594A
1595A
[flaml.automl: 09-17 03:18:47] {2390} INFO - task = regression
[flaml.automl: 09-17 03:18:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:18:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:18:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:18:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:18:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:18:48] {3025} INFO - Estimated sufficient time budget=50491s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 03:18:48] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.6144,	best estimator xgboost's best error=13.6144
[flaml.automl: 09-17 03:18:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:18:50] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.9540,	best estimator xgboost's best error=6.9540
[flaml.automl: 09-17 03:18:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:18:51] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.9540,	best estimator xgboost's best error=6.9540
[flaml.automl: 09-17 03:18:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:18:58] {3072} INFO -  at 11.0s,	estimator xgboost's best error=6.9540,	best estimator xgboost's best error=6.9540
[flaml.automl: 09-17 03:18:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:18:59] {3072} INFO -  at 12.1s,	estimator xgboost's best error=5.3061,	best estimator xgboost's best error=5.3061
[flaml.automl: 09-17 03:18:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:19:00] {3072} INFO -  at 13.7s,	estimator xgboost's best error=4.7920,	best estimator xgboost's best error=4.7920
[flaml.automl: 09-17 03:19:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:19:02] {3072} INFO -  at 15.3s,	estimator xgboost's best error=4.7920,	best estimator xgboost's best error=4.7920
[flaml.automl: 09-17 03:19:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:19:04] {3072} INFO -  at 17.8s,	estimator xgboost's best error=4.7920,	best estimator xgboost's best error=4.7920
[flaml.automl: 09-17 03:19:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:19:06] {3072} INFO -  at 18.9s,	estimator xgboost's best error=4.7920,	best estimator xgboost's best error=4.7920
[flaml.automl: 09-17 03:19:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:19:09] {3072} INFO -  at 21.9s,	estimator xgboost's best error=4.7920,	best estimator xgboost's best error=4.7920
[flaml.automl: 09-17 03:19:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:19:11] {3072} INFO -  at 24.8s,	estimator xgboost's best error=4.7779,	best estimator xgboost's best error=4.7779
[flaml.automl: 09-17 03:19:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:19:14] {3072} INFO -  at 26.9s,	estimator xgboost's best error=4.7779,	best estimator xgboost's best error=4.7779
[flaml.automl: 09-17 03:19:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:19:26] {3072} INFO -  at 38.9s,	estimator xgboost's best error=4.6832,	best estimator xgboost's best error=4.6832
[flaml.automl: 09-17 03:19:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:19:46] {3072} INFO -  at 59.0s,	estimator xgboost's best error=4.5749,	best estimator xgboost's best error=4.5749
[flaml.automl: 09-17 03:20:08] {3335} INFO - retrain xgboost for 22.4s
[flaml.automl: 09-17 03:20:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:20:08] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:20:08] {2637} INFO - Time taken to find the best model: 59.02098274230957
[flaml.automl: 09-17 03:20:08] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43108}
NO2(0)最佳损失：-3.574948091148584
NO2(0)最好结果：{'pred_time': 1.6375722865222143e-05, 'wall_clock_time': 59.02098274230957, 'metric_for_logging': {'pred_time': 1.6375722865222143e-05}, 'val_loss': 4.574948091148584, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43108}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43108, 'experiment_tag': 'exp', 'time_total_s': 20.143215656280518}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7116326651431879
NO2(0)的mse=48.936353658008045
NO2(0)的mae=4.631753264973089
NO2(0)的mar=0.2513809782179614
总共花费的时间为：82.13
营口市
1598A
3378A
3379A
3866A
[flaml.automl: 09-17 03:33:19] {2390} INFO - task = regression
[flaml.automl: 09-17 03:33:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:33:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:33:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:33:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:33:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:33:22] {3025} INFO - Estimated sufficient time budget=23714s. Estimated necessary time budget=24s.
[flaml.automl: 09-17 03:33:22] {3072} INFO -  at 2.6s,	estimator xgboost's best error=16.5572,	best estimator xgboost's best error=16.5572
[flaml.automl: 09-17 03:33:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:33:26] {3072} INFO -  at 6.6s,	estimator xgboost's best error=8.2018,	best estimator xgboost's best error=8.2018
[flaml.automl: 09-17 03:33:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:33:28] {3072} INFO -  at 8.8s,	estimator xgboost's best error=8.2018,	best estimator xgboost's best error=8.2018
[flaml.automl: 09-17 03:33:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:33:40] {3072} INFO -  at 20.6s,	estimator xgboost's best error=8.2018,	best estimator xgboost's best error=8.2018
[flaml.automl: 09-17 03:33:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:33:41] {3072} INFO -  at 21.7s,	estimator xgboost's best error=5.8146,	best estimator xgboost's best error=5.8146
[flaml.automl: 09-17 03:33:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:33:42] {3072} INFO -  at 23.3s,	estimator xgboost's best error=5.0356,	best estimator xgboost's best error=5.0356
[flaml.automl: 09-17 03:33:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:33:44] {3072} INFO -  at 25.0s,	estimator xgboost's best error=5.0356,	best estimator xgboost's best error=5.0356
[flaml.automl: 09-17 03:33:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:33:46] {3072} INFO -  at 27.4s,	estimator xgboost's best error=5.0356,	best estimator xgboost's best error=5.0356
[flaml.automl: 09-17 03:33:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:33:48] {3072} INFO -  at 28.6s,	estimator xgboost's best error=5.0356,	best estimator xgboost's best error=5.0356
[flaml.automl: 09-17 03:33:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:33:50] {3072} INFO -  at 31.3s,	estimator xgboost's best error=5.0356,	best estimator xgboost's best error=5.0356
[flaml.automl: 09-17 03:33:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:33:51] {3072} INFO -  at 32.4s,	estimator xgboost's best error=5.0356,	best estimator xgboost's best error=5.0356
[flaml.automl: 09-17 03:33:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:33:53] {3072} INFO -  at 33.6s,	estimator xgboost's best error=5.0356,	best estimator xgboost's best error=5.0356
[flaml.automl: 09-17 03:33:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:33:59] {3072} INFO -  at 40.0s,	estimator xgboost's best error=4.8250,	best estimator xgboost's best error=4.8250
[flaml.automl: 09-17 03:33:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:34:11] {3072} INFO -  at 52.1s,	estimator xgboost's best error=4.7355,	best estimator xgboost's best error=4.7355
[flaml.automl: 09-17 03:34:23] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 03:34:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:34:23] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:34:23] {2637} INFO - Time taken to find the best model: 52.090476751327515
[flaml.automl: 09-17 03:34:23] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.73548088240069
NO2(0)最好结果：{'pred_time': 9.481296982876091e-06, 'wall_clock_time': 52.090476751327515, 'metric_for_logging': {'pred_time': 9.481296982876091e-06}, 'val_loss': 4.73548088240069, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.039989233016968}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7976749135841459
NO2(0)的mse=51.38507856781418
NO2(0)的mae=4.794108504974525
NO2(0)的mar=0.22903683298784724
总共花费的时间为：64.92
丹东市
1600A
1602A
1603A
[flaml.automl: 09-17 03:44:23] {2390} INFO - task = regression
[flaml.automl: 09-17 03:44:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:44:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:44:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:44:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:44:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:44:24] {3025} INFO - Estimated sufficient time budget=12077s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:44:24] {3072} INFO -  at 1.4s,	estimator xgboost's best error=10.5581,	best estimator xgboost's best error=10.5581
[flaml.automl: 09-17 03:44:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:44:26] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.4290,	best estimator xgboost's best error=5.4290
[flaml.automl: 09-17 03:44:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:44:27] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.4290,	best estimator xgboost's best error=5.4290
[flaml.automl: 09-17 03:44:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:44:37] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.4290,	best estimator xgboost's best error=5.4290
[flaml.automl: 09-17 03:44:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:44:39] {3072} INFO -  at 15.9s,	estimator xgboost's best error=4.2762,	best estimator xgboost's best error=4.2762
[flaml.automl: 09-17 03:44:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:44:40] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.8761,	best estimator xgboost's best error=3.8761
[flaml.automl: 09-17 03:44:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:44:42] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.8761,	best estimator xgboost's best error=3.8761
[flaml.automl: 09-17 03:44:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:44:44] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.8761,	best estimator xgboost's best error=3.8761
[flaml.automl: 09-17 03:44:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:44:45] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.8761,	best estimator xgboost's best error=3.8761
[flaml.automl: 09-17 03:44:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:44:48] {3072} INFO -  at 25.5s,	estimator xgboost's best error=3.8761,	best estimator xgboost's best error=3.8761
[flaml.automl: 09-17 03:44:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:44:49] {3072} INFO -  at 26.6s,	estimator xgboost's best error=3.8761,	best estimator xgboost's best error=3.8761
[flaml.automl: 09-17 03:44:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:44:50] {3072} INFO -  at 27.8s,	estimator xgboost's best error=3.8761,	best estimator xgboost's best error=3.8761
[flaml.automl: 09-17 03:44:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:44:57] {3072} INFO -  at 34.3s,	estimator xgboost's best error=3.7615,	best estimator xgboost's best error=3.7615
[flaml.automl: 09-17 03:44:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:45:09] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.7024,	best estimator xgboost's best error=3.7024
[flaml.automl: 09-17 03:45:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 03:45:18] {3072} INFO -  at 55.5s,	estimator xgboost's best error=3.7024,	best estimator xgboost's best error=3.7024
[flaml.automl: 09-17 03:45:40] {3335} INFO - retrain xgboost for 22.3s
[flaml.automl: 09-17 03:45:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:45:40] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:45:40] {2637} INFO - Time taken to find the best model: 46.444053173065186
[flaml.automl: 09-17 03:45:40] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.702429931241668
NO2(0)最好结果：{'pred_time': 1.112825098909456e-05, 'wall_clock_time': 46.444053173065186, 'metric_for_logging': {'pred_time': 1.112825098909456e-05}, 'val_loss': 3.702429931241668, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.10810375213623}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.749472718364154
NO2(0)的mse=31.23642011314826
NO2(0)的mae=3.702753959407684
NO2(0)的mar=0.26221096018286916
总共花费的时间为：78.47
盘锦市
1604A
1605A
[flaml.automl: 09-17 03:52:07] {2390} INFO - task = regression
[flaml.automl: 09-17 03:52:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:52:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:52:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:52:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:52:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:52:08] {3025} INFO - Estimated sufficient time budget=12079s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:52:08] {3072} INFO -  at 1.3s,	estimator xgboost's best error=16.9386,	best estimator xgboost's best error=16.9386
[flaml.automl: 09-17 03:52:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:52:10] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.1031,	best estimator xgboost's best error=8.1031
[flaml.automl: 09-17 03:52:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:52:11] {3072} INFO -  at 4.6s,	estimator xgboost's best error=8.1031,	best estimator xgboost's best error=8.1031
[flaml.automl: 09-17 03:52:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:52:20] {3072} INFO -  at 14.0s,	estimator xgboost's best error=8.1031,	best estimator xgboost's best error=8.1031
[flaml.automl: 09-17 03:52:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:52:22] {3072} INFO -  at 15.1s,	estimator xgboost's best error=5.2432,	best estimator xgboost's best error=5.2432
[flaml.automl: 09-17 03:52:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:52:23] {3072} INFO -  at 16.7s,	estimator xgboost's best error=4.6217,	best estimator xgboost's best error=4.6217
[flaml.automl: 09-17 03:52:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:52:25] {3072} INFO -  at 18.3s,	estimator xgboost's best error=4.6217,	best estimator xgboost's best error=4.6217
[flaml.automl: 09-17 03:52:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:52:27] {3072} INFO -  at 20.8s,	estimator xgboost's best error=4.6217,	best estimator xgboost's best error=4.6217
[flaml.automl: 09-17 03:52:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:52:28] {3072} INFO -  at 22.0s,	estimator xgboost's best error=4.6217,	best estimator xgboost's best error=4.6217
[flaml.automl: 09-17 03:52:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:52:31] {3072} INFO -  at 24.4s,	estimator xgboost's best error=4.6217,	best estimator xgboost's best error=4.6217
[flaml.automl: 09-17 03:52:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:52:32] {3072} INFO -  at 25.6s,	estimator xgboost's best error=4.6217,	best estimator xgboost's best error=4.6217
[flaml.automl: 09-17 03:52:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:52:33] {3072} INFO -  at 26.7s,	estimator xgboost's best error=4.6217,	best estimator xgboost's best error=4.6217
[flaml.automl: 09-17 03:52:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:52:41] {3072} INFO -  at 34.2s,	estimator xgboost's best error=4.4352,	best estimator xgboost's best error=4.4352
[flaml.automl: 09-17 03:52:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:53:00] {3072} INFO -  at 53.5s,	estimator xgboost's best error=4.3453,	best estimator xgboost's best error=4.3453
[flaml.automl: 09-17 03:53:19] {3335} INFO - retrain xgboost for 18.8s
[flaml.automl: 09-17 03:53:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:53:19] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:53:19] {2637} INFO - Time taken to find the best model: 53.472588300704956
[flaml.automl: 09-17 03:53:19] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.3453274192825893
NO2(0)最好结果：{'pred_time': 3.635215284221644e-05, 'wall_clock_time': 53.472588300704956, 'metric_for_logging': {'pred_time': 3.635215284221644e-05}, 'val_loss': 4.345327419282589, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 19.29958152770996}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8176396245628155
NO2(0)的mse=43.11901241058883
NO2(0)的mae=4.439608040254954
NO2(0)的mar=0.186593816259387
总共花费的时间为：72.74
葫芦岛市
1607A
[flaml.automl: 09-17 03:56:39] {2390} INFO - task = regression
[flaml.automl: 09-17 03:56:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:56:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:56:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:56:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:56:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:56:41] {3025} INFO - Estimated sufficient time budget=11949s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:56:41] {3072} INFO -  at 1.3s,	estimator xgboost's best error=14.7764,	best estimator xgboost's best error=14.7764
[flaml.automl: 09-17 03:56:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:56:42] {3072} INFO -  at 3.1s,	estimator xgboost's best error=8.4467,	best estimator xgboost's best error=8.4467
[flaml.automl: 09-17 03:56:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:56:44] {3072} INFO -  at 4.3s,	estimator xgboost's best error=8.4467,	best estimator xgboost's best error=8.4467
[flaml.automl: 09-17 03:56:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:56:51] {3072} INFO -  at 11.4s,	estimator xgboost's best error=8.4467,	best estimator xgboost's best error=8.4467
[flaml.automl: 09-17 03:56:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:56:52] {3072} INFO -  at 12.5s,	estimator xgboost's best error=5.4058,	best estimator xgboost's best error=5.4058
[flaml.automl: 09-17 03:56:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:56:53] {3072} INFO -  at 14.1s,	estimator xgboost's best error=4.7506,	best estimator xgboost's best error=4.7506
[flaml.automl: 09-17 03:56:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:56:55] {3072} INFO -  at 15.7s,	estimator xgboost's best error=4.7506,	best estimator xgboost's best error=4.7506
[flaml.automl: 09-17 03:56:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:56:57] {3072} INFO -  at 18.0s,	estimator xgboost's best error=4.7506,	best estimator xgboost's best error=4.7506
[flaml.automl: 09-17 03:56:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:56:58] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.7506,	best estimator xgboost's best error=4.7506
[flaml.automl: 09-17 03:56:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:57:01] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.7506,	best estimator xgboost's best error=4.7506
[flaml.automl: 09-17 03:57:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:57:02] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.7506,	best estimator xgboost's best error=4.7506
[flaml.automl: 09-17 03:57:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:57:03] {3072} INFO -  at 23.8s,	estimator xgboost's best error=4.7506,	best estimator xgboost's best error=4.7506
[flaml.automl: 09-17 03:57:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:57:09] {3072} INFO -  at 29.4s,	estimator xgboost's best error=4.7506,	best estimator xgboost's best error=4.7506
[flaml.automl: 09-17 03:57:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:57:11] {3072} INFO -  at 32.2s,	estimator xgboost's best error=4.5704,	best estimator xgboost's best error=4.5704
[flaml.automl: 09-17 03:57:11] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 03:57:13] {3072} INFO -  at 33.8s,	estimator xgboost's best error=4.5704,	best estimator xgboost's best error=4.5704
[flaml.automl: 09-17 03:57:13] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 03:57:17] {3072} INFO -  at 38.1s,	estimator xgboost's best error=4.5704,	best estimator xgboost's best error=4.5704
[flaml.automl: 09-17 03:57:17] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 03:57:20] {3072} INFO -  at 40.4s,	estimator xgboost's best error=4.5704,	best estimator xgboost's best error=4.5704
[flaml.automl: 09-17 03:57:20] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 03:57:22] {3072} INFO -  at 42.5s,	estimator xgboost's best error=4.5704,	best estimator xgboost's best error=4.5704
[flaml.automl: 09-17 03:57:22] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 03:57:28] {3072} INFO -  at 48.4s,	estimator xgboost's best error=4.5054,	best estimator xgboost's best error=4.5054
[flaml.automl: 09-17 03:57:28] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 03:57:30] {3072} INFO -  at 51.0s,	estimator xgboost's best error=4.5054,	best estimator xgboost's best error=4.5054
[flaml.automl: 09-17 03:57:30] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 03:57:38] {3072} INFO -  at 59.2s,	estimator xgboost's best error=4.5054,	best estimator xgboost's best error=4.5054
[flaml.automl: 09-17 03:57:44] {3335} INFO - retrain xgboost for 5.8s
[flaml.automl: 09-17 03:57:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7178642069579442, colsample_bynode=1,
             colsample_bytree=0.7808640703646168, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9481937935550555,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.2628045260160708, missing=nan,
             monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9333220038816206, scale_pos_weight=1,
             subsample=0.8151775216506888, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:57:44] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:57:44] {2637} INFO - Time taken to find the best model: 48.359894037246704
[flaml.automl: 09-17 03:57:44] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 10, 'max_leaves': 10, 'min_child_weight': 0.2628045260160708, 'learning_rate': 0.9481937935550555, 'subsample': 0.8151775216506888, 'colsample_bylevel': 0.7178642069579442, 'colsample_bytree': 0.7808640703646168, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9333220038816206}
NO2(0)最佳损失：-3.505368578640712
NO2(0)最好结果：{'pred_time': 3.2693073830261986e-05, 'wall_clock_time': 48.359894037246704, 'metric_for_logging': {'pred_time': 3.2693073830261986e-05}, 'val_loss': 4.505368578640712, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 10, 'min_child_weight': 0.2628045260160708, 'learning_rate': 0.9481937935550555, 'subsample': 0.8151775216506888, 'colsample_bylevel': 0.7178642069579442, 'colsample_bytree': 0.7808640703646168, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9333220038816206}, 'config/n_estimators': 10, 'config/max_leaves': 10, 'config/min_child_weight': 0.2628045260160708, 'config/learning_rate': 0.9481937935550555, 'config/subsample': 0.8151775216506888, 'config/colsample_bylevel': 0.7178642069579442, 'config/colsample_bytree': 0.7808640703646168, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.9333220038816206, 'experiment_tag': 'exp', 'time_total_s': 5.8418121337890625}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7178642069579442, colsample_bynode=1,
             colsample_bytree=0.7808640703646168, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9481937935550555,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.2628045260160708, missing=nan,
             monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9333220038816206, scale_pos_weight=1,
             subsample=0.8151775216506888, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7802741881911822
NO2(0)的mse=54.03027741313122
NO2(0)的mae=4.850086604680916
NO2(0)的mar=0.2469289980514061
总共花费的时间为：65.26
泉州市
1614A
3529A
[flaml.automl: 09-17 04:04:09] {2390} INFO - task = regression
[flaml.automl: 09-17 04:04:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:04:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:04:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:04:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:04:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:04:10] {3025} INFO - Estimated sufficient time budget=11994s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 04:04:10] {3072} INFO -  at 1.3s,	estimator xgboost's best error=9.3903,	best estimator xgboost's best error=9.3903
[flaml.automl: 09-17 04:04:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:04:12] {3072} INFO -  at 3.2s,	estimator xgboost's best error=5.2470,	best estimator xgboost's best error=5.2470
[flaml.automl: 09-17 04:04:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:04:13] {3072} INFO -  at 4.4s,	estimator xgboost's best error=5.2470,	best estimator xgboost's best error=5.2470
[flaml.automl: 09-17 04:04:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:04:21] {3072} INFO -  at 12.8s,	estimator xgboost's best error=5.2470,	best estimator xgboost's best error=5.2470
[flaml.automl: 09-17 04:04:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:04:23] {3072} INFO -  at 14.0s,	estimator xgboost's best error=3.1911,	best estimator xgboost's best error=3.1911
[flaml.automl: 09-17 04:04:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:04:24] {3072} INFO -  at 15.5s,	estimator xgboost's best error=2.8844,	best estimator xgboost's best error=2.8844
[flaml.automl: 09-17 04:04:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:04:26] {3072} INFO -  at 17.2s,	estimator xgboost's best error=2.8844,	best estimator xgboost's best error=2.8844
[flaml.automl: 09-17 04:04:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:04:28] {3072} INFO -  at 19.6s,	estimator xgboost's best error=2.8844,	best estimator xgboost's best error=2.8844
[flaml.automl: 09-17 04:04:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:04:29] {3072} INFO -  at 20.8s,	estimator xgboost's best error=2.8844,	best estimator xgboost's best error=2.8844
[flaml.automl: 09-17 04:04:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:04:32] {3072} INFO -  at 23.2s,	estimator xgboost's best error=2.8844,	best estimator xgboost's best error=2.8844
[flaml.automl: 09-17 04:04:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:04:33] {3072} INFO -  at 24.4s,	estimator xgboost's best error=2.8844,	best estimator xgboost's best error=2.8844
[flaml.automl: 09-17 04:04:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:04:34] {3072} INFO -  at 25.5s,	estimator xgboost's best error=2.8844,	best estimator xgboost's best error=2.8844
[flaml.automl: 09-17 04:04:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:04:40] {3072} INFO -  at 31.6s,	estimator xgboost's best error=2.8364,	best estimator xgboost's best error=2.8364
[flaml.automl: 09-17 04:04:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:04:51] {3072} INFO -  at 42.1s,	estimator xgboost's best error=2.7620,	best estimator xgboost's best error=2.7620
[flaml.automl: 09-17 04:04:51] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 04:04:57] {3072} INFO -  at 48.2s,	estimator xgboost's best error=2.7620,	best estimator xgboost's best error=2.7620
[flaml.automl: 09-17 04:04:57] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 04:05:08] {3072} INFO -  at 59.8s,	estimator xgboost's best error=2.7620,	best estimator xgboost's best error=2.7620
[flaml.automl: 09-17 04:05:19] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-17 04:05:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:05:19] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:05:19] {2637} INFO - Time taken to find the best model: 42.06836271286011
[flaml.automl: 09-17 04:05:19] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.7620191602698276
NO2(0)最好结果：{'pred_time': 1.7541628069551676e-05, 'wall_clock_time': 42.06836271286011, 'metric_for_logging': {'pred_time': 1.7541628069551676e-05}, 'val_loss': 2.7620191602698276, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.44059681892395}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7405420903364521
NO2(0)的mse=16.03376654691822
NO2(0)的mae=2.604641792973287
NO2(0)的mar=0.21965763554157863
总共花费的时间为：70.55
临沂市
1618A
1619A
1620A
3496A
3860A
[flaml.automl: 09-17 04:21:35] {2390} INFO - task = regression
[flaml.automl: 09-17 04:21:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:21:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:21:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:21:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:21:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:21:37] {3025} INFO - Estimated sufficient time budget=111357s. Estimated necessary time budget=111s.
[flaml.automl: 09-17 04:21:37] {3072} INFO -  at 2.6s,	estimator xgboost's best error=18.7513,	best estimator xgboost's best error=18.7513
[flaml.automl: 09-17 04:21:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:21:41] {3072} INFO -  at 6.4s,	estimator xgboost's best error=8.8102,	best estimator xgboost's best error=8.8102
[flaml.automl: 09-17 04:21:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:21:43] {3072} INFO -  at 8.5s,	estimator xgboost's best error=8.8102,	best estimator xgboost's best error=8.8102
[flaml.automl: 09-17 04:21:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:21:48] {3072} INFO -  at 13.6s,	estimator xgboost's best error=8.8102,	best estimator xgboost's best error=8.8102
[flaml.automl: 09-17 04:21:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:21:50] {3072} INFO -  at 15.7s,	estimator xgboost's best error=5.3483,	best estimator xgboost's best error=5.3483
[flaml.automl: 09-17 04:21:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:21:53] {3072} INFO -  at 18.6s,	estimator xgboost's best error=4.4900,	best estimator xgboost's best error=4.4900
[flaml.automl: 09-17 04:21:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:21:56] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.4900,	best estimator xgboost's best error=4.4900
[flaml.automl: 09-17 04:21:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:21:59] {3072} INFO -  at 25.1s,	estimator xgboost's best error=4.4900,	best estimator xgboost's best error=4.4900
[flaml.automl: 09-17 04:21:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:22:02] {3072} INFO -  at 27.2s,	estimator xgboost's best error=4.4900,	best estimator xgboost's best error=4.4900
[flaml.automl: 09-17 04:22:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:22:04] {3072} INFO -  at 29.7s,	estimator xgboost's best error=4.4900,	best estimator xgboost's best error=4.4900
[flaml.automl: 09-17 04:22:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:22:06] {3072} INFO -  at 31.3s,	estimator xgboost's best error=4.4283,	best estimator xgboost's best error=4.4283
[flaml.automl: 09-17 04:22:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:22:07] {3072} INFO -  at 32.4s,	estimator xgboost's best error=4.4283,	best estimator xgboost's best error=4.4283
[flaml.automl: 09-17 04:22:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:22:13] {3072} INFO -  at 38.8s,	estimator xgboost's best error=4.1584,	best estimator xgboost's best error=4.1584
[flaml.automl: 09-17 04:22:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:22:25] {3072} INFO -  at 50.8s,	estimator xgboost's best error=4.0389,	best estimator xgboost's best error=4.0389
[flaml.automl: 09-17 04:22:37] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 04:22:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:22:37] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:22:37] {2637} INFO - Time taken to find the best model: 50.77632164955139
[flaml.automl: 09-17 04:22:37] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 48667}
NO2(0)最佳损失：-3.0389106504222347
NO2(0)最好结果：{'pred_time': 7.37303283792981e-06, 'wall_clock_time': 50.77632164955139, 'metric_for_logging': {'pred_time': 7.37303283792981e-06}, 'val_loss': 4.038910650422235, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 48667}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 48667, 'experiment_tag': 'exp', 'time_total_s': 11.934640884399414}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.9015057265407056
NO2(0)的mse=33.40295738491707
NO2(0)的mae=3.917790571015336
NO2(0)的mar=0.15721855281596792
总共花费的时间为：63.86
德州市
3066A
3372A
3511A
[flaml.automl: 09-17 04:31:49] {2390} INFO - task = regression
[flaml.automl: 09-17 04:31:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:31:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:31:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:31:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:31:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:31:50] {3025} INFO - Estimated sufficient time budget=12161s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 04:31:50] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.8344,	best estimator xgboost's best error=15.8344
[flaml.automl: 09-17 04:31:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:31:52] {3072} INFO -  at 3.6s,	estimator xgboost's best error=7.7095,	best estimator xgboost's best error=7.7095
[flaml.automl: 09-17 04:31:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:31:54] {3072} INFO -  at 4.8s,	estimator xgboost's best error=7.7095,	best estimator xgboost's best error=7.7095
[flaml.automl: 09-17 04:31:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:32:04] {3072} INFO -  at 14.8s,	estimator xgboost's best error=7.7095,	best estimator xgboost's best error=7.7095
[flaml.automl: 09-17 04:32:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:32:05] {3072} INFO -  at 15.9s,	estimator xgboost's best error=4.9329,	best estimator xgboost's best error=4.9329
[flaml.automl: 09-17 04:32:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:32:06] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.1573,	best estimator xgboost's best error=4.1573
[flaml.automl: 09-17 04:32:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:32:08] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.1573,	best estimator xgboost's best error=4.1573
[flaml.automl: 09-17 04:32:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:32:10] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.1573,	best estimator xgboost's best error=4.1573
[flaml.automl: 09-17 04:32:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:32:12] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.1573,	best estimator xgboost's best error=4.1573
[flaml.automl: 09-17 04:32:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:32:14] {3072} INFO -  at 25.4s,	estimator xgboost's best error=4.1573,	best estimator xgboost's best error=4.1573
[flaml.automl: 09-17 04:32:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:32:15] {3072} INFO -  at 26.6s,	estimator xgboost's best error=4.1573,	best estimator xgboost's best error=4.1573
[flaml.automl: 09-17 04:32:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:32:17] {3072} INFO -  at 27.8s,	estimator xgboost's best error=4.1573,	best estimator xgboost's best error=4.1573
[flaml.automl: 09-17 04:32:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:32:23] {3072} INFO -  at 34.3s,	estimator xgboost's best error=3.8362,	best estimator xgboost's best error=3.8362
[flaml.automl: 09-17 04:32:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:32:35] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.7452,	best estimator xgboost's best error=3.7452
[flaml.automl: 09-17 04:32:35] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 04:32:42] {3072} INFO -  at 52.9s,	estimator xgboost's best error=3.7452,	best estimator xgboost's best error=3.7452
[flaml.automl: 09-17 04:32:54] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-17 04:32:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:32:54] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:32:54] {2637} INFO - Time taken to find the best model: 46.36835312843323
[flaml.automl: 09-17 04:32:54] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.745191069225982
NO2(0)最好结果：{'pred_time': 1.1270550837842508e-05, 'wall_clock_time': 46.36835312843323, 'metric_for_logging': {'pred_time': 1.1270550837842508e-05}, 'val_loss': 3.745191069225982, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.093899965286255}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8817981514309644
NO2(0)的mse=29.62503372740518
NO2(0)的mae=3.623474982980818
NO2(0)的mar=0.18701225761994664
总共花费的时间为：65.33
聊城市
1625A
3513A
[flaml.automl: 09-17 04:40:08] {2390} INFO - task = regression
[flaml.automl: 09-17 04:40:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:40:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:40:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:40:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:40:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:40:09] {3025} INFO - Estimated sufficient time budget=12137s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 04:40:09] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.6219,	best estimator xgboost's best error=17.6219
[flaml.automl: 09-17 04:40:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:40:11] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.4861,	best estimator xgboost's best error=8.4861
[flaml.automl: 09-17 04:40:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:40:12] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.4861,	best estimator xgboost's best error=8.4861
[flaml.automl: 09-17 04:40:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:40:22] {3072} INFO -  at 14.1s,	estimator xgboost's best error=8.4861,	best estimator xgboost's best error=8.4861
[flaml.automl: 09-17 04:40:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:40:23] {3072} INFO -  at 15.2s,	estimator xgboost's best error=5.3041,	best estimator xgboost's best error=5.3041
[flaml.automl: 09-17 04:40:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:40:24] {3072} INFO -  at 16.8s,	estimator xgboost's best error=4.4458,	best estimator xgboost's best error=4.4458
[flaml.automl: 09-17 04:40:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:40:26] {3072} INFO -  at 18.4s,	estimator xgboost's best error=4.4458,	best estimator xgboost's best error=4.4458
[flaml.automl: 09-17 04:40:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:40:28] {3072} INFO -  at 20.9s,	estimator xgboost's best error=4.4458,	best estimator xgboost's best error=4.4458
[flaml.automl: 09-17 04:40:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:40:30] {3072} INFO -  at 22.0s,	estimator xgboost's best error=4.4458,	best estimator xgboost's best error=4.4458
[flaml.automl: 09-17 04:40:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:40:32] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.4458,	best estimator xgboost's best error=4.4458
[flaml.automl: 09-17 04:40:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:40:33] {3072} INFO -  at 25.6s,	estimator xgboost's best error=4.4458,	best estimator xgboost's best error=4.4458
[flaml.automl: 09-17 04:40:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:40:34] {3072} INFO -  at 26.8s,	estimator xgboost's best error=4.4458,	best estimator xgboost's best error=4.4458
[flaml.automl: 09-17 04:40:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:40:40] {3072} INFO -  at 32.9s,	estimator xgboost's best error=4.0960,	best estimator xgboost's best error=4.0960
[flaml.automl: 09-17 04:40:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:40:51] {3072} INFO -  at 43.2s,	estimator xgboost's best error=4.0677,	best estimator xgboost's best error=4.0677
[flaml.automl: 09-17 04:40:51] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 04:40:57] {3072} INFO -  at 49.2s,	estimator xgboost's best error=4.0677,	best estimator xgboost's best error=4.0677
[flaml.automl: 09-17 04:40:57] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 04:41:07] {3072} INFO -  at 59.8s,	estimator xgboost's best error=3.9742,	best estimator xgboost's best error=3.9742
[flaml.automl: 09-17 04:41:25] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-17 04:41:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:41:25] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:41:25] {2637} INFO - Time taken to find the best model: 59.83651089668274
[flaml.automl: 09-17 04:41:25] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-2.9742353384949496
NO2(0)最好结果：{'pred_time': 1.704170531833295e-05, 'wall_clock_time': 59.83651089668274, 'metric_for_logging': {'pred_time': 1.704170531833295e-05}, 'val_loss': 3.9742353384949496, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 10.597132921218872}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8861892753143069
NO2(0)的mse=34.524090061146254
NO2(0)的mae=3.8301531143734278
NO2(0)的mar=0.1835570067512119
总共花费的时间为：77.55
滨州市
1629A
1630A
3514A
3515A
3516A
[flaml.automl: 09-17 04:57:52] {2390} INFO - task = regression
[flaml.automl: 09-17 04:57:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:57:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:57:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:57:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:57:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:57:54] {3025} INFO - Estimated sufficient time budget=120555s. Estimated necessary time budget=121s.
[flaml.automl: 09-17 04:57:54] {3072} INFO -  at 2.6s,	estimator xgboost's best error=17.4780,	best estimator xgboost's best error=17.4780
[flaml.automl: 09-17 04:57:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:57:58] {3072} INFO -  at 6.5s,	estimator xgboost's best error=8.5919,	best estimator xgboost's best error=8.5919
[flaml.automl: 09-17 04:57:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:58:00] {3072} INFO -  at 8.6s,	estimator xgboost's best error=8.5919,	best estimator xgboost's best error=8.5919
[flaml.automl: 09-17 04:58:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:58:04] {3072} INFO -  at 12.7s,	estimator xgboost's best error=8.5919,	best estimator xgboost's best error=8.5919
[flaml.automl: 09-17 04:58:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:58:06] {3072} INFO -  at 14.8s,	estimator xgboost's best error=5.5944,	best estimator xgboost's best error=5.5944
[flaml.automl: 09-17 04:58:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:58:09] {3072} INFO -  at 17.7s,	estimator xgboost's best error=4.7639,	best estimator xgboost's best error=4.7639
[flaml.automl: 09-17 04:58:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:58:12] {3072} INFO -  at 20.7s,	estimator xgboost's best error=4.7639,	best estimator xgboost's best error=4.7639
[flaml.automl: 09-17 04:58:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:58:14] {3072} INFO -  at 23.2s,	estimator xgboost's best error=4.7639,	best estimator xgboost's best error=4.7639
[flaml.automl: 09-17 04:58:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:58:16] {3072} INFO -  at 25.3s,	estimator xgboost's best error=4.7639,	best estimator xgboost's best error=4.7639
[flaml.automl: 09-17 04:58:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:58:19] {3072} INFO -  at 27.9s,	estimator xgboost's best error=4.7639,	best estimator xgboost's best error=4.7639
[flaml.automl: 09-17 04:58:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:58:22] {3072} INFO -  at 30.9s,	estimator xgboost's best error=4.7639,	best estimator xgboost's best error=4.7639
[flaml.automl: 09-17 04:58:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:58:24] {3072} INFO -  at 33.0s,	estimator xgboost's best error=4.7639,	best estimator xgboost's best error=4.7639
[flaml.automl: 09-17 04:58:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:58:36] {3072} INFO -  at 45.0s,	estimator xgboost's best error=4.5051,	best estimator xgboost's best error=4.5051
[flaml.automl: 09-17 04:58:36] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:58:49] {3072} INFO -  at 57.5s,	estimator xgboost's best error=4.3672,	best estimator xgboost's best error=4.3672
[flaml.automl: 09-17 04:59:01] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 04:59:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:59:01] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:59:01] {2637} INFO - Time taken to find the best model: 57.50245642662048
[flaml.automl: 09-17 04:59:01] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54201}
NO2(0)最佳损失：-3.3672372855044435
NO2(0)最好结果：{'pred_time': 6.554591067898576e-06, 'wall_clock_time': 57.50245642662048, 'metric_for_logging': {'pred_time': 6.554591067898576e-06}, 'val_loss': 4.3672372855044435, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54201}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54201, 'experiment_tag': 'exp', 'time_total_s': 12.45817756652832}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8829318038445566
NO2(0)的mse=41.68609169686856
NO2(0)的mae=4.230452940660683
NO2(0)的mar=0.20352985183658392
总共花费的时间为：70.49
淄博市
1631A
3363A
3644A
3645A
[flaml.automl: 09-17 05:12:23] {2390} INFO - task = regression
[flaml.automl: 09-17 05:12:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:12:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:12:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:12:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:12:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:12:24] {3025} INFO - Estimated sufficient time budget=54905s. Estimated necessary time budget=55s.
[flaml.automl: 09-17 05:12:24] {3072} INFO -  at 1.5s,	estimator xgboost's best error=21.6236,	best estimator xgboost's best error=21.6236
[flaml.automl: 09-17 05:12:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:12:26] {3072} INFO -  at 3.6s,	estimator xgboost's best error=10.2648,	best estimator xgboost's best error=10.2648
[flaml.automl: 09-17 05:12:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:12:27] {3072} INFO -  at 4.8s,	estimator xgboost's best error=10.2648,	best estimator xgboost's best error=10.2648
[flaml.automl: 09-17 05:12:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:12:33] {3072} INFO -  at 10.7s,	estimator xgboost's best error=10.2648,	best estimator xgboost's best error=10.2648
[flaml.automl: 09-17 05:12:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:12:34] {3072} INFO -  at 11.8s,	estimator xgboost's best error=6.4454,	best estimator xgboost's best error=6.4454
[flaml.automl: 09-17 05:12:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:12:36] {3072} INFO -  at 13.4s,	estimator xgboost's best error=5.5089,	best estimator xgboost's best error=5.5089
[flaml.automl: 09-17 05:12:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:12:38] {3072} INFO -  at 15.0s,	estimator xgboost's best error=5.5089,	best estimator xgboost's best error=5.5089
[flaml.automl: 09-17 05:12:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:12:40] {3072} INFO -  at 17.5s,	estimator xgboost's best error=5.5089,	best estimator xgboost's best error=5.5089
[flaml.automl: 09-17 05:12:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:12:41] {3072} INFO -  at 18.6s,	estimator xgboost's best error=5.5089,	best estimator xgboost's best error=5.5089
[flaml.automl: 09-17 05:12:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:12:44] {3072} INFO -  at 21.3s,	estimator xgboost's best error=5.5089,	best estimator xgboost's best error=5.5089
[flaml.automl: 09-17 05:12:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:12:46] {3072} INFO -  at 22.9s,	estimator xgboost's best error=5.5089,	best estimator xgboost's best error=5.5089
[flaml.automl: 09-17 05:12:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:12:47] {3072} INFO -  at 24.0s,	estimator xgboost's best error=5.5089,	best estimator xgboost's best error=5.5089
[flaml.automl: 09-17 05:12:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:12:53] {3072} INFO -  at 30.4s,	estimator xgboost's best error=5.2614,	best estimator xgboost's best error=5.2614
[flaml.automl: 09-17 05:12:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:13:05] {3072} INFO -  at 42.5s,	estimator xgboost's best error=5.0912,	best estimator xgboost's best error=5.0912
[flaml.automl: 09-17 05:13:05] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 05:13:12] {3072} INFO -  at 49.0s,	estimator xgboost's best error=5.0912,	best estimator xgboost's best error=5.0912
[flaml.automl: 09-17 05:13:24] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 05:13:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:13:24] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:13:24] {2637} INFO - Time taken to find the best model: 42.496466875076294
[flaml.automl: 09-17 05:13:24] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43506}
NO2(0)最佳损失：-4.091218503924412
NO2(0)最好结果：{'pred_time': 8.386909776784272e-06, 'wall_clock_time': 42.496466875076294, 'metric_for_logging': {'pred_time': 8.386909776784272e-06}, 'val_loss': 5.091218503924412, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43506}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43506, 'experiment_tag': 'exp', 'time_total_s': 12.046829223632812}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8509586742598381
NO2(0)的mse=55.430717569172934
NO2(0)的mae=5.0605492375718955
NO2(0)的mar=0.18527512641756486
总共花费的时间为：61.87
枣庄市
1637A
1638A
1639A
1640A
3364A
[flaml.automl: 09-17 05:28:26] {2390} INFO - task = regression
[flaml.automl: 09-17 05:28:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:28:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:28:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:28:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:28:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:28:27] {3025} INFO - Estimated sufficient time budget=65288s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 05:28:27] {3072} INFO -  at 1.5s,	estimator xgboost's best error=17.0845,	best estimator xgboost's best error=17.0845
[flaml.automl: 09-17 05:28:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:28:29] {3072} INFO -  at 3.6s,	estimator xgboost's best error=8.1989,	best estimator xgboost's best error=8.1989
[flaml.automl: 09-17 05:28:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:28:30] {3072} INFO -  at 4.8s,	estimator xgboost's best error=8.1989,	best estimator xgboost's best error=8.1989
[flaml.automl: 09-17 05:28:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:28:35] {3072} INFO -  at 9.6s,	estimator xgboost's best error=8.1989,	best estimator xgboost's best error=8.1989
[flaml.automl: 09-17 05:28:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:28:36] {3072} INFO -  at 10.8s,	estimator xgboost's best error=5.1444,	best estimator xgboost's best error=5.1444
[flaml.automl: 09-17 05:28:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:28:38] {3072} INFO -  at 12.3s,	estimator xgboost's best error=4.4537,	best estimator xgboost's best error=4.4537
[flaml.automl: 09-17 05:28:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:28:39] {3072} INFO -  at 13.9s,	estimator xgboost's best error=4.4537,	best estimator xgboost's best error=4.4537
[flaml.automl: 09-17 05:28:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:28:42] {3072} INFO -  at 16.4s,	estimator xgboost's best error=4.4537,	best estimator xgboost's best error=4.4537
[flaml.automl: 09-17 05:28:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:28:43] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.4537,	best estimator xgboost's best error=4.4537
[flaml.automl: 09-17 05:28:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:28:46] {3072} INFO -  at 20.2s,	estimator xgboost's best error=4.4537,	best estimator xgboost's best error=4.4537
[flaml.automl: 09-17 05:28:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:28:47] {3072} INFO -  at 21.8s,	estimator xgboost's best error=4.4537,	best estimator xgboost's best error=4.4537
[flaml.automl: 09-17 05:28:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:28:48] {3072} INFO -  at 23.0s,	estimator xgboost's best error=4.4537,	best estimator xgboost's best error=4.4537
[flaml.automl: 09-17 05:28:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:28:55] {3072} INFO -  at 29.6s,	estimator xgboost's best error=4.2481,	best estimator xgboost's best error=4.2481
[flaml.automl: 09-17 05:28:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:29:07] {3072} INFO -  at 41.7s,	estimator xgboost's best error=4.1225,	best estimator xgboost's best error=4.1225
[flaml.automl: 09-17 05:29:07] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 05:29:14] {3072} INFO -  at 48.2s,	estimator xgboost's best error=4.1225,	best estimator xgboost's best error=4.1225
[flaml.automl: 09-17 05:29:26] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 05:29:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:29:26] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:29:26] {2637} INFO - Time taken to find the best model: 41.70055079460144
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53935}
NO2(0)最佳损失：-3.1224950788893526
NO2(0)最好结果：{'pred_time': 6.700902754568644e-06, 'wall_clock_time': 41.70055079460144, 'metric_for_logging': {'pred_time': 6.700902754568644e-06}, 'val_loss': 4.122495078889353, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53935}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53935, 'experiment_tag': 'exp', 'time_total_s': 12.129189252853394}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8322624370247955
NO2(0)的mse=38.87849881088971
NO2(0)的mae=4.068408963808803
NO2(0)的mar=0.17472621582289136
总共花费的时间为：61.28
烟台市
1642A
1643A
1644A
1646A
3366A
[flaml.automl: 09-17 05:45:10] {2390} INFO - task = regression
[flaml.automl: 09-17 05:45:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:45:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:45:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:45:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:45:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:45:12] {3025} INFO - Estimated sufficient time budget=64699s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 05:45:12] {3072} INFO -  at 1.5s,	estimator xgboost's best error=14.7799,	best estimator xgboost's best error=14.7799
[flaml.automl: 09-17 05:45:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:45:14] {3072} INFO -  at 3.6s,	estimator xgboost's best error=7.4108,	best estimator xgboost's best error=7.4108
[flaml.automl: 09-17 05:45:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:45:15] {3072} INFO -  at 4.8s,	estimator xgboost's best error=7.4108,	best estimator xgboost's best error=7.4108
[flaml.automl: 09-17 05:45:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:45:20] {3072} INFO -  at 9.7s,	estimator xgboost's best error=7.4108,	best estimator xgboost's best error=7.4108
[flaml.automl: 09-17 05:45:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:45:21] {3072} INFO -  at 10.8s,	estimator xgboost's best error=5.1041,	best estimator xgboost's best error=5.1041
[flaml.automl: 09-17 05:45:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:45:23] {3072} INFO -  at 12.4s,	estimator xgboost's best error=4.4205,	best estimator xgboost's best error=4.4205
[flaml.automl: 09-17 05:45:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:45:24] {3072} INFO -  at 14.0s,	estimator xgboost's best error=4.4205,	best estimator xgboost's best error=4.4205
[flaml.automl: 09-17 05:45:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:45:27] {3072} INFO -  at 16.5s,	estimator xgboost's best error=4.4205,	best estimator xgboost's best error=4.4205
[flaml.automl: 09-17 05:45:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:45:28] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.4205,	best estimator xgboost's best error=4.4205
[flaml.automl: 09-17 05:45:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:45:31] {3072} INFO -  at 20.3s,	estimator xgboost's best error=4.4205,	best estimator xgboost's best error=4.4205
[flaml.automl: 09-17 05:45:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:45:32] {3072} INFO -  at 21.9s,	estimator xgboost's best error=4.4164,	best estimator xgboost's best error=4.4164
[flaml.automl: 09-17 05:45:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:45:33] {3072} INFO -  at 23.1s,	estimator xgboost's best error=4.4164,	best estimator xgboost's best error=4.4164
[flaml.automl: 09-17 05:45:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:45:40] {3072} INFO -  at 29.6s,	estimator xgboost's best error=4.1940,	best estimator xgboost's best error=4.1940
[flaml.automl: 09-17 05:45:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:45:52] {3072} INFO -  at 41.8s,	estimator xgboost's best error=4.1234,	best estimator xgboost's best error=4.1234
[flaml.automl: 09-17 05:45:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 05:45:59] {3072} INFO -  at 48.3s,	estimator xgboost's best error=4.1234,	best estimator xgboost's best error=4.1234
[flaml.automl: 09-17 05:46:18] {3335} INFO - retrain xgboost for 19.5s
[flaml.automl: 09-17 05:46:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:46:18] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:46:18] {2637} INFO - Time taken to find the best model: 41.76415729522705
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54156}
NO2(0)最佳损失：-3.1234256654492487
NO2(0)最好结果：{'pred_time': 6.798812026952184e-06, 'wall_clock_time': 41.76415729522705, 'metric_for_logging': {'pred_time': 6.798812026952184e-06}, 'val_loss': 4.123425665449249, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54156}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54156, 'experiment_tag': 'exp', 'time_total_s': 12.164570569992065}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8114752027871339
NO2(0)的mse=40.47607500164436
NO2(0)的mae=4.178310050510836
NO2(0)的mar=0.22787576661259742
总共花费的时间为：68.76
潍坊市
3178A
3368A
3416A
3861A
[flaml.automl: 09-17 05:59:24] {2390} INFO - task = regression
[flaml.automl: 09-17 05:59:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:59:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:59:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:59:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:59:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:59:28] {3025} INFO - Estimated sufficient time budget=35204s. Estimated necessary time budget=35s.
[flaml.automl: 09-17 05:59:28] {3072} INFO -  at 4.0s,	estimator xgboost's best error=16.4297,	best estimator xgboost's best error=16.4297
[flaml.automl: 09-17 05:59:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:59:34] {3072} INFO -  at 10.3s,	estimator xgboost's best error=7.9548,	best estimator xgboost's best error=7.9548
[flaml.automl: 09-17 05:59:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:59:37] {3072} INFO -  at 13.8s,	estimator xgboost's best error=7.9548,	best estimator xgboost's best error=7.9548
[flaml.automl: 09-17 05:59:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:00:07] {3072} INFO -  at 43.7s,	estimator xgboost's best error=7.9548,	best estimator xgboost's best error=7.9548
[flaml.automl: 09-17 06:00:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:00:10] {3072} INFO -  at 46.5s,	estimator xgboost's best error=5.1328,	best estimator xgboost's best error=5.1328
[flaml.automl: 09-17 06:00:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:00:14] {3072} INFO -  at 50.4s,	estimator xgboost's best error=4.4028,	best estimator xgboost's best error=4.4028
[flaml.automl: 09-17 06:00:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:00:18] {3072} INFO -  at 54.5s,	estimator xgboost's best error=4.4028,	best estimator xgboost's best error=4.4028
[flaml.automl: 09-17 06:00:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:00:22] {3072} INFO -  at 58.3s,	estimator xgboost's best error=4.4028,	best estimator xgboost's best error=4.4028
[flaml.automl: 09-17 06:00:23] {3335} INFO - retrain xgboost for 1.5s
[flaml.automl: 09-17 06:00:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 06:00:24] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:00:24] {2637} INFO - Time taken to find the best model: 50.37297582626343
[flaml.automl: 09-17 06:00:24] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-3.4027720968003967
NO2(0)最好结果：{'pred_time': 2.500595811196025e-05, 'wall_clock_time': 50.37297582626343, 'metric_for_logging': {'pred_time': 2.500595811196025e-05}, 'val_loss': 4.402772096800397, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 3.825847864151001}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8375338560553395
NO2(0)的mse=42.01257810344858
NO2(0)的mae=4.4175014076428365
NO2(0)的mar=0.23522720466284075
总共花费的时间为：61.03
济宁市
1653A
3501A
3678A
[flaml.automl: 09-17 06:11:01] {2390} INFO - task = regression
[flaml.automl: 09-17 06:11:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:11:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:11:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:11:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:11:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:11:02] {3025} INFO - Estimated sufficient time budget=12267s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 06:11:02] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.6046,	best estimator xgboost's best error=15.6046
[flaml.automl: 09-17 06:11:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:11:04] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.4893,	best estimator xgboost's best error=7.4893
[flaml.automl: 09-17 06:11:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:11:05] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.4893,	best estimator xgboost's best error=7.4893
[flaml.automl: 09-17 06:11:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:11:15] {3072} INFO -  at 14.7s,	estimator xgboost's best error=7.4893,	best estimator xgboost's best error=7.4893
[flaml.automl: 09-17 06:11:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:11:16] {3072} INFO -  at 15.9s,	estimator xgboost's best error=4.6755,	best estimator xgboost's best error=4.6755
[flaml.automl: 09-17 06:11:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:11:18] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.8777,	best estimator xgboost's best error=3.8777
[flaml.automl: 09-17 06:11:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:11:19] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.8777,	best estimator xgboost's best error=3.8777
[flaml.automl: 09-17 06:11:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:11:22] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.8777,	best estimator xgboost's best error=3.8777
[flaml.automl: 09-17 06:11:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:11:23] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.8777,	best estimator xgboost's best error=3.8777
[flaml.automl: 09-17 06:11:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:11:26] {3072} INFO -  at 25.4s,	estimator xgboost's best error=3.8777,	best estimator xgboost's best error=3.8777
[flaml.automl: 09-17 06:11:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:11:27] {3072} INFO -  at 26.6s,	estimator xgboost's best error=3.8777,	best estimator xgboost's best error=3.8777
[flaml.automl: 09-17 06:11:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:11:28] {3072} INFO -  at 27.7s,	estimator xgboost's best error=3.8777,	best estimator xgboost's best error=3.8777
[flaml.automl: 09-17 06:11:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:11:35] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.6492,	best estimator xgboost's best error=3.6492
[flaml.automl: 09-17 06:11:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:11:49] {3072} INFO -  at 48.1s,	estimator xgboost's best error=3.5412,	best estimator xgboost's best error=3.5412
[flaml.automl: 09-17 06:12:09] {3335} INFO - retrain xgboost for 20.4s
[flaml.automl: 09-17 06:12:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:12:09] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:12:09] {2637} INFO - Time taken to find the best model: 48.14620041847229
[flaml.automl: 09-17 06:12:09] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.541198296104263
NO2(0)最好结果：{'pred_time': 2.4060336267978062e-05, 'wall_clock_time': 48.14620041847229, 'metric_for_logging': {'pred_time': 2.4060336267978062e-05}, 'val_loss': 3.541198296104263, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 13.925561904907227}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.9043624476563773
NO2(0)的mse=25.615876084091404
NO2(0)的mae=3.48505936769799
NO2(0)的mar=0.1816613937485577
总共花费的时间为：69.22
泰安市
3502A
3503A
3504A
[flaml.automl: 09-17 06:22:26] {2390} INFO - task = regression
[flaml.automl: 09-17 06:22:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:22:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:22:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:22:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:22:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:22:28] {3025} INFO - Estimated sufficient time budget=22575s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 06:22:28] {3072} INFO -  at 2.4s,	estimator xgboost's best error=15.5584,	best estimator xgboost's best error=15.5584
[flaml.automl: 09-17 06:22:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:22:32] {3072} INFO -  at 6.3s,	estimator xgboost's best error=7.5974,	best estimator xgboost's best error=7.5974
[flaml.automl: 09-17 06:22:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:22:35] {3072} INFO -  at 8.7s,	estimator xgboost's best error=7.5974,	best estimator xgboost's best error=7.5974
[flaml.automl: 09-17 06:22:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:22:53] {3072} INFO -  at 27.3s,	estimator xgboost's best error=7.5974,	best estimator xgboost's best error=7.5974
[flaml.automl: 09-17 06:22:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:22:55] {3072} INFO -  at 29.4s,	estimator xgboost's best error=5.1715,	best estimator xgboost's best error=5.1715
[flaml.automl: 09-17 06:22:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:22:58] {3072} INFO -  at 32.3s,	estimator xgboost's best error=4.5678,	best estimator xgboost's best error=4.5678
[flaml.automl: 09-17 06:22:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:23:01] {3072} INFO -  at 35.4s,	estimator xgboost's best error=4.5678,	best estimator xgboost's best error=4.5678
[flaml.automl: 09-17 06:23:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:23:06] {3072} INFO -  at 40.0s,	estimator xgboost's best error=4.5678,	best estimator xgboost's best error=4.5678
[flaml.automl: 09-17 06:23:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:23:08] {3072} INFO -  at 42.1s,	estimator xgboost's best error=4.5678,	best estimator xgboost's best error=4.5678
[flaml.automl: 09-17 06:23:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:23:13] {3072} INFO -  at 47.1s,	estimator xgboost's best error=4.5678,	best estimator xgboost's best error=4.5678
[flaml.automl: 09-17 06:23:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:23:15] {3072} INFO -  at 49.3s,	estimator xgboost's best error=4.5678,	best estimator xgboost's best error=4.5678
[flaml.automl: 09-17 06:23:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:23:17] {3072} INFO -  at 51.5s,	estimator xgboost's best error=4.5678,	best estimator xgboost's best error=4.5678
[flaml.automl: 09-17 06:23:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:23:25] {3072} INFO -  at 59.2s,	estimator xgboost's best error=4.5577,	best estimator xgboost's best error=4.5577
[flaml.automl: 09-17 06:23:38] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 06:23:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 06:23:38] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:23:38] {2637} INFO - Time taken to find the best model: 59.17520570755005
[flaml.automl: 09-17 06:23:38] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-3.55767706450077
NO2(0)最好结果：{'pred_time': 2.1096485764233033e-05, 'wall_clock_time': 59.17520570755005, 'metric_for_logging': {'pred_time': 2.1096485764233033e-05}, 'val_loss': 4.55767706450077, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 7.651245355606079}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8360773069771816
NO2(0)的mse=45.52870424952587
NO2(0)的mae=4.409999167800542
NO2(0)的mar=0.22211968025801354
总共花费的时间为：72.58
日照市
1659A
1661A
3507A
3604A
[flaml.automl: 09-17 06:36:35] {2390} INFO - task = regression
[flaml.automl: 09-17 06:36:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:36:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:36:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:36:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:36:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:36:38] {3025} INFO - Estimated sufficient time budget=101400s. Estimated necessary time budget=101s.
[flaml.automl: 09-17 06:36:38] {3072} INFO -  at 2.6s,	estimator xgboost's best error=17.1266,	best estimator xgboost's best error=17.1266
[flaml.automl: 09-17 06:36:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:36:41] {3072} INFO -  at 6.3s,	estimator xgboost's best error=8.7432,	best estimator xgboost's best error=8.7432
[flaml.automl: 09-17 06:36:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:36:44] {3072} INFO -  at 9.6s,	estimator xgboost's best error=8.7432,	best estimator xgboost's best error=8.7432
[flaml.automl: 09-17 06:36:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:36:49] {3072} INFO -  at 14.4s,	estimator xgboost's best error=8.7432,	best estimator xgboost's best error=8.7432
[flaml.automl: 09-17 06:36:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:36:53] {3072} INFO -  at 17.7s,	estimator xgboost's best error=6.2954,	best estimator xgboost's best error=6.2954
[flaml.automl: 09-17 06:36:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:36:57] {3072} INFO -  at 22.3s,	estimator xgboost's best error=5.6273,	best estimator xgboost's best error=5.6273
[flaml.automl: 09-17 06:36:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:37:01] {3072} INFO -  at 26.2s,	estimator xgboost's best error=5.6273,	best estimator xgboost's best error=5.6273
[flaml.automl: 09-17 06:37:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:37:03] {3072} INFO -  at 28.3s,	estimator xgboost's best error=5.6273,	best estimator xgboost's best error=5.6273
[flaml.automl: 09-17 06:37:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:37:06] {3072} INFO -  at 31.5s,	estimator xgboost's best error=5.6273,	best estimator xgboost's best error=5.6273
[flaml.automl: 09-17 06:37:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:37:09] {3072} INFO -  at 33.8s,	estimator xgboost's best error=5.6273,	best estimator xgboost's best error=5.6273
[flaml.automl: 09-17 06:37:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:37:13] {3072} INFO -  at 38.4s,	estimator xgboost's best error=5.6037,	best estimator xgboost's best error=5.6037
[flaml.automl: 09-17 06:37:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:37:17] {3072} INFO -  at 41.8s,	estimator xgboost's best error=5.6037,	best estimator xgboost's best error=5.6037
[flaml.automl: 09-17 06:37:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:37:33] {3072} INFO -  at 58.1s,	estimator xgboost's best error=5.4077,	best estimator xgboost's best error=5.4077
[flaml.automl: 09-17 06:37:45] {3335} INFO - retrain xgboost for 11.7s
[flaml.automl: 09-17 06:37:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 06:37:45] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:37:45] {2637} INFO - Time taken to find the best model: 58.12364172935486
[flaml.automl: 09-17 06:37:45] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 43355}
NO2(0)最佳损失：-4.407719359364734
NO2(0)最好结果：{'pred_time': 1.819215903592931e-05, 'wall_clock_time': 58.12364172935486, 'metric_for_logging': {'pred_time': 1.819215903592931e-05}, 'val_loss': 5.407719359364734, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 43355}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 43355, 'experiment_tag': 'exp', 'time_total_s': 16.33867359161377}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8148457754943552
NO2(0)的mse=63.31221377037538
NO2(0)的mae=5.260057132711327
NO2(0)的mar=0.2832111542246898
总共花费的时间为：70.70
威海市
1662A
1664A
1982A
3505A
[flaml.automl: 09-17 06:50:54] {2390} INFO - task = regression
[flaml.automl: 09-17 06:50:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:50:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:50:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:50:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:50:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:50:55] {3025} INFO - Estimated sufficient time budget=51433s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 06:50:55] {3072} INFO -  at 1.5s,	estimator xgboost's best error=9.9470,	best estimator xgboost's best error=9.9470
[flaml.automl: 09-17 06:50:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:50:57] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.0618,	best estimator xgboost's best error=5.0618
[flaml.automl: 09-17 06:50:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:50:58] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.0618,	best estimator xgboost's best error=5.0618
[flaml.automl: 09-17 06:50:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:51:04] {3072} INFO -  at 10.6s,	estimator xgboost's best error=5.0618,	best estimator xgboost's best error=5.0618
[flaml.automl: 09-17 06:51:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:51:05] {3072} INFO -  at 11.8s,	estimator xgboost's best error=3.7606,	best estimator xgboost's best error=3.7606
[flaml.automl: 09-17 06:51:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:51:07] {3072} INFO -  at 13.4s,	estimator xgboost's best error=3.3868,	best estimator xgboost's best error=3.3868
[flaml.automl: 09-17 06:51:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:51:09] {3072} INFO -  at 15.8s,	estimator xgboost's best error=3.3868,	best estimator xgboost's best error=3.3868
[flaml.automl: 09-17 06:51:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:51:14] {3072} INFO -  at 20.0s,	estimator xgboost's best error=3.3868,	best estimator xgboost's best error=3.3868
[flaml.automl: 09-17 06:51:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:51:16] {3072} INFO -  at 22.2s,	estimator xgboost's best error=3.3868,	best estimator xgboost's best error=3.3868
[flaml.automl: 09-17 06:51:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:51:19] {3072} INFO -  at 25.7s,	estimator xgboost's best error=3.3868,	best estimator xgboost's best error=3.3868
[flaml.automl: 09-17 06:51:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:51:22] {3072} INFO -  at 28.4s,	estimator xgboost's best error=3.3822,	best estimator xgboost's best error=3.3822
[flaml.automl: 09-17 06:51:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:51:24] {3072} INFO -  at 30.2s,	estimator xgboost's best error=3.3822,	best estimator xgboost's best error=3.3822
[flaml.automl: 09-17 06:51:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:51:35] {3072} INFO -  at 40.9s,	estimator xgboost's best error=3.3371,	best estimator xgboost's best error=3.3371
[flaml.automl: 09-17 06:51:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:51:53] {3072} INFO -  at 59.6s,	estimator xgboost's best error=3.2901,	best estimator xgboost's best error=3.2901
[flaml.automl: 09-17 06:52:14] {3335} INFO - retrain xgboost for 20.3s
[flaml.automl: 09-17 06:52:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:52:14] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:52:14] {2637} INFO - Time taken to find the best model: 59.56124663352966
[flaml.automl: 09-17 06:52:14] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42731}
NO2(0)最佳损失：-2.2901310629973213
NO2(0)最好结果：{'pred_time': 1.8410102198278592e-05, 'wall_clock_time': 59.56124663352966, 'metric_for_logging': {'pred_time': 1.8410102198278592e-05}, 'val_loss': 3.2901310629973213, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42731}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42731, 'experiment_tag': 'exp', 'time_total_s': 18.620954036712646}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7187312040947788
NO2(0)的mse=24.54078121071372
NO2(0)的mae=3.223550101633412
NO2(0)的mar=0.24529553759376693
总共花费的时间为：80.65
东营市
3365A
3498A
3734A
[flaml.automl: 09-17 07:02:35] {2390} INFO - task = regression
[flaml.automl: 09-17 07:02:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:02:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:02:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:02:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:02:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:02:37] {3025} INFO - Estimated sufficient time budget=20913s. Estimated necessary time budget=21s.
[flaml.automl: 09-17 07:02:37] {3072} INFO -  at 2.3s,	estimator xgboost's best error=15.9941,	best estimator xgboost's best error=15.9941
[flaml.automl: 09-17 07:02:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:02:41] {3072} INFO -  at 6.4s,	estimator xgboost's best error=7.7586,	best estimator xgboost's best error=7.7586
[flaml.automl: 09-17 07:02:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:02:43] {3072} INFO -  at 8.5s,	estimator xgboost's best error=7.7586,	best estimator xgboost's best error=7.7586
[flaml.automl: 09-17 07:02:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:03:01] {3072} INFO -  at 25.9s,	estimator xgboost's best error=7.7586,	best estimator xgboost's best error=7.7586
[flaml.automl: 09-17 07:03:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:03:03] {3072} INFO -  at 28.0s,	estimator xgboost's best error=5.0724,	best estimator xgboost's best error=5.0724
[flaml.automl: 09-17 07:03:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:03:06] {3072} INFO -  at 30.9s,	estimator xgboost's best error=4.3502,	best estimator xgboost's best error=4.3502
[flaml.automl: 09-17 07:03:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:03:09] {3072} INFO -  at 33.8s,	estimator xgboost's best error=4.3502,	best estimator xgboost's best error=4.3502
[flaml.automl: 09-17 07:03:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:03:13] {3072} INFO -  at 38.1s,	estimator xgboost's best error=4.3502,	best estimator xgboost's best error=4.3502
[flaml.automl: 09-17 07:03:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:03:15] {3072} INFO -  at 39.9s,	estimator xgboost's best error=4.3502,	best estimator xgboost's best error=4.3502
[flaml.automl: 09-17 07:03:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:03:20] {3072} INFO -  at 44.8s,	estimator xgboost's best error=4.3502,	best estimator xgboost's best error=4.3502
[flaml.automl: 09-17 07:03:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:03:22] {3072} INFO -  at 47.0s,	estimator xgboost's best error=4.3502,	best estimator xgboost's best error=4.3502
[flaml.automl: 09-17 07:03:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:03:24] {3072} INFO -  at 49.2s,	estimator xgboost's best error=4.3502,	best estimator xgboost's best error=4.3502
[flaml.automl: 09-17 07:03:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:03:33] {3072} INFO -  at 58.3s,	estimator xgboost's best error=4.1172,	best estimator xgboost's best error=4.1172
[flaml.automl: 09-17 07:03:40] {3335} INFO - retrain xgboost for 6.5s
[flaml.automl: 09-17 07:03:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 07:03:40] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:03:40] {2637} INFO - Time taken to find the best model: 58.26861548423767
[flaml.automl: 09-17 07:03:40] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-3.117191846422627
NO2(0)最好结果：{'pred_time': 1.0743757762756431e-05, 'wall_clock_time': 58.26861548423767, 'metric_for_logging': {'pred_time': 1.0743757762756431e-05}, 'val_loss': 4.117191846422627, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.058698892593384}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8583656625252831
NO2(0)的mse=43.769454152659385
NO2(0)的mae=4.25888409425248
NO2(0)的mar=0.2008291745380334
总共花费的时间为：65.43
韶关市
1669A
1673A
3622A
[flaml.automl: 09-17 07:12:45] {2390} INFO - task = regression
[flaml.automl: 09-17 07:12:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:12:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:12:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:12:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:12:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:12:46] {3025} INFO - Estimated sufficient time budget=12081s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 07:12:46] {3072} INFO -  at 1.4s,	estimator xgboost's best error=11.0445,	best estimator xgboost's best error=11.0445
[flaml.automl: 09-17 07:12:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:12:48] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.4206,	best estimator xgboost's best error=5.4206
[flaml.automl: 09-17 07:12:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:12:50] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.4206,	best estimator xgboost's best error=5.4206
[flaml.automl: 09-17 07:12:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:13:00] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.4206,	best estimator xgboost's best error=5.4206
[flaml.automl: 09-17 07:13:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:13:01] {3072} INFO -  at 15.8s,	estimator xgboost's best error=3.5408,	best estimator xgboost's best error=3.5408
[flaml.automl: 09-17 07:13:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:13:02] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.0298,	best estimator xgboost's best error=3.0298
[flaml.automl: 09-17 07:13:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:13:04] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.0298,	best estimator xgboost's best error=3.0298
[flaml.automl: 09-17 07:13:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:13:07] {3072} INFO -  at 21.5s,	estimator xgboost's best error=3.0298,	best estimator xgboost's best error=3.0298
[flaml.automl: 09-17 07:13:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:13:08] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.0298,	best estimator xgboost's best error=3.0298
[flaml.automl: 09-17 07:13:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:13:10] {3072} INFO -  at 25.3s,	estimator xgboost's best error=3.0298,	best estimator xgboost's best error=3.0298
[flaml.automl: 09-17 07:13:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:13:11] {3072} INFO -  at 26.5s,	estimator xgboost's best error=3.0298,	best estimator xgboost's best error=3.0298
[flaml.automl: 09-17 07:13:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:13:13] {3072} INFO -  at 27.6s,	estimator xgboost's best error=3.0298,	best estimator xgboost's best error=3.0298
[flaml.automl: 09-17 07:13:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:13:19] {3072} INFO -  at 34.1s,	estimator xgboost's best error=2.8562,	best estimator xgboost's best error=2.8562
[flaml.automl: 09-17 07:13:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:13:31] {3072} INFO -  at 46.1s,	estimator xgboost's best error=2.8066,	best estimator xgboost's best error=2.8066
[flaml.automl: 09-17 07:13:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 07:13:38] {3072} INFO -  at 52.7s,	estimator xgboost's best error=2.8066,	best estimator xgboost's best error=2.8066
[flaml.automl: 09-17 07:13:50] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 07:13:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:13:50] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:13:50] {2637} INFO - Time taken to find the best model: 46.14230704307556
[flaml.automl: 09-17 07:13:50] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.8065694008158686
NO2(0)最好结果：{'pred_time': 1.1407721742311057e-05, 'wall_clock_time': 46.14230704307556, 'metric_for_logging': {'pred_time': 1.1407721742311057e-05}, 'val_loss': 2.8065694008158686, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.01896357536316}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8320659575238097
NO2(0)的mse=15.415621478859716
NO2(0)的mae=2.6701661291010743
NO2(0)的mar=0.19299229195872059
总共花费的时间为：65.26
汕头市
1674A
1675A
3624A
[flaml.automl: 09-17 07:23:35] {2390} INFO - task = regression
[flaml.automl: 09-17 07:23:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:23:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:23:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:23:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:23:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:23:36] {3025} INFO - Estimated sufficient time budget=12176s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 07:23:36] {3072} INFO -  at 1.4s,	estimator xgboost's best error=10.0768,	best estimator xgboost's best error=10.0768
[flaml.automl: 09-17 07:23:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:23:38] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.7872,	best estimator xgboost's best error=4.7872
[flaml.automl: 09-17 07:23:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:23:39] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.7872,	best estimator xgboost's best error=4.7872
[flaml.automl: 09-17 07:23:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:23:49] {3072} INFO -  at 14.7s,	estimator xgboost's best error=4.7872,	best estimator xgboost's best error=4.7872
[flaml.automl: 09-17 07:23:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:23:50] {3072} INFO -  at 15.9s,	estimator xgboost's best error=2.8197,	best estimator xgboost's best error=2.8197
[flaml.automl: 09-17 07:23:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:23:52] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.4203,	best estimator xgboost's best error=2.4203
[flaml.automl: 09-17 07:23:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:23:54] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.4203,	best estimator xgboost's best error=2.4203
[flaml.automl: 09-17 07:23:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:23:56] {3072} INFO -  at 21.6s,	estimator xgboost's best error=2.4203,	best estimator xgboost's best error=2.4203
[flaml.automl: 09-17 07:23:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:23:57] {3072} INFO -  at 22.7s,	estimator xgboost's best error=2.4203,	best estimator xgboost's best error=2.4203
[flaml.automl: 09-17 07:23:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:24:00] {3072} INFO -  at 25.4s,	estimator xgboost's best error=2.4203,	best estimator xgboost's best error=2.4203
[flaml.automl: 09-17 07:24:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:24:01] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.4203,	best estimator xgboost's best error=2.4203
[flaml.automl: 09-17 07:24:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:24:02] {3072} INFO -  at 27.8s,	estimator xgboost's best error=2.4203,	best estimator xgboost's best error=2.4203
[flaml.automl: 09-17 07:24:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:24:09] {3072} INFO -  at 34.3s,	estimator xgboost's best error=2.2862,	best estimator xgboost's best error=2.2862
[flaml.automl: 09-17 07:24:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:24:21] {3072} INFO -  at 46.3s,	estimator xgboost's best error=2.2084,	best estimator xgboost's best error=2.2084
[flaml.automl: 09-17 07:24:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 07:24:27] {3072} INFO -  at 52.8s,	estimator xgboost's best error=2.2084,	best estimator xgboost's best error=2.2084
[flaml.automl: 09-17 07:24:39] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 07:24:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:24:39] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:24:39] {2637} INFO - Time taken to find the best model: 46.31879258155823
[flaml.automl: 09-17 07:24:39] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.2083977732331173
NO2(0)最好结果：{'pred_time': 1.1295393546378341e-05, 'wall_clock_time': 46.31879258155823, 'metric_for_logging': {'pred_time': 1.1295393546378341e-05}, 'val_loss': 2.2083977732331173, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.044864177703857}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8127173807466078
NO2(0)的mse=10.498072696684272
NO2(0)的mae=2.282881256533441
NO2(0)的mar=0.16772539699878736
总共花费的时间为：65.36
湛江市
1680A
1681A
1682A
1684A
1685A
[flaml.automl: 09-17 07:40:47] {2390} INFO - task = regression
[flaml.automl: 09-17 07:40:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:40:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:40:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:40:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:40:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:40:49] {3025} INFO - Estimated sufficient time budget=123623s. Estimated necessary time budget=124s.
[flaml.automl: 09-17 07:40:49] {3072} INFO -  at 2.6s,	estimator xgboost's best error=7.2512,	best estimator xgboost's best error=7.2512
[flaml.automl: 09-17 07:40:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:40:53] {3072} INFO -  at 6.5s,	estimator xgboost's best error=3.5701,	best estimator xgboost's best error=3.5701
[flaml.automl: 09-17 07:40:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:40:56] {3072} INFO -  at 8.8s,	estimator xgboost's best error=3.5701,	best estimator xgboost's best error=3.5701
[flaml.automl: 09-17 07:40:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:40:59] {3072} INFO -  at 12.5s,	estimator xgboost's best error=3.5701,	best estimator xgboost's best error=3.5701
[flaml.automl: 09-17 07:40:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:41:01] {3072} INFO -  at 14.5s,	estimator xgboost's best error=2.5278,	best estimator xgboost's best error=2.5278
[flaml.automl: 09-17 07:41:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:41:04] {3072} INFO -  at 17.1s,	estimator xgboost's best error=2.3021,	best estimator xgboost's best error=2.3021
[flaml.automl: 09-17 07:41:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:41:07] {3072} INFO -  at 20.2s,	estimator xgboost's best error=2.3021,	best estimator xgboost's best error=2.3021
[flaml.automl: 09-17 07:41:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:41:10] {3072} INFO -  at 23.1s,	estimator xgboost's best error=2.3021,	best estimator xgboost's best error=2.3021
[flaml.automl: 09-17 07:41:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:41:12] {3072} INFO -  at 25.1s,	estimator xgboost's best error=2.3021,	best estimator xgboost's best error=2.3021
[flaml.automl: 09-17 07:41:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:41:14] {3072} INFO -  at 27.4s,	estimator xgboost's best error=2.3021,	best estimator xgboost's best error=2.3021
[flaml.automl: 09-17 07:41:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:41:17] {3072} INFO -  at 30.3s,	estimator xgboost's best error=2.3021,	best estimator xgboost's best error=2.3021
[flaml.automl: 09-17 07:41:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:41:19] {3072} INFO -  at 32.3s,	estimator xgboost's best error=2.3021,	best estimator xgboost's best error=2.3021
[flaml.automl: 09-17 07:41:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:41:31] {3072} INFO -  at 44.5s,	estimator xgboost's best error=2.2529,	best estimator xgboost's best error=2.2529
[flaml.automl: 09-17 07:41:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:41:45] {3072} INFO -  at 58.3s,	estimator xgboost's best error=2.2330,	best estimator xgboost's best error=2.2330
[flaml.automl: 09-17 07:42:07] {3335} INFO - retrain xgboost for 21.9s
[flaml.automl: 09-17 07:42:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:42:07] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:42:07] {2637} INFO - Time taken to find the best model: 58.3184814453125
[flaml.automl: 09-17 07:42:07] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 55414}
NO2(0)最佳损失：-1.2330217053650956
NO2(0)最好结果：{'pred_time': 1.1149146565371963e-05, 'wall_clock_time': 58.3184814453125, 'metric_for_logging': {'pred_time': 1.1149146565371963e-05}, 'val_loss': 2.2330217053650956, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 55414}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 55414, 'experiment_tag': 'exp', 'time_total_s': 13.76555871963501}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7129662544924094
NO2(0)的mse=11.928008252290622
NO2(0)的mae=2.3083648303035265
NO2(0)的mar=0.2225452473030265
总共花费的时间为：81.38
茂名市
1686A
1688A
1689A
3450A
[flaml.automl: 09-17 07:55:23] {2390} INFO - task = regression
[flaml.automl: 09-17 07:55:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:55:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:55:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:55:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:55:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:55:25] {3025} INFO - Estimated sufficient time budget=87202s. Estimated necessary time budget=87s.
[flaml.automl: 09-17 07:55:25] {3072} INFO -  at 2.3s,	estimator xgboost's best error=7.1941,	best estimator xgboost's best error=7.1941
[flaml.automl: 09-17 07:55:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:55:29] {3072} INFO -  at 5.8s,	estimator xgboost's best error=3.4695,	best estimator xgboost's best error=3.4695
[flaml.automl: 09-17 07:55:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:55:31] {3072} INFO -  at 8.3s,	estimator xgboost's best error=3.4695,	best estimator xgboost's best error=3.4695
[flaml.automl: 09-17 07:55:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:55:37] {3072} INFO -  at 14.2s,	estimator xgboost's best error=3.4695,	best estimator xgboost's best error=3.4695
[flaml.automl: 09-17 07:55:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:55:39] {3072} INFO -  at 16.3s,	estimator xgboost's best error=2.3051,	best estimator xgboost's best error=2.3051
[flaml.automl: 09-17 07:55:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:55:42] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.1113,	best estimator xgboost's best error=2.1113
[flaml.automl: 09-17 07:55:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:55:46] {3072} INFO -  at 23.0s,	estimator xgboost's best error=2.1113,	best estimator xgboost's best error=2.1113
[flaml.automl: 09-17 07:55:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:55:49] {3072} INFO -  at 26.0s,	estimator xgboost's best error=2.1113,	best estimator xgboost's best error=2.1113
[flaml.automl: 09-17 07:55:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:55:51] {3072} INFO -  at 28.5s,	estimator xgboost's best error=2.1113,	best estimator xgboost's best error=2.1113
[flaml.automl: 09-17 07:55:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:55:55] {3072} INFO -  at 31.8s,	estimator xgboost's best error=2.1113,	best estimator xgboost's best error=2.1113
[flaml.automl: 09-17 07:55:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:55:58] {3072} INFO -  at 35.6s,	estimator xgboost's best error=2.1113,	best estimator xgboost's best error=2.1113
[flaml.automl: 09-17 07:55:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:56:01] {3072} INFO -  at 38.2s,	estimator xgboost's best error=2.1113,	best estimator xgboost's best error=2.1113
[flaml.automl: 09-17 07:56:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:56:12] {3072} INFO -  at 49.3s,	estimator xgboost's best error=2.0680,	best estimator xgboost's best error=2.0680
[flaml.automl: 09-17 07:56:24] {3335} INFO - retrain xgboost for 11.7s
[flaml.automl: 09-17 07:56:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 07:56:24] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:56:24] {2637} INFO - Time taken to find the best model: 49.325352907180786
[flaml.automl: 09-17 07:56:24] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 42399}
NO2(0)最佳损失：-1.0679705719288015
NO2(0)最好结果：{'pred_time': 1.8153459795059696e-05, 'wall_clock_time': 49.325352907180786, 'metric_for_logging': {'pred_time': 1.8153459795059696e-05}, 'val_loss': 2.0679705719288015, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 42399}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 42399, 'experiment_tag': 'exp', 'time_total_s': 11.151639461517334}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7523390297711432
NO2(0)的mse=9.085646352899577
NO2(0)的mae=2.0433286173637413
NO2(0)的mar=0.19796774451459112
总共花费的时间为：61.90
梅州市
1690A
1692A
3315A
[flaml.automl: 09-17 08:05:59] {2390} INFO - task = regression
[flaml.automl: 09-17 08:05:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:05:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:05:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:05:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:05:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:06:00] {3025} INFO - Estimated sufficient time budget=12173s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 08:06:00] {3072} INFO -  at 1.4s,	estimator xgboost's best error=10.4706,	best estimator xgboost's best error=10.4706
[flaml.automl: 09-17 08:06:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:06:02] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.2453,	best estimator xgboost's best error=5.2453
[flaml.automl: 09-17 08:06:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:06:03] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.2453,	best estimator xgboost's best error=5.2453
[flaml.automl: 09-17 08:06:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:06:13] {3072} INFO -  at 14.8s,	estimator xgboost's best error=5.2453,	best estimator xgboost's best error=5.2453
[flaml.automl: 09-17 08:06:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:06:15] {3072} INFO -  at 15.9s,	estimator xgboost's best error=3.8074,	best estimator xgboost's best error=3.8074
[flaml.automl: 09-17 08:06:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:06:16] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.4849,	best estimator xgboost's best error=3.4849
[flaml.automl: 09-17 08:06:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:06:18] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.4849,	best estimator xgboost's best error=3.4849
[flaml.automl: 09-17 08:06:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:06:20] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.4849,	best estimator xgboost's best error=3.4849
[flaml.automl: 09-17 08:06:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:06:21] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.4849,	best estimator xgboost's best error=3.4849
[flaml.automl: 09-17 08:06:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:06:24] {3072} INFO -  at 25.5s,	estimator xgboost's best error=3.4849,	best estimator xgboost's best error=3.4849
[flaml.automl: 09-17 08:06:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:06:25] {3072} INFO -  at 26.6s,	estimator xgboost's best error=3.4849,	best estimator xgboost's best error=3.4849
[flaml.automl: 09-17 08:06:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:06:26] {3072} INFO -  at 27.8s,	estimator xgboost's best error=3.4849,	best estimator xgboost's best error=3.4849
[flaml.automl: 09-17 08:06:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:06:33] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.4007,	best estimator xgboost's best error=3.4007
[flaml.automl: 09-17 08:06:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:06:45] {3072} INFO -  at 46.3s,	estimator xgboost's best error=3.3471,	best estimator xgboost's best error=3.3471
[flaml.automl: 09-17 08:06:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 08:06:51] {3072} INFO -  at 52.8s,	estimator xgboost's best error=3.3471,	best estimator xgboost's best error=3.3471
[flaml.automl: 09-17 08:07:04] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 08:07:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:07:04] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:07:04] {2637} INFO - Time taken to find the best model: 46.292423248291016
[flaml.automl: 09-17 08:07:04] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.34707557175625
NO2(0)最好结果：{'pred_time': 1.1592348707346146e-05, 'wall_clock_time': 46.292423248291016, 'metric_for_logging': {'pred_time': 1.1592348707346146e-05}, 'val_loss': 3.34707557175625, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.072622060775757}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7378818452982718
NO2(0)的mse=26.012593160663624
NO2(0)的mae=3.4334699734460554
NO2(0)的mar=0.24177858949585604
总共花费的时间为：65.37
汕尾市
1694A
1695A
[flaml.automl: 09-17 08:13:07] {2390} INFO - task = regression
[flaml.automl: 09-17 08:13:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:13:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:13:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:13:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:13:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:13:09] {3025} INFO - Estimated sufficient time budget=22110s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 08:13:09] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.4592,	best estimator xgboost's best error=5.4592
[flaml.automl: 09-17 08:13:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:13:13] {3072} INFO -  at 6.3s,	estimator xgboost's best error=2.6397,	best estimator xgboost's best error=2.6397
[flaml.automl: 09-17 08:13:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:13:15] {3072} INFO -  at 8.5s,	estimator xgboost's best error=2.6397,	best estimator xgboost's best error=2.6397
[flaml.automl: 09-17 08:13:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:13:32] {3072} INFO -  at 24.9s,	estimator xgboost's best error=2.6397,	best estimator xgboost's best error=2.6397
[flaml.automl: 09-17 08:13:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:13:34] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.7771,	best estimator xgboost's best error=1.7771
[flaml.automl: 09-17 08:13:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:13:36] {3072} INFO -  at 28.6s,	estimator xgboost's best error=1.6217,	best estimator xgboost's best error=1.6217
[flaml.automl: 09-17 08:13:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:13:38] {3072} INFO -  at 31.4s,	estimator xgboost's best error=1.6217,	best estimator xgboost's best error=1.6217
[flaml.automl: 09-17 08:13:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:13:42] {3072} INFO -  at 35.3s,	estimator xgboost's best error=1.6217,	best estimator xgboost's best error=1.6217
[flaml.automl: 09-17 08:13:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:13:44] {3072} INFO -  at 37.2s,	estimator xgboost's best error=1.6217,	best estimator xgboost's best error=1.6217
[flaml.automl: 09-17 08:13:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:13:48] {3072} INFO -  at 41.2s,	estimator xgboost's best error=1.6217,	best estimator xgboost's best error=1.6217
[flaml.automl: 09-17 08:13:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:13:50] {3072} INFO -  at 43.0s,	estimator xgboost's best error=1.6217,	best estimator xgboost's best error=1.6217
[flaml.automl: 09-17 08:13:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:13:52] {3072} INFO -  at 45.0s,	estimator xgboost's best error=1.6217,	best estimator xgboost's best error=1.6217
[flaml.automl: 09-17 08:13:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:14:01] {3072} INFO -  at 54.4s,	estimator xgboost's best error=1.5877,	best estimator xgboost's best error=1.5877
[flaml.automl: 09-17 08:14:12] {3335} INFO - retrain xgboost for 10.6s
[flaml.automl: 09-17 08:14:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 08:14:12] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:14:12] {2637} INFO - Time taken to find the best model: 54.44637203216553
[flaml.automl: 09-17 08:14:12] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-0.5876892015720347
NO2(0)最好结果：{'pred_time': 2.0256652318483447e-05, 'wall_clock_time': 54.44637203216553, 'metric_for_logging': {'pred_time': 2.0256652318483447e-05}, 'val_loss': 1.5876892015720347, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.400649070739746}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7496276989382399
NO2(0)的mse=5.788179981262989
NO2(0)的mae=1.59527957407838
NO2(0)的mar=0.20357154237333558
总共花费的时间为：65.49
河源市
1696A
1697A
[flaml.automl: 09-17 08:20:54] {2390} INFO - task = regression
[flaml.automl: 09-17 08:20:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:20:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:20:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:20:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:20:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:20:55] {3025} INFO - Estimated sufficient time budget=12118s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 08:20:55] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.4607,	best estimator xgboost's best error=11.4607
[flaml.automl: 09-17 08:20:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:20:57] {3072} INFO -  at 3.2s,	estimator xgboost's best error=6.1661,	best estimator xgboost's best error=6.1661
[flaml.automl: 09-17 08:20:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:20:59] {3072} INFO -  at 4.4s,	estimator xgboost's best error=6.1661,	best estimator xgboost's best error=6.1661
[flaml.automl: 09-17 08:20:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:21:07] {3072} INFO -  at 12.8s,	estimator xgboost's best error=6.1661,	best estimator xgboost's best error=6.1661
[flaml.automl: 09-17 08:21:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:21:08] {3072} INFO -  at 13.9s,	estimator xgboost's best error=3.5713,	best estimator xgboost's best error=3.5713
[flaml.automl: 09-17 08:21:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:21:10] {3072} INFO -  at 15.5s,	estimator xgboost's best error=3.2428,	best estimator xgboost's best error=3.2428
[flaml.automl: 09-17 08:21:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:21:11] {3072} INFO -  at 17.1s,	estimator xgboost's best error=3.2428,	best estimator xgboost's best error=3.2428
[flaml.automl: 09-17 08:21:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:21:14] {3072} INFO -  at 19.6s,	estimator xgboost's best error=3.2428,	best estimator xgboost's best error=3.2428
[flaml.automl: 09-17 08:21:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:21:15] {3072} INFO -  at 20.7s,	estimator xgboost's best error=3.2428,	best estimator xgboost's best error=3.2428
[flaml.automl: 09-17 08:21:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:21:17] {3072} INFO -  at 23.1s,	estimator xgboost's best error=3.2428,	best estimator xgboost's best error=3.2428
[flaml.automl: 09-17 08:21:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:21:18] {3072} INFO -  at 24.3s,	estimator xgboost's best error=3.2428,	best estimator xgboost's best error=3.2428
[flaml.automl: 09-17 08:21:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:21:20] {3072} INFO -  at 25.4s,	estimator xgboost's best error=3.2428,	best estimator xgboost's best error=3.2428
[flaml.automl: 09-17 08:21:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:21:26] {3072} INFO -  at 31.4s,	estimator xgboost's best error=3.1908,	best estimator xgboost's best error=3.1908
[flaml.automl: 09-17 08:21:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:21:36] {3072} INFO -  at 41.8s,	estimator xgboost's best error=3.1145,	best estimator xgboost's best error=3.1145
[flaml.automl: 09-17 08:21:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 08:21:42] {3072} INFO -  at 47.8s,	estimator xgboost's best error=3.1145,	best estimator xgboost's best error=3.1145
[flaml.automl: 09-17 08:21:42] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 08:21:54] {3072} INFO -  at 59.5s,	estimator xgboost's best error=3.1145,	best estimator xgboost's best error=3.1145
[flaml.automl: 09-17 08:22:04] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-17 08:22:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:22:04] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:22:04] {2637} INFO - Time taken to find the best model: 41.776180028915405
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.114457450940266
NO2(0)最好结果：{'pred_time': 1.9197293530066734e-05, 'wall_clock_time': 41.776180028915405, 'metric_for_logging': {'pred_time': 1.9197293530066734e-05}, 'val_loss': 3.114457450940266, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.390448093414307}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7013219323285911
NO2(0)的mse=23.8494997899981
NO2(0)的mae=3.1226978259625398
NO2(0)的mar=0.18796463845056383
总共花费的时间为：70.28
阳江市
1699A
3453A
[flaml.automl: 09-17 08:28:23] {2390} INFO - task = regression
[flaml.automl: 09-17 08:28:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:28:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:28:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:28:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:28:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:28:25] {3025} INFO - Estimated sufficient time budget=22139s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 08:28:25] {3072} INFO -  at 2.3s,	estimator xgboost's best error=8.1189,	best estimator xgboost's best error=8.1189
[flaml.automl: 09-17 08:28:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:28:29] {3072} INFO -  at 5.8s,	estimator xgboost's best error=4.7035,	best estimator xgboost's best error=4.7035
[flaml.automl: 09-17 08:28:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:28:31] {3072} INFO -  at 8.0s,	estimator xgboost's best error=4.7035,	best estimator xgboost's best error=4.7035
[flaml.automl: 09-17 08:28:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:28:47] {3072} INFO -  at 23.5s,	estimator xgboost's best error=4.7035,	best estimator xgboost's best error=4.7035
[flaml.automl: 09-17 08:28:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:28:49] {3072} INFO -  at 25.6s,	estimator xgboost's best error=3.1832,	best estimator xgboost's best error=3.1832
[flaml.automl: 09-17 08:28:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:28:52] {3072} INFO -  at 28.5s,	estimator xgboost's best error=2.7733,	best estimator xgboost's best error=2.7733
[flaml.automl: 09-17 08:28:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:28:55] {3072} INFO -  at 31.4s,	estimator xgboost's best error=2.7733,	best estimator xgboost's best error=2.7733
[flaml.automl: 09-17 08:28:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:28:59] {3072} INFO -  at 35.9s,	estimator xgboost's best error=2.7733,	best estimator xgboost's best error=2.7733
[flaml.automl: 09-17 08:28:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:29:00] {3072} INFO -  at 37.0s,	estimator xgboost's best error=2.7733,	best estimator xgboost's best error=2.7733
[flaml.automl: 09-17 08:29:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:29:03] {3072} INFO -  at 39.5s,	estimator xgboost's best error=2.7733,	best estimator xgboost's best error=2.7733
[flaml.automl: 09-17 08:29:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:29:04] {3072} INFO -  at 40.6s,	estimator xgboost's best error=2.7733,	best estimator xgboost's best error=2.7733
[flaml.automl: 09-17 08:29:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:29:05] {3072} INFO -  at 41.8s,	estimator xgboost's best error=2.7733,	best estimator xgboost's best error=2.7733
[flaml.automl: 09-17 08:29:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:29:11] {3072} INFO -  at 47.8s,	estimator xgboost's best error=2.7675,	best estimator xgboost's best error=2.7675
[flaml.automl: 09-17 08:29:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:29:21] {3072} INFO -  at 58.2s,	estimator xgboost's best error=2.6932,	best estimator xgboost's best error=2.6932
[flaml.automl: 09-17 08:29:33] {3335} INFO - retrain xgboost for 11.8s
[flaml.automl: 09-17 08:29:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:29:33] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:29:33] {2637} INFO - Time taken to find the best model: 58.19739246368408
[flaml.automl: 09-17 08:29:33] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.6932195648072068
NO2(0)最好结果：{'pred_time': 1.807821464713721e-05, 'wall_clock_time': 58.19739246368408, 'metric_for_logging': {'pred_time': 1.807821464713721e-05}, 'val_loss': 2.693219564807207, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.392721891403198}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8339364372358826
NO2(0)的mse=16.945623353704924
NO2(0)的mae=2.691023470498945
NO2(0)的mar=0.2769721906015082
总共花费的时间为：70.42
清远市
1702A
3318A
3455A
[flaml.automl: 09-17 08:39:04] {2390} INFO - task = regression
[flaml.automl: 09-17 08:39:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:39:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:39:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:39:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:39:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:39:08] {3025} INFO - Estimated sufficient time budget=33788s. Estimated necessary time budget=34s.
[flaml.automl: 09-17 08:39:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=12.0193,	best estimator xgboost's best error=12.0193
[flaml.automl: 09-17 08:39:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:39:14] {3072} INFO -  at 9.5s,	estimator xgboost's best error=5.7222,	best estimator xgboost's best error=5.7222
[flaml.automl: 09-17 08:39:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:39:17] {3072} INFO -  at 12.9s,	estimator xgboost's best error=5.7222,	best estimator xgboost's best error=5.7222
[flaml.automl: 09-17 08:39:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:39:44] {3072} INFO -  at 39.5s,	estimator xgboost's best error=5.7222,	best estimator xgboost's best error=5.7222
[flaml.automl: 09-17 08:39:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:39:46] {3072} INFO -  at 41.7s,	estimator xgboost's best error=3.6111,	best estimator xgboost's best error=3.6111
[flaml.automl: 09-17 08:39:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:39:49] {3072} INFO -  at 44.7s,	estimator xgboost's best error=3.1009,	best estimator xgboost's best error=3.1009
[flaml.automl: 09-17 08:39:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:39:52] {3072} INFO -  at 47.7s,	estimator xgboost's best error=3.1009,	best estimator xgboost's best error=3.1009
[flaml.automl: 09-17 08:39:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:39:56] {3072} INFO -  at 51.9s,	estimator xgboost's best error=3.1009,	best estimator xgboost's best error=3.1009
[flaml.automl: 09-17 08:39:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:39:57] {3072} INFO -  at 53.0s,	estimator xgboost's best error=3.1009,	best estimator xgboost's best error=3.1009
[flaml.automl: 09-17 08:39:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:40:00] {3072} INFO -  at 55.7s,	estimator xgboost's best error=3.1009,	best estimator xgboost's best error=3.1009
[flaml.automl: 09-17 08:40:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:40:01] {3072} INFO -  at 56.8s,	estimator xgboost's best error=3.1009,	best estimator xgboost's best error=3.1009
[flaml.automl: 09-17 08:40:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:40:02] {3072} INFO -  at 58.0s,	estimator xgboost's best error=3.1009,	best estimator xgboost's best error=3.1009
[flaml.automl: 09-17 08:40:03] {3335} INFO - retrain xgboost for 1.5s
[flaml.automl: 09-17 08:40:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 08:40:04] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:40:04] {2637} INFO - Time taken to find the best model: 44.67414093017578
[flaml.automl: 09-17 08:40:04] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-2.100912791937186
NO2(0)最好结果：{'pred_time': 2.2913223643135026e-05, 'wall_clock_time': 44.67414093017578, 'metric_for_logging': {'pred_time': 2.2913223643135026e-05}, 'val_loss': 3.100912791937186, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 3.001636028289795}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7711962340960139
NO2(0)的mse=21.967279690649782
NO2(0)的mae=3.183245242206895
NO2(0)的mar=0.19582061217565494
总共花费的时间为：60.08
潮州市
1705A
1706A
3026A
[flaml.automl: 09-17 08:51:14] {2390} INFO - task = regression
[flaml.automl: 09-17 08:51:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:51:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:51:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:51:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:51:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:51:17] {3025} INFO - Estimated sufficient time budget=21415s. Estimated necessary time budget=21s.
[flaml.automl: 09-17 08:51:17] {3072} INFO -  at 2.3s,	estimator xgboost's best error=9.1861,	best estimator xgboost's best error=9.1861
[flaml.automl: 09-17 08:51:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:51:20] {3072} INFO -  at 6.1s,	estimator xgboost's best error=4.3087,	best estimator xgboost's best error=4.3087
[flaml.automl: 09-17 08:51:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:51:22] {3072} INFO -  at 8.2s,	estimator xgboost's best error=4.3087,	best estimator xgboost's best error=4.3087
[flaml.automl: 09-17 08:51:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:51:39] {3072} INFO -  at 25.0s,	estimator xgboost's best error=4.3087,	best estimator xgboost's best error=4.3087
[flaml.automl: 09-17 08:51:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:51:41] {3072} INFO -  at 27.0s,	estimator xgboost's best error=2.6222,	best estimator xgboost's best error=2.6222
[flaml.automl: 09-17 08:51:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:51:44] {3072} INFO -  at 30.0s,	estimator xgboost's best error=2.3578,	best estimator xgboost's best error=2.3578
[flaml.automl: 09-17 08:51:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:51:47] {3072} INFO -  at 33.1s,	estimator xgboost's best error=2.3578,	best estimator xgboost's best error=2.3578
[flaml.automl: 09-17 08:51:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:51:52] {3072} INFO -  at 37.6s,	estimator xgboost's best error=2.3578,	best estimator xgboost's best error=2.3578
[flaml.automl: 09-17 08:51:52] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:51:54] {3072} INFO -  at 39.4s,	estimator xgboost's best error=2.3578,	best estimator xgboost's best error=2.3578
[flaml.automl: 09-17 08:51:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:51:58] {3072} INFO -  at 43.9s,	estimator xgboost's best error=2.3578,	best estimator xgboost's best error=2.3578
[flaml.automl: 09-17 08:51:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:52:00] {3072} INFO -  at 45.8s,	estimator xgboost's best error=2.3578,	best estimator xgboost's best error=2.3578
[flaml.automl: 09-17 08:52:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:52:02] {3072} INFO -  at 47.9s,	estimator xgboost's best error=2.3578,	best estimator xgboost's best error=2.3578
[flaml.automl: 09-17 08:52:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:52:11] {3072} INFO -  at 57.0s,	estimator xgboost's best error=2.2648,	best estimator xgboost's best error=2.2648
[flaml.automl: 09-17 08:52:18] {3335} INFO - retrain xgboost for 6.5s
[flaml.automl: 09-17 08:52:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 08:52:18] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:52:18] {2637} INFO - Time taken to find the best model: 57.03280854225159
[flaml.automl: 09-17 08:52:18] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-1.2647691007497315
NO2(0)最好结果：{'pred_time': 1.2291966113662271e-05, 'wall_clock_time': 57.03280854225159, 'metric_for_logging': {'pred_time': 1.2291966113662271e-05}, 'val_loss': 2.2647691007497315, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.135768413543701}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7345787004029292
NO2(0)的mse=10.56699380218341
NO2(0)的mae=2.3090619960009438
NO2(0)的mar=0.1760734172045736
总共花费的时间为：64.14
揭阳市
1708A
1709A
1710A
3320A
[flaml.automl: 09-17 09:05:23] {2390} INFO - task = regression
[flaml.automl: 09-17 09:05:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:05:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:05:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:05:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:05:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:05:25] {3025} INFO - Estimated sufficient time budget=52144s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 09:05:25] {3072} INFO -  at 1.4s,	estimator xgboost's best error=10.5793,	best estimator xgboost's best error=10.5793
[flaml.automl: 09-17 09:05:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:05:27] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.1736,	best estimator xgboost's best error=5.1736
[flaml.automl: 09-17 09:05:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:05:28] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.1736,	best estimator xgboost's best error=5.1736
[flaml.automl: 09-17 09:05:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:05:34] {3072} INFO -  at 10.6s,	estimator xgboost's best error=5.1736,	best estimator xgboost's best error=5.1736
[flaml.automl: 09-17 09:05:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:05:35] {3072} INFO -  at 11.7s,	estimator xgboost's best error=3.4775,	best estimator xgboost's best error=3.4775
[flaml.automl: 09-17 09:05:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:05:37] {3072} INFO -  at 13.3s,	estimator xgboost's best error=3.0801,	best estimator xgboost's best error=3.0801
[flaml.automl: 09-17 09:05:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:05:38] {3072} INFO -  at 14.9s,	estimator xgboost's best error=3.0801,	best estimator xgboost's best error=3.0801
[flaml.automl: 09-17 09:05:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:05:41] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.0801,	best estimator xgboost's best error=3.0801
[flaml.automl: 09-17 09:05:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:05:42] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.0801,	best estimator xgboost's best error=3.0801
[flaml.automl: 09-17 09:05:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:05:44] {3072} INFO -  at 21.2s,	estimator xgboost's best error=3.0801,	best estimator xgboost's best error=3.0801
[flaml.automl: 09-17 09:05:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:05:46] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.0405,	best estimator xgboost's best error=3.0405
[flaml.automl: 09-17 09:05:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:05:47] {3072} INFO -  at 23.9s,	estimator xgboost's best error=3.0405,	best estimator xgboost's best error=3.0405
[flaml.automl: 09-17 09:05:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:05:54] {3072} INFO -  at 30.5s,	estimator xgboost's best error=2.9636,	best estimator xgboost's best error=2.9636
[flaml.automl: 09-17 09:05:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:06:06] {3072} INFO -  at 42.6s,	estimator xgboost's best error=2.8604,	best estimator xgboost's best error=2.8604
[flaml.automl: 09-17 09:06:06] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 09:06:12] {3072} INFO -  at 49.1s,	estimator xgboost's best error=2.8604,	best estimator xgboost's best error=2.8604
[flaml.automl: 09-17 09:06:24] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 09:06:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:06:24] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:06:24] {2637} INFO - Time taken to find the best model: 42.56236505508423
[flaml.automl: 09-17 09:06:24] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43476}
NO2(0)最佳损失：-1.8604263156572256
NO2(0)最好结果：{'pred_time': 8.398542441682179e-06, 'wall_clock_time': 42.56236505508423, 'metric_for_logging': {'pred_time': 8.398542441682179e-06}, 'val_loss': 2.8604263156572256, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43476}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43476, 'experiment_tag': 'exp', 'time_total_s': 12.089096069335938}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8256190021069596
NO2(0)的mse=17.14174323434307
NO2(0)的mae=2.901006771293198
NO2(0)的mar=0.21533231662513377
总共花费的时间为：61.92
云浮市
1712A
[flaml.automl: 09-17 09:09:45] {2390} INFO - task = regression
[flaml.automl: 09-17 09:09:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:09:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:09:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:09:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:09:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:09:46] {3025} INFO - Estimated sufficient time budget=11961s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 09:09:46] {3072} INFO -  at 1.2s,	estimator xgboost's best error=8.8908,	best estimator xgboost's best error=8.8908
[flaml.automl: 09-17 09:09:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:09:48] {3072} INFO -  at 3.1s,	estimator xgboost's best error=4.9035,	best estimator xgboost's best error=4.9035
[flaml.automl: 09-17 09:09:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:09:49] {3072} INFO -  at 4.3s,	estimator xgboost's best error=4.9035,	best estimator xgboost's best error=4.9035
[flaml.automl: 09-17 09:09:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:09:56] {3072} INFO -  at 11.4s,	estimator xgboost's best error=4.9035,	best estimator xgboost's best error=4.9035
[flaml.automl: 09-17 09:09:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:09:57] {3072} INFO -  at 12.5s,	estimator xgboost's best error=2.8636,	best estimator xgboost's best error=2.8636
[flaml.automl: 09-17 09:09:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:09:59] {3072} INFO -  at 14.1s,	estimator xgboost's best error=2.5672,	best estimator xgboost's best error=2.5672
[flaml.automl: 09-17 09:09:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:10:01] {3072} INFO -  at 15.7s,	estimator xgboost's best error=2.5342,	best estimator xgboost's best error=2.5342
[flaml.automl: 09-17 09:10:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:10:03] {3072} INFO -  at 18.1s,	estimator xgboost's best error=2.5342,	best estimator xgboost's best error=2.5342
[flaml.automl: 09-17 09:10:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:10:05] {3072} INFO -  at 19.7s,	estimator xgboost's best error=2.5179,	best estimator xgboost's best error=2.5179
[flaml.automl: 09-17 09:10:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:10:07] {3072} INFO -  at 22.0s,	estimator xgboost's best error=2.5179,	best estimator xgboost's best error=2.5179
[flaml.automl: 09-17 09:10:07] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:10:09] {3072} INFO -  at 23.7s,	estimator xgboost's best error=2.3962,	best estimator xgboost's best error=2.3962
[flaml.automl: 09-17 09:10:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:10:10] {3072} INFO -  at 24.8s,	estimator xgboost's best error=2.3962,	best estimator xgboost's best error=2.3962
[flaml.automl: 09-17 09:10:10] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:10:15] {3072} INFO -  at 30.2s,	estimator xgboost's best error=2.2724,	best estimator xgboost's best error=2.2724
[flaml.automl: 09-17 09:10:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:10:23] {3072} INFO -  at 38.5s,	estimator xgboost's best error=2.2129,	best estimator xgboost's best error=2.2129
[flaml.automl: 09-17 09:10:23] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 09:10:28] {3072} INFO -  at 43.2s,	estimator xgboost's best error=2.2129,	best estimator xgboost's best error=2.2129
[flaml.automl: 09-17 09:10:28] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 09:10:41] {3072} INFO -  at 56.5s,	estimator xgboost's best error=2.2129,	best estimator xgboost's best error=2.2129
[flaml.automl: 09-17 09:10:49] {3335} INFO - retrain xgboost for 8.2s
[flaml.automl: 09-17 09:10:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 09:10:49] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:10:49] {2637} INFO - Time taken to find the best model: 38.46391034126282
NO2(0)最佳参数：{'n_estimators': 23, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}
NO2(0)最佳损失：-1.2129417733837196
NO2(0)最好结果：{'pred_time': 4.039662758758449e-05, 'wall_clock_time': 38.46391034126282, 'metric_for_logging': {'pred_time': 4.039662758758449e-05}, 'val_loss': 2.2129417733837196, 'training_iteration': 1, 'config': {'n_estimators': 23, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}, 'config/n_estimators': 23, 'config/max_leaves': 6, 'config/min_child_weight': 2.978767810916454, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532448312341252, 'config/reg_alpha': 0.0015779591896894877, 'config/reg_lambda': 1.3500086298278042, 'experiment_tag': 'exp', 'time_total_s': 8.248564958572388}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8337264261713002
NO2(0)的mse=10.247437227831044
NO2(0)的mae=2.2306920462533046
NO2(0)的mar=0.2069937877717954
总共花费的时间为：64.90
玉溪市
2882A
2883A
[flaml.automl: 09-17 09:17:14] {2390} INFO - task = regression
[flaml.automl: 09-17 09:17:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:17:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:17:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:17:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:17:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:17:15] {3025} INFO - Estimated sufficient time budget=12004s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 09:17:15] {3072} INFO -  at 1.3s,	estimator xgboost's best error=12.5368,	best estimator xgboost's best error=12.5368
[flaml.automl: 09-17 09:17:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:17:17] {3072} INFO -  at 3.4s,	estimator xgboost's best error=6.0646,	best estimator xgboost's best error=6.0646
[flaml.automl: 09-17 09:17:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:17:18] {3072} INFO -  at 4.6s,	estimator xgboost's best error=6.0646,	best estimator xgboost's best error=6.0646
[flaml.automl: 09-17 09:17:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:17:28] {3072} INFO -  at 14.1s,	estimator xgboost's best error=6.0646,	best estimator xgboost's best error=6.0646
[flaml.automl: 09-17 09:17:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:17:29] {3072} INFO -  at 15.3s,	estimator xgboost's best error=4.0256,	best estimator xgboost's best error=4.0256
[flaml.automl: 09-17 09:17:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:17:30] {3072} INFO -  at 16.9s,	estimator xgboost's best error=3.6527,	best estimator xgboost's best error=3.6527
[flaml.automl: 09-17 09:17:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:17:32] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.6527,	best estimator xgboost's best error=3.6527
[flaml.automl: 09-17 09:17:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:17:35] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.6527,	best estimator xgboost's best error=3.6527
[flaml.automl: 09-17 09:17:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:17:36] {3072} INFO -  at 22.1s,	estimator xgboost's best error=3.6527,	best estimator xgboost's best error=3.6527
[flaml.automl: 09-17 09:17:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:17:38] {3072} INFO -  at 24.6s,	estimator xgboost's best error=3.6527,	best estimator xgboost's best error=3.6527
[flaml.automl: 09-17 09:17:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:17:39] {3072} INFO -  at 25.8s,	estimator xgboost's best error=3.6527,	best estimator xgboost's best error=3.6527
[flaml.automl: 09-17 09:17:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:17:40] {3072} INFO -  at 26.9s,	estimator xgboost's best error=3.6527,	best estimator xgboost's best error=3.6527
[flaml.automl: 09-17 09:17:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:17:47] {3072} INFO -  at 33.0s,	estimator xgboost's best error=3.5779,	best estimator xgboost's best error=3.5779
[flaml.automl: 09-17 09:17:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:17:57] {3072} INFO -  at 43.5s,	estimator xgboost's best error=3.4256,	best estimator xgboost's best error=3.4256
[flaml.automl: 09-17 09:17:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 09:18:03] {3072} INFO -  at 49.5s,	estimator xgboost's best error=3.4256,	best estimator xgboost's best error=3.4256
[flaml.automl: 09-17 09:18:14] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-17 09:18:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:18:14] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:18:14] {2637} INFO - Time taken to find the best model: 43.47605657577515
[flaml.automl: 09-17 09:18:14] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.4256228108180866
NO2(0)最好结果：{'pred_time': 1.645293813466536e-05, 'wall_clock_time': 43.47605657577515, 'metric_for_logging': {'pred_time': 1.645293813466536e-05}, 'val_loss': 3.4256228108180866, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.480009317398071}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7304653940319136
NO2(0)的mse=25.33950626018414
NO2(0)的mae=3.430245009688034
NO2(0)的mar=0.1924019215136701
总共花费的时间为：60.42
菏泽市
1719A
[flaml.automl: 09-17 09:21:22] {2390} INFO - task = regression
[flaml.automl: 09-17 09:21:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:21:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:21:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:21:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:21:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:21:24] {3025} INFO - Estimated sufficient time budget=21059s. Estimated necessary time budget=21s.
[flaml.automl: 09-17 09:21:24] {3072} INFO -  at 2.2s,	estimator xgboost's best error=14.9494,	best estimator xgboost's best error=14.9494
[flaml.automl: 09-17 09:21:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:21:27] {3072} INFO -  at 5.1s,	estimator xgboost's best error=7.9744,	best estimator xgboost's best error=7.9744
[flaml.automl: 09-17 09:21:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:21:29] {3072} INFO -  at 6.3s,	estimator xgboost's best error=7.9744,	best estimator xgboost's best error=7.9744
[flaml.automl: 09-17 09:21:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:21:36] {3072} INFO -  at 13.4s,	estimator xgboost's best error=7.9744,	best estimator xgboost's best error=7.9744
[flaml.automl: 09-17 09:21:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:21:37] {3072} INFO -  at 14.6s,	estimator xgboost's best error=4.0945,	best estimator xgboost's best error=4.0945
[flaml.automl: 09-17 09:21:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:21:38] {3072} INFO -  at 16.1s,	estimator xgboost's best error=3.6074,	best estimator xgboost's best error=3.6074
[flaml.automl: 09-17 09:21:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:21:40] {3072} INFO -  at 17.7s,	estimator xgboost's best error=3.5562,	best estimator xgboost's best error=3.5562
[flaml.automl: 09-17 09:21:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:21:42] {3072} INFO -  at 20.1s,	estimator xgboost's best error=3.5562,	best estimator xgboost's best error=3.5562
[flaml.automl: 09-17 09:21:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:21:44] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.4394,	best estimator xgboost's best error=3.4394
[flaml.automl: 09-17 09:21:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:21:47] {3072} INFO -  at 24.2s,	estimator xgboost's best error=3.4394,	best estimator xgboost's best error=3.4394
[flaml.automl: 09-17 09:21:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:21:48] {3072} INFO -  at 25.9s,	estimator xgboost's best error=3.4394,	best estimator xgboost's best error=3.4394
[flaml.automl: 09-17 09:21:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:21:49] {3072} INFO -  at 27.0s,	estimator xgboost's best error=3.4394,	best estimator xgboost's best error=3.4394
[flaml.automl: 09-17 09:21:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:21:55] {3072} INFO -  at 32.7s,	estimator xgboost's best error=3.0575,	best estimator xgboost's best error=3.0575
[flaml.automl: 09-17 09:21:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:22:04] {3072} INFO -  at 41.6s,	estimator xgboost's best error=2.9994,	best estimator xgboost's best error=2.9994
[flaml.automl: 09-17 09:22:04] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 09:22:09] {3072} INFO -  at 46.7s,	estimator xgboost's best error=2.9994,	best estimator xgboost's best error=2.9994
[flaml.automl: 09-17 09:22:09] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 09:22:22] {3072} INFO -  at 59.4s,	estimator xgboost's best error=2.9994,	best estimator xgboost's best error=2.9994
[flaml.automl: 09-17 09:22:31] {3335} INFO - retrain xgboost for 8.9s
[flaml.automl: 09-17 09:22:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 09:22:31] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:22:31] {2637} INFO - Time taken to find the best model: 41.61842155456543
NO2(0)最佳参数：{'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}
NO2(0)最佳损失：-1.999421545579557
NO2(0)最好结果：{'pred_time': 3.2833104141566514e-05, 'wall_clock_time': 41.61842155456543, 'metric_for_logging': {'pred_time': 3.2833104141566514e-05}, 'val_loss': 2.999421545579557, 'training_iteration': 1, 'config': {'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}, 'config/n_estimators': 25, 'config/max_leaves': 6, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'experiment_tag': 'exp', 'time_total_s': 8.893322706222534}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8952662448834797
NO2(0)的mse=21.369464243341998
NO2(0)的mae=3.053942212478099
NO2(0)的mar=0.14110253573630324
总共花费的时间为：68.56
大同市
1721A
1725A
1726A
3565A
3566A
3567A
[flaml.automl: 09-17 09:41:00] {2390} INFO - task = regression
[flaml.automl: 09-17 09:41:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:41:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:41:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:41:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:41:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:41:03] {3025} INFO - Estimated sufficient time budget=183046s. Estimated necessary time budget=183s.
[flaml.automl: 09-17 09:41:03] {3072} INFO -  at 3.3s,	estimator xgboost's best error=13.6635,	best estimator xgboost's best error=13.6635
[flaml.automl: 09-17 09:41:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:41:06] {3072} INFO -  at 7.1s,	estimator xgboost's best error=8.6685,	best estimator xgboost's best error=8.6685
[flaml.automl: 09-17 09:41:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:41:09] {3072} INFO -  at 10.0s,	estimator xgboost's best error=8.6685,	best estimator xgboost's best error=8.6685
[flaml.automl: 09-17 09:41:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:41:12] {3072} INFO -  at 13.0s,	estimator xgboost's best error=8.6685,	best estimator xgboost's best error=8.6685
[flaml.automl: 09-17 09:41:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:41:15] {3072} INFO -  at 15.8s,	estimator xgboost's best error=4.9549,	best estimator xgboost's best error=4.9549
[flaml.automl: 09-17 09:41:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:41:18] {3072} INFO -  at 18.9s,	estimator xgboost's best error=4.7176,	best estimator xgboost's best error=4.7176
[flaml.automl: 09-17 09:41:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:41:21] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.7176,	best estimator xgboost's best error=4.7176
[flaml.automl: 09-17 09:41:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:41:23] {3072} INFO -  at 23.4s,	estimator xgboost's best error=4.7176,	best estimator xgboost's best error=4.7176
[flaml.automl: 09-17 09:41:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:41:25] {3072} INFO -  at 25.5s,	estimator xgboost's best error=4.7176,	best estimator xgboost's best error=4.7176
[flaml.automl: 09-17 09:41:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:41:27] {3072} INFO -  at 27.4s,	estimator xgboost's best error=4.7176,	best estimator xgboost's best error=4.7176
[flaml.automl: 09-17 09:41:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:41:31] {3072} INFO -  at 31.4s,	estimator xgboost's best error=4.4091,	best estimator xgboost's best error=4.4091
[flaml.automl: 09-17 09:41:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:41:33] {3072} INFO -  at 34.0s,	estimator xgboost's best error=4.4091,	best estimator xgboost's best error=4.4091
[flaml.automl: 09-17 09:41:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:41:43] {3072} INFO -  at 43.9s,	estimator xgboost's best error=4.2582,	best estimator xgboost's best error=4.2582
[flaml.automl: 09-17 09:41:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:41:55] {3072} INFO -  at 56.0s,	estimator xgboost's best error=4.2148,	best estimator xgboost's best error=4.2148
[flaml.automl: 09-17 09:42:07] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 09:42:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:42:07] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:42:07] {2637} INFO - Time taken to find the best model: 55.96463966369629
[flaml.automl: 09-17 09:42:07] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64909}
NO2(0)最佳损失：-3.2148380354243082
NO2(0)最好结果：{'pred_time': 5.4719923075733215e-06, 'wall_clock_time': 55.96463966369629, 'metric_for_logging': {'pred_time': 5.4719923075733215e-06}, 'val_loss': 4.214838035424308, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64909}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 64909, 'experiment_tag': 'exp', 'time_total_s': 12.07589864730835}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.798416866443552
NO2(0)的mse=41.055334900818714
NO2(0)的mae=4.269938872212747
NO2(0)的mar=0.2610351411305431
总共花费的时间为：69.46
长治市
1728A
1731A
2845A
3568A
3569A
3570A
[flaml.automl: 09-17 10:01:55] {2390} INFO - task = regression
[flaml.automl: 09-17 10:01:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:01:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:01:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:01:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:01:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:01:56] {3025} INFO - Estimated sufficient time budget=77657s. Estimated necessary time budget=78s.
[flaml.automl: 09-17 10:01:56] {3072} INFO -  at 1.5s,	estimator xgboost's best error=15.7810,	best estimator xgboost's best error=15.7810
[flaml.automl: 09-17 10:01:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:01:58] {3072} INFO -  at 3.7s,	estimator xgboost's best error=7.8265,	best estimator xgboost's best error=7.8265
[flaml.automl: 09-17 10:01:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:01:59] {3072} INFO -  at 4.9s,	estimator xgboost's best error=7.8265,	best estimator xgboost's best error=7.8265
[flaml.automl: 09-17 10:01:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:02:03] {3072} INFO -  at 8.6s,	estimator xgboost's best error=7.8265,	best estimator xgboost's best error=7.8265
[flaml.automl: 09-17 10:02:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:02:04] {3072} INFO -  at 9.8s,	estimator xgboost's best error=5.6538,	best estimator xgboost's best error=5.6538
[flaml.automl: 09-17 10:02:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:02:06] {3072} INFO -  at 11.4s,	estimator xgboost's best error=4.9976,	best estimator xgboost's best error=4.9976
[flaml.automl: 09-17 10:02:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:02:08] {3072} INFO -  at 13.0s,	estimator xgboost's best error=4.9976,	best estimator xgboost's best error=4.9976
[flaml.automl: 09-17 10:02:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:02:10] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.9976,	best estimator xgboost's best error=4.9976
[flaml.automl: 09-17 10:02:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:02:11] {3072} INFO -  at 16.6s,	estimator xgboost's best error=4.9976,	best estimator xgboost's best error=4.9976
[flaml.automl: 09-17 10:02:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:02:14] {3072} INFO -  at 19.2s,	estimator xgboost's best error=4.9976,	best estimator xgboost's best error=4.9976
[flaml.automl: 09-17 10:02:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:02:15] {3072} INFO -  at 20.9s,	estimator xgboost's best error=4.9949,	best estimator xgboost's best error=4.9949
[flaml.automl: 09-17 10:02:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:02:17] {3072} INFO -  at 22.1s,	estimator xgboost's best error=4.9949,	best estimator xgboost's best error=4.9949
[flaml.automl: 09-17 10:02:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:02:23] {3072} INFO -  at 28.6s,	estimator xgboost's best error=4.8718,	best estimator xgboost's best error=4.8718
[flaml.automl: 09-17 10:02:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:02:35] {3072} INFO -  at 40.7s,	estimator xgboost's best error=4.7566,	best estimator xgboost's best error=4.7566
[flaml.automl: 09-17 10:02:35] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 10:02:42] {3072} INFO -  at 47.3s,	estimator xgboost's best error=4.7566,	best estimator xgboost's best error=4.7566
[flaml.automl: 09-17 10:02:42] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 10:02:54] {3072} INFO -  at 59.8s,	estimator xgboost's best error=4.7366,	best estimator xgboost's best error=4.7366
[flaml.automl: 09-17 10:03:16] {3335} INFO - retrain xgboost for 21.4s
[flaml.automl: 09-17 10:03:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 10:03:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:03:16] {2637} INFO - Time taken to find the best model: 59.79365611076355
[flaml.automl: 09-17 10:03:16] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 65107}
NO2(0)最佳损失：-3.7365964506606684
NO2(0)最好结果：{'pred_time': 5.4272109257738265e-06, 'wall_clock_time': 59.79365611076355, 'metric_for_logging': {'pred_time': 5.4272109257738265e-06}, 'val_loss': 4.736596450660668, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 65107}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 65107, 'experiment_tag': 'exp', 'time_total_s': 12.505737543106079}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7840657178623234
NO2(0)的mse=50.6259561395627
NO2(0)的mae=4.777016403648977
NO2(0)的mar=0.2608296990239669
总共花费的时间为：82.43
临汾市
3668A
[flaml.automl: 09-17 10:07:19] {2390} INFO - task = regression
[flaml.automl: 09-17 10:07:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:07:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:07:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:07:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:07:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:07:21] {3025} INFO - Estimated sufficient time budget=21822s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 10:07:21] {3072} INFO -  at 2.3s,	estimator xgboost's best error=22.4690,	best estimator xgboost's best error=22.4690
[flaml.automl: 09-17 10:07:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:07:24] {3072} INFO -  at 5.4s,	estimator xgboost's best error=12.1540,	best estimator xgboost's best error=12.1540
[flaml.automl: 09-17 10:07:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:07:26] {3072} INFO -  at 7.4s,	estimator xgboost's best error=12.1540,	best estimator xgboost's best error=12.1540
[flaml.automl: 09-17 10:07:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:07:38] {3072} INFO -  at 19.5s,	estimator xgboost's best error=12.1540,	best estimator xgboost's best error=12.1540
[flaml.automl: 09-17 10:07:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:07:40] {3072} INFO -  at 21.5s,	estimator xgboost's best error=6.6895,	best estimator xgboost's best error=6.6895
[flaml.automl: 09-17 10:07:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:07:44] {3072} INFO -  at 25.1s,	estimator xgboost's best error=5.7952,	best estimator xgboost's best error=5.7952
[flaml.automl: 09-17 10:07:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:07:48] {3072} INFO -  at 29.8s,	estimator xgboost's best error=5.7952,	best estimator xgboost's best error=5.7952
[flaml.automl: 09-17 10:07:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:07:55] {3072} INFO -  at 36.0s,	estimator xgboost's best error=5.7952,	best estimator xgboost's best error=5.7952
[flaml.automl: 09-17 10:07:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:07:58] {3072} INFO -  at 39.4s,	estimator xgboost's best error=5.7952,	best estimator xgboost's best error=5.7952
[flaml.automl: 09-17 10:07:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:08:05] {3072} INFO -  at 46.3s,	estimator xgboost's best error=5.7952,	best estimator xgboost's best error=5.7952
[flaml.automl: 09-17 10:08:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:08:08] {3072} INFO -  at 49.5s,	estimator xgboost's best error=5.7952,	best estimator xgboost's best error=5.7952
[flaml.automl: 09-17 10:08:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:08:11] {3072} INFO -  at 52.6s,	estimator xgboost's best error=5.7952,	best estimator xgboost's best error=5.7952
[flaml.automl: 09-17 10:08:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:08:17] {3072} INFO -  at 58.2s,	estimator xgboost's best error=5.7952,	best estimator xgboost's best error=5.7952
[flaml.automl: 09-17 10:08:19] {3335} INFO - retrain xgboost for 2.8s
[flaml.automl: 09-17 10:08:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 10:08:19] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:08:19] {2637} INFO - Time taken to find the best model: 25.129616260528564
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-4.795205637624627
NO2(0)最好结果：{'pred_time': 0.00011324541622896571, 'wall_clock_time': 25.129616260528564, 'metric_for_logging': {'pred_time': 0.00011324541622896571}, 'val_loss': 5.795205637624627, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 3.64005446434021}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.6074594501335295
NO2(0)的mse=88.33498198999189
NO2(0)的mae=6.708592218490011
NO2(0)的mar=0.28182626467010513
总共花费的时间为：61.30
阳泉市
1738A
1739A
1743A
3619A
[flaml.automl: 09-17 10:20:47] {2390} INFO - task = regression
[flaml.automl: 09-17 10:20:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:20:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:20:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:20:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:20:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:20:49] {3025} INFO - Estimated sufficient time budget=96292s. Estimated necessary time budget=96s.
[flaml.automl: 09-17 10:20:49] {3072} INFO -  at 2.5s,	estimator xgboost's best error=22.3064,	best estimator xgboost's best error=22.3064
[flaml.automl: 09-17 10:20:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:20:53] {3072} INFO -  at 6.4s,	estimator xgboost's best error=10.4973,	best estimator xgboost's best error=10.4973
[flaml.automl: 09-17 10:20:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:20:55] {3072} INFO -  at 8.6s,	estimator xgboost's best error=10.4973,	best estimator xgboost's best error=10.4973
[flaml.automl: 09-17 10:20:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:21:00] {3072} INFO -  at 13.5s,	estimator xgboost's best error=10.4973,	best estimator xgboost's best error=10.4973
[flaml.automl: 09-17 10:21:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:21:01] {3072} INFO -  at 14.6s,	estimator xgboost's best error=6.4589,	best estimator xgboost's best error=6.4589
[flaml.automl: 09-17 10:21:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:21:03] {3072} INFO -  at 16.2s,	estimator xgboost's best error=5.4553,	best estimator xgboost's best error=5.4553
[flaml.automl: 09-17 10:21:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:21:05] {3072} INFO -  at 17.8s,	estimator xgboost's best error=5.4553,	best estimator xgboost's best error=5.4553
[flaml.automl: 09-17 10:21:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:21:07] {3072} INFO -  at 20.3s,	estimator xgboost's best error=5.4553,	best estimator xgboost's best error=5.4553
[flaml.automl: 09-17 10:21:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:21:08] {3072} INFO -  at 21.4s,	estimator xgboost's best error=5.4553,	best estimator xgboost's best error=5.4553
[flaml.automl: 09-17 10:21:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:21:11] {3072} INFO -  at 24.1s,	estimator xgboost's best error=5.4553,	best estimator xgboost's best error=5.4553
[flaml.automl: 09-17 10:21:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:21:13] {3072} INFO -  at 25.7s,	estimator xgboost's best error=5.4540,	best estimator xgboost's best error=5.4540
[flaml.automl: 09-17 10:21:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:21:14] {3072} INFO -  at 26.9s,	estimator xgboost's best error=5.4540,	best estimator xgboost's best error=5.4540
[flaml.automl: 09-17 10:21:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:21:20] {3072} INFO -  at 33.4s,	estimator xgboost's best error=5.1699,	best estimator xgboost's best error=5.1699
[flaml.automl: 09-17 10:21:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:21:32] {3072} INFO -  at 45.5s,	estimator xgboost's best error=4.9814,	best estimator xgboost's best error=4.9814
[flaml.automl: 09-17 10:21:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 10:21:39] {3072} INFO -  at 52.0s,	estimator xgboost's best error=4.9814,	best estimator xgboost's best error=4.9814
[flaml.automl: 09-17 10:21:51] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 10:21:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 10:21:51] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:21:51] {2637} INFO - Time taken to find the best model: 45.470237255096436
[flaml.automl: 09-17 10:21:51] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43811}
NO2(0)最佳损失：-3.981430074966825
NO2(0)最好结果：{'pred_time': 8.08061687497693e-06, 'wall_clock_time': 45.470237255096436, 'metric_for_logging': {'pred_time': 8.08061687497693e-06}, 'val_loss': 4.981430074966825, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43811}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43811, 'experiment_tag': 'exp', 'time_total_s': 12.085630893707275}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8393534107959044
NO2(0)的mse=62.017274983513545
NO2(0)的mae=5.173571281411948
NO2(0)的mar=0.17357110566978742
总共花费的时间为：64.87
赤峰市
1744A
1745A
3286A
[flaml.automl: 09-17 10:31:25] {2390} INFO - task = regression
[flaml.automl: 09-17 10:31:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:31:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:31:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:31:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:31:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:31:27] {3025} INFO - Estimated sufficient time budget=21237s. Estimated necessary time budget=21s.
[flaml.automl: 09-17 10:31:27] {3072} INFO -  at 2.3s,	estimator xgboost's best error=14.7501,	best estimator xgboost's best error=14.7501
[flaml.automl: 09-17 10:31:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:31:31] {3072} INFO -  at 6.2s,	estimator xgboost's best error=7.2808,	best estimator xgboost's best error=7.2808
[flaml.automl: 09-17 10:31:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:31:33] {3072} INFO -  at 8.5s,	estimator xgboost's best error=7.2808,	best estimator xgboost's best error=7.2808
[flaml.automl: 09-17 10:31:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:31:51] {3072} INFO -  at 26.0s,	estimator xgboost's best error=7.2808,	best estimator xgboost's best error=7.2808
[flaml.automl: 09-17 10:31:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:31:53] {3072} INFO -  at 28.2s,	estimator xgboost's best error=5.3114,	best estimator xgboost's best error=5.3114
[flaml.automl: 09-17 10:31:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:31:56] {3072} INFO -  at 31.3s,	estimator xgboost's best error=4.8365,	best estimator xgboost's best error=4.8365
[flaml.automl: 09-17 10:31:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:31:59] {3072} INFO -  at 34.0s,	estimator xgboost's best error=4.8365,	best estimator xgboost's best error=4.8365
[flaml.automl: 09-17 10:31:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:32:03] {3072} INFO -  at 38.1s,	estimator xgboost's best error=4.8365,	best estimator xgboost's best error=4.8365
[flaml.automl: 09-17 10:32:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:32:04] {3072} INFO -  at 39.6s,	estimator xgboost's best error=4.8365,	best estimator xgboost's best error=4.8365
[flaml.automl: 09-17 10:32:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:32:07] {3072} INFO -  at 42.2s,	estimator xgboost's best error=4.8365,	best estimator xgboost's best error=4.8365
[flaml.automl: 09-17 10:32:07] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:32:08] {3072} INFO -  at 43.4s,	estimator xgboost's best error=4.8365,	best estimator xgboost's best error=4.8365
[flaml.automl: 09-17 10:32:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:32:09] {3072} INFO -  at 44.5s,	estimator xgboost's best error=4.8365,	best estimator xgboost's best error=4.8365
[flaml.automl: 09-17 10:32:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:32:16] {3072} INFO -  at 50.9s,	estimator xgboost's best error=4.8116,	best estimator xgboost's best error=4.8116
[flaml.automl: 09-17 10:32:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:32:25] {3072} INFO -  at 59.8s,	estimator xgboost's best error=4.7308,	best estimator xgboost's best error=4.7308
[flaml.automl: 09-17 10:32:37] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-17 10:32:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 10:32:37] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:32:37] {2637} INFO - Time taken to find the best model: 59.77857255935669
[flaml.automl: 09-17 10:32:37] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.7308236876748015
NO2(0)最好结果：{'pred_time': 1.0868984835223399e-05, 'wall_clock_time': 59.77857255935669, 'metric_for_logging': {'pred_time': 1.0868984835223399e-05}, 'val_loss': 4.7308236876748015, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 8.858870029449463}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6901407155483521
NO2(0)的mse=44.75833625288642
NO2(0)的mae=4.687233246165911
NO2(0)的mar=0.2495057364927917
总共花费的时间为：72.29
鞍山市
1749A
1750A
1751A
1752A
1753A
1754A
[flaml.automl: 09-17 10:52:14] {2390} INFO - task = regression
[flaml.automl: 09-17 10:52:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:52:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:52:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:52:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:52:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:52:16] {3025} INFO - Estimated sufficient time budget=76101s. Estimated necessary time budget=76s.
[flaml.automl: 09-17 10:52:16] {3072} INFO -  at 1.5s,	estimator xgboost's best error=15.8879,	best estimator xgboost's best error=15.8879
[flaml.automl: 09-17 10:52:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:52:18] {3072} INFO -  at 3.6s,	estimator xgboost's best error=8.0713,	best estimator xgboost's best error=8.0713
[flaml.automl: 09-17 10:52:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:52:19] {3072} INFO -  at 4.8s,	estimator xgboost's best error=8.0713,	best estimator xgboost's best error=8.0713
[flaml.automl: 09-17 10:52:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:52:23] {3072} INFO -  at 9.1s,	estimator xgboost's best error=8.0713,	best estimator xgboost's best error=8.0713
[flaml.automl: 09-17 10:52:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:52:24] {3072} INFO -  at 10.2s,	estimator xgboost's best error=5.7820,	best estimator xgboost's best error=5.7820
[flaml.automl: 09-17 10:52:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:52:26] {3072} INFO -  at 11.7s,	estimator xgboost's best error=5.2169,	best estimator xgboost's best error=5.2169
[flaml.automl: 09-17 10:52:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:52:28] {3072} INFO -  at 13.3s,	estimator xgboost's best error=5.2169,	best estimator xgboost's best error=5.2169
[flaml.automl: 09-17 10:52:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:52:30] {3072} INFO -  at 15.8s,	estimator xgboost's best error=5.2169,	best estimator xgboost's best error=5.2169
[flaml.automl: 09-17 10:52:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:52:31] {3072} INFO -  at 16.9s,	estimator xgboost's best error=5.2169,	best estimator xgboost's best error=5.2169
[flaml.automl: 09-17 10:52:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:52:34] {3072} INFO -  at 19.6s,	estimator xgboost's best error=5.2169,	best estimator xgboost's best error=5.2169
[flaml.automl: 09-17 10:52:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:52:35] {3072} INFO -  at 21.2s,	estimator xgboost's best error=5.2169,	best estimator xgboost's best error=5.2169
[flaml.automl: 09-17 10:52:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:52:37] {3072} INFO -  at 22.4s,	estimator xgboost's best error=5.2169,	best estimator xgboost's best error=5.2169
[flaml.automl: 09-17 10:52:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:52:43] {3072} INFO -  at 29.0s,	estimator xgboost's best error=5.0202,	best estimator xgboost's best error=5.0202
[flaml.automl: 09-17 10:52:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:52:55] {3072} INFO -  at 41.1s,	estimator xgboost's best error=4.8524,	best estimator xgboost's best error=4.8524
[flaml.automl: 09-17 10:52:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 10:53:02] {3072} INFO -  at 47.6s,	estimator xgboost's best error=4.8524,	best estimator xgboost's best error=4.8524
[flaml.automl: 09-17 10:53:02] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 10:53:13] {3072} INFO -  at 59.0s,	estimator xgboost's best error=4.8524,	best estimator xgboost's best error=4.8524
[flaml.automl: 09-17 10:53:25] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 10:53:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 10:53:25] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:53:25] {2637} INFO - Time taken to find the best model: 41.087509632110596
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 63652}
NO2(0)最佳损失：-3.8523713538051076
NO2(0)最好结果：{'pred_time': 5.83048539638317e-06, 'wall_clock_time': 41.087509632110596, 'metric_for_logging': {'pred_time': 5.83048539638317e-06}, 'val_loss': 4.852371353805108, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 63652}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 63652, 'experiment_tag': 'exp', 'time_total_s': 12.112470626831055}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7722989353595922
NO2(0)的mse=50.279894400637225
NO2(0)的mae=4.866551562398577
NO2(0)的mar=0.24742607066843628
总共花费的时间为：72.21
抚顺市
1755A
1756A
1757A
1758A
1760A
[flaml.automl: 09-17 11:09:53] {2390} INFO - task = regression
[flaml.automl: 09-17 11:09:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:09:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:09:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:09:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:09:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:09:54] {3025} INFO - Estimated sufficient time budget=63706s. Estimated necessary time budget=64s.
[flaml.automl: 09-17 11:09:54] {3072} INFO -  at 1.5s,	estimator xgboost's best error=16.7913,	best estimator xgboost's best error=16.7913
[flaml.automl: 09-17 11:09:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:09:56] {3072} INFO -  at 3.6s,	estimator xgboost's best error=8.2544,	best estimator xgboost's best error=8.2544
[flaml.automl: 09-17 11:09:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:09:57] {3072} INFO -  at 4.8s,	estimator xgboost's best error=8.2544,	best estimator xgboost's best error=8.2544
[flaml.automl: 09-17 11:09:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:10:02] {3072} INFO -  at 9.6s,	estimator xgboost's best error=8.2544,	best estimator xgboost's best error=8.2544
[flaml.automl: 09-17 11:10:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:10:03] {3072} INFO -  at 10.7s,	estimator xgboost's best error=5.5040,	best estimator xgboost's best error=5.5040
[flaml.automl: 09-17 11:10:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:10:05] {3072} INFO -  at 12.3s,	estimator xgboost's best error=4.8347,	best estimator xgboost's best error=4.8347
[flaml.automl: 09-17 11:10:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:10:07] {3072} INFO -  at 13.9s,	estimator xgboost's best error=4.8347,	best estimator xgboost's best error=4.8347
[flaml.automl: 09-17 11:10:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:10:09] {3072} INFO -  at 16.4s,	estimator xgboost's best error=4.8347,	best estimator xgboost's best error=4.8347
[flaml.automl: 09-17 11:10:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:10:10] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.8347,	best estimator xgboost's best error=4.8347
[flaml.automl: 09-17 11:10:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:10:13] {3072} INFO -  at 20.2s,	estimator xgboost's best error=4.8347,	best estimator xgboost's best error=4.8347
[flaml.automl: 09-17 11:10:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:10:14] {3072} INFO -  at 21.8s,	estimator xgboost's best error=4.7705,	best estimator xgboost's best error=4.7705
[flaml.automl: 09-17 11:10:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:10:16] {3072} INFO -  at 23.0s,	estimator xgboost's best error=4.7705,	best estimator xgboost's best error=4.7705
[flaml.automl: 09-17 11:10:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:10:22] {3072} INFO -  at 29.5s,	estimator xgboost's best error=4.5645,	best estimator xgboost's best error=4.5645
[flaml.automl: 09-17 11:10:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:10:34] {3072} INFO -  at 41.6s,	estimator xgboost's best error=4.4654,	best estimator xgboost's best error=4.4654
[flaml.automl: 09-17 11:10:34] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 11:10:41] {3072} INFO -  at 48.1s,	estimator xgboost's best error=4.4654,	best estimator xgboost's best error=4.4654
[flaml.automl: 09-17 11:10:53] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 11:10:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:10:53] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:10:53] {2637} INFO - Time taken to find the best model: 41.607879877090454
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53137}
NO2(0)最佳损失：-3.4653993935629437
NO2(0)最好结果：{'pred_time': 7.231330387073488e-06, 'wall_clock_time': 41.607879877090454, 'metric_for_logging': {'pred_time': 7.231330387073488e-06}, 'val_loss': 4.465399393562944, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53137}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53137, 'experiment_tag': 'exp', 'time_total_s': 12.063126564025879}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7989711217675988
NO2(0)的mse=43.554012240124855
NO2(0)的mae=4.530269057676025
NO2(0)的mar=0.2171790379503503
总共花费的时间为：61.15
本溪市
1761A
1762A
1763A
1764A
1765A
[flaml.automl: 09-17 11:26:25] {2390} INFO - task = regression
[flaml.automl: 09-17 11:26:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:26:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:26:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:26:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:26:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:26:27] {3025} INFO - Estimated sufficient time budget=65873s. Estimated necessary time budget=66s.
[flaml.automl: 09-17 11:26:27] {3072} INFO -  at 1.5s,	estimator xgboost's best error=17.6761,	best estimator xgboost's best error=17.6761
[flaml.automl: 09-17 11:26:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:26:29] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.0441,	best estimator xgboost's best error=9.0441
[flaml.automl: 09-17 11:26:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:26:30] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.0441,	best estimator xgboost's best error=9.0441
[flaml.automl: 09-17 11:26:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:26:35] {3072} INFO -  at 9.5s,	estimator xgboost's best error=9.0441,	best estimator xgboost's best error=9.0441
[flaml.automl: 09-17 11:26:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:26:36] {3072} INFO -  at 10.6s,	estimator xgboost's best error=6.2874,	best estimator xgboost's best error=6.2874
[flaml.automl: 09-17 11:26:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:26:37] {3072} INFO -  at 12.1s,	estimator xgboost's best error=5.4760,	best estimator xgboost's best error=5.4760
[flaml.automl: 09-17 11:26:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:26:39] {3072} INFO -  at 13.7s,	estimator xgboost's best error=5.4760,	best estimator xgboost's best error=5.4760
[flaml.automl: 09-17 11:26:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:26:41] {3072} INFO -  at 16.2s,	estimator xgboost's best error=5.4760,	best estimator xgboost's best error=5.4760
[flaml.automl: 09-17 11:26:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:26:42] {3072} INFO -  at 17.3s,	estimator xgboost's best error=5.4760,	best estimator xgboost's best error=5.4760
[flaml.automl: 09-17 11:26:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:26:45] {3072} INFO -  at 19.9s,	estimator xgboost's best error=5.4760,	best estimator xgboost's best error=5.4760
[flaml.automl: 09-17 11:26:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:26:47] {3072} INFO -  at 21.5s,	estimator xgboost's best error=5.4760,	best estimator xgboost's best error=5.4760
[flaml.automl: 09-17 11:26:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:26:48] {3072} INFO -  at 22.7s,	estimator xgboost's best error=5.4760,	best estimator xgboost's best error=5.4760
[flaml.automl: 09-17 11:26:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:26:54] {3072} INFO -  at 29.2s,	estimator xgboost's best error=5.2026,	best estimator xgboost's best error=5.2026
[flaml.automl: 09-17 11:26:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:27:06] {3072} INFO -  at 41.3s,	estimator xgboost's best error=5.1104,	best estimator xgboost's best error=5.1104
[flaml.automl: 09-17 11:27:06] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 11:27:13] {3072} INFO -  at 47.9s,	estimator xgboost's best error=5.1104,	best estimator xgboost's best error=5.1104
[flaml.automl: 09-17 11:27:13] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 11:27:24] {3072} INFO -  at 59.3s,	estimator xgboost's best error=5.1104,	best estimator xgboost's best error=5.1104
[flaml.automl: 09-17 11:27:36] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 11:27:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:27:36] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:27:36] {2637} INFO - Time taken to find the best model: 41.300798177719116
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 55428}
NO2(0)最佳损失：-4.110423644345038
NO2(0)最好结果：{'pred_time': 7.4790425648219325e-06, 'wall_clock_time': 41.300798177719116, 'metric_for_logging': {'pred_time': 7.4790425648219325e-06}, 'val_loss': 5.110423644345038, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 55428}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 55428, 'experiment_tag': 'exp', 'time_total_s': 12.119426250457764}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7800450830118171
NO2(0)的mse=55.08888601439071
NO2(0)的mae=5.025322997521267
NO2(0)的mar=0.2493019450714468
总共花费的时间为：72.25
锦州市
1767A
1768A
1770A
1771A
[flaml.automl: 09-17 11:41:09] {2390} INFO - task = regression
[flaml.automl: 09-17 11:41:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:41:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:41:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:41:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:41:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:41:10] {3025} INFO - Estimated sufficient time budget=52238s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 11:41:10] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.1459,	best estimator xgboost's best error=17.1459
[flaml.automl: 09-17 11:41:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:41:13] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.7623,	best estimator xgboost's best error=8.7623
[flaml.automl: 09-17 11:41:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:41:15] {3072} INFO -  at 6.6s,	estimator xgboost's best error=8.7623,	best estimator xgboost's best error=8.7623
[flaml.automl: 09-17 11:41:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:41:21] {3072} INFO -  at 12.1s,	estimator xgboost's best error=8.7623,	best estimator xgboost's best error=8.7623
[flaml.automl: 09-17 11:41:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:41:23] {3072} INFO -  at 14.2s,	estimator xgboost's best error=6.2700,	best estimator xgboost's best error=6.2700
[flaml.automl: 09-17 11:41:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:41:26] {3072} INFO -  at 17.0s,	estimator xgboost's best error=5.4280,	best estimator xgboost's best error=5.4280
[flaml.automl: 09-17 11:41:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:41:28] {3072} INFO -  at 19.4s,	estimator xgboost's best error=5.4280,	best estimator xgboost's best error=5.4280
[flaml.automl: 09-17 11:41:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:41:32] {3072} INFO -  at 23.7s,	estimator xgboost's best error=5.4280,	best estimator xgboost's best error=5.4280
[flaml.automl: 09-17 11:41:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:41:35] {3072} INFO -  at 25.9s,	estimator xgboost's best error=5.4280,	best estimator xgboost's best error=5.4280
[flaml.automl: 09-17 11:41:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:41:38] {3072} INFO -  at 29.1s,	estimator xgboost's best error=5.4280,	best estimator xgboost's best error=5.4280
[flaml.automl: 09-17 11:41:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:41:40] {3072} INFO -  at 31.2s,	estimator xgboost's best error=5.3934,	best estimator xgboost's best error=5.3934
[flaml.automl: 09-17 11:41:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:41:41] {3072} INFO -  at 32.9s,	estimator xgboost's best error=5.3934,	best estimator xgboost's best error=5.3934
[flaml.automl: 09-17 11:41:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:41:50] {3072} INFO -  at 41.8s,	estimator xgboost's best error=5.2302,	best estimator xgboost's best error=5.2302
[flaml.automl: 09-17 11:41:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:42:08] {3072} INFO -  at 59.0s,	estimator xgboost's best error=5.0207,	best estimator xgboost's best error=5.0207
[flaml.automl: 09-17 11:42:24] {3335} INFO - retrain xgboost for 16.3s
[flaml.automl: 09-17 11:42:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:42:24] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:42:24] {2637} INFO - Time taken to find the best model: 59.008193016052246
[flaml.automl: 09-17 11:42:24] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43780}
NO2(0)最佳损失：-4.020679051756002
NO2(0)最好结果：{'pred_time': 1.481165008701744e-05, 'wall_clock_time': 59.008193016052246, 'metric_for_logging': {'pred_time': 1.481165008701744e-05}, 'val_loss': 5.020679051756002, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43780}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43780, 'experiment_tag': 'exp', 'time_total_s': 17.255675554275513}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8205168438113593
NO2(0)的mse=56.67643563674857
NO2(0)的mae=4.975554848462619
NO2(0)的mar=0.2603923565761103
总共花费的时间为：76.07
吉林市
1772A
1774A
1775A
1776A
2868A
[flaml.automl: 09-17 11:58:15] {2390} INFO - task = regression
[flaml.automl: 09-17 11:58:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:58:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:58:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:58:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:58:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:58:17] {3025} INFO - Estimated sufficient time budget=120551s. Estimated necessary time budget=121s.
[flaml.automl: 09-17 11:58:17] {3072} INFO -  at 2.7s,	estimator xgboost's best error=13.0130,	best estimator xgboost's best error=13.0130
[flaml.automl: 09-17 11:58:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:58:21] {3072} INFO -  at 6.7s,	estimator xgboost's best error=6.3731,	best estimator xgboost's best error=6.3731
[flaml.automl: 09-17 11:58:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:58:23] {3072} INFO -  at 8.9s,	estimator xgboost's best error=6.3731,	best estimator xgboost's best error=6.3731
[flaml.automl: 09-17 11:58:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:58:27] {3072} INFO -  at 12.9s,	estimator xgboost's best error=6.3731,	best estimator xgboost's best error=6.3731
[flaml.automl: 09-17 11:58:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:58:29] {3072} INFO -  at 15.0s,	estimator xgboost's best error=4.4360,	best estimator xgboost's best error=4.4360
[flaml.automl: 09-17 11:58:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:58:32] {3072} INFO -  at 18.0s,	estimator xgboost's best error=3.9691,	best estimator xgboost's best error=3.9691
[flaml.automl: 09-17 11:58:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:58:35] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.9691,	best estimator xgboost's best error=3.9691
[flaml.automl: 09-17 11:58:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:58:38] {3072} INFO -  at 23.4s,	estimator xgboost's best error=3.9691,	best estimator xgboost's best error=3.9691
[flaml.automl: 09-17 11:58:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:58:39] {3072} INFO -  at 24.6s,	estimator xgboost's best error=3.9691,	best estimator xgboost's best error=3.9691
[flaml.automl: 09-17 11:58:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:58:41] {3072} INFO -  at 27.2s,	estimator xgboost's best error=3.9691,	best estimator xgboost's best error=3.9691
[flaml.automl: 09-17 11:58:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:58:43] {3072} INFO -  at 28.8s,	estimator xgboost's best error=3.9509,	best estimator xgboost's best error=3.9509
[flaml.automl: 09-17 11:58:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:58:44] {3072} INFO -  at 30.0s,	estimator xgboost's best error=3.9509,	best estimator xgboost's best error=3.9509
[flaml.automl: 09-17 11:58:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:58:51] {3072} INFO -  at 36.5s,	estimator xgboost's best error=3.7981,	best estimator xgboost's best error=3.7981
[flaml.automl: 09-17 11:58:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:59:08] {3072} INFO -  at 53.4s,	estimator xgboost's best error=3.7325,	best estimator xgboost's best error=3.7325
[flaml.automl: 09-17 11:59:41] {3335} INFO - retrain xgboost for 33.3s
[flaml.automl: 09-17 11:59:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:59:41] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:59:41] {2637} INFO - Time taken to find the best model: 53.38357424736023
[flaml.automl: 09-17 11:59:41] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53589}
NO2(0)最佳损失：-2.7325002414053174
NO2(0)最好结果：{'pred_time': 1.5480192041917573e-05, 'wall_clock_time': 53.38357424736023, 'metric_for_logging': {'pred_time': 1.5480192041917573e-05}, 'val_loss': 3.7325002414053174, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53589}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53589, 'experiment_tag': 'exp', 'time_total_s': 16.85624647140503}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7656885137811555
NO2(0)的mse=32.77625588442445
NO2(0)的mae=3.807786062726878
NO2(0)的mar=0.20924524958726454
总共花费的时间为：87.78
齐齐哈尔市
1779A
1781A
3662A
[flaml.automl: 09-17 12:08:43] {2390} INFO - task = regression
[flaml.automl: 09-17 12:08:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:08:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:08:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:08:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:08:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:08:44] {3025} INFO - Estimated sufficient time budget=12269s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 12:08:44] {3072} INFO -  at 1.4s,	estimator xgboost's best error=9.6418,	best estimator xgboost's best error=9.6418
[flaml.automl: 09-17 12:08:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:08:46] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.0992,	best estimator xgboost's best error=5.0992
[flaml.automl: 09-17 12:08:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:08:48] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.0992,	best estimator xgboost's best error=5.0992
[flaml.automl: 09-17 12:08:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:08:58] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.0992,	best estimator xgboost's best error=5.0992
[flaml.automl: 09-17 12:08:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:08:59] {3072} INFO -  at 15.9s,	estimator xgboost's best error=4.0001,	best estimator xgboost's best error=4.0001
[flaml.automl: 09-17 12:08:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:09:00] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.7308,	best estimator xgboost's best error=3.7308
[flaml.automl: 09-17 12:09:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:09:02] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.7308,	best estimator xgboost's best error=3.7308
[flaml.automl: 09-17 12:09:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:09:04] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.7308,	best estimator xgboost's best error=3.7308
[flaml.automl: 09-17 12:09:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:09:06] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.7308,	best estimator xgboost's best error=3.7308
[flaml.automl: 09-17 12:09:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:09:08] {3072} INFO -  at 25.4s,	estimator xgboost's best error=3.7308,	best estimator xgboost's best error=3.7308
[flaml.automl: 09-17 12:09:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:09:09] {3072} INFO -  at 26.6s,	estimator xgboost's best error=3.7308,	best estimator xgboost's best error=3.7308
[flaml.automl: 09-17 12:09:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:09:11] {3072} INFO -  at 27.8s,	estimator xgboost's best error=3.7308,	best estimator xgboost's best error=3.7308
[flaml.automl: 09-17 12:09:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:09:17] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.6317,	best estimator xgboost's best error=3.6317
[flaml.automl: 09-17 12:09:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:09:33] {3072} INFO -  at 49.9s,	estimator xgboost's best error=3.5968,	best estimator xgboost's best error=3.5968
[flaml.automl: 09-17 12:09:52] {3335} INFO - retrain xgboost for 19.6s
[flaml.automl: 09-17 12:09:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:09:52] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:09:52] {2637} INFO - Time taken to find the best model: 49.94146800041199
[flaml.automl: 09-17 12:09:52] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.5968494897322465
NO2(0)最好结果：{'pred_time': 2.272691832077878e-05, 'wall_clock_time': 49.94146800041199, 'metric_for_logging': {'pred_time': 2.272691832077878e-05}, 'val_loss': 3.5968494897322465, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 15.74701189994812}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.714016713504804
NO2(0)的mse=26.18288635276669
NO2(0)的mae=3.50593467253559
NO2(0)的mar=0.28537168861003986
总共花费的时间为：70.19
牡丹江市
1784A
1785A
1786A
1787A
[flaml.automl: 09-17 12:22:13] {2390} INFO - task = regression
[flaml.automl: 09-17 12:22:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:22:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:22:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:22:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:22:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:22:15] {3025} INFO - Estimated sufficient time budget=12367s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 12:22:15] {3072} INFO -  at 1.5s,	estimator xgboost's best error=12.7415,	best estimator xgboost's best error=12.7415
[flaml.automl: 09-17 12:22:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:22:17] {3072} INFO -  at 3.6s,	estimator xgboost's best error=6.4362,	best estimator xgboost's best error=6.4362
[flaml.automl: 09-17 12:22:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:22:18] {3072} INFO -  at 4.9s,	estimator xgboost's best error=6.4362,	best estimator xgboost's best error=6.4362
[flaml.automl: 09-17 12:22:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:22:28] {3072} INFO -  at 14.9s,	estimator xgboost's best error=6.4362,	best estimator xgboost's best error=6.4362
[flaml.automl: 09-17 12:22:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:22:29] {3072} INFO -  at 16.0s,	estimator xgboost's best error=4.7039,	best estimator xgboost's best error=4.7039
[flaml.automl: 09-17 12:22:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:22:31] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.2055,	best estimator xgboost's best error=4.2055
[flaml.automl: 09-17 12:22:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:22:32] {3072} INFO -  at 19.3s,	estimator xgboost's best error=4.2055,	best estimator xgboost's best error=4.2055
[flaml.automl: 09-17 12:22:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:22:35] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.2055,	best estimator xgboost's best error=4.2055
[flaml.automl: 09-17 12:22:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:22:36] {3072} INFO -  at 22.9s,	estimator xgboost's best error=4.2055,	best estimator xgboost's best error=4.2055
[flaml.automl: 09-17 12:22:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:22:39] {3072} INFO -  at 25.6s,	estimator xgboost's best error=4.2055,	best estimator xgboost's best error=4.2055
[flaml.automl: 09-17 12:22:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:22:40] {3072} INFO -  at 26.7s,	estimator xgboost's best error=4.2055,	best estimator xgboost's best error=4.2055
[flaml.automl: 09-17 12:22:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:22:41] {3072} INFO -  at 27.9s,	estimator xgboost's best error=4.2055,	best estimator xgboost's best error=4.2055
[flaml.automl: 09-17 12:22:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:22:47] {3072} INFO -  at 34.4s,	estimator xgboost's best error=4.0224,	best estimator xgboost's best error=4.0224
[flaml.automl: 09-17 12:22:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:22:59] {3072} INFO -  at 46.3s,	estimator xgboost's best error=3.8812,	best estimator xgboost's best error=3.8812
[flaml.automl: 09-17 12:22:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 12:23:06] {3072} INFO -  at 52.7s,	estimator xgboost's best error=3.8812,	best estimator xgboost's best error=3.8812
[flaml.automl: 09-17 12:23:18] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 12:23:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:23:18] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:23:18] {2637} INFO - Time taken to find the best model: 46.31030225753784
[flaml.automl: 09-17 12:23:18] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.881232069651141
NO2(0)最好结果：{'pred_time': 8.854638859001249e-06, 'wall_clock_time': 46.31030225753784, 'metric_for_logging': {'pred_time': 8.854638859001249e-06}, 'val_loss': 3.881232069651141, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.90944242477417}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7917879996902824
NO2(0)的mse=34.77483764661798
NO2(0)的mae=3.9714629061772273
NO2(0)的mar=0.24696614796217536
总共花费的时间为：65.47
大庆市
1789A
1790A
1792A
1793A
3481A
[flaml.automl: 09-17 12:38:31] {2390} INFO - task = regression
[flaml.automl: 09-17 12:38:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:38:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:38:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:38:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:38:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:38:34] {3025} INFO - Estimated sufficient time budget=146426s. Estimated necessary time budget=146s.
[flaml.automl: 09-17 12:38:34] {3072} INFO -  at 3.2s,	estimator xgboost's best error=10.1684,	best estimator xgboost's best error=10.1684
[flaml.automl: 09-17 12:38:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:38:39] {3072} INFO -  at 7.9s,	estimator xgboost's best error=5.2061,	best estimator xgboost's best error=5.2061
[flaml.automl: 09-17 12:38:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:38:41] {3072} INFO -  at 10.4s,	estimator xgboost's best error=5.2061,	best estimator xgboost's best error=5.2061
[flaml.automl: 09-17 12:38:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:38:45] {3072} INFO -  at 14.1s,	estimator xgboost's best error=5.2061,	best estimator xgboost's best error=5.2061
[flaml.automl: 09-17 12:38:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:38:48] {3072} INFO -  at 16.8s,	estimator xgboost's best error=3.8683,	best estimator xgboost's best error=3.8683
[flaml.automl: 09-17 12:38:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:38:51] {3072} INFO -  at 20.3s,	estimator xgboost's best error=3.5383,	best estimator xgboost's best error=3.5383
[flaml.automl: 09-17 12:38:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:38:54] {3072} INFO -  at 23.2s,	estimator xgboost's best error=3.5383,	best estimator xgboost's best error=3.5383
[flaml.automl: 09-17 12:38:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:38:56] {3072} INFO -  at 25.7s,	estimator xgboost's best error=3.5383,	best estimator xgboost's best error=3.5383
[flaml.automl: 09-17 12:38:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:38:58] {3072} INFO -  at 26.8s,	estimator xgboost's best error=3.5383,	best estimator xgboost's best error=3.5383
[flaml.automl: 09-17 12:38:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:39:00] {3072} INFO -  at 29.5s,	estimator xgboost's best error=3.5383,	best estimator xgboost's best error=3.5383
[flaml.automl: 09-17 12:39:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:39:02] {3072} INFO -  at 31.1s,	estimator xgboost's best error=3.5369,	best estimator xgboost's best error=3.5369
[flaml.automl: 09-17 12:39:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:39:03] {3072} INFO -  at 32.2s,	estimator xgboost's best error=3.5369,	best estimator xgboost's best error=3.5369
[flaml.automl: 09-17 12:39:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:39:09] {3072} INFO -  at 38.7s,	estimator xgboost's best error=3.4109,	best estimator xgboost's best error=3.4109
[flaml.automl: 09-17 12:39:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:39:22] {3072} INFO -  at 50.8s,	estimator xgboost's best error=3.3903,	best estimator xgboost's best error=3.3903
[flaml.automl: 09-17 12:39:34] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 12:39:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:39:34] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:39:34] {2637} INFO - Time taken to find the best model: 50.80369281768799
[flaml.automl: 09-17 12:39:34] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53514}
NO2(0)最佳损失：-2.390252393923181
NO2(0)最好结果：{'pred_time': 6.8363901183856205e-06, 'wall_clock_time': 50.80369281768799, 'metric_for_logging': {'pred_time': 6.8363901183856205e-06}, 'val_loss': 3.390252393923181, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53514}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53514, 'experiment_tag': 'exp', 'time_total_s': 12.099003553390503}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7506207111804102
NO2(0)的mse=27.103378349103853
NO2(0)的mae=3.491214160535237
NO2(0)的mar=0.2563486532868153
总共花费的时间为：63.92
芜湖市
1795A
1796A
3328A
3465A
3466A
[flaml.automl: 09-17 12:55:19] {2390} INFO - task = regression
[flaml.automl: 09-17 12:55:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:55:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:55:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:55:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:55:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:55:22] {3025} INFO - Estimated sufficient time budget=150033s. Estimated necessary time budget=150s.
[flaml.automl: 09-17 12:55:22] {3072} INFO -  at 3.1s,	estimator xgboost's best error=18.1745,	best estimator xgboost's best error=18.1745
[flaml.automl: 09-17 12:55:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:55:27] {3072} INFO -  at 7.8s,	estimator xgboost's best error=8.8344,	best estimator xgboost's best error=8.8344
[flaml.automl: 09-17 12:55:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:55:29] {3072} INFO -  at 10.3s,	estimator xgboost's best error=8.8344,	best estimator xgboost's best error=8.8344
[flaml.automl: 09-17 12:55:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:55:33] {3072} INFO -  at 14.0s,	estimator xgboost's best error=8.8344,	best estimator xgboost's best error=8.8344
[flaml.automl: 09-17 12:55:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:55:35] {3072} INFO -  at 16.5s,	estimator xgboost's best error=5.7705,	best estimator xgboost's best error=5.7705
[flaml.automl: 09-17 12:55:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:55:38] {3072} INFO -  at 19.5s,	estimator xgboost's best error=4.9301,	best estimator xgboost's best error=4.9301
[flaml.automl: 09-17 12:55:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:55:41] {3072} INFO -  at 22.2s,	estimator xgboost's best error=4.9301,	best estimator xgboost's best error=4.9301
[flaml.automl: 09-17 12:55:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:55:44] {3072} INFO -  at 24.7s,	estimator xgboost's best error=4.9301,	best estimator xgboost's best error=4.9301
[flaml.automl: 09-17 12:55:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:55:46] {3072} INFO -  at 26.6s,	estimator xgboost's best error=4.9301,	best estimator xgboost's best error=4.9301
[flaml.automl: 09-17 12:55:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:55:48] {3072} INFO -  at 28.7s,	estimator xgboost's best error=4.9301,	best estimator xgboost's best error=4.9301
[flaml.automl: 09-17 12:55:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:55:51] {3072} INFO -  at 31.7s,	estimator xgboost's best error=4.9301,	best estimator xgboost's best error=4.9301
[flaml.automl: 09-17 12:55:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:55:53] {3072} INFO -  at 33.9s,	estimator xgboost's best error=4.9301,	best estimator xgboost's best error=4.9301
[flaml.automl: 09-17 12:55:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:56:02] {3072} INFO -  at 42.6s,	estimator xgboost's best error=4.6879,	best estimator xgboost's best error=4.6879
[flaml.automl: 09-17 12:56:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:56:14] {3072} INFO -  at 54.7s,	estimator xgboost's best error=4.5722,	best estimator xgboost's best error=4.5722
[flaml.automl: 09-17 12:56:26] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 12:56:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:56:26] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:56:26] {2637} INFO - Time taken to find the best model: 54.70953392982483
[flaml.automl: 09-17 12:56:26] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54577}
NO2(0)最佳损失：-3.572160046806241
NO2(0)最好结果：{'pred_time': 6.704338306449016e-06, 'wall_clock_time': 54.70953392982483, 'metric_for_logging': {'pred_time': 6.704338306449016e-06}, 'val_loss': 4.572160046806241, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54577}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54577, 'experiment_tag': 'exp', 'time_total_s': 12.089580774307251}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8369739724587977
NO2(0)的mse=46.993880097048546
NO2(0)的mae=4.472456380959327
NO2(0)的mar=0.18867137781467783
总共花费的时间为：67.81
马鞍山市
1798A
1800A
3633A
[flaml.automl: 09-17 13:05:59] {2390} INFO - task = regression
[flaml.automl: 09-17 13:05:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:05:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:06:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:06:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:06:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:06:02] {3025} INFO - Estimated sufficient time budget=21710s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 13:06:02] {3072} INFO -  at 2.4s,	estimator xgboost's best error=18.6417,	best estimator xgboost's best error=18.6417
[flaml.automl: 09-17 13:06:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:06:04] {3072} INFO -  at 4.5s,	estimator xgboost's best error=9.0013,	best estimator xgboost's best error=9.0013
[flaml.automl: 09-17 13:06:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:06:05] {3072} INFO -  at 5.7s,	estimator xgboost's best error=9.0013,	best estimator xgboost's best error=9.0013
[flaml.automl: 09-17 13:06:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:06:15] {3072} INFO -  at 15.7s,	estimator xgboost's best error=9.0013,	best estimator xgboost's best error=9.0013
[flaml.automl: 09-17 13:06:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:06:16] {3072} INFO -  at 16.9s,	estimator xgboost's best error=5.7257,	best estimator xgboost's best error=5.7257
[flaml.automl: 09-17 13:06:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:06:18] {3072} INFO -  at 18.4s,	estimator xgboost's best error=5.0644,	best estimator xgboost's best error=5.0644
[flaml.automl: 09-17 13:06:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:06:19] {3072} INFO -  at 20.1s,	estimator xgboost's best error=5.0644,	best estimator xgboost's best error=5.0644
[flaml.automl: 09-17 13:06:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:06:22] {3072} INFO -  at 22.5s,	estimator xgboost's best error=5.0644,	best estimator xgboost's best error=5.0644
[flaml.automl: 09-17 13:06:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:06:23] {3072} INFO -  at 23.7s,	estimator xgboost's best error=5.0644,	best estimator xgboost's best error=5.0644
[flaml.automl: 09-17 13:06:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:06:26] {3072} INFO -  at 26.3s,	estimator xgboost's best error=5.0644,	best estimator xgboost's best error=5.0644
[flaml.automl: 09-17 13:06:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:06:27] {3072} INFO -  at 27.5s,	estimator xgboost's best error=5.0644,	best estimator xgboost's best error=5.0644
[flaml.automl: 09-17 13:06:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:06:28] {3072} INFO -  at 28.6s,	estimator xgboost's best error=5.0644,	best estimator xgboost's best error=5.0644
[flaml.automl: 09-17 13:06:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:06:34] {3072} INFO -  at 35.1s,	estimator xgboost's best error=4.7831,	best estimator xgboost's best error=4.7831
[flaml.automl: 09-17 13:06:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:06:47] {3072} INFO -  at 47.2s,	estimator xgboost's best error=4.6897,	best estimator xgboost's best error=4.6897
[flaml.automl: 09-17 13:06:47] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 13:06:53] {3072} INFO -  at 53.8s,	estimator xgboost's best error=4.6897,	best estimator xgboost's best error=4.6897
[flaml.automl: 09-17 13:07:05] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 13:07:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:07:05] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:07:05] {2637} INFO - Time taken to find the best model: 47.23984742164612
[flaml.automl: 09-17 13:07:05] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.6897481093499653
NO2(0)最好结果：{'pred_time': 1.1440439144548932e-05, 'wall_clock_time': 47.23984742164612, 'metric_for_logging': {'pred_time': 1.1440439144548932e-05}, 'val_loss': 4.689748109349965, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.098170518875122}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8179287359479294
NO2(0)的mse=46.82959698337246
NO2(0)的mae=4.687841730089109
NO2(0)的mar=0.19054099203802882
总共花费的时间为：66.42
九江市
1803A
1804A
1805A
1806A
1810A
[flaml.automl: 09-17 13:22:22] {2390} INFO - task = regression
[flaml.automl: 09-17 13:22:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:22:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:22:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:22:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:22:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:22:23] {3025} INFO - Estimated sufficient time budget=64218s. Estimated necessary time budget=64s.
[flaml.automl: 09-17 13:22:23] {3072} INFO -  at 1.5s,	estimator xgboost's best error=13.8993,	best estimator xgboost's best error=13.8993
[flaml.automl: 09-17 13:22:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:22:25] {3072} INFO -  at 3.6s,	estimator xgboost's best error=7.1355,	best estimator xgboost's best error=7.1355
[flaml.automl: 09-17 13:22:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:22:27] {3072} INFO -  at 4.8s,	estimator xgboost's best error=7.1355,	best estimator xgboost's best error=7.1355
[flaml.automl: 09-17 13:22:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:22:31] {3072} INFO -  at 9.6s,	estimator xgboost's best error=7.1355,	best estimator xgboost's best error=7.1355
[flaml.automl: 09-17 13:22:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:22:33] {3072} INFO -  at 10.8s,	estimator xgboost's best error=5.0818,	best estimator xgboost's best error=5.0818
[flaml.automl: 09-17 13:22:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:22:34] {3072} INFO -  at 12.4s,	estimator xgboost's best error=4.4000,	best estimator xgboost's best error=4.4000
[flaml.automl: 09-17 13:22:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:22:36] {3072} INFO -  at 14.0s,	estimator xgboost's best error=4.4000,	best estimator xgboost's best error=4.4000
[flaml.automl: 09-17 13:22:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:22:38] {3072} INFO -  at 16.4s,	estimator xgboost's best error=4.4000,	best estimator xgboost's best error=4.4000
[flaml.automl: 09-17 13:22:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:22:39] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.4000,	best estimator xgboost's best error=4.4000
[flaml.automl: 09-17 13:22:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:22:42] {3072} INFO -  at 20.3s,	estimator xgboost's best error=4.4000,	best estimator xgboost's best error=4.4000
[flaml.automl: 09-17 13:22:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:22:44] {3072} INFO -  at 21.8s,	estimator xgboost's best error=4.3486,	best estimator xgboost's best error=4.3486
[flaml.automl: 09-17 13:22:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:22:45] {3072} INFO -  at 23.0s,	estimator xgboost's best error=4.3486,	best estimator xgboost's best error=4.3486
[flaml.automl: 09-17 13:22:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:22:51] {3072} INFO -  at 29.5s,	estimator xgboost's best error=4.1509,	best estimator xgboost's best error=4.1509
[flaml.automl: 09-17 13:22:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:23:03] {3072} INFO -  at 41.6s,	estimator xgboost's best error=4.0458,	best estimator xgboost's best error=4.0458
[flaml.automl: 09-17 13:23:03] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 13:23:10] {3072} INFO -  at 48.2s,	estimator xgboost's best error=4.0458,	best estimator xgboost's best error=4.0458
[flaml.automl: 09-17 13:23:22] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 13:23:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:23:22] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:23:22] {2637} INFO - Time taken to find the best model: 41.64420127868652
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51410}
NO2(0)最佳损失：-3.0457952109565047
NO2(0)最好结果：{'pred_time': 7.129604420394003e-06, 'wall_clock_time': 41.64420127868652, 'metric_for_logging': {'pred_time': 7.129604420394003e-06}, 'val_loss': 4.045795210956505, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51410}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51410, 'experiment_tag': 'exp', 'time_total_s': 12.10590648651123}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8424827264482964
NO2(0)的mse=44.449947757216414
NO2(0)的mae=4.049992224181607
NO2(0)的mar=0.2659558847578936
总共花费的时间为：61.02
洛阳市
1815A
1817A
3341A
3593A
3635A
3636A
[flaml.automl: 09-17 13:43:13] {2390} INFO - task = regression
[flaml.automl: 09-17 13:43:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:43:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:43:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:43:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:43:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:43:14] {3025} INFO - Estimated sufficient time budget=77807s. Estimated necessary time budget=78s.
[flaml.automl: 09-17 13:43:14] {3072} INFO -  at 1.5s,	estimator xgboost's best error=16.6525,	best estimator xgboost's best error=16.6525
[flaml.automl: 09-17 13:43:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:43:16] {3072} INFO -  at 3.7s,	estimator xgboost's best error=7.9819,	best estimator xgboost's best error=7.9819
[flaml.automl: 09-17 13:43:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:43:17] {3072} INFO -  at 4.9s,	estimator xgboost's best error=7.9819,	best estimator xgboost's best error=7.9819
[flaml.automl: 09-17 13:43:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:43:21] {3072} INFO -  at 8.6s,	estimator xgboost's best error=7.9819,	best estimator xgboost's best error=7.9819
[flaml.automl: 09-17 13:43:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:43:23] {3072} INFO -  at 10.7s,	estimator xgboost's best error=4.9707,	best estimator xgboost's best error=4.9707
[flaml.automl: 09-17 13:43:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:43:26] {3072} INFO -  at 13.5s,	estimator xgboost's best error=4.2372,	best estimator xgboost's best error=4.2372
[flaml.automl: 09-17 13:43:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:43:29] {3072} INFO -  at 16.3s,	estimator xgboost's best error=4.2372,	best estimator xgboost's best error=4.2372
[flaml.automl: 09-17 13:43:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:43:31] {3072} INFO -  at 18.5s,	estimator xgboost's best error=4.2372,	best estimator xgboost's best error=4.2372
[flaml.automl: 09-17 13:43:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:43:33] {3072} INFO -  at 20.4s,	estimator xgboost's best error=4.2372,	best estimator xgboost's best error=4.2372
[flaml.automl: 09-17 13:43:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:43:35] {3072} INFO -  at 22.9s,	estimator xgboost's best error=4.2372,	best estimator xgboost's best error=4.2372
[flaml.automl: 09-17 13:43:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:43:38] {3072} INFO -  at 26.0s,	estimator xgboost's best error=4.2372,	best estimator xgboost's best error=4.2372
[flaml.automl: 09-17 13:43:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:43:40] {3072} INFO -  at 28.0s,	estimator xgboost's best error=4.2372,	best estimator xgboost's best error=4.2372
[flaml.automl: 09-17 13:43:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:43:50] {3072} INFO -  at 38.0s,	estimator xgboost's best error=3.9750,	best estimator xgboost's best error=3.9750
[flaml.automl: 09-17 13:43:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:44:11] {3072} INFO -  at 58.2s,	estimator xgboost's best error=3.8384,	best estimator xgboost's best error=3.8384
[flaml.automl: 09-17 13:44:30] {3335} INFO - retrain xgboost for 18.9s
[flaml.automl: 09-17 13:44:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:44:30] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:44:30] {2637} INFO - Time taken to find the best model: 58.159584760665894
[flaml.automl: 09-17 13:44:30] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65028}
NO2(0)最佳损失：-2.838425287209222
NO2(0)最好结果：{'pred_time': 1.225727534247936e-05, 'wall_clock_time': 58.159584760665894, 'metric_for_logging': {'pred_time': 1.225727534247936e-05}, 'val_loss': 3.838425287209222, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65028}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 65028, 'experiment_tag': 'exp', 'time_total_s': 20.158610343933105}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8623786836958858
NO2(0)的mse=35.74977599394507
NO2(0)的mae=3.9305909851120306
NO2(0)的mar=0.19942530155255797
总共花费的时间为：78.21
安阳市
1818A
1819A
3141A
3669A
[flaml.automl: 09-17 13:57:25] {2390} INFO - task = regression
[flaml.automl: 09-17 13:57:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:57:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:57:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:57:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:57:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:57:26] {3025} INFO - Estimated sufficient time budget=49998s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 13:57:26] {3072} INFO -  at 1.5s,	estimator xgboost's best error=18.3737,	best estimator xgboost's best error=18.3737
[flaml.automl: 09-17 13:57:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:57:29] {3072} INFO -  at 3.6s,	estimator xgboost's best error=8.6686,	best estimator xgboost's best error=8.6686
[flaml.automl: 09-17 13:57:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:57:30] {3072} INFO -  at 4.8s,	estimator xgboost's best error=8.6686,	best estimator xgboost's best error=8.6686
[flaml.automl: 09-17 13:57:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:57:36] {3072} INFO -  at 11.1s,	estimator xgboost's best error=8.6686,	best estimator xgboost's best error=8.6686
[flaml.automl: 09-17 13:57:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:57:37] {3072} INFO -  at 12.3s,	estimator xgboost's best error=5.0935,	best estimator xgboost's best error=5.0935
[flaml.automl: 09-17 13:57:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:57:39] {3072} INFO -  at 13.9s,	estimator xgboost's best error=4.2660,	best estimator xgboost's best error=4.2660
[flaml.automl: 09-17 13:57:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:57:40] {3072} INFO -  at 15.5s,	estimator xgboost's best error=4.2660,	best estimator xgboost's best error=4.2660
[flaml.automl: 09-17 13:57:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:57:43] {3072} INFO -  at 17.9s,	estimator xgboost's best error=4.2660,	best estimator xgboost's best error=4.2660
[flaml.automl: 09-17 13:57:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:57:44] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.2660,	best estimator xgboost's best error=4.2660
[flaml.automl: 09-17 13:57:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:57:47] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.2660,	best estimator xgboost's best error=4.2660
[flaml.automl: 09-17 13:57:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:57:48] {3072} INFO -  at 23.3s,	estimator xgboost's best error=4.2454,	best estimator xgboost's best error=4.2454
[flaml.automl: 09-17 13:57:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:57:49] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.2454,	best estimator xgboost's best error=4.2454
[flaml.automl: 09-17 13:57:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:57:56] {3072} INFO -  at 31.0s,	estimator xgboost's best error=3.8915,	best estimator xgboost's best error=3.8915
[flaml.automl: 09-17 13:57:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:58:08] {3072} INFO -  at 43.1s,	estimator xgboost's best error=3.7783,	best estimator xgboost's best error=3.7783
[flaml.automl: 09-17 13:58:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 13:58:14] {3072} INFO -  at 49.5s,	estimator xgboost's best error=3.7783,	best estimator xgboost's best error=3.7783
[flaml.automl: 09-17 13:58:27] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 13:58:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:58:27] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:58:27] {2637} INFO - Time taken to find the best model: 43.06462502479553
[flaml.automl: 09-17 13:58:27] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41642}
NO2(0)最佳损失：-2.778262408990079
NO2(0)最好结果：{'pred_time': 8.5577172286984e-06, 'wall_clock_time': 43.06462502479553, 'metric_for_logging': {'pred_time': 8.5577172286984e-06}, 'val_loss': 3.778262408990079, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41642}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41642, 'experiment_tag': 'exp', 'time_total_s': 12.063233375549316}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.9010941883545505
NO2(0)的mse=29.87199410449178
NO2(0)的mae=3.688446814658715
NO2(0)的mar=0.15478731432778375
总共花费的时间为：62.26
开封市
3210A
3473A
3592A
[flaml.automl: 09-17 14:08:15] {2390} INFO - task = regression
[flaml.automl: 09-17 14:08:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:08:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:08:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:08:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:08:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:08:16] {3025} INFO - Estimated sufficient time budget=12136s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 14:08:16] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.0843,	best estimator xgboost's best error=15.0843
[flaml.automl: 09-17 14:08:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:08:18] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.3839,	best estimator xgboost's best error=7.3839
[flaml.automl: 09-17 14:08:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:08:20] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.3839,	best estimator xgboost's best error=7.3839
[flaml.automl: 09-17 14:08:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:08:30] {3072} INFO -  at 14.8s,	estimator xgboost's best error=7.3839,	best estimator xgboost's best error=7.3839
[flaml.automl: 09-17 14:08:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:08:31] {3072} INFO -  at 15.9s,	estimator xgboost's best error=4.6727,	best estimator xgboost's best error=4.6727
[flaml.automl: 09-17 14:08:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:08:32] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.9279,	best estimator xgboost's best error=3.9279
[flaml.automl: 09-17 14:08:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:08:34] {3072} INFO -  at 19.2s,	estimator xgboost's best error=3.9279,	best estimator xgboost's best error=3.9279
[flaml.automl: 09-17 14:08:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:08:36] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.9279,	best estimator xgboost's best error=3.9279
[flaml.automl: 09-17 14:08:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:08:38] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.9279,	best estimator xgboost's best error=3.9279
[flaml.automl: 09-17 14:08:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:08:40] {3072} INFO -  at 25.5s,	estimator xgboost's best error=3.9279,	best estimator xgboost's best error=3.9279
[flaml.automl: 09-17 14:08:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:08:41] {3072} INFO -  at 26.7s,	estimator xgboost's best error=3.9279,	best estimator xgboost's best error=3.9279
[flaml.automl: 09-17 14:08:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:08:43] {3072} INFO -  at 27.8s,	estimator xgboost's best error=3.9279,	best estimator xgboost's best error=3.9279
[flaml.automl: 09-17 14:08:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:08:49] {3072} INFO -  at 34.3s,	estimator xgboost's best error=3.7970,	best estimator xgboost's best error=3.7970
[flaml.automl: 09-17 14:08:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:09:01] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.6844,	best estimator xgboost's best error=3.6844
[flaml.automl: 09-17 14:09:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 14:09:08] {3072} INFO -  at 52.9s,	estimator xgboost's best error=3.6844,	best estimator xgboost's best error=3.6844
[flaml.automl: 09-17 14:09:20] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 14:09:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:09:20] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:09:20] {2637} INFO - Time taken to find the best model: 46.39407753944397
[flaml.automl: 09-17 14:09:20] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.6843927715268556
NO2(0)最好结果：{'pred_time': 1.1573800009362807e-05, 'wall_clock_time': 46.39407753944397, 'metric_for_logging': {'pred_time': 1.1573800009362807e-05}, 'val_loss': 3.6843927715268556, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.058066844940186}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8672203282750858
NO2(0)的mse=32.38579655214164
NO2(0)的mae=3.623584832528016
NO2(0)的mar=0.18662109298680898
总共花费的时间为：65.53
焦作市
1830A
3169A
3335A
3336A
3477A
[flaml.automl: 09-17 14:26:01] {2390} INFO - task = regression
[flaml.automl: 09-17 14:26:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:26:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:26:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:26:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:26:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:26:03] {3025} INFO - Estimated sufficient time budget=61508s. Estimated necessary time budget=62s.
[flaml.automl: 09-17 14:26:03] {3072} INFO -  at 1.4s,	estimator xgboost's best error=14.1958,	best estimator xgboost's best error=14.1958
[flaml.automl: 09-17 14:26:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:26:05] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.0271,	best estimator xgboost's best error=7.0271
[flaml.automl: 09-17 14:26:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:26:06] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.0271,	best estimator xgboost's best error=7.0271
[flaml.automl: 09-17 14:26:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:26:11] {3072} INFO -  at 9.5s,	estimator xgboost's best error=7.0271,	best estimator xgboost's best error=7.0271
[flaml.automl: 09-17 14:26:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:26:12] {3072} INFO -  at 10.6s,	estimator xgboost's best error=4.7447,	best estimator xgboost's best error=4.7447
[flaml.automl: 09-17 14:26:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:26:13] {3072} INFO -  at 12.2s,	estimator xgboost's best error=4.2045,	best estimator xgboost's best error=4.2045
[flaml.automl: 09-17 14:26:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:26:15] {3072} INFO -  at 13.8s,	estimator xgboost's best error=4.2045,	best estimator xgboost's best error=4.2045
[flaml.automl: 09-17 14:26:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:26:17] {3072} INFO -  at 16.3s,	estimator xgboost's best error=4.2045,	best estimator xgboost's best error=4.2045
[flaml.automl: 09-17 14:26:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:26:19] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.2045,	best estimator xgboost's best error=4.2045
[flaml.automl: 09-17 14:26:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:26:21] {3072} INFO -  at 20.1s,	estimator xgboost's best error=4.2045,	best estimator xgboost's best error=4.2045
[flaml.automl: 09-17 14:26:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:26:23] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.1746,	best estimator xgboost's best error=4.1746
[flaml.automl: 09-17 14:26:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:26:24] {3072} INFO -  at 22.9s,	estimator xgboost's best error=4.1746,	best estimator xgboost's best error=4.1746
[flaml.automl: 09-17 14:26:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:26:31] {3072} INFO -  at 29.4s,	estimator xgboost's best error=4.0192,	best estimator xgboost's best error=4.0192
[flaml.automl: 09-17 14:26:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:26:43] {3072} INFO -  at 41.5s,	estimator xgboost's best error=3.9084,	best estimator xgboost's best error=3.9084
[flaml.automl: 09-17 14:26:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 14:26:49] {3072} INFO -  at 48.1s,	estimator xgboost's best error=3.9084,	best estimator xgboost's best error=3.9084
[flaml.automl: 09-17 14:27:01] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 14:27:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:27:01] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:27:01] {2637} INFO - Time taken to find the best model: 41.54841899871826
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52289}
NO2(0)最佳损失：-2.9084437714181135
NO2(0)最好结果：{'pred_time': 6.981889064874173e-06, 'wall_clock_time': 41.54841899871826, 'metric_for_logging': {'pred_time': 6.981889064874173e-06}, 'val_loss': 3.9084437714181135, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52289}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52289, 'experiment_tag': 'exp', 'time_total_s': 12.114012956619263}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8165300597229789
NO2(0)的mse=33.31243955874801
NO2(0)的mae=3.907454087499349
NO2(0)的mar=0.23204290547679504
总共花费的时间为：61.42
平顶山市
1833A
3204A
3594A
[flaml.automl: 09-17 14:36:26] {2390} INFO - task = regression
[flaml.automl: 09-17 14:36:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:36:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:36:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:36:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:36:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:36:27] {3025} INFO - Estimated sufficient time budget=12068s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 14:36:27] {3072} INFO -  at 1.3s,	estimator xgboost's best error=16.6043,	best estimator xgboost's best error=16.6043
[flaml.automl: 09-17 14:36:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:36:29] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.0948,	best estimator xgboost's best error=8.0948
[flaml.automl: 09-17 14:36:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:36:30] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.0948,	best estimator xgboost's best error=8.0948
[flaml.automl: 09-17 14:36:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:36:40] {3072} INFO -  at 14.7s,	estimator xgboost's best error=8.0948,	best estimator xgboost's best error=8.0948
[flaml.automl: 09-17 14:36:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:36:42] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.3872,	best estimator xgboost's best error=5.3872
[flaml.automl: 09-17 14:36:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:36:43] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.5843,	best estimator xgboost's best error=4.5843
[flaml.automl: 09-17 14:36:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:36:45] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.5843,	best estimator xgboost's best error=4.5843
[flaml.automl: 09-17 14:36:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:36:47] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.5843,	best estimator xgboost's best error=4.5843
[flaml.automl: 09-17 14:36:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:36:48] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.5843,	best estimator xgboost's best error=4.5843
[flaml.automl: 09-17 14:36:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:36:51] {3072} INFO -  at 25.2s,	estimator xgboost's best error=4.5843,	best estimator xgboost's best error=4.5843
[flaml.automl: 09-17 14:36:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:36:52] {3072} INFO -  at 26.4s,	estimator xgboost's best error=4.5843,	best estimator xgboost's best error=4.5843
[flaml.automl: 09-17 14:36:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:36:53] {3072} INFO -  at 27.5s,	estimator xgboost's best error=4.5843,	best estimator xgboost's best error=4.5843
[flaml.automl: 09-17 14:36:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:36:59] {3072} INFO -  at 33.6s,	estimator xgboost's best error=4.3639,	best estimator xgboost's best error=4.3639
[flaml.automl: 09-17 14:36:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:37:10] {3072} INFO -  at 44.7s,	estimator xgboost's best error=4.2418,	best estimator xgboost's best error=4.2418
[flaml.automl: 09-17 14:37:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 14:37:16] {3072} INFO -  at 50.7s,	estimator xgboost's best error=4.2418,	best estimator xgboost's best error=4.2418
[flaml.automl: 09-17 14:37:27] {3335} INFO - retrain xgboost for 11.0s
[flaml.automl: 09-17 14:37:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:37:27] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:37:27] {2637} INFO - Time taken to find the best model: 44.693274974823
[flaml.automl: 09-17 14:37:27] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.2417780000223884
NO2(0)最好结果：{'pred_time': 1.2611700114297928e-05, 'wall_clock_time': 44.693274974823, 'metric_for_logging': {'pred_time': 1.2611700114297928e-05}, 'val_loss': 4.241778000022388, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.052769184112549}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8383917775192313
NO2(0)的mse=39.956832704646466
NO2(0)的mae=4.073137506524102
NO2(0)的mar=0.19235086559111925
总共花费的时间为：62.28
三门峡市
1835A
1836A
1838A
3598A
[flaml.automl: 09-17 14:49:45] {2390} INFO - task = regression
[flaml.automl: 09-17 14:49:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:49:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:49:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:49:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:49:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:49:46] {3025} INFO - Estimated sufficient time budget=50806s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 14:49:46] {3072} INFO -  at 1.5s,	estimator xgboost's best error=16.1863,	best estimator xgboost's best error=16.1863
[flaml.automl: 09-17 14:49:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:49:48] {3072} INFO -  at 3.6s,	estimator xgboost's best error=7.8869,	best estimator xgboost's best error=7.8869
[flaml.automl: 09-17 14:49:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:49:50] {3072} INFO -  at 4.8s,	estimator xgboost's best error=7.8869,	best estimator xgboost's best error=7.8869
[flaml.automl: 09-17 14:49:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:49:56] {3072} INFO -  at 11.1s,	estimator xgboost's best error=7.8869,	best estimator xgboost's best error=7.8869
[flaml.automl: 09-17 14:49:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:49:57] {3072} INFO -  at 12.2s,	estimator xgboost's best error=5.0808,	best estimator xgboost's best error=5.0808
[flaml.automl: 09-17 14:49:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:49:59] {3072} INFO -  at 13.8s,	estimator xgboost's best error=4.3527,	best estimator xgboost's best error=4.3527
[flaml.automl: 09-17 14:49:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:50:00] {3072} INFO -  at 15.5s,	estimator xgboost's best error=4.3527,	best estimator xgboost's best error=4.3527
[flaml.automl: 09-17 14:50:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:50:03] {3072} INFO -  at 17.9s,	estimator xgboost's best error=4.3527,	best estimator xgboost's best error=4.3527
[flaml.automl: 09-17 14:50:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:50:04] {3072} INFO -  at 19.0s,	estimator xgboost's best error=4.3527,	best estimator xgboost's best error=4.3527
[flaml.automl: 09-17 14:50:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:50:07] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.3527,	best estimator xgboost's best error=4.3527
[flaml.automl: 09-17 14:50:07] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:50:08] {3072} INFO -  at 23.3s,	estimator xgboost's best error=4.3527,	best estimator xgboost's best error=4.3527
[flaml.automl: 09-17 14:50:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:50:09] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.3527,	best estimator xgboost's best error=4.3527
[flaml.automl: 09-17 14:50:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:50:16] {3072} INFO -  at 31.0s,	estimator xgboost's best error=4.1822,	best estimator xgboost's best error=4.1822
[flaml.automl: 09-17 14:50:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:50:28] {3072} INFO -  at 43.0s,	estimator xgboost's best error=4.0635,	best estimator xgboost's best error=4.0635
[flaml.automl: 09-17 14:50:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 14:50:34] {3072} INFO -  at 49.5s,	estimator xgboost's best error=4.0635,	best estimator xgboost's best error=4.0635
[flaml.automl: 09-17 14:50:46] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 14:50:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:50:46] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:50:46] {2637} INFO - Time taken to find the best model: 43.01553797721863
[flaml.automl: 09-17 14:50:46] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41886}
NO2(0)最佳损失：-3.0634561457959784
NO2(0)最好结果：{'pred_time': 8.57968775048807e-06, 'wall_clock_time': 43.01553797721863, 'metric_for_logging': {'pred_time': 8.57968775048807e-06}, 'val_loss': 4.063456145795978, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41886}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41886, 'experiment_tag': 'exp', 'time_total_s': 12.046925067901611}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8075266245238933
NO2(0)的mse=39.659503634092154
NO2(0)的mae=4.093999126855477
NO2(0)的mar=0.20145579673096203
总共花费的时间为：62.25
宜昌市
1840A
1841A
1842A
1843A
3546A
3653A
[flaml.automl: 09-17 15:09:42] {2390} INFO - task = regression
[flaml.automl: 09-17 15:09:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:09:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:09:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:09:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:09:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:09:43] {3025} INFO - Estimated sufficient time budget=76185s. Estimated necessary time budget=76s.
[flaml.automl: 09-17 15:09:43] {3072} INFO -  at 1.5s,	estimator xgboost's best error=14.6358,	best estimator xgboost's best error=14.6358
[flaml.automl: 09-17 15:09:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:09:45] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.0686,	best estimator xgboost's best error=7.0686
[flaml.automl: 09-17 15:09:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:09:46] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.0686,	best estimator xgboost's best error=7.0686
[flaml.automl: 09-17 15:09:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:09:50] {3072} INFO -  at 8.9s,	estimator xgboost's best error=7.0686,	best estimator xgboost's best error=7.0686
[flaml.automl: 09-17 15:09:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:09:52] {3072} INFO -  at 10.0s,	estimator xgboost's best error=4.6323,	best estimator xgboost's best error=4.6323
[flaml.automl: 09-17 15:09:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:09:53] {3072} INFO -  at 11.5s,	estimator xgboost's best error=4.2070,	best estimator xgboost's best error=4.2070
[flaml.automl: 09-17 15:09:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:09:55] {3072} INFO -  at 13.1s,	estimator xgboost's best error=4.2070,	best estimator xgboost's best error=4.2070
[flaml.automl: 09-17 15:09:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:09:57] {3072} INFO -  at 15.7s,	estimator xgboost's best error=4.2070,	best estimator xgboost's best error=4.2070
[flaml.automl: 09-17 15:09:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:09:59] {3072} INFO -  at 17.1s,	estimator xgboost's best error=4.2070,	best estimator xgboost's best error=4.2070
[flaml.automl: 09-17 15:09:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:10:01] {3072} INFO -  at 20.0s,	estimator xgboost's best error=4.2070,	best estimator xgboost's best error=4.2070
[flaml.automl: 09-17 15:10:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:10:04] {3072} INFO -  at 22.4s,	estimator xgboost's best error=4.1964,	best estimator xgboost's best error=4.1964
[flaml.automl: 09-17 15:10:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:10:05] {3072} INFO -  at 23.9s,	estimator xgboost's best error=4.1964,	best estimator xgboost's best error=4.1964
[flaml.automl: 09-17 15:10:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:10:15] {3072} INFO -  at 33.5s,	estimator xgboost's best error=4.0455,	best estimator xgboost's best error=4.0455
[flaml.automl: 09-17 15:10:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:10:35] {3072} INFO -  at 53.1s,	estimator xgboost's best error=3.9433,	best estimator xgboost's best error=3.9433
[flaml.automl: 09-17 15:10:59] {3335} INFO - retrain xgboost for 24.3s
[flaml.automl: 09-17 15:10:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:10:59] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:10:59] {2637} INFO - Time taken to find the best model: 53.06219291687012
[flaml.automl: 09-17 15:10:59] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64668}
NO2(0)最佳损失：-2.943296930931287
NO2(0)最好结果：{'pred_time': 8.612497915240871e-06, 'wall_clock_time': 53.06219291687012, 'metric_for_logging': {'pred_time': 8.612497915240871e-06}, 'val_loss': 3.943296930931287, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64668}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 64668, 'experiment_tag': 'exp', 'time_total_s': 19.5675847530365}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7044966689020199
NO2(0)的mse=32.15637875265751
NO2(0)的mae=4.0174914595180145
NO2(0)的mar=0.2094183651674819
总共花费的时间为：78.50
荆州市
1845A
3548A
[flaml.automl: 09-17 15:17:23] {2390} INFO - task = regression
[flaml.automl: 09-17 15:17:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:17:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:17:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:17:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:17:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:17:24] {3025} INFO - Estimated sufficient time budget=12080s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 15:17:24] {3072} INFO -  at 1.3s,	estimator xgboost's best error=13.3897,	best estimator xgboost's best error=13.3897
[flaml.automl: 09-17 15:17:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:17:26] {3072} INFO -  at 3.4s,	estimator xgboost's best error=6.5494,	best estimator xgboost's best error=6.5494
[flaml.automl: 09-17 15:17:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:17:28] {3072} INFO -  at 4.6s,	estimator xgboost's best error=6.5494,	best estimator xgboost's best error=6.5494
[flaml.automl: 09-17 15:17:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:17:37] {3072} INFO -  at 14.1s,	estimator xgboost's best error=6.5494,	best estimator xgboost's best error=6.5494
[flaml.automl: 09-17 15:17:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:17:38] {3072} INFO -  at 15.2s,	estimator xgboost's best error=4.2979,	best estimator xgboost's best error=4.2979
[flaml.automl: 09-17 15:17:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:17:40] {3072} INFO -  at 16.8s,	estimator xgboost's best error=3.7421,	best estimator xgboost's best error=3.7421
[flaml.automl: 09-17 15:17:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:17:41] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.7421,	best estimator xgboost's best error=3.7421
[flaml.automl: 09-17 15:17:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:17:44] {3072} INFO -  at 20.9s,	estimator xgboost's best error=3.7421,	best estimator xgboost's best error=3.7421
[flaml.automl: 09-17 15:17:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:17:45] {3072} INFO -  at 22.1s,	estimator xgboost's best error=3.7421,	best estimator xgboost's best error=3.7421
[flaml.automl: 09-17 15:17:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:17:48] {3072} INFO -  at 24.5s,	estimator xgboost's best error=3.7421,	best estimator xgboost's best error=3.7421
[flaml.automl: 09-17 15:17:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:17:49] {3072} INFO -  at 25.7s,	estimator xgboost's best error=3.7421,	best estimator xgboost's best error=3.7421
[flaml.automl: 09-17 15:17:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:17:50] {3072} INFO -  at 26.8s,	estimator xgboost's best error=3.7421,	best estimator xgboost's best error=3.7421
[flaml.automl: 09-17 15:17:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:17:56] {3072} INFO -  at 32.9s,	estimator xgboost's best error=3.6029,	best estimator xgboost's best error=3.6029
[flaml.automl: 09-17 15:17:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:18:06] {3072} INFO -  at 43.4s,	estimator xgboost's best error=3.4179,	best estimator xgboost's best error=3.4179
[flaml.automl: 09-17 15:18:06] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 15:18:12] {3072} INFO -  at 49.4s,	estimator xgboost's best error=3.4179,	best estimator xgboost's best error=3.4179
[flaml.automl: 09-17 15:18:12] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 15:18:22] {3072} INFO -  at 59.1s,	estimator xgboost's best error=3.4179,	best estimator xgboost's best error=3.4179
[flaml.automl: 09-17 15:18:33] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-17 15:18:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:18:33] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:18:33] {2637} INFO - Time taken to find the best model: 43.357627391815186
[flaml.automl: 09-17 15:18:33] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.4179122582566643
NO2(0)最好结果：{'pred_time': 1.6739379325413812e-05, 'wall_clock_time': 43.357627391815186, 'metric_for_logging': {'pred_time': 1.6739379325413812e-05}, 'val_loss': 3.4179122582566643, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.461185455322266}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8227963617785214
NO2(0)的mse=27.734311218096888
NO2(0)的mae=3.5193802336057622
NO2(0)的mar=0.20856981078482179
总共花费的时间为：69.98
岳阳市
1847A
1848A
1850A
1851A
1852A
[flaml.automl: 09-17 15:34:15] {2390} INFO - task = regression
[flaml.automl: 09-17 15:34:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:34:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:34:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:34:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:34:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:34:16] {3025} INFO - Estimated sufficient time budget=61288s. Estimated necessary time budget=61s.
[flaml.automl: 09-17 15:34:16] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.5778,	best estimator xgboost's best error=13.5778
[flaml.automl: 09-17 15:34:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:34:18] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.5052,	best estimator xgboost's best error=6.5052
[flaml.automl: 09-17 15:34:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:34:19] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.5052,	best estimator xgboost's best error=6.5052
[flaml.automl: 09-17 15:34:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:34:24] {3072} INFO -  at 10.0s,	estimator xgboost's best error=6.5052,	best estimator xgboost's best error=6.5052
[flaml.automl: 09-17 15:34:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:34:25] {3072} INFO -  at 11.1s,	estimator xgboost's best error=4.3732,	best estimator xgboost's best error=4.3732
[flaml.automl: 09-17 15:34:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:34:27] {3072} INFO -  at 12.7s,	estimator xgboost's best error=3.9628,	best estimator xgboost's best error=3.9628
[flaml.automl: 09-17 15:34:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:34:29] {3072} INFO -  at 14.3s,	estimator xgboost's best error=3.9628,	best estimator xgboost's best error=3.9628
[flaml.automl: 09-17 15:34:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:34:31] {3072} INFO -  at 16.7s,	estimator xgboost's best error=3.9628,	best estimator xgboost's best error=3.9628
[flaml.automl: 09-17 15:34:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:34:32] {3072} INFO -  at 17.8s,	estimator xgboost's best error=3.9628,	best estimator xgboost's best error=3.9628
[flaml.automl: 09-17 15:34:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:34:35] {3072} INFO -  at 20.5s,	estimator xgboost's best error=3.9628,	best estimator xgboost's best error=3.9628
[flaml.automl: 09-17 15:34:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:34:36] {3072} INFO -  at 22.1s,	estimator xgboost's best error=3.9606,	best estimator xgboost's best error=3.9606
[flaml.automl: 09-17 15:34:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:34:38] {3072} INFO -  at 23.3s,	estimator xgboost's best error=3.9606,	best estimator xgboost's best error=3.9606
[flaml.automl: 09-17 15:34:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:34:44] {3072} INFO -  at 29.8s,	estimator xgboost's best error=3.8633,	best estimator xgboost's best error=3.8633
[flaml.automl: 09-17 15:34:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:35:03] {3072} INFO -  at 49.1s,	estimator xgboost's best error=3.7608,	best estimator xgboost's best error=3.7608
[flaml.automl: 09-17 15:35:24] {3335} INFO - retrain xgboost for 20.4s
[flaml.automl: 09-17 15:35:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:35:24] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:35:24] {2637} INFO - Time taken to find the best model: 49.10119390487671
[flaml.automl: 09-17 15:35:24] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51350}
NO2(0)最佳损失：-2.7608254525354776
NO2(0)最好结果：{'pred_time': 1.5133521116452596e-05, 'wall_clock_time': 49.10119390487671, 'metric_for_logging': {'pred_time': 1.5133521116452596e-05}, 'val_loss': 3.7608254525354776, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51350}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51350, 'experiment_tag': 'exp', 'time_total_s': 19.274165153503418}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7890238259090202
NO2(0)的mse=33.00823317309225
NO2(0)的mae=3.691763829508038
NO2(0)的mar=0.19810916978272264
总共花费的时间为：70.38
常德市
1854A
1857A
3138A
3139A
3140A
[flaml.automl: 09-17 15:50:49] {2390} INFO - task = regression
[flaml.automl: 09-17 15:50:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:50:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:50:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:50:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:50:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:50:50] {3025} INFO - Estimated sufficient time budget=60700s. Estimated necessary time budget=61s.
[flaml.automl: 09-17 15:50:50] {3072} INFO -  at 1.4s,	estimator xgboost's best error=9.8298,	best estimator xgboost's best error=9.8298
[flaml.automl: 09-17 15:50:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:50:52] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.7911,	best estimator xgboost's best error=4.7911
[flaml.automl: 09-17 15:50:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:50:53] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.7911,	best estimator xgboost's best error=4.7911
[flaml.automl: 09-17 15:50:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:50:58] {3072} INFO -  at 9.6s,	estimator xgboost's best error=4.7911,	best estimator xgboost's best error=4.7911
[flaml.automl: 09-17 15:50:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:50:59] {3072} INFO -  at 10.7s,	estimator xgboost's best error=3.1989,	best estimator xgboost's best error=3.1989
[flaml.automl: 09-17 15:50:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:51:01] {3072} INFO -  at 12.3s,	estimator xgboost's best error=2.8547,	best estimator xgboost's best error=2.8547
[flaml.automl: 09-17 15:51:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:51:03] {3072} INFO -  at 13.9s,	estimator xgboost's best error=2.8547,	best estimator xgboost's best error=2.8547
[flaml.automl: 09-17 15:51:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:51:05] {3072} INFO -  at 16.4s,	estimator xgboost's best error=2.8547,	best estimator xgboost's best error=2.8547
[flaml.automl: 09-17 15:51:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:51:06] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.8547,	best estimator xgboost's best error=2.8547
[flaml.automl: 09-17 15:51:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:51:09] {3072} INFO -  at 20.2s,	estimator xgboost's best error=2.8547,	best estimator xgboost's best error=2.8547
[flaml.automl: 09-17 15:51:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:51:11] {3072} INFO -  at 21.8s,	estimator xgboost's best error=2.8418,	best estimator xgboost's best error=2.8418
[flaml.automl: 09-17 15:51:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:51:12] {3072} INFO -  at 23.0s,	estimator xgboost's best error=2.8418,	best estimator xgboost's best error=2.8418
[flaml.automl: 09-17 15:51:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:51:18] {3072} INFO -  at 29.5s,	estimator xgboost's best error=2.7369,	best estimator xgboost's best error=2.7369
[flaml.automl: 09-17 15:51:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:51:31] {3072} INFO -  at 42.3s,	estimator xgboost's best error=2.6865,	best estimator xgboost's best error=2.6865
[flaml.automl: 09-17 15:51:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 15:51:42] {3072} INFO -  at 53.1s,	estimator xgboost's best error=2.6865,	best estimator xgboost's best error=2.6865
[flaml.automl: 09-17 15:52:03] {3335} INFO - retrain xgboost for 21.0s
[flaml.automl: 09-17 15:52:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:52:03] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:52:03] {2637} INFO - Time taken to find the best model: 42.26165962219238
[flaml.automl: 09-17 15:52:03] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51336}
NO2(0)最佳损失：-1.6865314680764252
NO2(0)最好结果：{'pred_time': 1.301538058116426e-05, 'wall_clock_time': 42.26165962219238, 'metric_for_logging': {'pred_time': 1.301538058116426e-05}, 'val_loss': 2.686531468076425, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51336}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51336, 'experiment_tag': 'exp', 'time_total_s': 12.719000816345215}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7894085449003532
NO2(0)的mse=17.98955230277052
NO2(0)的mae=2.652440228007802
NO2(0)的mar=0.19404148787281073
总共花费的时间为：75.10
张家界市
1858A
1859A
1861A
[flaml.automl: 09-17 16:01:13] {2390} INFO - task = regression
[flaml.automl: 09-17 16:01:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:01:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:01:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:01:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:01:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:01:14] {3025} INFO - Estimated sufficient time budget=12218s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 16:01:14] {3072} INFO -  at 1.4s,	estimator xgboost's best error=7.5692,	best estimator xgboost's best error=7.5692
[flaml.automl: 09-17 16:01:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:01:16] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.7677,	best estimator xgboost's best error=3.7677
[flaml.automl: 09-17 16:01:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:01:18] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.7677,	best estimator xgboost's best error=3.7677
[flaml.automl: 09-17 16:01:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:01:28] {3072} INFO -  at 14.7s,	estimator xgboost's best error=3.7677,	best estimator xgboost's best error=3.7677
[flaml.automl: 09-17 16:01:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:01:29] {3072} INFO -  at 15.9s,	estimator xgboost's best error=2.5856,	best estimator xgboost's best error=2.5856
[flaml.automl: 09-17 16:01:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:01:30] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.3519,	best estimator xgboost's best error=2.3519
[flaml.automl: 09-17 16:01:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:01:32] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.3519,	best estimator xgboost's best error=2.3519
[flaml.automl: 09-17 16:01:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:01:35] {3072} INFO -  at 21.6s,	estimator xgboost's best error=2.3519,	best estimator xgboost's best error=2.3519
[flaml.automl: 09-17 16:01:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:01:36] {3072} INFO -  at 22.7s,	estimator xgboost's best error=2.3519,	best estimator xgboost's best error=2.3519
[flaml.automl: 09-17 16:01:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:01:38] {3072} INFO -  at 25.4s,	estimator xgboost's best error=2.3519,	best estimator xgboost's best error=2.3519
[flaml.automl: 09-17 16:01:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:01:40] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.3519,	best estimator xgboost's best error=2.3519
[flaml.automl: 09-17 16:01:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:01:41] {3072} INFO -  at 27.7s,	estimator xgboost's best error=2.3519,	best estimator xgboost's best error=2.3519
[flaml.automl: 09-17 16:01:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:01:47] {3072} INFO -  at 34.2s,	estimator xgboost's best error=2.3335,	best estimator xgboost's best error=2.3335
[flaml.automl: 09-17 16:01:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:01:59] {3072} INFO -  at 46.3s,	estimator xgboost's best error=2.2514,	best estimator xgboost's best error=2.2514
[flaml.automl: 09-17 16:01:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 16:02:06] {3072} INFO -  at 52.8s,	estimator xgboost's best error=2.2514,	best estimator xgboost's best error=2.2514
[flaml.automl: 09-17 16:02:18] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 16:02:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:02:18] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:02:18] {2637} INFO - Time taken to find the best model: 46.30747175216675
[flaml.automl: 09-17 16:02:18] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.2513996648064851
NO2(0)最好结果：{'pred_time': 1.251309897811762e-05, 'wall_clock_time': 46.30747175216675, 'metric_for_logging': {'pred_time': 1.251309897811762e-05}, 'val_loss': 2.251399664806485, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.067866325378418}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8101123360855739
NO2(0)的mse=12.961237245550278
NO2(0)的mae=2.2873343648254005
NO2(0)的mar=0.2152402104884682
总共花费的时间为：65.40
桂林市
1862A
1864A
1865A
3403A
3531A
[flaml.automl: 09-17 16:17:43] {2390} INFO - task = regression
[flaml.automl: 09-17 16:17:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:17:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:17:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:17:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:17:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:17:44] {3025} INFO - Estimated sufficient time budget=61142s. Estimated necessary time budget=61s.
[flaml.automl: 09-17 16:17:44] {3072} INFO -  at 1.5s,	estimator xgboost's best error=9.2933,	best estimator xgboost's best error=9.2933
[flaml.automl: 09-17 16:17:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:17:46] {3072} INFO -  at 3.6s,	estimator xgboost's best error=4.4983,	best estimator xgboost's best error=4.4983
[flaml.automl: 09-17 16:17:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:17:47] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.4983,	best estimator xgboost's best error=4.4983
[flaml.automl: 09-17 16:17:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:17:52] {3072} INFO -  at 10.1s,	estimator xgboost's best error=4.4983,	best estimator xgboost's best error=4.4983
[flaml.automl: 09-17 16:17:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:17:54] {3072} INFO -  at 11.2s,	estimator xgboost's best error=2.8468,	best estimator xgboost's best error=2.8468
[flaml.automl: 09-17 16:17:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:17:55] {3072} INFO -  at 12.8s,	estimator xgboost's best error=2.4587,	best estimator xgboost's best error=2.4587
[flaml.automl: 09-17 16:17:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:17:57] {3072} INFO -  at 14.5s,	estimator xgboost's best error=2.4587,	best estimator xgboost's best error=2.4587
[flaml.automl: 09-17 16:17:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:17:59] {3072} INFO -  at 16.9s,	estimator xgboost's best error=2.4587,	best estimator xgboost's best error=2.4587
[flaml.automl: 09-17 16:17:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:18:00] {3072} INFO -  at 18.1s,	estimator xgboost's best error=2.4587,	best estimator xgboost's best error=2.4587
[flaml.automl: 09-17 16:18:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:18:03] {3072} INFO -  at 20.7s,	estimator xgboost's best error=2.4587,	best estimator xgboost's best error=2.4587
[flaml.automl: 09-17 16:18:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:18:05] {3072} INFO -  at 22.3s,	estimator xgboost's best error=2.4587,	best estimator xgboost's best error=2.4587
[flaml.automl: 09-17 16:18:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:18:06] {3072} INFO -  at 23.5s,	estimator xgboost's best error=2.4587,	best estimator xgboost's best error=2.4587
[flaml.automl: 09-17 16:18:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:18:12] {3072} INFO -  at 30.0s,	estimator xgboost's best error=2.3429,	best estimator xgboost's best error=2.3429
[flaml.automl: 09-17 16:18:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:18:24] {3072} INFO -  at 42.1s,	estimator xgboost's best error=2.3391,	best estimator xgboost's best error=2.3391
[flaml.automl: 09-17 16:18:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 16:18:31] {3072} INFO -  at 48.6s,	estimator xgboost's best error=2.3391,	best estimator xgboost's best error=2.3391
[flaml.automl: 09-17 16:18:43] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 16:18:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:18:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:18:43] {2637} INFO - Time taken to find the best model: 42.057748317718506
[flaml.automl: 09-17 16:18:43] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 50848}
NO2(0)最佳损失：-1.3391338228124434
NO2(0)最好结果：{'pred_time': 7.103818707761511e-06, 'wall_clock_time': 42.057748317718506, 'metric_for_logging': {'pred_time': 7.103818707761511e-06}, 'val_loss': 2.3391338228124434, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 50848}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 50848, 'experiment_tag': 'exp', 'time_total_s': 12.044393301010132}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8326630164597218
NO2(0)的mse=14.743129541184166
NO2(0)的mae=2.3508559270207385
NO2(0)的mar=0.176203797123482
总共花费的时间为：61.67
北海市
1866A
1868A
1869A
3400A
[flaml.automl: 09-17 16:31:42] {2390} INFO - task = regression
[flaml.automl: 09-17 16:31:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:31:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:31:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:31:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:31:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:31:43] {3025} INFO - Estimated sufficient time budget=48914s. Estimated necessary time budget=49s.
[flaml.automl: 09-17 16:31:43] {3072} INFO -  at 1.5s,	estimator xgboost's best error=6.4144,	best estimator xgboost's best error=6.4144
[flaml.automl: 09-17 16:31:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:31:45] {3072} INFO -  at 3.6s,	estimator xgboost's best error=3.2512,	best estimator xgboost's best error=3.2512
[flaml.automl: 09-17 16:31:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:31:46] {3072} INFO -  at 4.8s,	estimator xgboost's best error=3.2512,	best estimator xgboost's best error=3.2512
[flaml.automl: 09-17 16:31:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:31:53] {3072} INFO -  at 11.1s,	estimator xgboost's best error=3.2512,	best estimator xgboost's best error=3.2512
[flaml.automl: 09-17 16:31:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:31:54] {3072} INFO -  at 12.3s,	estimator xgboost's best error=2.1486,	best estimator xgboost's best error=2.1486
[flaml.automl: 09-17 16:31:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:31:55] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.8674,	best estimator xgboost's best error=1.8674
[flaml.automl: 09-17 16:31:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:31:57] {3072} INFO -  at 15.5s,	estimator xgboost's best error=1.8674,	best estimator xgboost's best error=1.8674
[flaml.automl: 09-17 16:31:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:31:59] {3072} INFO -  at 17.9s,	estimator xgboost's best error=1.8674,	best estimator xgboost's best error=1.8674
[flaml.automl: 09-17 16:31:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:32:01] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.8674,	best estimator xgboost's best error=1.8674
[flaml.automl: 09-17 16:32:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:32:03] {3072} INFO -  at 21.7s,	estimator xgboost's best error=1.8674,	best estimator xgboost's best error=1.8674
[flaml.automl: 09-17 16:32:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:32:05] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.8674,	best estimator xgboost's best error=1.8674
[flaml.automl: 09-17 16:32:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:32:06] {3072} INFO -  at 24.5s,	estimator xgboost's best error=1.8674,	best estimator xgboost's best error=1.8674
[flaml.automl: 09-17 16:32:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:32:13] {3072} INFO -  at 31.1s,	estimator xgboost's best error=1.8187,	best estimator xgboost's best error=1.8187
[flaml.automl: 09-17 16:32:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:32:35] {3072} INFO -  at 53.2s,	estimator xgboost's best error=1.7810,	best estimator xgboost's best error=1.7810
[flaml.automl: 09-17 16:32:57] {3335} INFO - retrain xgboost for 22.1s
[flaml.automl: 09-17 16:32:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:32:57] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:32:57] {2637} INFO - Time taken to find the best model: 53.22525405883789
[flaml.automl: 09-17 16:32:57] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40880}
NO2(0)最佳损失：-0.7809646829697048
NO2(0)最好结果：{'pred_time': 1.72676348408072e-05, 'wall_clock_time': 53.22525405883789, 'metric_for_logging': {'pred_time': 1.72676348408072e-05}, 'val_loss': 1.7809646829697048, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40880}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40880, 'experiment_tag': 'exp', 'time_total_s': 22.091840744018555}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8130466379411114
NO2(0)的mse=7.888222602716268
NO2(0)的mae=1.7895660996555172
NO2(0)的mar=0.203870521167905
总共花费的时间为：76.12
柳州市
1870A
1872A
1873A
1874A
1875A
3402A
[flaml.automl: 09-17 16:52:33] {2390} INFO - task = regression
[flaml.automl: 09-17 16:52:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:52:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:52:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:52:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:52:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:52:35] {3025} INFO - Estimated sufficient time budget=82406s. Estimated necessary time budget=82s.
[flaml.automl: 09-17 16:52:35] {3072} INFO -  at 1.6s,	estimator xgboost's best error=10.9874,	best estimator xgboost's best error=10.9874
[flaml.automl: 09-17 16:52:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:52:37] {3072} INFO -  at 3.7s,	estimator xgboost's best error=5.3396,	best estimator xgboost's best error=5.3396
[flaml.automl: 09-17 16:52:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:52:38] {3072} INFO -  at 4.9s,	estimator xgboost's best error=5.3396,	best estimator xgboost's best error=5.3396
[flaml.automl: 09-17 16:52:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:52:42] {3072} INFO -  at 8.7s,	estimator xgboost's best error=5.3396,	best estimator xgboost's best error=5.3396
[flaml.automl: 09-17 16:52:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:52:43] {3072} INFO -  at 9.9s,	estimator xgboost's best error=3.6777,	best estimator xgboost's best error=3.6777
[flaml.automl: 09-17 16:52:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:52:45] {3072} INFO -  at 11.4s,	estimator xgboost's best error=3.1736,	best estimator xgboost's best error=3.1736
[flaml.automl: 09-17 16:52:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:52:47] {3072} INFO -  at 14.2s,	estimator xgboost's best error=3.1736,	best estimator xgboost's best error=3.1736
[flaml.automl: 09-17 16:52:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:52:50] {3072} INFO -  at 16.6s,	estimator xgboost's best error=3.1736,	best estimator xgboost's best error=3.1736
[flaml.automl: 09-17 16:52:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:52:52] {3072} INFO -  at 18.7s,	estimator xgboost's best error=3.1736,	best estimator xgboost's best error=3.1736
[flaml.automl: 09-17 16:52:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:52:55] {3072} INFO -  at 21.4s,	estimator xgboost's best error=3.1736,	best estimator xgboost's best error=3.1736
[flaml.automl: 09-17 16:52:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:52:58] {3072} INFO -  at 24.4s,	estimator xgboost's best error=3.1677,	best estimator xgboost's best error=3.1677
[flaml.automl: 09-17 16:52:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:53:00] {3072} INFO -  at 26.6s,	estimator xgboost's best error=3.1677,	best estimator xgboost's best error=3.1677
[flaml.automl: 09-17 16:53:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:53:12] {3072} INFO -  at 38.7s,	estimator xgboost's best error=3.1075,	best estimator xgboost's best error=3.1075
[flaml.automl: 09-17 16:53:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:53:32] {3072} INFO -  at 59.0s,	estimator xgboost's best error=2.9764,	best estimator xgboost's best error=2.9764
[flaml.automl: 09-17 16:53:55] {3335} INFO - retrain xgboost for 22.5s
[flaml.automl: 09-17 16:53:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:53:55] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:53:55] {2637} INFO - Time taken to find the best model: 58.977388858795166
[flaml.automl: 09-17 16:53:55] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64431}
NO2(0)最佳损失：-1.9763862836127841
NO2(0)最好结果：{'pred_time': 9.891007865607406e-06, 'wall_clock_time': 58.977388858795166, 'metric_for_logging': {'pred_time': 9.891007865607406e-06}, 'val_loss': 2.976386283612784, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64431}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 64431, 'experiment_tag': 'exp', 'time_total_s': 20.250659942626953}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8054894086086564
NO2(0)的mse=22.968805591923644
NO2(0)的mae=3.038074501680174
NO2(0)的mar=0.19249117126541468
总共花费的时间为：82.51
三亚市
1876A
3540A
[flaml.automl: 09-17 17:00:57] {2390} INFO - task = regression
[flaml.automl: 09-17 17:00:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:00:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:00:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:00:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:00:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:00:59] {3025} INFO - Estimated sufficient time budget=12055s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 17:00:59] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.0862,	best estimator xgboost's best error=4.0862
[flaml.automl: 09-17 17:00:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:01:01] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.0008,	best estimator xgboost's best error=2.0008
[flaml.automl: 09-17 17:01:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:01:02] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.0008,	best estimator xgboost's best error=2.0008
[flaml.automl: 09-17 17:01:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:01:11] {3072} INFO -  at 14.1s,	estimator xgboost's best error=2.0008,	best estimator xgboost's best error=2.0008
[flaml.automl: 09-17 17:01:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:01:13] {3072} INFO -  at 15.3s,	estimator xgboost's best error=1.3678,	best estimator xgboost's best error=1.3678
[flaml.automl: 09-17 17:01:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:01:14] {3072} INFO -  at 16.9s,	estimator xgboost's best error=1.2732,	best estimator xgboost's best error=1.2732
[flaml.automl: 09-17 17:01:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:01:16] {3072} INFO -  at 18.5s,	estimator xgboost's best error=1.2732,	best estimator xgboost's best error=1.2732
[flaml.automl: 09-17 17:01:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:01:18] {3072} INFO -  at 21.0s,	estimator xgboost's best error=1.2732,	best estimator xgboost's best error=1.2732
[flaml.automl: 09-17 17:01:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:01:19] {3072} INFO -  at 22.1s,	estimator xgboost's best error=1.2732,	best estimator xgboost's best error=1.2732
[flaml.automl: 09-17 17:01:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:01:22] {3072} INFO -  at 24.6s,	estimator xgboost's best error=1.2732,	best estimator xgboost's best error=1.2732
[flaml.automl: 09-17 17:01:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:01:23] {3072} INFO -  at 25.8s,	estimator xgboost's best error=1.2732,	best estimator xgboost's best error=1.2732
[flaml.automl: 09-17 17:01:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:01:24] {3072} INFO -  at 27.1s,	estimator xgboost's best error=1.2732,	best estimator xgboost's best error=1.2732
[flaml.automl: 09-17 17:01:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:01:35] {3072} INFO -  at 38.3s,	estimator xgboost's best error=1.2732,	best estimator xgboost's best error=1.2732
[flaml.automl: 09-17 17:01:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:01:43] {3072} INFO -  at 46.2s,	estimator xgboost's best error=1.2732,	best estimator xgboost's best error=1.2732
[flaml.automl: 09-17 17:01:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 17:01:49] {3072} INFO -  at 51.8s,	estimator xgboost's best error=1.2732,	best estimator xgboost's best error=1.2732
[flaml.automl: 09-17 17:01:49] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 17:01:56] {3072} INFO -  at 58.6s,	estimator xgboost's best error=1.2732,	best estimator xgboost's best error=1.2732
[flaml.automl: 09-17 17:02:00] {3335} INFO - retrain xgboost for 4.2s
[flaml.automl: 09-17 17:02:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 17:02:00] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:02:00] {2637} INFO - Time taken to find the best model: 16.881953239440918
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-0.27315677390308
NO2(0)最好结果：{'pred_time': 1.6712256380029628e-05, 'wall_clock_time': 16.881953239440918, 'metric_for_logging': {'pred_time': 1.6712256380029628e-05}, 'val_loss': 1.27315677390308, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 1.5842680931091309}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.5263554335663854
NO2(0)的mse=4.050637706308825
NO2(0)的mae=1.3366361548809078
NO2(0)的mar=0.21257290371635967
总共花费的时间为：63.37
德阳市
1902A
3639A
[flaml.automl: 09-17 17:08:36] {2390} INFO - task = regression
[flaml.automl: 09-17 17:08:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:08:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:08:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:08:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:08:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:08:39] {3025} INFO - Estimated sufficient time budget=23064s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 17:08:39] {3072} INFO -  at 2.4s,	estimator xgboost's best error=20.0824,	best estimator xgboost's best error=20.0824
[flaml.automl: 09-17 17:08:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:08:43] {3072} INFO -  at 6.4s,	estimator xgboost's best error=9.7706,	best estimator xgboost's best error=9.7706
[flaml.automl: 09-17 17:08:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:08:45] {3072} INFO -  at 8.6s,	estimator xgboost's best error=9.7706,	best estimator xgboost's best error=9.7706
[flaml.automl: 09-17 17:08:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:09:01] {3072} INFO -  at 24.6s,	estimator xgboost's best error=9.7706,	best estimator xgboost's best error=9.7706
[flaml.automl: 09-17 17:09:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:09:02] {3072} INFO -  at 25.7s,	estimator xgboost's best error=6.4338,	best estimator xgboost's best error=6.4338
[flaml.automl: 09-17 17:09:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:09:03] {3072} INFO -  at 27.2s,	estimator xgboost's best error=5.5804,	best estimator xgboost's best error=5.5804
[flaml.automl: 09-17 17:09:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:09:05] {3072} INFO -  at 28.9s,	estimator xgboost's best error=5.5804,	best estimator xgboost's best error=5.5804
[flaml.automl: 09-17 17:09:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:09:07] {3072} INFO -  at 31.3s,	estimator xgboost's best error=5.5804,	best estimator xgboost's best error=5.5804
[flaml.automl: 09-17 17:09:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:09:09] {3072} INFO -  at 32.4s,	estimator xgboost's best error=5.5804,	best estimator xgboost's best error=5.5804
[flaml.automl: 09-17 17:09:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:09:11] {3072} INFO -  at 34.8s,	estimator xgboost's best error=5.5804,	best estimator xgboost's best error=5.5804
[flaml.automl: 09-17 17:09:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:09:12] {3072} INFO -  at 35.9s,	estimator xgboost's best error=5.5804,	best estimator xgboost's best error=5.5804
[flaml.automl: 09-17 17:09:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:09:13] {3072} INFO -  at 37.0s,	estimator xgboost's best error=5.5804,	best estimator xgboost's best error=5.5804
[flaml.automl: 09-17 17:09:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:09:19] {3072} INFO -  at 43.0s,	estimator xgboost's best error=5.3461,	best estimator xgboost's best error=5.3461
[flaml.automl: 09-17 17:09:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:09:29] {3072} INFO -  at 53.2s,	estimator xgboost's best error=5.1837,	best estimator xgboost's best error=5.1837
[flaml.automl: 09-17 17:09:40] {3335} INFO - retrain xgboost for 10.2s
[flaml.automl: 09-17 17:09:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:09:40] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:09:40] {2637} INFO - Time taken to find the best model: 53.21306610107422
[flaml.automl: 09-17 17:09:40] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-4.1836684105161535
NO2(0)最好结果：{'pred_time': 1.600052497559587e-05, 'wall_clock_time': 53.21306610107422, 'metric_for_logging': {'pred_time': 1.600052497559587e-05}, 'val_loss': 5.1836684105161535, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.25920844078064}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8098950528538548
NO2(0)的mse=53.453685490940096
NO2(0)的mae=5.1114641435414345
NO2(0)的mar=0.2040521002903709
总共花费的时间为：64.00
南充市
1905A
1906A
1907A
1908A
1909A
[flaml.automl: 09-17 17:25:50] {2390} INFO - task = regression
[flaml.automl: 09-17 17:25:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:25:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:25:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:25:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:25:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:25:52] {3025} INFO - Estimated sufficient time budget=61821s. Estimated necessary time budget=62s.
[flaml.automl: 09-17 17:25:52] {3072} INFO -  at 1.5s,	estimator xgboost's best error=10.6965,	best estimator xgboost's best error=10.6965
[flaml.automl: 09-17 17:25:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:25:54] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.5455,	best estimator xgboost's best error=5.5455
[flaml.automl: 09-17 17:25:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:25:55] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.5455,	best estimator xgboost's best error=5.5455
[flaml.automl: 09-17 17:25:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:26:00] {3072} INFO -  at 9.6s,	estimator xgboost's best error=5.5455,	best estimator xgboost's best error=5.5455
[flaml.automl: 09-17 17:26:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:26:01] {3072} INFO -  at 10.8s,	estimator xgboost's best error=4.1057,	best estimator xgboost's best error=4.1057
[flaml.automl: 09-17 17:26:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:26:02] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-17 17:26:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:26:04] {3072} INFO -  at 14.0s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-17 17:26:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:26:07] {3072} INFO -  at 16.4s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-17 17:26:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:26:08] {3072} INFO -  at 17.6s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-17 17:26:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:26:10] {3072} INFO -  at 20.2s,	estimator xgboost's best error=3.7187,	best estimator xgboost's best error=3.7187
[flaml.automl: 09-17 17:26:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:26:12] {3072} INFO -  at 21.9s,	estimator xgboost's best error=3.7137,	best estimator xgboost's best error=3.7137
[flaml.automl: 09-17 17:26:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:26:14] {3072} INFO -  at 23.9s,	estimator xgboost's best error=3.7137,	best estimator xgboost's best error=3.7137
[flaml.automl: 09-17 17:26:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:26:24] {3072} INFO -  at 34.3s,	estimator xgboost's best error=3.6082,	best estimator xgboost's best error=3.6082
[flaml.automl: 09-17 17:26:24] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:26:45] {3072} INFO -  at 54.5s,	estimator xgboost's best error=3.5678,	best estimator xgboost's best error=3.5678
[flaml.automl: 09-17 17:27:05] {3335} INFO - retrain xgboost for 20.1s
[flaml.automl: 09-17 17:27:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:27:05] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:27:05] {2637} INFO - Time taken to find the best model: 54.51097917556763
[flaml.automl: 09-17 17:27:05] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51143}
NO2(0)最佳损失：-2.567762677232795
NO2(0)最好结果：{'pred_time': 9.76521889550489e-06, 'wall_clock_time': 54.51097917556763, 'metric_for_logging': {'pred_time': 9.76521889550489e-06}, 'val_loss': 3.567762677232795, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51143}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51143, 'experiment_tag': 'exp', 'time_total_s': 20.190863370895386}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6886590592764515
NO2(0)的mse=28.793224360633975
NO2(0)的mae=3.612820272095263
NO2(0)的mar=0.2733574814560714
总共花费的时间为：75.54
遵义市
1911A
1912A
1913A
1914A
3536A
[flaml.automl: 09-17 17:43:11] {2390} INFO - task = regression
[flaml.automl: 09-17 17:43:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:43:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:43:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:43:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:43:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:43:12] {3025} INFO - Estimated sufficient time budget=64514s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 17:43:12] {3072} INFO -  at 1.5s,	estimator xgboost's best error=9.8603,	best estimator xgboost's best error=9.8603
[flaml.automl: 09-17 17:43:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:43:15] {3072} INFO -  at 3.6s,	estimator xgboost's best error=4.7737,	best estimator xgboost's best error=4.7737
[flaml.automl: 09-17 17:43:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:43:16] {3072} INFO -  at 4.8s,	estimator xgboost's best error=4.7737,	best estimator xgboost's best error=4.7737
[flaml.automl: 09-17 17:43:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:43:21] {3072} INFO -  at 9.7s,	estimator xgboost's best error=4.7737,	best estimator xgboost's best error=4.7737
[flaml.automl: 09-17 17:43:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:43:22] {3072} INFO -  at 10.8s,	estimator xgboost's best error=3.2265,	best estimator xgboost's best error=3.2265
[flaml.automl: 09-17 17:43:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:43:23] {3072} INFO -  at 12.4s,	estimator xgboost's best error=2.8545,	best estimator xgboost's best error=2.8545
[flaml.automl: 09-17 17:43:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:43:25] {3072} INFO -  at 14.0s,	estimator xgboost's best error=2.8545,	best estimator xgboost's best error=2.8545
[flaml.automl: 09-17 17:43:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:43:27] {3072} INFO -  at 16.5s,	estimator xgboost's best error=2.8545,	best estimator xgboost's best error=2.8545
[flaml.automl: 09-17 17:43:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:43:29] {3072} INFO -  at 17.6s,	estimator xgboost's best error=2.8545,	best estimator xgboost's best error=2.8545
[flaml.automl: 09-17 17:43:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:43:31] {3072} INFO -  at 20.3s,	estimator xgboost's best error=2.8545,	best estimator xgboost's best error=2.8545
[flaml.automl: 09-17 17:43:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:43:33] {3072} INFO -  at 21.9s,	estimator xgboost's best error=2.8545,	best estimator xgboost's best error=2.8545
[flaml.automl: 09-17 17:43:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:43:34] {3072} INFO -  at 23.1s,	estimator xgboost's best error=2.8545,	best estimator xgboost's best error=2.8545
[flaml.automl: 09-17 17:43:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:43:41] {3072} INFO -  at 29.6s,	estimator xgboost's best error=2.8194,	best estimator xgboost's best error=2.8194
[flaml.automl: 09-17 17:43:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:43:53] {3072} INFO -  at 41.7s,	estimator xgboost's best error=2.7436,	best estimator xgboost's best error=2.7436
[flaml.automl: 09-17 17:43:53] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 17:43:59] {3072} INFO -  at 48.3s,	estimator xgboost's best error=2.7436,	best estimator xgboost's best error=2.7436
[flaml.automl: 09-17 17:44:11] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 17:44:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:44:11] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:44:11] {2637} INFO - Time taken to find the best model: 41.74818015098572
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53782}
NO2(0)最佳损失：-1.7435991306860283
NO2(0)最好结果：{'pred_time': 6.651982086569749e-06, 'wall_clock_time': 41.74818015098572, 'metric_for_logging': {'pred_time': 6.651982086569749e-06}, 'val_loss': 2.7435991306860283, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53782}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53782, 'experiment_tag': 'exp', 'time_total_s': 12.104504108428955}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7559025352866433
NO2(0)的mse=15.496935438222186
NO2(0)的mae=2.695050611122545
NO2(0)的mar=0.19761651357363316
总共花费的时间为：61.17
曲靖市
1917A
3376A
3377A
[flaml.automl: 09-17 17:54:15] {2390} INFO - task = regression
[flaml.automl: 09-17 17:54:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:54:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:54:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:54:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:54:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:54:16] {3025} INFO - Estimated sufficient time budget=12209s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 17:54:16] {3072} INFO -  at 1.4s,	estimator xgboost's best error=8.8657,	best estimator xgboost's best error=8.8657
[flaml.automl: 09-17 17:54:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:54:19] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.2981,	best estimator xgboost's best error=4.2981
[flaml.automl: 09-17 17:54:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:54:20] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.2981,	best estimator xgboost's best error=4.2981
[flaml.automl: 09-17 17:54:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:54:30] {3072} INFO -  at 14.7s,	estimator xgboost's best error=4.2981,	best estimator xgboost's best error=4.2981
[flaml.automl: 09-17 17:54:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:54:31] {3072} INFO -  at 15.9s,	estimator xgboost's best error=2.9457,	best estimator xgboost's best error=2.9457
[flaml.automl: 09-17 17:54:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:54:33] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.7203,	best estimator xgboost's best error=2.7203
[flaml.automl: 09-17 17:54:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:54:34] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.7203,	best estimator xgboost's best error=2.7203
[flaml.automl: 09-17 17:54:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:54:37] {3072} INFO -  at 21.6s,	estimator xgboost's best error=2.7203,	best estimator xgboost's best error=2.7203
[flaml.automl: 09-17 17:54:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:54:38] {3072} INFO -  at 22.8s,	estimator xgboost's best error=2.7203,	best estimator xgboost's best error=2.7203
[flaml.automl: 09-17 17:54:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:54:41] {3072} INFO -  at 25.5s,	estimator xgboost's best error=2.7203,	best estimator xgboost's best error=2.7203
[flaml.automl: 09-17 17:54:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:54:42] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.7203,	best estimator xgboost's best error=2.7203
[flaml.automl: 09-17 17:54:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:54:43] {3072} INFO -  at 27.8s,	estimator xgboost's best error=2.7203,	best estimator xgboost's best error=2.7203
[flaml.automl: 09-17 17:54:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:54:49] {3072} INFO -  at 34.3s,	estimator xgboost's best error=2.7139,	best estimator xgboost's best error=2.7139
[flaml.automl: 09-17 17:54:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:55:01] {3072} INFO -  at 46.4s,	estimator xgboost's best error=2.6370,	best estimator xgboost's best error=2.6370
[flaml.automl: 09-17 17:55:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 17:55:08] {3072} INFO -  at 52.9s,	estimator xgboost's best error=2.6370,	best estimator xgboost's best error=2.6370
[flaml.automl: 09-17 17:55:20] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 17:55:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:55:20] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:55:20] {2637} INFO - Time taken to find the best model: 46.39817523956299
[flaml.automl: 09-17 17:55:20] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.6370036711691482
NO2(0)最好结果：{'pred_time': 1.1449940928450394e-05, 'wall_clock_time': 46.39817523956299, 'metric_for_logging': {'pred_time': 1.1449940928450394e-05}, 'val_loss': 2.6370036711691482, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.099401473999023}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6609710296628342
NO2(0)的mse=16.90845199359949
NO2(0)的mae=2.6115589295567667
NO2(0)的mar=0.19600464620874908
总共花费的时间为：65.50
咸阳市
1918A
1919A
1920A
3525A
[flaml.automl: 09-17 18:08:08] {2390} INFO - task = regression
[flaml.automl: 09-17 18:08:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:08:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:08:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:08:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:08:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:08:09] {3025} INFO - Estimated sufficient time budget=49537s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 18:08:09] {3072} INFO -  at 1.4s,	estimator xgboost's best error=23.8530,	best estimator xgboost's best error=23.8530
[flaml.automl: 09-17 18:08:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:08:11] {3072} INFO -  at 3.5s,	estimator xgboost's best error=11.2540,	best estimator xgboost's best error=11.2540
[flaml.automl: 09-17 18:08:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:08:13] {3072} INFO -  at 5.8s,	estimator xgboost's best error=11.2540,	best estimator xgboost's best error=11.2540
[flaml.automl: 09-17 18:08:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:08:19] {3072} INFO -  at 11.7s,	estimator xgboost's best error=11.2540,	best estimator xgboost's best error=11.2540
[flaml.automl: 09-17 18:08:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:08:21] {3072} INFO -  at 13.8s,	estimator xgboost's best error=6.9674,	best estimator xgboost's best error=6.9674
[flaml.automl: 09-17 18:08:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:08:24] {3072} INFO -  at 16.7s,	estimator xgboost's best error=5.8470,	best estimator xgboost's best error=5.8470
[flaml.automl: 09-17 18:08:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:08:27] {3072} INFO -  at 19.7s,	estimator xgboost's best error=5.8470,	best estimator xgboost's best error=5.8470
[flaml.automl: 09-17 18:08:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:08:32] {3072} INFO -  at 24.2s,	estimator xgboost's best error=5.8470,	best estimator xgboost's best error=5.8470
[flaml.automl: 09-17 18:08:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:08:34] {3072} INFO -  at 26.2s,	estimator xgboost's best error=5.8470,	best estimator xgboost's best error=5.8470
[flaml.automl: 09-17 18:08:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:08:37] {3072} INFO -  at 29.9s,	estimator xgboost's best error=5.8470,	best estimator xgboost's best error=5.8470
[flaml.automl: 09-17 18:08:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:08:40] {3072} INFO -  at 32.8s,	estimator xgboost's best error=5.8364,	best estimator xgboost's best error=5.8364
[flaml.automl: 09-17 18:08:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:08:42] {3072} INFO -  at 35.0s,	estimator xgboost's best error=5.8364,	best estimator xgboost's best error=5.8364
[flaml.automl: 09-17 18:08:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:08:55] {3072} INFO -  at 47.2s,	estimator xgboost's best error=5.5170,	best estimator xgboost's best error=5.5170
[flaml.automl: 09-17 18:08:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:09:07] {3072} INFO -  at 59.1s,	estimator xgboost's best error=5.3423,	best estimator xgboost's best error=5.3423
[flaml.automl: 09-17 18:09:25] {3335} INFO - retrain xgboost for 18.5s
[flaml.automl: 09-17 18:09:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:09:25] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:09:25] {2637} INFO - Time taken to find the best model: 59.11960029602051
[flaml.automl: 09-17 18:09:25] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41755}
NO2(0)最佳损失：-4.342293444481389
NO2(0)最好结果：{'pred_time': 1.544787965971848e-05, 'wall_clock_time': 59.11960029602051, 'metric_for_logging': {'pred_time': 1.544787965971848e-05}, 'val_loss': 5.342293444481389, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41755}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41755, 'experiment_tag': 'exp', 'time_total_s': 11.900571584701538}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8592088727312486
NO2(0)的mse=67.41792398278174
NO2(0)的mae=5.492752890747772
NO2(0)的mar=0.184547697601904
总共花费的时间为：78.32
铜川市
1922A
1923A
[flaml.automl: 09-17 18:16:06] {2390} INFO - task = regression
[flaml.automl: 09-17 18:16:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:16:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:16:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:16:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:16:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:16:07] {3025} INFO - Estimated sufficient time budget=12174s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 18:16:07] {3072} INFO -  at 1.3s,	estimator xgboost's best error=19.2442,	best estimator xgboost's best error=19.2442
[flaml.automl: 09-17 18:16:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:16:09] {3072} INFO -  at 3.2s,	estimator xgboost's best error=10.7153,	best estimator xgboost's best error=10.7153
[flaml.automl: 09-17 18:16:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:16:10] {3072} INFO -  at 4.4s,	estimator xgboost's best error=10.7153,	best estimator xgboost's best error=10.7153
[flaml.automl: 09-17 18:16:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:16:19] {3072} INFO -  at 12.9s,	estimator xgboost's best error=10.7153,	best estimator xgboost's best error=10.7153
[flaml.automl: 09-17 18:16:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:16:20] {3072} INFO -  at 14.0s,	estimator xgboost's best error=6.2643,	best estimator xgboost's best error=6.2643
[flaml.automl: 09-17 18:16:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:16:21] {3072} INFO -  at 15.6s,	estimator xgboost's best error=5.3858,	best estimator xgboost's best error=5.3858
[flaml.automl: 09-17 18:16:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:16:23] {3072} INFO -  at 17.2s,	estimator xgboost's best error=5.3858,	best estimator xgboost's best error=5.3858
[flaml.automl: 09-17 18:16:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:16:25] {3072} INFO -  at 19.7s,	estimator xgboost's best error=5.3858,	best estimator xgboost's best error=5.3858
[flaml.automl: 09-17 18:16:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:16:27] {3072} INFO -  at 20.8s,	estimator xgboost's best error=5.3858,	best estimator xgboost's best error=5.3858
[flaml.automl: 09-17 18:16:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:16:29] {3072} INFO -  at 23.3s,	estimator xgboost's best error=5.3858,	best estimator xgboost's best error=5.3858
[flaml.automl: 09-17 18:16:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:16:30] {3072} INFO -  at 24.4s,	estimator xgboost's best error=5.3858,	best estimator xgboost's best error=5.3858
[flaml.automl: 09-17 18:16:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:16:31] {3072} INFO -  at 25.6s,	estimator xgboost's best error=5.3858,	best estimator xgboost's best error=5.3858
[flaml.automl: 09-17 18:16:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:16:37] {3072} INFO -  at 31.6s,	estimator xgboost's best error=5.2125,	best estimator xgboost's best error=5.2125
[flaml.automl: 09-17 18:16:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:16:48] {3072} INFO -  at 42.1s,	estimator xgboost's best error=4.9906,	best estimator xgboost's best error=4.9906
[flaml.automl: 09-17 18:16:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 18:16:54] {3072} INFO -  at 48.1s,	estimator xgboost's best error=4.9906,	best estimator xgboost's best error=4.9906
[flaml.automl: 09-17 18:16:54] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 18:17:06] {3072} INFO -  at 59.7s,	estimator xgboost's best error=4.9756,	best estimator xgboost's best error=4.9756
[flaml.automl: 09-17 18:17:23] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-17 18:17:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:17:23] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:17:23] {2637} INFO - Time taken to find the best model: 59.743326902389526
[flaml.automl: 09-17 18:17:23] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-3.9756226118999383
NO2(0)最好结果：{'pred_time': 1.7679563843377747e-05, 'wall_clock_time': 59.743326902389526, 'metric_for_logging': {'pred_time': 1.7679563843377747e-05}, 'val_loss': 4.975622611899938, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 11.601097822189331}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8185930373256392
NO2(0)的mse=57.22700557673698
NO2(0)的mae=5.061352226404876
NO2(0)的mar=0.23102636363848011
总共花费的时间为：77.65
延安市
1926A
1927A
1929A
3652A
[flaml.automl: 09-17 18:30:03] {2390} INFO - task = regression
[flaml.automl: 09-17 18:30:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:30:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:30:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:30:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:30:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:30:06] {3025} INFO - Estimated sufficient time budget=91765s. Estimated necessary time budget=92s.
[flaml.automl: 09-17 18:30:06] {3072} INFO -  at 2.5s,	estimator xgboost's best error=20.1224,	best estimator xgboost's best error=20.1224
[flaml.automl: 09-17 18:30:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:30:09] {3072} INFO -  at 6.2s,	estimator xgboost's best error=9.9632,	best estimator xgboost's best error=9.9632
[flaml.automl: 09-17 18:30:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:30:11] {3072} INFO -  at 8.4s,	estimator xgboost's best error=9.9632,	best estimator xgboost's best error=9.9632
[flaml.automl: 09-17 18:30:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:30:17] {3072} INFO -  at 13.6s,	estimator xgboost's best error=9.9632,	best estimator xgboost's best error=9.9632
[flaml.automl: 09-17 18:30:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:30:19] {3072} INFO -  at 15.7s,	estimator xgboost's best error=6.3302,	best estimator xgboost's best error=6.3302
[flaml.automl: 09-17 18:30:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:30:22] {3072} INFO -  at 18.5s,	estimator xgboost's best error=5.4300,	best estimator xgboost's best error=5.4300
[flaml.automl: 09-17 18:30:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:30:25] {3072} INFO -  at 21.5s,	estimator xgboost's best error=5.4300,	best estimator xgboost's best error=5.4300
[flaml.automl: 09-17 18:30:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:30:28] {3072} INFO -  at 25.0s,	estimator xgboost's best error=5.4300,	best estimator xgboost's best error=5.4300
[flaml.automl: 09-17 18:30:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:30:30] {3072} INFO -  at 27.2s,	estimator xgboost's best error=5.4300,	best estimator xgboost's best error=5.4300
[flaml.automl: 09-17 18:30:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:30:33] {3072} INFO -  at 29.8s,	estimator xgboost's best error=5.4300,	best estimator xgboost's best error=5.4300
[flaml.automl: 09-17 18:30:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:30:36] {3072} INFO -  at 32.7s,	estimator xgboost's best error=5.3564,	best estimator xgboost's best error=5.3564
[flaml.automl: 09-17 18:30:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:30:38] {3072} INFO -  at 34.9s,	estimator xgboost's best error=5.3564,	best estimator xgboost's best error=5.3564
[flaml.automl: 09-17 18:30:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:30:50] {3072} INFO -  at 46.8s,	estimator xgboost's best error=5.0239,	best estimator xgboost's best error=5.0239
[flaml.automl: 09-17 18:30:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:31:03] {3072} INFO -  at 59.7s,	estimator xgboost's best error=4.9190,	best estimator xgboost's best error=4.9190
[flaml.automl: 09-17 18:31:23] {3335} INFO - retrain xgboost for 20.7s
[flaml.automl: 09-17 18:31:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:31:23] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:31:23] {2637} INFO - Time taken to find the best model: 59.71369814872742
[flaml.automl: 09-17 18:31:23] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42693}
NO2(0)最佳损失：-3.919016767613602
NO2(0)最好结果：{'pred_time': 1.7047933543110377e-05, 'wall_clock_time': 59.71369814872742, 'metric_for_logging': {'pred_time': 1.7047933543110377e-05}, 'val_loss': 4.919016767613602, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42693}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42693, 'experiment_tag': 'exp', 'time_total_s': 12.885017395019531}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8308738238755604
NO2(0)的mse=57.02549603779522
NO2(0)的mae=4.960958733829595
NO2(0)的mar=0.23676732081530147
总共花费的时间为：81.95
宝鸡市
1930A
1931A
1932A
1933A
1934A
1935A
1937A
[flaml.automl: 09-17 18:53:32] {2390} INFO - task = regression
[flaml.automl: 09-17 18:53:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:53:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:53:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:53:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:53:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:53:33] {3025} INFO - Estimated sufficient time budget=88736s. Estimated necessary time budget=89s.
[flaml.automl: 09-17 18:53:33] {3072} INFO -  at 1.6s,	estimator xgboost's best error=15.8586,	best estimator xgboost's best error=15.8586
[flaml.automl: 09-17 18:53:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:53:35] {3072} INFO -  at 3.7s,	estimator xgboost's best error=7.5621,	best estimator xgboost's best error=7.5621
[flaml.automl: 09-17 18:53:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:53:37] {3072} INFO -  at 4.9s,	estimator xgboost's best error=7.5621,	best estimator xgboost's best error=7.5621
[flaml.automl: 09-17 18:53:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:53:40] {3072} INFO -  at 8.2s,	estimator xgboost's best error=7.5621,	best estimator xgboost's best error=7.5621
[flaml.automl: 09-17 18:53:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:53:41] {3072} INFO -  at 9.3s,	estimator xgboost's best error=4.6580,	best estimator xgboost's best error=4.6580
[flaml.automl: 09-17 18:53:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:53:42] {3072} INFO -  at 10.8s,	estimator xgboost's best error=4.0384,	best estimator xgboost's best error=4.0384
[flaml.automl: 09-17 18:53:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:53:44] {3072} INFO -  at 12.4s,	estimator xgboost's best error=4.0384,	best estimator xgboost's best error=4.0384
[flaml.automl: 09-17 18:53:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:53:46] {3072} INFO -  at 14.9s,	estimator xgboost's best error=4.0384,	best estimator xgboost's best error=4.0384
[flaml.automl: 09-17 18:53:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:53:48] {3072} INFO -  at 16.0s,	estimator xgboost's best error=4.0384,	best estimator xgboost's best error=4.0384
[flaml.automl: 09-17 18:53:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:53:50] {3072} INFO -  at 18.6s,	estimator xgboost's best error=4.0384,	best estimator xgboost's best error=4.0384
[flaml.automl: 09-17 18:53:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:53:52] {3072} INFO -  at 20.2s,	estimator xgboost's best error=4.0384,	best estimator xgboost's best error=4.0384
[flaml.automl: 09-17 18:53:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:53:53] {3072} INFO -  at 21.4s,	estimator xgboost's best error=4.0384,	best estimator xgboost's best error=4.0384
[flaml.automl: 09-17 18:53:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:54:00] {3072} INFO -  at 27.9s,	estimator xgboost's best error=3.8393,	best estimator xgboost's best error=3.8393
[flaml.automl: 09-17 18:54:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:54:12] {3072} INFO -  at 40.0s,	estimator xgboost's best error=3.7446,	best estimator xgboost's best error=3.7446
[flaml.automl: 09-17 18:54:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 18:54:18] {3072} INFO -  at 46.6s,	estimator xgboost's best error=3.7446,	best estimator xgboost's best error=3.7446
[flaml.automl: 09-17 18:54:18] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 18:54:32] {3072} INFO -  at 60.1s,	estimator xgboost's best error=3.7298,	best estimator xgboost's best error=3.7298
[flaml.automl: 09-17 18:55:12] {3335} INFO - retrain xgboost for 40.5s
[flaml.automl: 09-17 18:55:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:55:12] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:55:12] {2637} INFO - Time taken to find the best model: 60.138920545578
[flaml.automl: 09-17 18:55:12] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 73998}
NO2(0)最佳损失：-2.7298018044434857
NO2(0)最好结果：{'pred_time': 1.0508135037247035e-05, 'wall_clock_time': 60.138920545578, 'metric_for_logging': {'pred_time': 1.0508135037247035e-05}, 'val_loss': 3.7298018044434857, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 73998}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 73998, 'experiment_tag': 'exp', 'time_total_s': 13.532115936279297}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8166400991604041
NO2(0)的mse=30.77187432704063
NO2(0)的mae=3.765654136989455
NO2(0)的mar=0.17152072757683706
总共花费的时间为：101.93
渭南市
1938A
1939A
1941A
[flaml.automl: 09-17 19:04:43] {2390} INFO - task = regression
[flaml.automl: 09-17 19:04:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:04:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:04:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:04:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:04:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:04:45] {3025} INFO - Estimated sufficient time budget=12158s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 19:04:45] {3072} INFO -  at 1.4s,	estimator xgboost's best error=21.1690,	best estimator xgboost's best error=21.1690
[flaml.automl: 09-17 19:04:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:04:47] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.5555,	best estimator xgboost's best error=10.5555
[flaml.automl: 09-17 19:04:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:04:48] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.5555,	best estimator xgboost's best error=10.5555
[flaml.automl: 09-17 19:04:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:04:58] {3072} INFO -  at 14.7s,	estimator xgboost's best error=10.5555,	best estimator xgboost's best error=10.5555
[flaml.automl: 09-17 19:04:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:04:59] {3072} INFO -  at 15.8s,	estimator xgboost's best error=7.2996,	best estimator xgboost's best error=7.2996
[flaml.automl: 09-17 19:04:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:05:01] {3072} INFO -  at 17.5s,	estimator xgboost's best error=6.6157,	best estimator xgboost's best error=6.6157
[flaml.automl: 09-17 19:05:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:05:04] {3072} INFO -  at 20.3s,	estimator xgboost's best error=6.6157,	best estimator xgboost's best error=6.6157
[flaml.automl: 09-17 19:05:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:05:08] {3072} INFO -  at 24.3s,	estimator xgboost's best error=6.6157,	best estimator xgboost's best error=6.6157
[flaml.automl: 09-17 19:05:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:05:10] {3072} INFO -  at 26.4s,	estimator xgboost's best error=6.6157,	best estimator xgboost's best error=6.6157
[flaml.automl: 09-17 19:05:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:05:14] {3072} INFO -  at 30.8s,	estimator xgboost's best error=6.6157,	best estimator xgboost's best error=6.6157
[flaml.automl: 09-17 19:05:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:05:16] {3072} INFO -  at 32.9s,	estimator xgboost's best error=6.6157,	best estimator xgboost's best error=6.6157
[flaml.automl: 09-17 19:05:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:05:18] {3072} INFO -  at 35.1s,	estimator xgboost's best error=6.6157,	best estimator xgboost's best error=6.6157
[flaml.automl: 09-17 19:05:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:05:32] {3072} INFO -  at 49.1s,	estimator xgboost's best error=6.5674,	best estimator xgboost's best error=6.5674
[flaml.automl: 09-17 19:05:47] {3335} INFO - retrain xgboost for 14.7s
[flaml.automl: 09-17 19:05:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 19:05:47] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:05:47] {2637} INFO - Time taken to find the best model: 49.05323839187622
[flaml.automl: 09-17 19:05:47] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-5.5673768610109455
NO2(0)最好结果：{'pred_time': 2.6917080042929943e-05, 'wall_clock_time': 49.05323839187622, 'metric_for_logging': {'pred_time': 2.6917080042929943e-05}, 'val_loss': 6.5673768610109455, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 13.996114253997803}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7408924130405372
NO2(0)的mse=82.30116759185204
NO2(0)的mae=6.214371769041681
NO2(0)的mar=0.23845639407434574
总共花费的时间为：64.42
金昌市
金昌市没有数据
嘉峪关市
3248A
[flaml.automl: 09-17 19:09:19] {2390} INFO - task = regression
[flaml.automl: 09-17 19:09:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:09:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:09:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:09:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:09:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:09:20] {3025} INFO - Estimated sufficient time budget=11950s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 19:09:20] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.6700,	best estimator xgboost's best error=11.6700
[flaml.automl: 09-17 19:09:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:09:22] {3072} INFO -  at 3.1s,	estimator xgboost's best error=7.0897,	best estimator xgboost's best error=7.0897
[flaml.automl: 09-17 19:09:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:09:23] {3072} INFO -  at 4.3s,	estimator xgboost's best error=7.0897,	best estimator xgboost's best error=7.0897
[flaml.automl: 09-17 19:09:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:09:30] {3072} INFO -  at 11.3s,	estimator xgboost's best error=7.0897,	best estimator xgboost's best error=7.0897
[flaml.automl: 09-17 19:09:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:09:31] {3072} INFO -  at 12.5s,	estimator xgboost's best error=5.7871,	best estimator xgboost's best error=5.7871
[flaml.automl: 09-17 19:09:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:09:33] {3072} INFO -  at 14.1s,	estimator xgboost's best error=5.7332,	best estimator xgboost's best error=5.7332
[flaml.automl: 09-17 19:09:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:09:34] {3072} INFO -  at 15.7s,	estimator xgboost's best error=5.7318,	best estimator xgboost's best error=5.7318
[flaml.automl: 09-17 19:09:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:09:37] {3072} INFO -  at 18.0s,	estimator xgboost's best error=5.7318,	best estimator xgboost's best error=5.7318
[flaml.automl: 09-17 19:09:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:09:38] {3072} INFO -  at 19.7s,	estimator xgboost's best error=5.5643,	best estimator xgboost's best error=5.5643
[flaml.automl: 09-17 19:09:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:09:41] {3072} INFO -  at 22.6s,	estimator xgboost's best error=5.5643,	best estimator xgboost's best error=5.5643
[flaml.automl: 09-17 19:09:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:09:44] {3072} INFO -  at 25.9s,	estimator xgboost's best error=5.5145,	best estimator xgboost's best error=5.5145
[flaml.automl: 09-17 19:09:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:09:46] {3072} INFO -  at 27.8s,	estimator xgboost's best error=5.5145,	best estimator xgboost's best error=5.5145
[flaml.automl: 09-17 19:09:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:09:57] {3072} INFO -  at 38.0s,	estimator xgboost's best error=5.5145,	best estimator xgboost's best error=5.5145
[flaml.automl: 09-17 19:09:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:10:03] {3072} INFO -  at 44.0s,	estimator xgboost's best error=5.5145,	best estimator xgboost's best error=5.5145
[flaml.automl: 09-17 19:10:03] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 19:10:05] {3072} INFO -  at 46.4s,	estimator xgboost's best error=5.5145,	best estimator xgboost's best error=5.5145
[flaml.automl: 09-17 19:10:05] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 19:10:10] {3072} INFO -  at 51.0s,	estimator xgboost's best error=5.5145,	best estimator xgboost's best error=5.5145
[flaml.automl: 09-17 19:10:10] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 19:10:12] {3072} INFO -  at 53.4s,	estimator xgboost's best error=5.5145,	best estimator xgboost's best error=5.5145
[flaml.automl: 09-17 19:10:12] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 19:10:14] {3072} INFO -  at 55.7s,	estimator xgboost's best error=5.5145,	best estimator xgboost's best error=5.5145
[flaml.automl: 09-17 19:10:14] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 19:10:18] {3072} INFO -  at 59.1s,	estimator xgboost's best error=5.5145,	best estimator xgboost's best error=5.5145
[flaml.automl: 09-17 19:10:21] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-17 19:10:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7913434973835253, colsample_bynode=1,
             colsample_bytree=0.7701244418932739, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=4, min_child_weight=24.325922861664836,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0030487135381191033, reg_lambda=23.851820090366814,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:10:21] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:10:21] {2637} INFO - Time taken to find the best model: 25.889108419418335
NO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 24.325922861664836, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7913434973835253, 'colsample_bytree': 0.7701244418932739, 'reg_alpha': 0.0030487135381191033, 'reg_lambda': 23.851820090366814}
NO2(0)最佳损失：-4.514499911811595
NO2(0)最好结果：{'pred_time': 6.769527160341125e-05, 'wall_clock_time': 25.889108419418335, 'metric_for_logging': {'pred_time': 6.769527160341125e-05}, 'val_loss': 5.514499911811595, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 24.325922861664836, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7913434973835253, 'colsample_bytree': 0.7701244418932739, 'reg_alpha': 0.0030487135381191033, 'reg_lambda': 23.851820090366814}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 24.325922861664836, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7913434973835253, 'config/colsample_bytree': 0.7701244418932739, 'config/reg_alpha': 0.0030487135381191033, 'config/reg_lambda': 23.851820090366814, 'experiment_tag': 'exp', 'time_total_s': 3.2517707347869873}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7913434973835253, colsample_bynode=1,
             colsample_bytree=0.7701244418932739, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=4, min_child_weight=24.325922861664836,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0030487135381191033, reg_lambda=23.851820090366814,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.37899309586590146
NO2(0)的mse=65.56012733651899
NO2(0)的mae=5.695552309265251
NO2(0)的mar=0.37050212028587104
总共花费的时间为：62.24
石嘴山市
1947A
1949A
1950A
3520A
3521A
[flaml.automl: 09-17 19:25:31] {2390} INFO - task = regression
[flaml.automl: 09-17 19:25:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:25:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:25:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:25:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:25:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:25:33] {3025} INFO - Estimated sufficient time budget=65223s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 19:25:33] {3072} INFO -  at 1.5s,	estimator xgboost's best error=17.6492,	best estimator xgboost's best error=17.6492
[flaml.automl: 09-17 19:25:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:25:35] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.2573,	best estimator xgboost's best error=9.2573
[flaml.automl: 09-17 19:25:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:25:36] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.2573,	best estimator xgboost's best error=9.2573
[flaml.automl: 09-17 19:25:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:25:41] {3072} INFO -  at 9.6s,	estimator xgboost's best error=9.2573,	best estimator xgboost's best error=9.2573
[flaml.automl: 09-17 19:25:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:25:42] {3072} INFO -  at 10.8s,	estimator xgboost's best error=6.8474,	best estimator xgboost's best error=6.8474
[flaml.automl: 09-17 19:25:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:25:44] {3072} INFO -  at 12.3s,	estimator xgboost's best error=6.2448,	best estimator xgboost's best error=6.2448
[flaml.automl: 09-17 19:25:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:25:45] {3072} INFO -  at 14.0s,	estimator xgboost's best error=6.2448,	best estimator xgboost's best error=6.2448
[flaml.automl: 09-17 19:25:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:25:48] {3072} INFO -  at 16.4s,	estimator xgboost's best error=6.2448,	best estimator xgboost's best error=6.2448
[flaml.automl: 09-17 19:25:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:25:49] {3072} INFO -  at 17.6s,	estimator xgboost's best error=6.2448,	best estimator xgboost's best error=6.2448
[flaml.automl: 09-17 19:25:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:25:51] {3072} INFO -  at 20.3s,	estimator xgboost's best error=6.2448,	best estimator xgboost's best error=6.2448
[flaml.automl: 09-17 19:25:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:25:53] {3072} INFO -  at 21.9s,	estimator xgboost's best error=6.1370,	best estimator xgboost's best error=6.1370
[flaml.automl: 09-17 19:25:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:25:54] {3072} INFO -  at 23.0s,	estimator xgboost's best error=6.1370,	best estimator xgboost's best error=6.1370
[flaml.automl: 09-17 19:25:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:26:01] {3072} INFO -  at 29.6s,	estimator xgboost's best error=6.0287,	best estimator xgboost's best error=6.0287
[flaml.automl: 09-17 19:26:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:26:14] {3072} INFO -  at 42.7s,	estimator xgboost's best error=5.8834,	best estimator xgboost's best error=5.8834
[flaml.automl: 09-17 19:26:14] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 19:26:25] {3072} INFO -  at 54.3s,	estimator xgboost's best error=5.8834,	best estimator xgboost's best error=5.8834
[flaml.automl: 09-17 19:26:53] {3335} INFO - retrain xgboost for 27.2s
[flaml.automl: 09-17 19:26:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:26:53] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:26:53] {2637} INFO - Time taken to find the best model: 42.728145360946655
[flaml.automl: 09-17 19:26:53] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52005}
NO2(0)最佳损失：-4.883403108820905
NO2(0)最好结果：{'pred_time': 1.4351197728072715e-05, 'wall_clock_time': 42.728145360946655, 'metric_for_logging': {'pred_time': 1.4351197728072715e-05}, 'val_loss': 5.883403108820905, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52005}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52005, 'experiment_tag': 'exp', 'time_total_s': 13.17426323890686}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.69245616305908
NO2(0)的mse=76.49648368246127
NO2(0)的mae=5.829214900922337
NO2(0)的mar=0.2816728562165954
总共花费的时间为：82.77
克拉玛依市
1951A
1955A
3612A
[flaml.automl: 09-17 19:36:16] {2390} INFO - task = regression
[flaml.automl: 09-17 19:36:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:36:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:36:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:36:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:36:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:36:18] {3025} INFO - Estimated sufficient time budget=21691s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 19:36:18] {3072} INFO -  at 2.3s,	estimator xgboost's best error=11.7462,	best estimator xgboost's best error=11.7462
[flaml.automl: 09-17 19:36:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:36:22] {3072} INFO -  at 6.3s,	estimator xgboost's best error=6.4096,	best estimator xgboost's best error=6.4096
[flaml.automl: 09-17 19:36:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:36:25] {3072} INFO -  at 8.5s,	estimator xgboost's best error=6.4096,	best estimator xgboost's best error=6.4096
[flaml.automl: 09-17 19:36:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:36:43] {3072} INFO -  at 26.8s,	estimator xgboost's best error=6.4096,	best estimator xgboost's best error=6.4096
[flaml.automl: 09-17 19:36:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:36:45] {3072} INFO -  at 28.9s,	estimator xgboost's best error=4.7973,	best estimator xgboost's best error=4.7973
[flaml.automl: 09-17 19:36:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:36:48] {3072} INFO -  at 31.8s,	estimator xgboost's best error=4.3526,	best estimator xgboost's best error=4.3526
[flaml.automl: 09-17 19:36:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:36:51] {3072} INFO -  at 35.0s,	estimator xgboost's best error=4.3526,	best estimator xgboost's best error=4.3526
[flaml.automl: 09-17 19:36:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:36:56] {3072} INFO -  at 39.6s,	estimator xgboost's best error=4.3526,	best estimator xgboost's best error=4.3526
[flaml.automl: 09-17 19:36:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:36:58] {3072} INFO -  at 41.8s,	estimator xgboost's best error=4.3526,	best estimator xgboost's best error=4.3526
[flaml.automl: 09-17 19:36:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:37:03] {3072} INFO -  at 46.7s,	estimator xgboost's best error=4.3526,	best estimator xgboost's best error=4.3526
[flaml.automl: 09-17 19:37:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:37:05] {3072} INFO -  at 48.9s,	estimator xgboost's best error=4.3526,	best estimator xgboost's best error=4.3526
[flaml.automl: 09-17 19:37:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:37:07] {3072} INFO -  at 51.0s,	estimator xgboost's best error=4.3526,	best estimator xgboost's best error=4.3526
[flaml.automl: 09-17 19:37:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:37:15] {3072} INFO -  at 58.7s,	estimator xgboost's best error=4.2570,	best estimator xgboost's best error=4.2570
[flaml.automl: 09-17 19:37:22] {3335} INFO - retrain xgboost for 7.5s
[flaml.automl: 09-17 19:37:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 19:37:22] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:37:22] {2637} INFO - Time taken to find the best model: 58.71553635597229
[flaml.automl: 09-17 19:37:22] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-3.257036311073252
NO2(0)最好结果：{'pred_time': 2.413617707334024e-05, 'wall_clock_time': 58.71553635597229, 'metric_for_logging': {'pred_time': 2.413617707334024e-05}, 'val_loss': 4.257036311073252, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 7.72030234336853}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7875951647377379
NO2(0)的mse=41.079229555984064
NO2(0)的mae=4.201380593843658
NO2(0)的mar=0.353654149801319
总共花费的时间为：66.74
巴音郭楞州
1957A
1958A
[flaml.automl: 09-17 19:44:11] {2390} INFO - task = regression
[flaml.automl: 09-17 19:44:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:44:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:44:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:44:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:44:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:44:12] {3025} INFO - Estimated sufficient time budget=12127s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 19:44:12] {3072} INFO -  at 1.3s,	estimator xgboost's best error=12.2861,	best estimator xgboost's best error=12.2861
[flaml.automl: 09-17 19:44:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:44:14] {3072} INFO -  at 3.4s,	estimator xgboost's best error=7.2315,	best estimator xgboost's best error=7.2315
[flaml.automl: 09-17 19:44:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:44:16] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.2315,	best estimator xgboost's best error=7.2315
[flaml.automl: 09-17 19:44:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:44:32] {3072} INFO -  at 20.7s,	estimator xgboost's best error=7.2315,	best estimator xgboost's best error=7.2315
[flaml.automl: 09-17 19:44:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:44:34] {3072} INFO -  at 22.8s,	estimator xgboost's best error=6.2435,	best estimator xgboost's best error=6.2435
[flaml.automl: 09-17 19:44:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:44:37] {3072} INFO -  at 25.6s,	estimator xgboost's best error=5.9647,	best estimator xgboost's best error=5.9647
[flaml.automl: 09-17 19:44:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:44:39] {3072} INFO -  at 27.9s,	estimator xgboost's best error=5.9647,	best estimator xgboost's best error=5.9647
[flaml.automl: 09-17 19:44:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:44:43] {3072} INFO -  at 31.6s,	estimator xgboost's best error=5.9647,	best estimator xgboost's best error=5.9647
[flaml.automl: 09-17 19:44:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:44:45] {3072} INFO -  at 33.6s,	estimator xgboost's best error=5.9647,	best estimator xgboost's best error=5.9647
[flaml.automl: 09-17 19:44:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:44:49] {3072} INFO -  at 37.9s,	estimator xgboost's best error=5.9121,	best estimator xgboost's best error=5.9121
[flaml.automl: 09-17 19:44:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:44:52] {3072} INFO -  at 40.7s,	estimator xgboost's best error=5.9121,	best estimator xgboost's best error=5.9121
[flaml.automl: 09-17 19:44:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:44:54] {3072} INFO -  at 43.1s,	estimator xgboost's best error=5.9121,	best estimator xgboost's best error=5.9121
[flaml.automl: 09-17 19:44:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:45:09] {3072} INFO -  at 57.9s,	estimator xgboost's best error=5.9121,	best estimator xgboost's best error=5.9121
[flaml.automl: 09-17 19:45:13] {3335} INFO - retrain xgboost for 4.2s
[flaml.automl: 09-17 19:45:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:45:13] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:45:13] {2637} INFO - Time taken to find the best model: 37.92033863067627
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}
NO2(0)最佳损失：-4.912131316038582
NO2(0)最好结果：{'pred_time': 3.500607993098215e-05, 'wall_clock_time': 37.92033863067627, 'metric_for_logging': {'pred_time': 3.500607993098215e-05}, 'val_loss': 5.912131316038582, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}, 'config/n_estimators': 4, 'config/max_leaves': 10, 'config/min_child_weight': 0.06372323163910634, 'config/learning_rate': 0.4086634770112844, 'config/subsample': 0.770412309752864, 'config/colsample_bylevel': 0.6720329282771347, 'config/colsample_bytree': 0.752099095598003, 'config/reg_alpha': 0.00271725862127768, 'config/reg_lambda': 3.9335607770117846, 'experiment_tag': 'exp', 'time_total_s': 4.269060134887695}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.40269462267221035
NO2(0)的mse=95.47899024246364
NO2(0)的mae=5.857508334764534
NO2(0)的mar=0.3179486167032504
总共花费的时间为：62.76
信阳市
2054A
2064A
2065A
2066A
[flaml.automl: 09-17 19:58:24] {2390} INFO - task = regression
[flaml.automl: 09-17 19:58:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:58:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:58:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:58:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:58:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:58:26] {3025} INFO - Estimated sufficient time budget=100165s. Estimated necessary time budget=100s.
[flaml.automl: 09-17 19:58:26] {3072} INFO -  at 2.6s,	estimator xgboost's best error=11.4066,	best estimator xgboost's best error=11.4066
[flaml.automl: 09-17 19:58:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:58:30] {3072} INFO -  at 6.1s,	estimator xgboost's best error=5.5456,	best estimator xgboost's best error=5.5456
[flaml.automl: 09-17 19:58:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:58:32] {3072} INFO -  at 8.3s,	estimator xgboost's best error=5.5456,	best estimator xgboost's best error=5.5456
[flaml.automl: 09-17 19:58:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:58:37] {3072} INFO -  at 13.4s,	estimator xgboost's best error=5.5456,	best estimator xgboost's best error=5.5456
[flaml.automl: 09-17 19:58:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:58:39] {3072} INFO -  at 15.1s,	estimator xgboost's best error=3.3783,	best estimator xgboost's best error=3.3783
[flaml.automl: 09-17 19:58:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:58:41] {3072} INFO -  at 17.8s,	estimator xgboost's best error=2.8525,	best estimator xgboost's best error=2.8525
[flaml.automl: 09-17 19:58:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:58:44] {3072} INFO -  at 20.0s,	estimator xgboost's best error=2.8525,	best estimator xgboost's best error=2.8525
[flaml.automl: 09-17 19:58:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:58:48] {3072} INFO -  at 24.0s,	estimator xgboost's best error=2.8525,	best estimator xgboost's best error=2.8525
[flaml.automl: 09-17 19:58:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:58:49] {3072} INFO -  at 25.7s,	estimator xgboost's best error=2.8525,	best estimator xgboost's best error=2.8525
[flaml.automl: 09-17 19:58:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:58:53] {3072} INFO -  at 29.0s,	estimator xgboost's best error=2.8525,	best estimator xgboost's best error=2.8525
[flaml.automl: 09-17 19:58:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:58:56] {3072} INFO -  at 31.9s,	estimator xgboost's best error=2.8525,	best estimator xgboost's best error=2.8525
[flaml.automl: 09-17 19:58:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:58:58] {3072} INFO -  at 33.9s,	estimator xgboost's best error=2.8525,	best estimator xgboost's best error=2.8525
[flaml.automl: 09-17 19:58:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:59:04] {3072} INFO -  at 40.5s,	estimator xgboost's best error=2.7425,	best estimator xgboost's best error=2.7425
[flaml.automl: 09-17 19:59:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:59:16] {3072} INFO -  at 52.6s,	estimator xgboost's best error=2.6319,	best estimator xgboost's best error=2.6319
[flaml.automl: 09-17 19:59:28] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 19:59:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:59:28] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:59:28] {2637} INFO - Time taken to find the best model: 52.58853888511658
[flaml.automl: 09-17 19:59:28] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42921}
NO2(0)最佳损失：-1.6319360240700358
NO2(0)最好结果：{'pred_time': 8.961639564242254e-06, 'wall_clock_time': 52.58853888511658, 'metric_for_logging': {'pred_time': 8.961639564242254e-06}, 'val_loss': 2.631936024070036, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42921}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42921, 'experiment_tag': 'exp', 'time_total_s': 12.099925518035889}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8684955038490145
NO2(0)的mse=18.429683178271326
NO2(0)的mae=2.6240594245352833
NO2(0)的mar=0.18519111988129128
总共花费的时间为：65.49
周口市
2067A
2068A
2069A
2070A
[flaml.automl: 09-17 20:11:23] {2390} INFO - task = regression
[flaml.automl: 09-17 20:11:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:11:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:11:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:11:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:11:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:11:25] {3025} INFO - Estimated sufficient time budget=94717s. Estimated necessary time budget=95s.
[flaml.automl: 09-17 20:11:25] {3072} INFO -  at 2.5s,	estimator xgboost's best error=11.7481,	best estimator xgboost's best error=11.7481
[flaml.automl: 09-17 20:11:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:11:29] {3072} INFO -  at 6.1s,	estimator xgboost's best error=5.5752,	best estimator xgboost's best error=5.5752
[flaml.automl: 09-17 20:11:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:11:31] {3072} INFO -  at 8.3s,	estimator xgboost's best error=5.5752,	best estimator xgboost's best error=5.5752
[flaml.automl: 09-17 20:11:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:11:37] {3072} INFO -  at 14.1s,	estimator xgboost's best error=5.5752,	best estimator xgboost's best error=5.5752
[flaml.automl: 09-17 20:11:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:11:38] {3072} INFO -  at 15.3s,	estimator xgboost's best error=3.3050,	best estimator xgboost's best error=3.3050
[flaml.automl: 09-17 20:11:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:11:40] {3072} INFO -  at 16.8s,	estimator xgboost's best error=2.8633,	best estimator xgboost's best error=2.8633
[flaml.automl: 09-17 20:11:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:11:41] {3072} INFO -  at 18.5s,	estimator xgboost's best error=2.8633,	best estimator xgboost's best error=2.8633
[flaml.automl: 09-17 20:11:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:11:44] {3072} INFO -  at 20.9s,	estimator xgboost's best error=2.8633,	best estimator xgboost's best error=2.8633
[flaml.automl: 09-17 20:11:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:11:45] {3072} INFO -  at 22.1s,	estimator xgboost's best error=2.8633,	best estimator xgboost's best error=2.8633
[flaml.automl: 09-17 20:11:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:11:48] {3072} INFO -  at 24.7s,	estimator xgboost's best error=2.8633,	best estimator xgboost's best error=2.8633
[flaml.automl: 09-17 20:11:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:11:49] {3072} INFO -  at 26.3s,	estimator xgboost's best error=2.8227,	best estimator xgboost's best error=2.8227
[flaml.automl: 09-17 20:11:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:11:50] {3072} INFO -  at 27.5s,	estimator xgboost's best error=2.8227,	best estimator xgboost's best error=2.8227
[flaml.automl: 09-17 20:11:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:11:57] {3072} INFO -  at 34.0s,	estimator xgboost's best error=2.5786,	best estimator xgboost's best error=2.5786
[flaml.automl: 09-17 20:11:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:12:09] {3072} INFO -  at 46.1s,	estimator xgboost's best error=2.5211,	best estimator xgboost's best error=2.5211
[flaml.automl: 09-17 20:12:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:12:15] {3072} INFO -  at 52.7s,	estimator xgboost's best error=2.5211,	best estimator xgboost's best error=2.5211
[flaml.automl: 09-17 20:12:28] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 20:12:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:12:28] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:12:28] {2637} INFO - Time taken to find the best model: 46.13984966278076
[flaml.automl: 09-17 20:12:28] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42667}
NO2(0)最佳损失：-1.5210806129099925
NO2(0)最好结果：{'pred_time': 8.364262849413074e-06, 'wall_clock_time': 46.13984966278076, 'metric_for_logging': {'pred_time': 8.364262849413074e-06}, 'val_loss': 2.5210806129099925, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42667}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42667, 'experiment_tag': 'exp', 'time_total_s': 12.119966506958008}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8828374276006656
NO2(0)的mse=14.711443778950898
NO2(0)的mae=2.547513782434963
NO2(0)的mar=0.16107923589038223
总共花费的时间为：65.56
漳州市
2075A
2920A
3216A
3530A
[flaml.automl: 09-17 20:24:39] {2390} INFO - task = regression
[flaml.automl: 09-17 20:24:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:24:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:24:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:24:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:24:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:24:41] {3025} INFO - Estimated sufficient time budget=51680s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 20:24:41] {3072} INFO -  at 1.5s,	estimator xgboost's best error=13.9983,	best estimator xgboost's best error=13.9983
[flaml.automl: 09-17 20:24:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:24:43] {3072} INFO -  at 3.6s,	estimator xgboost's best error=6.8351,	best estimator xgboost's best error=6.8351
[flaml.automl: 09-17 20:24:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:24:44] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.8351,	best estimator xgboost's best error=6.8351
[flaml.automl: 09-17 20:24:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:24:50] {3072} INFO -  at 11.1s,	estimator xgboost's best error=6.8351,	best estimator xgboost's best error=6.8351
[flaml.automl: 09-17 20:24:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:24:51] {3072} INFO -  at 12.3s,	estimator xgboost's best error=4.3046,	best estimator xgboost's best error=4.3046
[flaml.automl: 09-17 20:24:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:24:53] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.7334,	best estimator xgboost's best error=3.7334
[flaml.automl: 09-17 20:24:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:24:55] {3072} INFO -  at 15.4s,	estimator xgboost's best error=3.7334,	best estimator xgboost's best error=3.7334
[flaml.automl: 09-17 20:24:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:24:57] {3072} INFO -  at 17.9s,	estimator xgboost's best error=3.7334,	best estimator xgboost's best error=3.7334
[flaml.automl: 09-17 20:24:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:24:58] {3072} INFO -  at 19.0s,	estimator xgboost's best error=3.7334,	best estimator xgboost's best error=3.7334
[flaml.automl: 09-17 20:24:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:25:01] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.7334,	best estimator xgboost's best error=3.7334
[flaml.automl: 09-17 20:25:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:25:02] {3072} INFO -  at 23.3s,	estimator xgboost's best error=3.7215,	best estimator xgboost's best error=3.7215
[flaml.automl: 09-17 20:25:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:25:04] {3072} INFO -  at 24.5s,	estimator xgboost's best error=3.7215,	best estimator xgboost's best error=3.7215
[flaml.automl: 09-17 20:25:04] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:25:10] {3072} INFO -  at 31.0s,	estimator xgboost's best error=3.5100,	best estimator xgboost's best error=3.5100
[flaml.automl: 09-17 20:25:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:25:22] {3072} INFO -  at 43.1s,	estimator xgboost's best error=3.4244,	best estimator xgboost's best error=3.4244
[flaml.automl: 09-17 20:25:22] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:25:29] {3072} INFO -  at 49.6s,	estimator xgboost's best error=3.4244,	best estimator xgboost's best error=3.4244
[flaml.automl: 09-17 20:25:41] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 20:25:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:25:41] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:25:41] {2637} INFO - Time taken to find the best model: 43.093488693237305
[flaml.automl: 09-17 20:25:41] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41609}
NO2(0)最佳损失：-2.424392821492827
NO2(0)最好结果：{'pred_time': 8.816021948949689e-06, 'wall_clock_time': 43.093488693237305, 'metric_for_logging': {'pred_time': 8.816021948949689e-06}, 'val_loss': 3.424392821492827, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41609}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41609, 'experiment_tag': 'exp', 'time_total_s': 12.080219745635986}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8261002839087541
NO2(0)的mse=23.223179387562872
NO2(0)的mae=3.3710476327100216
NO2(0)的mar=0.20910603764916072
总共花费的时间为：62.47
晋城市
2160A
2161A
2162A
2163A
3620A
[flaml.automl: 09-17 20:41:37] {2390} INFO - task = regression
[flaml.automl: 09-17 20:41:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:41:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:41:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:41:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:41:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:41:39] {3025} INFO - Estimated sufficient time budget=119965s. Estimated necessary time budget=120s.
[flaml.automl: 09-17 20:41:39] {3072} INFO -  at 2.5s,	estimator xgboost's best error=17.1056,	best estimator xgboost's best error=17.1056
[flaml.automl: 09-17 20:41:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:41:43] {3072} INFO -  at 6.3s,	estimator xgboost's best error=8.4178,	best estimator xgboost's best error=8.4178
[flaml.automl: 09-17 20:41:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:41:45] {3072} INFO -  at 8.4s,	estimator xgboost's best error=8.4178,	best estimator xgboost's best error=8.4178
[flaml.automl: 09-17 20:41:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:41:49] {3072} INFO -  at 12.5s,	estimator xgboost's best error=8.4178,	best estimator xgboost's best error=8.4178
[flaml.automl: 09-17 20:41:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:41:52] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.5526,	best estimator xgboost's best error=5.5526
[flaml.automl: 09-17 20:41:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:41:54] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.8690,	best estimator xgboost's best error=4.8690
[flaml.automl: 09-17 20:41:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:41:57] {3072} INFO -  at 20.4s,	estimator xgboost's best error=4.8690,	best estimator xgboost's best error=4.8690
[flaml.automl: 09-17 20:41:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:42:01] {3072} INFO -  at 23.8s,	estimator xgboost's best error=4.8690,	best estimator xgboost's best error=4.8690
[flaml.automl: 09-17 20:42:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:42:03] {3072} INFO -  at 25.7s,	estimator xgboost's best error=4.8690,	best estimator xgboost's best error=4.8690
[flaml.automl: 09-17 20:42:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:42:05] {3072} INFO -  at 28.4s,	estimator xgboost's best error=4.8690,	best estimator xgboost's best error=4.8690
[flaml.automl: 09-17 20:42:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:42:07] {3072} INFO -  at 30.0s,	estimator xgboost's best error=4.7880,	best estimator xgboost's best error=4.7880
[flaml.automl: 09-17 20:42:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:42:08] {3072} INFO -  at 31.2s,	estimator xgboost's best error=4.7880,	best estimator xgboost's best error=4.7880
[flaml.automl: 09-17 20:42:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:42:15] {3072} INFO -  at 37.7s,	estimator xgboost's best error=4.5427,	best estimator xgboost's best error=4.5427
[flaml.automl: 09-17 20:42:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:42:27] {3072} INFO -  at 49.9s,	estimator xgboost's best error=4.3963,	best estimator xgboost's best error=4.3963
[flaml.automl: 09-17 20:42:39] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 20:42:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:42:39] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:42:39] {2637} INFO - Time taken to find the best model: 49.85644316673279
[flaml.automl: 09-17 20:42:39] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54018}
NO2(0)最佳损失：-3.396323783823039
NO2(0)最好结果：{'pred_time': 7.679357978278114e-06, 'wall_clock_time': 49.85644316673279, 'metric_for_logging': {'pred_time': 7.679357978278114e-06}, 'val_loss': 4.396323783823039, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54018}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54018, 'experiment_tag': 'exp', 'time_total_s': 12.12141489982605}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.816829366168724
NO2(0)的mse=42.15235892113963
NO2(0)的mae=4.337464941514988
NO2(0)的mar=0.19884340051285174
总共花费的时间为：63.07
朔州市
2166A
2167A
2168A
2169A
2170A
3571A
[flaml.automl: 09-17 21:01:23] {2390} INFO - task = regression
[flaml.automl: 09-17 21:01:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:01:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:01:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:01:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:01:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:01:26] {3025} INFO - Estimated sufficient time budget=201269s. Estimated necessary time budget=201s.
[flaml.automl: 09-17 21:01:26] {3072} INFO -  at 3.4s,	estimator xgboost's best error=17.4503,	best estimator xgboost's best error=17.4503
[flaml.automl: 09-17 21:01:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:01:30] {3072} INFO -  at 7.2s,	estimator xgboost's best error=12.5899,	best estimator xgboost's best error=12.5899
[flaml.automl: 09-17 21:01:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:01:33] {3072} INFO -  at 10.4s,	estimator xgboost's best error=12.5899,	best estimator xgboost's best error=12.5899
[flaml.automl: 09-17 21:01:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:01:36] {3072} INFO -  at 13.7s,	estimator xgboost's best error=12.5899,	best estimator xgboost's best error=12.5899
[flaml.automl: 09-17 21:01:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:01:39] {3072} INFO -  at 16.7s,	estimator xgboost's best error=6.5013,	best estimator xgboost's best error=6.5013
[flaml.automl: 09-17 21:01:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:01:42] {3072} INFO -  at 19.2s,	estimator xgboost's best error=6.5013,	best estimator xgboost's best error=6.5013
[flaml.automl: 09-17 21:01:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:01:44] {3072} INFO -  at 21.7s,	estimator xgboost's best error=6.5013,	best estimator xgboost's best error=6.5013
[flaml.automl: 09-17 21:01:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:01:46] {3072} INFO -  at 23.3s,	estimator xgboost's best error=6.5013,	best estimator xgboost's best error=6.5013
[flaml.automl: 09-17 21:01:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:01:48] {3072} INFO -  at 25.8s,	estimator xgboost's best error=6.4167,	best estimator xgboost's best error=6.4167
[flaml.automl: 09-17 21:01:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:01:50] {3072} INFO -  at 27.3s,	estimator xgboost's best error=6.4167,	best estimator xgboost's best error=6.4167
[flaml.automl: 09-17 21:01:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:01:52] {3072} INFO -  at 29.2s,	estimator xgboost's best error=6.4167,	best estimator xgboost's best error=6.4167
[flaml.automl: 09-17 21:01:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:01:54] {3072} INFO -  at 31.0s,	estimator xgboost's best error=6.4167,	best estimator xgboost's best error=6.4167
[flaml.automl: 09-17 21:01:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:01:56] {3072} INFO -  at 32.9s,	estimator xgboost's best error=6.1703,	best estimator xgboost's best error=6.1703
[flaml.automl: 09-17 21:01:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:01:57] {3072} INFO -  at 34.5s,	estimator xgboost's best error=6.1703,	best estimator xgboost's best error=6.1703
[flaml.automl: 09-17 21:01:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 21:02:00] {3072} INFO -  at 36.9s,	estimator xgboost's best error=6.1703,	best estimator xgboost's best error=6.1703
[flaml.automl: 09-17 21:02:00] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 21:02:02] {3072} INFO -  at 39.7s,	estimator xgboost's best error=6.0431,	best estimator xgboost's best error=6.0431
[flaml.automl: 09-17 21:02:02] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 21:02:04] {3072} INFO -  at 41.5s,	estimator xgboost's best error=6.0431,	best estimator xgboost's best error=6.0431
[flaml.automl: 09-17 21:02:04] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 21:02:06] {3072} INFO -  at 43.1s,	estimator xgboost's best error=6.0431,	best estimator xgboost's best error=6.0431
[flaml.automl: 09-17 21:02:06] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 21:02:09] {3072} INFO -  at 46.2s,	estimator xgboost's best error=5.9638,	best estimator xgboost's best error=5.9638
[flaml.automl: 09-17 21:02:09] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 21:02:12] {3072} INFO -  at 48.9s,	estimator xgboost's best error=5.9638,	best estimator xgboost's best error=5.9638
[flaml.automl: 09-17 21:02:12] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 21:02:21] {3072} INFO -  at 58.1s,	estimator xgboost's best error=5.8455,	best estimator xgboost's best error=5.8455
[flaml.automl: 09-17 21:03:19] {3335} INFO - retrain xgboost for 57.9s
[flaml.automl: 09-17 21:03:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5161922612101613, colsample_bynode=1,
             colsample_bytree=0.6118906562279557, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=69, min_child_weight=0.07069787149048533,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.002426712663282027, reg_lambda=1.0618132296291931,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:03:19] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:03:19] {2637} INFO - Time taken to find the best model: 58.14494585990906
[flaml.automl: 09-17 21:03:19] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 69, 'min_child_weight': 0.07069787149048533, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.5161922612101613, 'colsample_bytree': 0.6118906562279557, 'reg_alpha': 0.002426712663282027, 'reg_lambda': 1.0618132296291931, 'FLAML_sample_size': 10000}
NO2(0)最佳损失：-4.845497348040269
NO2(0)最好结果：{'pred_time': 1.3965437936536577e-05, 'wall_clock_time': 58.14494585990906, 'metric_for_logging': {'pred_time': 1.3965437936536577e-05}, 'val_loss': 5.845497348040269, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 69, 'min_child_weight': 0.07069787149048533, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.5161922612101613, 'colsample_bytree': 0.6118906562279557, 'reg_alpha': 0.002426712663282027, 'reg_lambda': 1.0618132296291931, 'FLAML_sample_size': 10000}, 'config/n_estimators': 15, 'config/max_leaves': 69, 'config/min_child_weight': 0.07069787149048533, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.5161922612101613, 'config/colsample_bytree': 0.6118906562279557, 'config/reg_alpha': 0.002426712663282027, 'config/reg_lambda': 1.0618132296291931, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 9.282151937484741}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5161922612101613, colsample_bynode=1,
             colsample_bytree=0.6118906562279557, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=69, min_child_weight=0.07069787149048533,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.002426712663282027, reg_lambda=1.0618132296291931,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7766689672658688
NO2(0)的mse=73.19088085803631
NO2(0)的mae=5.81969104868921
NO2(0)的mar=0.26579157992634317
总共花费的时间为：117.99
晋中市
2171A
2174A
2865A
[flaml.automl: 09-17 21:12:12] {2390} INFO - task = regression
[flaml.automl: 09-17 21:12:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:12:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:12:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:12:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:12:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:12:13] {3025} INFO - Estimated sufficient time budget=12100s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:12:13] {3072} INFO -  at 1.4s,	estimator xgboost's best error=18.6791,	best estimator xgboost's best error=18.6791
[flaml.automl: 09-17 21:12:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:12:16] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.0566,	best estimator xgboost's best error=9.0566
[flaml.automl: 09-17 21:12:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:12:17] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.0566,	best estimator xgboost's best error=9.0566
[flaml.automl: 09-17 21:12:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:12:27] {3072} INFO -  at 14.7s,	estimator xgboost's best error=9.0566,	best estimator xgboost's best error=9.0566
[flaml.automl: 09-17 21:12:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:12:28] {3072} INFO -  at 15.8s,	estimator xgboost's best error=5.7541,	best estimator xgboost's best error=5.7541
[flaml.automl: 09-17 21:12:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:12:29] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.9812,	best estimator xgboost's best error=4.9812
[flaml.automl: 09-17 21:12:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:12:31] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.9812,	best estimator xgboost's best error=4.9812
[flaml.automl: 09-17 21:12:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:12:34] {3072} INFO -  at 21.5s,	estimator xgboost's best error=4.9812,	best estimator xgboost's best error=4.9812
[flaml.automl: 09-17 21:12:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:12:35] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.9812,	best estimator xgboost's best error=4.9812
[flaml.automl: 09-17 21:12:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:12:37] {3072} INFO -  at 25.3s,	estimator xgboost's best error=4.9812,	best estimator xgboost's best error=4.9812
[flaml.automl: 09-17 21:12:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:12:39] {3072} INFO -  at 26.5s,	estimator xgboost's best error=4.9812,	best estimator xgboost's best error=4.9812
[flaml.automl: 09-17 21:12:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:12:40] {3072} INFO -  at 27.6s,	estimator xgboost's best error=4.9812,	best estimator xgboost's best error=4.9812
[flaml.automl: 09-17 21:12:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:12:46] {3072} INFO -  at 34.1s,	estimator xgboost's best error=4.7123,	best estimator xgboost's best error=4.7123
[flaml.automl: 09-17 21:12:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:12:58] {3072} INFO -  at 46.1s,	estimator xgboost's best error=4.6109,	best estimator xgboost's best error=4.6109
[flaml.automl: 09-17 21:12:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 21:13:05] {3072} INFO -  at 52.6s,	estimator xgboost's best error=4.6109,	best estimator xgboost's best error=4.6109
[flaml.automl: 09-17 21:13:17] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 21:13:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:13:17] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:13:17] {2637} INFO - Time taken to find the best model: 46.13663125038147
[flaml.automl: 09-17 21:13:17] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.61093614333069
NO2(0)最好结果：{'pred_time': 1.2391925415912673e-05, 'wall_clock_time': 46.13663125038147, 'metric_for_logging': {'pred_time': 1.2391925415912673e-05}, 'val_loss': 4.61093614333069, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.012732744216919}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8080330612123983
NO2(0)的mse=51.34762123588207
NO2(0)的mae=4.807865560822728
NO2(0)的mar=0.19576486671254473
总共花费的时间为：65.15
运城市
2175A
2178A
2179A
3670A
[flaml.automl: 09-17 21:26:14] {2390} INFO - task = regression
[flaml.automl: 09-17 21:26:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:26:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:26:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:26:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:26:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:26:15] {3025} INFO - Estimated sufficient time budget=51484s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 21:26:15] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.8152,	best estimator xgboost's best error=13.8152
[flaml.automl: 09-17 21:26:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:26:17] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.6490,	best estimator xgboost's best error=6.6490
[flaml.automl: 09-17 21:26:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:26:18] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.6490,	best estimator xgboost's best error=6.6490
[flaml.automl: 09-17 21:26:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:26:24] {3072} INFO -  at 10.6s,	estimator xgboost's best error=6.6490,	best estimator xgboost's best error=6.6490
[flaml.automl: 09-17 21:26:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:26:25] {3072} INFO -  at 11.7s,	estimator xgboost's best error=4.4910,	best estimator xgboost's best error=4.4910
[flaml.automl: 09-17 21:26:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:26:27] {3072} INFO -  at 13.3s,	estimator xgboost's best error=3.8988,	best estimator xgboost's best error=3.8988
[flaml.automl: 09-17 21:26:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:26:28] {3072} INFO -  at 14.9s,	estimator xgboost's best error=3.8988,	best estimator xgboost's best error=3.8988
[flaml.automl: 09-17 21:26:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:26:31] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.8988,	best estimator xgboost's best error=3.8988
[flaml.automl: 09-17 21:26:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:26:32] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.8988,	best estimator xgboost's best error=3.8988
[flaml.automl: 09-17 21:26:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:26:35] {3072} INFO -  at 21.2s,	estimator xgboost's best error=3.8988,	best estimator xgboost's best error=3.8988
[flaml.automl: 09-17 21:26:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:26:36] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.8988,	best estimator xgboost's best error=3.8988
[flaml.automl: 09-17 21:26:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:26:38] {3072} INFO -  at 24.0s,	estimator xgboost's best error=3.8988,	best estimator xgboost's best error=3.8988
[flaml.automl: 09-17 21:26:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:26:44] {3072} INFO -  at 30.5s,	estimator xgboost's best error=3.7466,	best estimator xgboost's best error=3.7466
[flaml.automl: 09-17 21:26:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:26:56] {3072} INFO -  at 42.6s,	estimator xgboost's best error=3.6153,	best estimator xgboost's best error=3.6153
[flaml.automl: 09-17 21:26:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 21:27:03] {3072} INFO -  at 49.1s,	estimator xgboost's best error=3.6153,	best estimator xgboost's best error=3.6153
[flaml.automl: 09-17 21:27:15] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 21:27:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:27:15] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:27:15] {2637} INFO - Time taken to find the best model: 42.60538959503174
[flaml.automl: 09-17 21:27:15] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42950}
NO2(0)最佳损失：-2.6152698171480933
NO2(0)最好结果：{'pred_time': 8.562089211531663e-06, 'wall_clock_time': 42.60538959503174, 'metric_for_logging': {'pred_time': 8.562089211531663e-06}, 'val_loss': 3.6152698171480933, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42950}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42950, 'experiment_tag': 'exp', 'time_total_s': 12.093071699142456}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8580302042648691
NO2(0)的mse=32.415984438462985
NO2(0)的mae=3.6157890887119266
NO2(0)的mar=0.18931649371087686
总共花费的时间为：61.99
忻州市
2182A
3208A
[flaml.automl: 09-17 21:33:32] {2390} INFO - task = regression
[flaml.automl: 09-17 21:33:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:33:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:33:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:33:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:33:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:33:33] {3025} INFO - Estimated sufficient time budget=12184s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:33:33] {3072} INFO -  at 1.3s,	estimator xgboost's best error=18.0164,	best estimator xgboost's best error=18.0164
[flaml.automl: 09-17 21:33:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:33:36] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.9381,	best estimator xgboost's best error=8.9381
[flaml.automl: 09-17 21:33:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:33:37] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.9381,	best estimator xgboost's best error=8.9381
[flaml.automl: 09-17 21:33:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:33:46] {3072} INFO -  at 14.2s,	estimator xgboost's best error=8.9381,	best estimator xgboost's best error=8.9381
[flaml.automl: 09-17 21:33:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:33:47] {3072} INFO -  at 15.3s,	estimator xgboost's best error=6.1702,	best estimator xgboost's best error=6.1702
[flaml.automl: 09-17 21:33:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:33:49] {3072} INFO -  at 16.9s,	estimator xgboost's best error=5.3900,	best estimator xgboost's best error=5.3900
[flaml.automl: 09-17 21:33:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:33:51] {3072} INFO -  at 18.5s,	estimator xgboost's best error=5.3900,	best estimator xgboost's best error=5.3900
[flaml.automl: 09-17 21:33:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:33:53] {3072} INFO -  at 21.0s,	estimator xgboost's best error=5.3900,	best estimator xgboost's best error=5.3900
[flaml.automl: 09-17 21:33:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:33:54] {3072} INFO -  at 22.2s,	estimator xgboost's best error=5.3900,	best estimator xgboost's best error=5.3900
[flaml.automl: 09-17 21:33:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:33:57] {3072} INFO -  at 24.6s,	estimator xgboost's best error=5.3900,	best estimator xgboost's best error=5.3900
[flaml.automl: 09-17 21:33:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:33:58] {3072} INFO -  at 25.8s,	estimator xgboost's best error=5.3900,	best estimator xgboost's best error=5.3900
[flaml.automl: 09-17 21:33:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:33:59] {3072} INFO -  at 26.9s,	estimator xgboost's best error=5.3900,	best estimator xgboost's best error=5.3900
[flaml.automl: 09-17 21:33:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:34:05] {3072} INFO -  at 32.9s,	estimator xgboost's best error=5.3183,	best estimator xgboost's best error=5.3183
[flaml.automl: 09-17 21:34:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:34:15] {3072} INFO -  at 43.3s,	estimator xgboost's best error=5.1583,	best estimator xgboost's best error=5.1583
[flaml.automl: 09-17 21:34:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 21:34:21] {3072} INFO -  at 49.3s,	estimator xgboost's best error=5.1583,	best estimator xgboost's best error=5.1583
[flaml.automl: 09-17 21:34:21] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 21:34:31] {3072} INFO -  at 59.1s,	estimator xgboost's best error=5.1470,	best estimator xgboost's best error=5.1470
[flaml.automl: 09-17 21:34:49] {3335} INFO - retrain xgboost for 17.4s
[flaml.automl: 09-17 21:34:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:34:49] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:34:49] {2637} INFO - Time taken to find the best model: 59.07930827140808
[flaml.automl: 09-17 21:34:49] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-4.146983765256395
NO2(0)最好结果：{'pred_time': 1.709319460401842e-05, 'wall_clock_time': 59.07930827140808, 'metric_for_logging': {'pred_time': 1.709319460401842e-05}, 'val_loss': 5.146983765256395, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.74449372291565}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7703351363094757
NO2(0)的mse=56.40181041317414
NO2(0)的mae=5.111970726330012
NO2(0)的mar=0.23091507518596233
总共花费的时间为：76.83
吕梁市
2183A
2867A
[flaml.automl: 09-17 21:41:47] {2390} INFO - task = regression
[flaml.automl: 09-17 21:41:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:41:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:41:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:41:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:41:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:41:50] {3025} INFO - Estimated sufficient time budget=28480s. Estimated necessary time budget=28s.
[flaml.automl: 09-17 21:41:50] {3072} INFO -  at 3.0s,	estimator xgboost's best error=27.2827,	best estimator xgboost's best error=27.2827
[flaml.automl: 09-17 21:41:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:41:55] {3072} INFO -  at 8.1s,	estimator xgboost's best error=13.7635,	best estimator xgboost's best error=13.7635
[flaml.automl: 09-17 21:41:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:41:58] {3072} INFO -  at 11.0s,	estimator xgboost's best error=13.7635,	best estimator xgboost's best error=13.7635
[flaml.automl: 09-17 21:41:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:42:18] {3072} INFO -  at 30.7s,	estimator xgboost's best error=13.7635,	best estimator xgboost's best error=13.7635
[flaml.automl: 09-17 21:42:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:42:20] {3072} INFO -  at 32.9s,	estimator xgboost's best error=8.6067,	best estimator xgboost's best error=8.6067
[flaml.automl: 09-17 21:42:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:42:23] {3072} INFO -  at 35.7s,	estimator xgboost's best error=7.3236,	best estimator xgboost's best error=7.3236
[flaml.automl: 09-17 21:42:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:42:25] {3072} INFO -  at 38.6s,	estimator xgboost's best error=7.3225,	best estimator xgboost's best error=7.3225
[flaml.automl: 09-17 21:42:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:42:29] {3072} INFO -  at 41.7s,	estimator xgboost's best error=7.3225,	best estimator xgboost's best error=7.3225
[flaml.automl: 09-17 21:42:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:42:30] {3072} INFO -  at 43.3s,	estimator xgboost's best error=7.3225,	best estimator xgboost's best error=7.3225
[flaml.automl: 09-17 21:42:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:42:33] {3072} INFO -  at 46.3s,	estimator xgboost's best error=7.2891,	best estimator xgboost's best error=7.2891
[flaml.automl: 09-17 21:42:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:42:35] {3072} INFO -  at 48.0s,	estimator xgboost's best error=7.2891,	best estimator xgboost's best error=7.2891
[flaml.automl: 09-17 21:42:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:42:36] {3072} INFO -  at 49.1s,	estimator xgboost's best error=7.2891,	best estimator xgboost's best error=7.2891
[flaml.automl: 09-17 21:42:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:42:46] {3072} INFO -  at 59.4s,	estimator xgboost's best error=6.6816,	best estimator xgboost's best error=6.6816
[flaml.automl: 09-17 21:42:58] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-17 21:42:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 21:42:58] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:42:58] {2637} INFO - Time taken to find the best model: 59.39717245101929
[flaml.automl: 09-17 21:42:58] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
NO2(0)最佳损失：-5.681624750747093
NO2(0)最好结果：{'pred_time': 1.586913289668301e-05, 'wall_clock_time': 59.39717245101929, 'metric_for_logging': {'pred_time': 1.586913289668301e-05}, 'val_loss': 6.681624750747093, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 10.292663812637329}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7949346270631692
NO2(0)的mse=95.45957514487942
NO2(0)的mae=6.418401998540062
NO2(0)的mar=0.22792225535106556
总共花费的时间为：71.85
乌海市
2188A
3284A
3621A
[flaml.automl: 09-17 21:53:22] {2390} INFO - task = regression
[flaml.automl: 09-17 21:53:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:53:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:53:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:53:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:53:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:53:24] {3025} INFO - Estimated sufficient time budget=20469s. Estimated necessary time budget=20s.
[flaml.automl: 09-17 21:53:24] {3072} INFO -  at 2.2s,	estimator xgboost's best error=16.6340,	best estimator xgboost's best error=16.6340
[flaml.automl: 09-17 21:53:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:53:30] {3072} INFO -  at 7.5s,	estimator xgboost's best error=9.1803,	best estimator xgboost's best error=9.1803
[flaml.automl: 09-17 21:53:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:53:33] {3072} INFO -  at 10.9s,	estimator xgboost's best error=9.1803,	best estimator xgboost's best error=9.1803
[flaml.automl: 09-17 21:53:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:54:00] {3072} INFO -  at 37.8s,	estimator xgboost's best error=9.1803,	best estimator xgboost's best error=9.1803
[flaml.automl: 09-17 21:54:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:54:03] {3072} INFO -  at 40.9s,	estimator xgboost's best error=7.2183,	best estimator xgboost's best error=7.2183
[flaml.automl: 09-17 21:54:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:54:07] {3072} INFO -  at 45.2s,	estimator xgboost's best error=6.4871,	best estimator xgboost's best error=6.4871
[flaml.automl: 09-17 21:54:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:54:12] {3072} INFO -  at 49.5s,	estimator xgboost's best error=6.4871,	best estimator xgboost's best error=6.4871
[flaml.automl: 09-17 21:54:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:54:18] {3072} INFO -  at 56.2s,	estimator xgboost's best error=6.4871,	best estimator xgboost's best error=6.4871
[flaml.automl: 09-17 21:54:23] {3335} INFO - retrain xgboost for 4.2s
[flaml.automl: 09-17 21:54:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 21:54:23] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:54:23] {2637} INFO - Time taken to find the best model: 45.178839445114136
[flaml.automl: 09-17 21:54:23] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-5.487102678069821
NO2(0)最好结果：{'pred_time': 3.123767190165334e-05, 'wall_clock_time': 45.178839445114136, 'metric_for_logging': {'pred_time': 3.123767190165334e-05}, 'val_loss': 6.487102678069821, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 4.258527994155884}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.6802791642082875
NO2(0)的mse=105.39155886108081
NO2(0)的mae=6.719915198211633
NO2(0)的mar=0.38992331331244834
总共花费的时间为：61.07
通辽市
2191A
3706A
3708A
[flaml.automl: 09-17 22:03:45] {2390} INFO - task = regression
[flaml.automl: 09-17 22:03:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:03:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:03:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:03:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:03:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:03:46] {3025} INFO - Estimated sufficient time budget=12130s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 22:03:46] {3072} INFO -  at 1.4s,	estimator xgboost's best error=11.5950,	best estimator xgboost's best error=11.5950
[flaml.automl: 09-17 22:03:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:03:49] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.3264,	best estimator xgboost's best error=5.3264
[flaml.automl: 09-17 22:03:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:03:50] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.3264,	best estimator xgboost's best error=5.3264
[flaml.automl: 09-17 22:03:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:04:00] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.3264,	best estimator xgboost's best error=5.3264
[flaml.automl: 09-17 22:04:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:04:01] {3072} INFO -  at 15.9s,	estimator xgboost's best error=3.3936,	best estimator xgboost's best error=3.3936
[flaml.automl: 09-17 22:04:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:04:03] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.0753,	best estimator xgboost's best error=3.0753
[flaml.automl: 09-17 22:04:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:04:04] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.0753,	best estimator xgboost's best error=3.0753
[flaml.automl: 09-17 22:04:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:04:07] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.0753,	best estimator xgboost's best error=3.0753
[flaml.automl: 09-17 22:04:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:04:08] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.0753,	best estimator xgboost's best error=3.0753
[flaml.automl: 09-17 22:04:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:04:11] {3072} INFO -  at 25.4s,	estimator xgboost's best error=3.0753,	best estimator xgboost's best error=3.0753
[flaml.automl: 09-17 22:04:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:04:12] {3072} INFO -  at 26.6s,	estimator xgboost's best error=3.0753,	best estimator xgboost's best error=3.0753
[flaml.automl: 09-17 22:04:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:04:13] {3072} INFO -  at 27.7s,	estimator xgboost's best error=3.0753,	best estimator xgboost's best error=3.0753
[flaml.automl: 09-17 22:04:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:04:19] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.0166,	best estimator xgboost's best error=3.0166
[flaml.automl: 09-17 22:04:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:04:31] {3072} INFO -  at 46.3s,	estimator xgboost's best error=2.9966,	best estimator xgboost's best error=2.9966
[flaml.automl: 09-17 22:04:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:04:38] {3072} INFO -  at 53.3s,	estimator xgboost's best error=2.9966,	best estimator xgboost's best error=2.9966
[flaml.automl: 09-17 22:05:00] {3335} INFO - retrain xgboost for 21.9s
[flaml.automl: 09-17 22:05:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:05:00] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:05:00] {2637} INFO - Time taken to find the best model: 46.32526731491089
[flaml.automl: 09-17 22:05:00] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.9965546079228682
NO2(0)最好结果：{'pred_time': 1.3808830383035025e-05, 'wall_clock_time': 46.32526731491089, 'metric_for_logging': {'pred_time': 1.3808830383035025e-05}, 'val_loss': 2.996554607922868, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.075626611709595}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6892264079427779
NO2(0)的mse=21.830717961022007
NO2(0)的mae=3.0712913543585545
NO2(0)的mar=0.16424070328018486
总共花费的时间为：75.87
呼伦贝尔市
2192A
[flaml.automl: 09-17 22:08:35] {2390} INFO - task = regression
[flaml.automl: 09-17 22:08:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:08:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:08:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:08:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:08:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:08:36] {3025} INFO - Estimated sufficient time budget=12006s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 22:08:36] {3072} INFO -  at 1.3s,	estimator xgboost's best error=8.5311,	best estimator xgboost's best error=8.5311
[flaml.automl: 09-17 22:08:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:08:38] {3072} INFO -  at 3.1s,	estimator xgboost's best error=4.8426,	best estimator xgboost's best error=4.8426
[flaml.automl: 09-17 22:08:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:08:40] {3072} INFO -  at 4.3s,	estimator xgboost's best error=4.8426,	best estimator xgboost's best error=4.8426
[flaml.automl: 09-17 22:08:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:08:47] {3072} INFO -  at 11.4s,	estimator xgboost's best error=4.8426,	best estimator xgboost's best error=4.8426
[flaml.automl: 09-17 22:08:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:08:48] {3072} INFO -  at 12.5s,	estimator xgboost's best error=3.4878,	best estimator xgboost's best error=3.4878
[flaml.automl: 09-17 22:08:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:08:49] {3072} INFO -  at 14.1s,	estimator xgboost's best error=3.4539,	best estimator xgboost's best error=3.4539
[flaml.automl: 09-17 22:08:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:08:51] {3072} INFO -  at 15.7s,	estimator xgboost's best error=3.3941,	best estimator xgboost's best error=3.3941
[flaml.automl: 09-17 22:08:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:08:53] {3072} INFO -  at 18.1s,	estimator xgboost's best error=3.3941,	best estimator xgboost's best error=3.3941
[flaml.automl: 09-17 22:08:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:08:55] {3072} INFO -  at 19.7s,	estimator xgboost's best error=3.2272,	best estimator xgboost's best error=3.2272
[flaml.automl: 09-17 22:08:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:08:58] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.2272,	best estimator xgboost's best error=3.2272
[flaml.automl: 09-17 22:08:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:09:00] {3072} INFO -  at 25.1s,	estimator xgboost's best error=3.2203,	best estimator xgboost's best error=3.2203
[flaml.automl: 09-17 22:09:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:09:02] {3072} INFO -  at 26.9s,	estimator xgboost's best error=3.2203,	best estimator xgboost's best error=3.2203
[flaml.automl: 09-17 22:09:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:09:10] {3072} INFO -  at 35.2s,	estimator xgboost's best error=3.1026,	best estimator xgboost's best error=3.1026
[flaml.automl: 09-17 22:09:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:09:26] {3072} INFO -  at 50.8s,	estimator xgboost's best error=3.0945,	best estimator xgboost's best error=3.0945
[flaml.automl: 09-17 22:09:46] {3335} INFO - retrain xgboost for 19.7s
[flaml.automl: 09-17 22:09:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 22:09:46] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:09:46] {2637} INFO - Time taken to find the best model: 50.77744650840759
[flaml.automl: 09-17 22:09:46] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}
NO2(0)最佳损失：-2.0944971945483473
NO2(0)最好结果：{'pred_time': 7.41598771509987e-05, 'wall_clock_time': 50.77744650840759, 'metric_for_logging': {'pred_time': 7.41598771509987e-05}, 'val_loss': 3.0944971945483473, 'training_iteration': 1, 'config': {'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}, 'config/n_estimators': 25, 'config/max_leaves': 6, 'config/min_child_weight': 2.978767810916454, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532448312341252, 'config/reg_alpha': 0.0015779591896894877, 'config/reg_lambda': 1.3500086298278042, 'experiment_tag': 'exp', 'time_total_s': 15.620135068893433}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.6486639498498609
NO2(0)的mse=21.763219558722692
NO2(0)的mae=3.141208174931395
NO2(0)的mar=0.25372692106421607
总共花费的时间为：70.77
巴彦淖尔市
2196A
[flaml.automl: 09-17 22:13:03] {2390} INFO - task = regression
[flaml.automl: 09-17 22:13:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:13:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:13:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:13:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:13:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:13:05] {3025} INFO - Estimated sufficient time budget=22553s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 22:13:05] {3072} INFO -  at 2.3s,	estimator xgboost's best error=8.7550,	best estimator xgboost's best error=8.7550
[flaml.automl: 09-17 22:13:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:13:09] {3072} INFO -  at 5.7s,	estimator xgboost's best error=5.1492,	best estimator xgboost's best error=5.1492
[flaml.automl: 09-17 22:13:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:13:11] {3072} INFO -  at 7.9s,	estimator xgboost's best error=5.1492,	best estimator xgboost's best error=5.1492
[flaml.automl: 09-17 22:13:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:13:24] {3072} INFO -  at 20.8s,	estimator xgboost's best error=5.1492,	best estimator xgboost's best error=5.1492
[flaml.automl: 09-17 22:13:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:13:26] {3072} INFO -  at 22.9s,	estimator xgboost's best error=3.6456,	best estimator xgboost's best error=3.6456
[flaml.automl: 09-17 22:13:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:13:29] {3072} INFO -  at 25.8s,	estimator xgboost's best error=3.6344,	best estimator xgboost's best error=3.6344
[flaml.automl: 09-17 22:13:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:13:31] {3072} INFO -  at 27.6s,	estimator xgboost's best error=3.4984,	best estimator xgboost's best error=3.4984
[flaml.automl: 09-17 22:13:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:13:33] {3072} INFO -  at 30.0s,	estimator xgboost's best error=3.4984,	best estimator xgboost's best error=3.4984
[flaml.automl: 09-17 22:13:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:13:34] {3072} INFO -  at 31.6s,	estimator xgboost's best error=3.3529,	best estimator xgboost's best error=3.3529
[flaml.automl: 09-17 22:13:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:13:37] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.3529,	best estimator xgboost's best error=3.3529
[flaml.automl: 09-17 22:13:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:13:39] {3072} INFO -  at 35.9s,	estimator xgboost's best error=3.3269,	best estimator xgboost's best error=3.3269
[flaml.automl: 09-17 22:13:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:13:40] {3072} INFO -  at 37.0s,	estimator xgboost's best error=3.3269,	best estimator xgboost's best error=3.3269
[flaml.automl: 09-17 22:13:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:13:46] {3072} INFO -  at 42.8s,	estimator xgboost's best error=3.3048,	best estimator xgboost's best error=3.3048
[flaml.automl: 09-17 22:13:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:13:55] {3072} INFO -  at 51.6s,	estimator xgboost's best error=3.2168,	best estimator xgboost's best error=3.2168
[flaml.automl: 09-17 22:14:03] {3335} INFO - retrain xgboost for 8.9s
[flaml.automl: 09-17 22:14:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 22:14:03] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:14:03] {2637} INFO - Time taken to find the best model: 51.62858605384827
[flaml.automl: 09-17 22:14:03] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}
NO2(0)最佳损失：-2.2167850485426253
NO2(0)最好结果：{'pred_time': 3.328250067341949e-05, 'wall_clock_time': 51.62858605384827, 'metric_for_logging': {'pred_time': 3.328250067341949e-05}, 'val_loss': 3.2167850485426253, 'training_iteration': 1, 'config': {'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}, 'config/n_estimators': 25, 'config/max_leaves': 6, 'config/min_child_weight': 2.978767810916454, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532448312341252, 'config/reg_alpha': 0.0015779591896894877, 'config/reg_lambda': 1.3500086298278042, 'experiment_tag': 'exp', 'time_total_s': 8.872598886489868}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.70327186558893
NO2(0)的mse=23.085927403324025
NO2(0)的mae=3.110388498687451
NO2(0)的mar=0.261410409671406
总共花费的时间为：60.71
乌兰察布市
2197A
3285A
3421A
[flaml.automl: 09-17 22:23:57] {2390} INFO - task = regression
[flaml.automl: 09-17 22:23:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:23:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:23:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:23:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:23:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:23:59] {3025} INFO - Estimated sufficient time budget=23293s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 22:23:59] {3072} INFO -  at 2.5s,	estimator xgboost's best error=11.9932,	best estimator xgboost's best error=11.9932
[flaml.automl: 09-17 22:23:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:24:03] {3072} INFO -  at 6.1s,	estimator xgboost's best error=6.3190,	best estimator xgboost's best error=6.3190
[flaml.automl: 09-17 22:24:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:24:05] {3072} INFO -  at 8.2s,	estimator xgboost's best error=6.3190,	best estimator xgboost's best error=6.3190
[flaml.automl: 09-17 22:24:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:24:22] {3072} INFO -  at 24.9s,	estimator xgboost's best error=6.3190,	best estimator xgboost's best error=6.3190
[flaml.automl: 09-17 22:24:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:24:23] {3072} INFO -  at 26.4s,	estimator xgboost's best error=4.9204,	best estimator xgboost's best error=4.9204
[flaml.automl: 09-17 22:24:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:24:25] {3072} INFO -  at 28.3s,	estimator xgboost's best error=4.4193,	best estimator xgboost's best error=4.4193
[flaml.automl: 09-17 22:24:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:24:28] {3072} INFO -  at 31.3s,	estimator xgboost's best error=4.4193,	best estimator xgboost's best error=4.4193
[flaml.automl: 09-17 22:24:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:24:33] {3072} INFO -  at 35.8s,	estimator xgboost's best error=4.4193,	best estimator xgboost's best error=4.4193
[flaml.automl: 09-17 22:24:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:24:35] {3072} INFO -  at 37.9s,	estimator xgboost's best error=4.4193,	best estimator xgboost's best error=4.4193
[flaml.automl: 09-17 22:24:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:24:40] {3072} INFO -  at 42.9s,	estimator xgboost's best error=4.4193,	best estimator xgboost's best error=4.4193
[flaml.automl: 09-17 22:24:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:24:42] {3072} INFO -  at 45.0s,	estimator xgboost's best error=4.4193,	best estimator xgboost's best error=4.4193
[flaml.automl: 09-17 22:24:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:24:44] {3072} INFO -  at 47.2s,	estimator xgboost's best error=4.4193,	best estimator xgboost's best error=4.4193
[flaml.automl: 09-17 22:24:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:24:56] {3072} INFO -  at 59.3s,	estimator xgboost's best error=4.3313,	best estimator xgboost's best error=4.3313
[flaml.automl: 09-17 22:25:11] {3335} INFO - retrain xgboost for 15.2s
[flaml.automl: 09-17 22:25:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 22:25:11] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:25:11] {2637} INFO - Time taken to find the best model: 59.2877676486969
[flaml.automl: 09-17 22:25:11] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-3.331295893128722
NO2(0)最好结果：{'pred_time': 2.2301967488729802e-05, 'wall_clock_time': 59.2877676486969, 'metric_for_logging': {'pred_time': 2.2301967488729802e-05}, 'val_loss': 4.331295893128722, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 12.086236476898193}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7964602352100334
NO2(0)的mse=39.14461055178491
NO2(0)的mae=4.201782408340446
NO2(0)的mar=0.31991927020756056
总共花费的时间为：75.26
阜新市
2207A
2208A
2209A
2210A
2211A
[flaml.automl: 09-17 22:40:59] {2390} INFO - task = regression
[flaml.automl: 09-17 22:40:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:40:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:40:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:40:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:40:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:41:00] {3025} INFO - Estimated sufficient time budget=69567s. Estimated necessary time budget=70s.
[flaml.automl: 09-17 22:41:00] {3072} INFO -  at 1.5s,	estimator xgboost's best error=12.6361,	best estimator xgboost's best error=12.6361
[flaml.automl: 09-17 22:41:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:41:02] {3072} INFO -  at 3.6s,	estimator xgboost's best error=6.3180,	best estimator xgboost's best error=6.3180
[flaml.automl: 09-17 22:41:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:41:04] {3072} INFO -  at 4.8s,	estimator xgboost's best error=6.3180,	best estimator xgboost's best error=6.3180
[flaml.automl: 09-17 22:41:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:41:08] {3072} INFO -  at 9.6s,	estimator xgboost's best error=6.3180,	best estimator xgboost's best error=6.3180
[flaml.automl: 09-17 22:41:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:41:10] {3072} INFO -  at 10.8s,	estimator xgboost's best error=4.2790,	best estimator xgboost's best error=4.2790
[flaml.automl: 09-17 22:41:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:41:11] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.7600,	best estimator xgboost's best error=3.7600
[flaml.automl: 09-17 22:41:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:41:13] {3072} INFO -  at 14.0s,	estimator xgboost's best error=3.7600,	best estimator xgboost's best error=3.7600
[flaml.automl: 09-17 22:41:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:41:15] {3072} INFO -  at 16.4s,	estimator xgboost's best error=3.7600,	best estimator xgboost's best error=3.7600
[flaml.automl: 09-17 22:41:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:41:16] {3072} INFO -  at 17.6s,	estimator xgboost's best error=3.7600,	best estimator xgboost's best error=3.7600
[flaml.automl: 09-17 22:41:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:41:19] {3072} INFO -  at 20.2s,	estimator xgboost's best error=3.7600,	best estimator xgboost's best error=3.7600
[flaml.automl: 09-17 22:41:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:41:21] {3072} INFO -  at 21.9s,	estimator xgboost's best error=3.7434,	best estimator xgboost's best error=3.7434
[flaml.automl: 09-17 22:41:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:41:22] {3072} INFO -  at 23.0s,	estimator xgboost's best error=3.7434,	best estimator xgboost's best error=3.7434
[flaml.automl: 09-17 22:41:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:41:28] {3072} INFO -  at 29.6s,	estimator xgboost's best error=3.5477,	best estimator xgboost's best error=3.5477
[flaml.automl: 09-17 22:41:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:41:40] {3072} INFO -  at 41.7s,	estimator xgboost's best error=3.4925,	best estimator xgboost's best error=3.4925
[flaml.automl: 09-17 22:41:40] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:41:47] {3072} INFO -  at 48.2s,	estimator xgboost's best error=3.4925,	best estimator xgboost's best error=3.4925
[flaml.automl: 09-17 22:41:59] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 22:41:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:41:59] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:41:59] {2637} INFO - Time taken to find the best model: 41.68050980567932
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54623}
NO2(0)最佳损失：-2.492525048350188
NO2(0)最好结果：{'pred_time': 7.013276931285073e-06, 'wall_clock_time': 41.68050980567932, 'metric_for_logging': {'pred_time': 7.013276931285073e-06}, 'val_loss': 3.492525048350188, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54623}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54623, 'experiment_tag': 'exp', 'time_total_s': 12.125991344451904}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8493936412821035
NO2(0)的mse=27.38635587771409
NO2(0)的mae=3.465983789144603
NO2(0)的mar=0.2254849872621006
总共花费的时间为：61.14
辽阳市
2212A
2213A
2214A
2215A
[flaml.automl: 09-17 22:54:34] {2390} INFO - task = regression
[flaml.automl: 09-17 22:54:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:54:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:54:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:54:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:54:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:54:35] {3025} INFO - Estimated sufficient time budget=52490s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 22:54:35] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.8046,	best estimator xgboost's best error=15.8046
[flaml.automl: 09-17 22:54:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:54:37] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.8982,	best estimator xgboost's best error=7.8982
[flaml.automl: 09-17 22:54:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:54:38] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.8982,	best estimator xgboost's best error=7.8982
[flaml.automl: 09-17 22:54:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:54:44] {3072} INFO -  at 10.6s,	estimator xgboost's best error=7.8982,	best estimator xgboost's best error=7.8982
[flaml.automl: 09-17 22:54:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:54:45] {3072} INFO -  at 11.7s,	estimator xgboost's best error=5.6064,	best estimator xgboost's best error=5.6064
[flaml.automl: 09-17 22:54:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:54:47] {3072} INFO -  at 13.3s,	estimator xgboost's best error=4.9417,	best estimator xgboost's best error=4.9417
[flaml.automl: 09-17 22:54:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:54:48] {3072} INFO -  at 14.9s,	estimator xgboost's best error=4.9417,	best estimator xgboost's best error=4.9417
[flaml.automl: 09-17 22:54:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:54:51] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.9417,	best estimator xgboost's best error=4.9417
[flaml.automl: 09-17 22:54:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:54:52] {3072} INFO -  at 18.5s,	estimator xgboost's best error=4.9417,	best estimator xgboost's best error=4.9417
[flaml.automl: 09-17 22:54:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:54:55] {3072} INFO -  at 21.2s,	estimator xgboost's best error=4.9417,	best estimator xgboost's best error=4.9417
[flaml.automl: 09-17 22:54:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:54:56] {3072} INFO -  at 22.8s,	estimator xgboost's best error=4.8664,	best estimator xgboost's best error=4.8664
[flaml.automl: 09-17 22:54:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:54:57] {3072} INFO -  at 24.0s,	estimator xgboost's best error=4.8664,	best estimator xgboost's best error=4.8664
[flaml.automl: 09-17 22:54:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:55:04] {3072} INFO -  at 30.6s,	estimator xgboost's best error=4.7268,	best estimator xgboost's best error=4.7268
[flaml.automl: 09-17 22:55:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:55:16] {3072} INFO -  at 42.6s,	estimator xgboost's best error=4.6058,	best estimator xgboost's best error=4.6058
[flaml.automl: 09-17 22:55:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:55:22] {3072} INFO -  at 49.1s,	estimator xgboost's best error=4.6058,	best estimator xgboost's best error=4.6058
[flaml.automl: 09-17 22:55:35] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 22:55:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:55:35] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:55:35] {2637} INFO - Time taken to find the best model: 42.60748481750488
[flaml.automl: 09-17 22:55:35] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43758}
NO2(0)最佳损失：-3.605788379202844
NO2(0)最好结果：{'pred_time': 8.079266808000142e-06, 'wall_clock_time': 42.60748481750488, 'metric_for_logging': {'pred_time': 8.079266808000142e-06}, 'val_loss': 4.605788379202844, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43758}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43758, 'experiment_tag': 'exp', 'time_total_s': 12.04803991317749}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7883301898668309
NO2(0)的mse=48.77803873179001
NO2(0)的mae=4.611809620587181
NO2(0)的mar=0.2311269143524677
总共花费的时间为：61.98
铁岭市
2216A
2217A
2218A
2219A
[flaml.automl: 09-17 23:08:46] {2390} INFO - task = regression
[flaml.automl: 09-17 23:08:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:08:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:08:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:08:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:08:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:08:48] {3025} INFO - Estimated sufficient time budget=96634s. Estimated necessary time budget=97s.
[flaml.automl: 09-17 23:08:48] {3072} INFO -  at 2.5s,	estimator xgboost's best error=16.0041,	best estimator xgboost's best error=16.0041
[flaml.automl: 09-17 23:08:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:08:52] {3072} INFO -  at 6.4s,	estimator xgboost's best error=7.7768,	best estimator xgboost's best error=7.7768
[flaml.automl: 09-17 23:08:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:08:54] {3072} INFO -  at 8.7s,	estimator xgboost's best error=7.7768,	best estimator xgboost's best error=7.7768
[flaml.automl: 09-17 23:08:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:08:59] {3072} INFO -  at 13.8s,	estimator xgboost's best error=7.7768,	best estimator xgboost's best error=7.7768
[flaml.automl: 09-17 23:08:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:09:02] {3072} INFO -  at 16.0s,	estimator xgboost's best error=5.2967,	best estimator xgboost's best error=5.2967
[flaml.automl: 09-17 23:09:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:09:04] {3072} INFO -  at 18.8s,	estimator xgboost's best error=4.7829,	best estimator xgboost's best error=4.7829
[flaml.automl: 09-17 23:09:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:09:07] {3072} INFO -  at 21.8s,	estimator xgboost's best error=4.7829,	best estimator xgboost's best error=4.7829
[flaml.automl: 09-17 23:09:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:09:11] {3072} INFO -  at 25.3s,	estimator xgboost's best error=4.7829,	best estimator xgboost's best error=4.7829
[flaml.automl: 09-17 23:09:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:09:13] {3072} INFO -  at 27.4s,	estimator xgboost's best error=4.7829,	best estimator xgboost's best error=4.7829
[flaml.automl: 09-17 23:09:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:09:16] {3072} INFO -  at 30.1s,	estimator xgboost's best error=4.7829,	best estimator xgboost's best error=4.7829
[flaml.automl: 09-17 23:09:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:09:19] {3072} INFO -  at 33.2s,	estimator xgboost's best error=4.7470,	best estimator xgboost's best error=4.7470
[flaml.automl: 09-17 23:09:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:09:21] {3072} INFO -  at 35.3s,	estimator xgboost's best error=4.7470,	best estimator xgboost's best error=4.7470
[flaml.automl: 09-17 23:09:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:09:33] {3072} INFO -  at 47.2s,	estimator xgboost's best error=4.7108,	best estimator xgboost's best error=4.7108
[flaml.automl: 09-17 23:09:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:09:45] {3072} INFO -  at 59.3s,	estimator xgboost's best error=4.6051,	best estimator xgboost's best error=4.6051
[flaml.automl: 09-17 23:09:57] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 23:09:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:09:57] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:09:57] {2637} INFO - Time taken to find the best model: 59.266947507858276
[flaml.automl: 09-17 23:09:57] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43239}
NO2(0)最佳损失：-3.605062431574612
NO2(0)最好结果：{'pred_time': 8.155255114250699e-06, 'wall_clock_time': 59.266947507858276, 'metric_for_logging': {'pred_time': 8.155255114250699e-06}, 'val_loss': 4.605062431574612, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43239}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43239, 'experiment_tag': 'exp', 'time_total_s': 12.043831825256348}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7407676855254535
NO2(0)的mse=44.2821972530728
NO2(0)的mae=4.547203987697288
NO2(0)的mar=0.21751684613901615
总共花费的时间为：72.09
朝阳市
2220A
2221A
2222A
2223A
[flaml.automl: 09-17 23:23:45] {2390} INFO - task = regression
[flaml.automl: 09-17 23:23:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:23:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:23:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:23:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:23:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:23:47] {3025} INFO - Estimated sufficient time budget=52677s. Estimated necessary time budget=53s.
[flaml.automl: 09-17 23:23:47] {3072} INFO -  at 1.4s,	estimator xgboost's best error=11.8658,	best estimator xgboost's best error=11.8658
[flaml.automl: 09-17 23:23:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:23:49] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.1758,	best estimator xgboost's best error=6.1758
[flaml.automl: 09-17 23:23:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:23:50] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.1758,	best estimator xgboost's best error=6.1758
[flaml.automl: 09-17 23:23:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:23:56] {3072} INFO -  at 10.6s,	estimator xgboost's best error=6.1758,	best estimator xgboost's best error=6.1758
[flaml.automl: 09-17 23:23:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:23:57] {3072} INFO -  at 11.8s,	estimator xgboost's best error=4.5049,	best estimator xgboost's best error=4.5049
[flaml.automl: 09-17 23:23:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:23:59] {3072} INFO -  at 13.3s,	estimator xgboost's best error=4.0490,	best estimator xgboost's best error=4.0490
[flaml.automl: 09-17 23:23:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:24:00] {3072} INFO -  at 15.0s,	estimator xgboost's best error=4.0490,	best estimator xgboost's best error=4.0490
[flaml.automl: 09-17 23:24:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:24:03] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.0490,	best estimator xgboost's best error=4.0490
[flaml.automl: 09-17 23:24:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:24:04] {3072} INFO -  at 18.5s,	estimator xgboost's best error=4.0490,	best estimator xgboost's best error=4.0490
[flaml.automl: 09-17 23:24:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:24:06] {3072} INFO -  at 21.2s,	estimator xgboost's best error=4.0490,	best estimator xgboost's best error=4.0490
[flaml.automl: 09-17 23:24:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:24:08] {3072} INFO -  at 22.8s,	estimator xgboost's best error=4.0328,	best estimator xgboost's best error=4.0328
[flaml.automl: 09-17 23:24:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:24:09] {3072} INFO -  at 24.0s,	estimator xgboost's best error=4.0328,	best estimator xgboost's best error=4.0328
[flaml.automl: 09-17 23:24:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:24:16] {3072} INFO -  at 30.5s,	estimator xgboost's best error=3.9387,	best estimator xgboost's best error=3.9387
[flaml.automl: 09-17 23:24:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:24:28] {3072} INFO -  at 42.6s,	estimator xgboost's best error=3.8508,	best estimator xgboost's best error=3.8508
[flaml.automl: 09-17 23:24:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 23:24:34] {3072} INFO -  at 49.2s,	estimator xgboost's best error=3.8508,	best estimator xgboost's best error=3.8508
[flaml.automl: 09-17 23:24:47] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 23:24:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:24:47] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:24:47] {2637} INFO - Time taken to find the best model: 42.63089394569397
[flaml.automl: 09-17 23:24:47] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43826}
NO2(0)最佳损失：-2.850753452988376
NO2(0)最好结果：{'pred_time': 8.310282744421362e-06, 'wall_clock_time': 42.63089394569397, 'metric_for_logging': {'pred_time': 8.310282744421362e-06}, 'val_loss': 3.850753452988376, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43826}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43826, 'experiment_tag': 'exp', 'time_total_s': 12.108195066452026}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7820448928943854
NO2(0)的mse=30.806857257668312
NO2(0)的mae=3.7264425053743664
NO2(0)的mar=0.256195878636804
总共花费的时间为：62.03
四平市
2226A
3486A
3713A
[flaml.automl: 09-17 23:34:04] {2390} INFO - task = regression
[flaml.automl: 09-17 23:34:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:34:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:34:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:34:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:34:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:34:06] {3025} INFO - Estimated sufficient time budget=12085s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 23:34:06] {3072} INFO -  at 1.4s,	estimator xgboost's best error=14.1846,	best estimator xgboost's best error=14.1846
[flaml.automl: 09-17 23:34:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:34:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.0533,	best estimator xgboost's best error=7.0533
[flaml.automl: 09-17 23:34:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:34:09] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.0533,	best estimator xgboost's best error=7.0533
[flaml.automl: 09-17 23:34:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:34:19] {3072} INFO -  at 14.7s,	estimator xgboost's best error=7.0533,	best estimator xgboost's best error=7.0533
[flaml.automl: 09-17 23:34:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:34:20] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.0845,	best estimator xgboost's best error=5.0845
[flaml.automl: 09-17 23:34:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:34:22] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.5842,	best estimator xgboost's best error=4.5842
[flaml.automl: 09-17 23:34:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:34:23] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.5842,	best estimator xgboost's best error=4.5842
[flaml.automl: 09-17 23:34:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:34:26] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.5842,	best estimator xgboost's best error=4.5842
[flaml.automl: 09-17 23:34:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:34:27] {3072} INFO -  at 22.8s,	estimator xgboost's best error=4.5842,	best estimator xgboost's best error=4.5842
[flaml.automl: 09-17 23:34:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:34:30] {3072} INFO -  at 25.5s,	estimator xgboost's best error=4.5842,	best estimator xgboost's best error=4.5842
[flaml.automl: 09-17 23:34:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:34:31] {3072} INFO -  at 26.6s,	estimator xgboost's best error=4.5842,	best estimator xgboost's best error=4.5842
[flaml.automl: 09-17 23:34:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:34:32] {3072} INFO -  at 27.8s,	estimator xgboost's best error=4.5842,	best estimator xgboost's best error=4.5842
[flaml.automl: 09-17 23:34:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:34:44] {3072} INFO -  at 39.5s,	estimator xgboost's best error=4.4683,	best estimator xgboost's best error=4.4683
[flaml.automl: 09-17 23:34:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:35:04] {3072} INFO -  at 59.6s,	estimator xgboost's best error=4.3796,	best estimator xgboost's best error=4.3796
[flaml.automl: 09-17 23:35:26] {3335} INFO - retrain xgboost for 22.2s
[flaml.automl: 09-17 23:35:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:35:26] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:35:26] {2637} INFO - Time taken to find the best model: 59.57283139228821
[flaml.automl: 09-17 23:35:26] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.3795707181857573
NO2(0)最好结果：{'pred_time': 2.3659099183645836e-05, 'wall_clock_time': 59.57283139228821, 'metric_for_logging': {'pred_time': 2.3659099183645836e-05}, 'val_loss': 4.379570718185757, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 20.078751802444458}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7863251688256873
NO2(0)的mse=42.598758421133724
NO2(0)的mae=4.510071003068539
NO2(0)的mar=0.2510987716447974
总共花费的时间为：82.36
辽源市
2227A
[flaml.automl: 09-17 23:39:04] {2390} INFO - task = regression
[flaml.automl: 09-17 23:39:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:39:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:39:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:39:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:39:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:39:06] {3025} INFO - Estimated sufficient time budget=11949s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 23:39:06] {3072} INFO -  at 1.3s,	estimator xgboost's best error=10.8120,	best estimator xgboost's best error=10.8120
[flaml.automl: 09-17 23:39:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:39:08] {3072} INFO -  at 3.1s,	estimator xgboost's best error=6.0608,	best estimator xgboost's best error=6.0608
[flaml.automl: 09-17 23:39:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:39:09] {3072} INFO -  at 4.3s,	estimator xgboost's best error=6.0608,	best estimator xgboost's best error=6.0608
[flaml.automl: 09-17 23:39:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:39:16] {3072} INFO -  at 11.4s,	estimator xgboost's best error=6.0608,	best estimator xgboost's best error=6.0608
[flaml.automl: 09-17 23:39:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:39:17] {3072} INFO -  at 12.5s,	estimator xgboost's best error=3.7863,	best estimator xgboost's best error=3.7863
[flaml.automl: 09-17 23:39:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:39:19] {3072} INFO -  at 14.1s,	estimator xgboost's best error=3.5714,	best estimator xgboost's best error=3.5714
[flaml.automl: 09-17 23:39:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:39:20] {3072} INFO -  at 15.7s,	estimator xgboost's best error=3.4410,	best estimator xgboost's best error=3.4410
[flaml.automl: 09-17 23:39:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:39:22] {3072} INFO -  at 18.1s,	estimator xgboost's best error=3.4410,	best estimator xgboost's best error=3.4410
[flaml.automl: 09-17 23:39:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:39:24] {3072} INFO -  at 19.7s,	estimator xgboost's best error=3.4114,	best estimator xgboost's best error=3.4114
[flaml.automl: 09-17 23:39:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:39:27] {3072} INFO -  at 22.3s,	estimator xgboost's best error=3.4114,	best estimator xgboost's best error=3.4114
[flaml.automl: 09-17 23:39:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:39:28] {3072} INFO -  at 24.0s,	estimator xgboost's best error=3.4114,	best estimator xgboost's best error=3.4114
[flaml.automl: 09-17 23:39:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:39:30] {3072} INFO -  at 25.2s,	estimator xgboost's best error=3.4114,	best estimator xgboost's best error=3.4114
[flaml.automl: 09-17 23:39:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:39:35] {3072} INFO -  at 30.9s,	estimator xgboost's best error=3.2726,	best estimator xgboost's best error=3.2726
[flaml.automl: 09-17 23:39:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:39:44] {3072} INFO -  at 39.8s,	estimator xgboost's best error=3.2026,	best estimator xgboost's best error=3.2026
[flaml.automl: 09-17 23:39:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 23:39:49] {3072} INFO -  at 44.9s,	estimator xgboost's best error=3.2026,	best estimator xgboost's best error=3.2026
[flaml.automl: 09-17 23:39:49] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 23:40:04] {3072} INFO -  at 59.3s,	estimator xgboost's best error=3.2026,	best estimator xgboost's best error=3.2026
[flaml.automl: 09-17 23:40:13] {3335} INFO - retrain xgboost for 8.9s
[flaml.automl: 09-17 23:40:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 23:40:13] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:40:13] {2637} INFO - Time taken to find the best model: 39.83705711364746
NO2(0)最佳参数：{'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}
NO2(0)最佳损失：-2.202581463120847
NO2(0)最好结果：{'pred_time': 3.498195120393489e-05, 'wall_clock_time': 39.83705711364746, 'metric_for_logging': {'pred_time': 3.498195120393489e-05}, 'val_loss': 3.202581463120847, 'training_iteration': 1, 'config': {'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}, 'config/n_estimators': 25, 'config/max_leaves': 6, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'experiment_tag': 'exp', 'time_total_s': 8.980913400650024}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7496318723807427
NO2(0)的mse=27.19794545112798
NO2(0)的mae=3.481042062294643
NO2(0)的mar=0.2545276529242393
总共花费的时间为：68.58
通化市
2229A
2230A
[flaml.automl: 09-17 23:46:25] {2390} INFO - task = regression
[flaml.automl: 09-17 23:46:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:46:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:46:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:46:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:46:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:46:29] {3025} INFO - Estimated sufficient time budget=33499s. Estimated necessary time budget=33s.
[flaml.automl: 09-17 23:46:29] {3072} INFO -  at 3.5s,	estimator xgboost's best error=11.9755,	best estimator xgboost's best error=11.9755
[flaml.automl: 09-17 23:46:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:46:32] {3072} INFO -  at 7.4s,	estimator xgboost's best error=6.1965,	best estimator xgboost's best error=6.1965
[flaml.automl: 09-17 23:46:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:46:35] {3072} INFO -  at 9.8s,	estimator xgboost's best error=6.1965,	best estimator xgboost's best error=6.1965
[flaml.automl: 09-17 23:46:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:46:44] {3072} INFO -  at 19.4s,	estimator xgboost's best error=6.1965,	best estimator xgboost's best error=6.1965
[flaml.automl: 09-17 23:46:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:46:46] {3072} INFO -  at 20.6s,	estimator xgboost's best error=4.3875,	best estimator xgboost's best error=4.3875
[flaml.automl: 09-17 23:46:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:46:47] {3072} INFO -  at 22.1s,	estimator xgboost's best error=3.9545,	best estimator xgboost's best error=3.9545
[flaml.automl: 09-17 23:46:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:46:49] {3072} INFO -  at 23.7s,	estimator xgboost's best error=3.9545,	best estimator xgboost's best error=3.9545
[flaml.automl: 09-17 23:46:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:46:51] {3072} INFO -  at 26.1s,	estimator xgboost's best error=3.9545,	best estimator xgboost's best error=3.9545
[flaml.automl: 09-17 23:46:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:46:52] {3072} INFO -  at 27.3s,	estimator xgboost's best error=3.9545,	best estimator xgboost's best error=3.9545
[flaml.automl: 09-17 23:46:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:46:55] {3072} INFO -  at 29.7s,	estimator xgboost's best error=3.9545,	best estimator xgboost's best error=3.9545
[flaml.automl: 09-17 23:46:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:46:56] {3072} INFO -  at 30.8s,	estimator xgboost's best error=3.9545,	best estimator xgboost's best error=3.9545
[flaml.automl: 09-17 23:46:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:46:57] {3072} INFO -  at 31.9s,	estimator xgboost's best error=3.9545,	best estimator xgboost's best error=3.9545
[flaml.automl: 09-17 23:46:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:47:03] {3072} INFO -  at 37.9s,	estimator xgboost's best error=3.9437,	best estimator xgboost's best error=3.9437
[flaml.automl: 09-17 23:47:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:47:13] {3072} INFO -  at 48.3s,	estimator xgboost's best error=3.8622,	best estimator xgboost's best error=3.8622
[flaml.automl: 09-17 23:47:13] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 23:47:19] {3072} INFO -  at 54.3s,	estimator xgboost's best error=3.8622,	best estimator xgboost's best error=3.8622
[flaml.automl: 09-17 23:47:30] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-17 23:47:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:47:30] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:47:30] {2637} INFO - Time taken to find the best model: 48.28811430931091
[flaml.automl: 09-17 23:47:30] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.8621765261205407
NO2(0)最好结果：{'pred_time': 1.6183561168034608e-05, 'wall_clock_time': 48.28811430931091, 'metric_for_logging': {'pred_time': 1.6183561168034608e-05}, 'val_loss': 3.8621765261205407, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.398069858551025}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7476750315042241
NO2(0)的mse=33.18838249446019
NO2(0)的mae=3.7933417287396765
NO2(0)的mar=0.24632344254716865
总共花费的时间为：65.31
白山市
2231A
2232A
[flaml.automl: 09-17 23:54:06] {2390} INFO - task = regression
[flaml.automl: 09-17 23:54:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:54:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:54:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:54:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:54:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:54:07] {3025} INFO - Estimated sufficient time budget=12150s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 23:54:07] {3072} INFO -  at 1.3s,	estimator xgboost's best error=12.8481,	best estimator xgboost's best error=12.8481
[flaml.automl: 09-17 23:54:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:54:09] {3072} INFO -  at 3.4s,	estimator xgboost's best error=6.4438,	best estimator xgboost's best error=6.4438
[flaml.automl: 09-17 23:54:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:54:11] {3072} INFO -  at 4.6s,	estimator xgboost's best error=6.4438,	best estimator xgboost's best error=6.4438
[flaml.automl: 09-17 23:54:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:54:20] {3072} INFO -  at 14.0s,	estimator xgboost's best error=6.4438,	best estimator xgboost's best error=6.4438
[flaml.automl: 09-17 23:54:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:54:21] {3072} INFO -  at 15.1s,	estimator xgboost's best error=4.4620,	best estimator xgboost's best error=4.4620
[flaml.automl: 09-17 23:54:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:54:23] {3072} INFO -  at 16.7s,	estimator xgboost's best error=3.9883,	best estimator xgboost's best error=3.9883
[flaml.automl: 09-17 23:54:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:54:24] {3072} INFO -  at 18.4s,	estimator xgboost's best error=3.9883,	best estimator xgboost's best error=3.9883
[flaml.automl: 09-17 23:54:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:54:27] {3072} INFO -  at 20.8s,	estimator xgboost's best error=3.9883,	best estimator xgboost's best error=3.9883
[flaml.automl: 09-17 23:54:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:54:28] {3072} INFO -  at 22.0s,	estimator xgboost's best error=3.9883,	best estimator xgboost's best error=3.9883
[flaml.automl: 09-17 23:54:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:54:30] {3072} INFO -  at 24.4s,	estimator xgboost's best error=3.9883,	best estimator xgboost's best error=3.9883
[flaml.automl: 09-17 23:54:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:54:32] {3072} INFO -  at 25.6s,	estimator xgboost's best error=3.9883,	best estimator xgboost's best error=3.9883
[flaml.automl: 09-17 23:54:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:54:33] {3072} INFO -  at 26.7s,	estimator xgboost's best error=3.9883,	best estimator xgboost's best error=3.9883
[flaml.automl: 09-17 23:54:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:54:39] {3072} INFO -  at 32.8s,	estimator xgboost's best error=3.9093,	best estimator xgboost's best error=3.9093
[flaml.automl: 09-17 23:54:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:54:49] {3072} INFO -  at 43.2s,	estimator xgboost's best error=3.8197,	best estimator xgboost's best error=3.8197
[flaml.automl: 09-17 23:54:49] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 23:54:55] {3072} INFO -  at 49.3s,	estimator xgboost's best error=3.8197,	best estimator xgboost's best error=3.8197
[flaml.automl: 09-17 23:54:55] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 23:55:05] {3072} INFO -  at 59.0s,	estimator xgboost's best error=3.7902,	best estimator xgboost's best error=3.7902
[flaml.automl: 09-17 23:55:22] {3335} INFO - retrain xgboost for 17.2s
[flaml.automl: 09-17 23:55:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:55:22] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:55:22] {2637} INFO - Time taken to find the best model: 58.978039264678955
[flaml.automl: 09-17 23:55:22] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-2.7901929372704197
NO2(0)最好结果：{'pred_time': 1.6545969132712467e-05, 'wall_clock_time': 58.978039264678955, 'metric_for_logging': {'pred_time': 1.6545969132712467e-05}, 'val_loss': 3.7901929372704197, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.688440084457397}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.764757990476083
NO2(0)的mse=29.64612219282186
NO2(0)的mae=3.655556814470529
NO2(0)的mar=0.2245153415975787
总共花费的时间为：76.59
松原市
2233A
2234A
[flaml.automl: 09-18 00:02:35] {2390} INFO - task = regression
[flaml.automl: 09-18 00:02:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:02:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:02:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:02:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:02:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:02:37] {3025} INFO - Estimated sufficient time budget=21750s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 00:02:37] {3072} INFO -  at 2.3s,	estimator xgboost's best error=10.0557,	best estimator xgboost's best error=10.0557
[flaml.automl: 09-18 00:02:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:02:40] {3072} INFO -  at 5.7s,	estimator xgboost's best error=5.5254,	best estimator xgboost's best error=5.5254
[flaml.automl: 09-18 00:02:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:02:43] {3072} INFO -  at 7.9s,	estimator xgboost's best error=5.5254,	best estimator xgboost's best error=5.5254
[flaml.automl: 09-18 00:02:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:03:05] {3072} INFO -  at 30.6s,	estimator xgboost's best error=5.5254,	best estimator xgboost's best error=5.5254
[flaml.automl: 09-18 00:03:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:03:09] {3072} INFO -  at 34.6s,	estimator xgboost's best error=3.5140,	best estimator xgboost's best error=3.5140
[flaml.automl: 09-18 00:03:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:03:15] {3072} INFO -  at 39.9s,	estimator xgboost's best error=3.1142,	best estimator xgboost's best error=3.1142
[flaml.automl: 09-18 00:03:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:03:21] {3072} INFO -  at 46.0s,	estimator xgboost's best error=3.1142,	best estimator xgboost's best error=3.1142
[flaml.automl: 09-18 00:03:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:03:30] {3072} INFO -  at 55.6s,	estimator xgboost's best error=3.1142,	best estimator xgboost's best error=3.1142
[flaml.automl: 09-18 00:03:36] {3335} INFO - retrain xgboost for 5.5s
[flaml.automl: 09-18 00:03:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 00:03:36] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:03:36] {2637} INFO - Time taken to find the best model: 39.856560707092285
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-2.11420220206522
NO2(0)最好结果：{'pred_time': 6.665357340432953e-05, 'wall_clock_time': 39.856560707092285, 'metric_for_logging': {'pred_time': 6.665357340432953e-05}, 'val_loss': 3.11420220206522, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 5.284254550933838}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7330029177788272
NO2(0)的mse=20.67770498882768
NO2(0)的mae=2.9638867478857005
NO2(0)的mar=0.19676309263611497
总共花费的时间为：61.82
白城市
2235A
2236A
[flaml.automl: 09-18 00:09:36] {2390} INFO - task = regression
[flaml.automl: 09-18 00:09:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:09:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:09:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:09:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:09:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:09:37] {3025} INFO - Estimated sufficient time budget=12664s. Estimated necessary time budget=13s.
[flaml.automl: 09-18 00:09:37] {3072} INFO -  at 1.4s,	estimator xgboost's best error=8.7537,	best estimator xgboost's best error=8.7537
[flaml.automl: 09-18 00:09:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:09:39] {3072} INFO -  at 3.2s,	estimator xgboost's best error=4.9770,	best estimator xgboost's best error=4.9770
[flaml.automl: 09-18 00:09:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:09:40] {3072} INFO -  at 4.4s,	estimator xgboost's best error=4.9770,	best estimator xgboost's best error=4.9770
[flaml.automl: 09-18 00:09:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:09:49] {3072} INFO -  at 12.9s,	estimator xgboost's best error=4.9770,	best estimator xgboost's best error=4.9770
[flaml.automl: 09-18 00:09:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:09:50] {3072} INFO -  at 14.0s,	estimator xgboost's best error=3.2753,	best estimator xgboost's best error=3.2753
[flaml.automl: 09-18 00:09:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:09:52] {3072} INFO -  at 15.6s,	estimator xgboost's best error=2.9068,	best estimator xgboost's best error=2.9068
[flaml.automl: 09-18 00:09:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:09:53] {3072} INFO -  at 17.2s,	estimator xgboost's best error=2.9068,	best estimator xgboost's best error=2.9068
[flaml.automl: 09-18 00:09:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:09:56] {3072} INFO -  at 19.7s,	estimator xgboost's best error=2.9068,	best estimator xgboost's best error=2.9068
[flaml.automl: 09-18 00:09:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:09:57] {3072} INFO -  at 20.9s,	estimator xgboost's best error=2.9068,	best estimator xgboost's best error=2.9068
[flaml.automl: 09-18 00:09:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:09:59] {3072} INFO -  at 23.3s,	estimator xgboost's best error=2.9068,	best estimator xgboost's best error=2.9068
[flaml.automl: 09-18 00:09:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:10:00] {3072} INFO -  at 24.5s,	estimator xgboost's best error=2.9068,	best estimator xgboost's best error=2.9068
[flaml.automl: 09-18 00:10:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:10:02] {3072} INFO -  at 25.6s,	estimator xgboost's best error=2.9068,	best estimator xgboost's best error=2.9068
[flaml.automl: 09-18 00:10:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:10:08] {3072} INFO -  at 31.7s,	estimator xgboost's best error=2.8316,	best estimator xgboost's best error=2.8316
[flaml.automl: 09-18 00:10:08] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:10:18] {3072} INFO -  at 42.1s,	estimator xgboost's best error=2.7594,	best estimator xgboost's best error=2.7594
[flaml.automl: 09-18 00:10:18] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 00:10:24] {3072} INFO -  at 48.2s,	estimator xgboost's best error=2.7594,	best estimator xgboost's best error=2.7594
[flaml.automl: 09-18 00:10:24] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 00:10:36] {3072} INFO -  at 59.7s,	estimator xgboost's best error=2.7594,	best estimator xgboost's best error=2.7594
[flaml.automl: 09-18 00:10:46] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 00:10:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:10:46] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:10:46] {2637} INFO - Time taken to find the best model: 42.10884404182434
[flaml.automl: 09-18 00:10:46] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.7593568969448654
NO2(0)最好结果：{'pred_time': 1.7568746217063722e-05, 'wall_clock_time': 42.10884404182434, 'metric_for_logging': {'pred_time': 1.7568746217063722e-05}, 'val_loss': 2.7593568969448654, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.441540002822876}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7886984850630778
NO2(0)的mse=18.911638043150347
NO2(0)的mae=2.751193918617373
NO2(0)的mar=0.23610136915728766
总共花费的时间为：70.53
延边朝鲜族自治州
2237A
2238A
2239A
[flaml.automl: 09-18 00:20:20] {2390} INFO - task = regression
[flaml.automl: 09-18 00:20:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:20:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:20:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:20:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:20:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:20:22] {3025} INFO - Estimated sufficient time budget=22374s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 00:20:22] {3072} INFO -  at 2.4s,	estimator xgboost's best error=7.9747,	best estimator xgboost's best error=7.9747
[flaml.automl: 09-18 00:20:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:20:26] {3072} INFO -  at 6.3s,	estimator xgboost's best error=4.1393,	best estimator xgboost's best error=4.1393
[flaml.automl: 09-18 00:20:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:20:28] {3072} INFO -  at 8.5s,	estimator xgboost's best error=4.1393,	best estimator xgboost's best error=4.1393
[flaml.automl: 09-18 00:20:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:20:46] {3072} INFO -  at 26.8s,	estimator xgboost's best error=4.1393,	best estimator xgboost's best error=4.1393
[flaml.automl: 09-18 00:20:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:20:49] {3072} INFO -  at 28.9s,	estimator xgboost's best error=3.2649,	best estimator xgboost's best error=3.2649
[flaml.automl: 09-18 00:20:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:20:52] {3072} INFO -  at 31.9s,	estimator xgboost's best error=2.9983,	best estimator xgboost's best error=2.9983
[flaml.automl: 09-18 00:20:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:20:55] {3072} INFO -  at 34.9s,	estimator xgboost's best error=2.9983,	best estimator xgboost's best error=2.9983
[flaml.automl: 09-18 00:20:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:20:59] {3072} INFO -  at 39.5s,	estimator xgboost's best error=2.9983,	best estimator xgboost's best error=2.9983
[flaml.automl: 09-18 00:20:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:21:01] {3072} INFO -  at 41.6s,	estimator xgboost's best error=2.9983,	best estimator xgboost's best error=2.9983
[flaml.automl: 09-18 00:21:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:21:06] {3072} INFO -  at 46.4s,	estimator xgboost's best error=2.9983,	best estimator xgboost's best error=2.9983
[flaml.automl: 09-18 00:21:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:21:07] {3072} INFO -  at 47.6s,	estimator xgboost's best error=2.9983,	best estimator xgboost's best error=2.9983
[flaml.automl: 09-18 00:21:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:21:08] {3072} INFO -  at 48.7s,	estimator xgboost's best error=2.9983,	best estimator xgboost's best error=2.9983
[flaml.automl: 09-18 00:21:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:21:15] {3072} INFO -  at 55.2s,	estimator xgboost's best error=2.9495,	best estimator xgboost's best error=2.9495
[flaml.automl: 09-18 00:21:21] {3335} INFO - retrain xgboost for 6.4s
[flaml.automl: 09-18 00:21:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 00:21:21] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:21:21] {2637} INFO - Time taken to find the best model: 55.18884062767029
[flaml.automl: 09-18 00:21:21] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-1.9495040492700788
NO2(0)最好结果：{'pred_time': 1.1137119433697492e-05, 'wall_clock_time': 55.18884062767029, 'metric_for_logging': {'pred_time': 1.1137119433697492e-05}, 'val_loss': 2.949504049270079, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 6.469780206680298}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7095836539696534
NO2(0)的mse=20.759737033899757
NO2(0)的mae=3.0312686940196336
NO2(0)的mar=0.27429450603081007
总共花费的时间为：62.24
鸡西市
2240A
2243A
3707A
3709A
[flaml.automl: 09-18 00:35:07] {2390} INFO - task = regression
[flaml.automl: 09-18 00:35:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:35:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:35:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:35:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:35:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:35:09] {3025} INFO - Estimated sufficient time budget=93060s. Estimated necessary time budget=93s.
[flaml.automl: 09-18 00:35:09] {3072} INFO -  at 2.5s,	estimator xgboost's best error=16.4412,	best estimator xgboost's best error=16.4412
[flaml.automl: 09-18 00:35:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:35:13] {3072} INFO -  at 6.5s,	estimator xgboost's best error=7.6238,	best estimator xgboost's best error=7.6238
[flaml.automl: 09-18 00:35:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:35:15] {3072} INFO -  at 8.7s,	estimator xgboost's best error=7.6238,	best estimator xgboost's best error=7.6238
[flaml.automl: 09-18 00:35:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:35:20] {3072} INFO -  at 13.7s,	estimator xgboost's best error=7.6238,	best estimator xgboost's best error=7.6238
[flaml.automl: 09-18 00:35:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:35:22] {3072} INFO -  at 15.7s,	estimator xgboost's best error=4.6699,	best estimator xgboost's best error=4.6699
[flaml.automl: 09-18 00:35:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:35:25] {3072} INFO -  at 18.6s,	estimator xgboost's best error=4.0688,	best estimator xgboost's best error=4.0688
[flaml.automl: 09-18 00:35:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:35:28] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.0688,	best estimator xgboost's best error=4.0688
[flaml.automl: 09-18 00:35:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:35:31] {3072} INFO -  at 25.0s,	estimator xgboost's best error=4.0688,	best estimator xgboost's best error=4.0688
[flaml.automl: 09-18 00:35:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:35:33] {3072} INFO -  at 27.1s,	estimator xgboost's best error=4.0688,	best estimator xgboost's best error=4.0688
[flaml.automl: 09-18 00:35:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:35:36] {3072} INFO -  at 29.8s,	estimator xgboost's best error=4.0688,	best estimator xgboost's best error=4.0688
[flaml.automl: 09-18 00:35:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:35:39] {3072} INFO -  at 32.7s,	estimator xgboost's best error=4.0330,	best estimator xgboost's best error=4.0330
[flaml.automl: 09-18 00:35:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:35:41] {3072} INFO -  at 35.0s,	estimator xgboost's best error=4.0330,	best estimator xgboost's best error=4.0330
[flaml.automl: 09-18 00:35:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:35:53] {3072} INFO -  at 47.1s,	estimator xgboost's best error=3.8640,	best estimator xgboost's best error=3.8640
[flaml.automl: 09-18 00:35:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:36:05] {3072} INFO -  at 59.2s,	estimator xgboost's best error=3.8408,	best estimator xgboost's best error=3.8408
[flaml.automl: 09-18 00:36:21] {3335} INFO - retrain xgboost for 15.6s
[flaml.automl: 09-18 00:36:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:36:21] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:36:21] {2637} INFO - Time taken to find the best model: 59.15295910835266
[flaml.automl: 09-18 00:36:21] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42907}
NO2(0)最佳损失：-2.840755902980798
NO2(0)最好结果：{'pred_time': 1.4949214938502984e-05, 'wall_clock_time': 59.15295910835266, 'metric_for_logging': {'pred_time': 1.4949214938502984e-05}, 'val_loss': 3.840755902980798, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42907}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42907, 'experiment_tag': 'exp', 'time_total_s': 12.067726135253906}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7786122859436134
NO2(0)的mse=34.460109607154585
NO2(0)的mae=3.886576108727288
NO2(0)的mar=0.16103602819264923
总共花费的时间为：75.68
鹤岗市
2244A
2245A
2246A
2247A
[flaml.automl: 09-18 00:49:18] {2390} INFO - task = regression
[flaml.automl: 09-18 00:49:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:49:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:49:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:49:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:49:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:49:21] {3025} INFO - Estimated sufficient time budget=23137s. Estimated necessary time budget=23s.
[flaml.automl: 09-18 00:49:21] {3072} INFO -  at 2.6s,	estimator xgboost's best error=7.9767,	best estimator xgboost's best error=7.9767
[flaml.automl: 09-18 00:49:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:49:25] {3072} INFO -  at 6.5s,	estimator xgboost's best error=4.1843,	best estimator xgboost's best error=4.1843
[flaml.automl: 09-18 00:49:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:49:27] {3072} INFO -  at 8.7s,	estimator xgboost's best error=4.1843,	best estimator xgboost's best error=4.1843
[flaml.automl: 09-18 00:49:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:49:40] {3072} INFO -  at 21.3s,	estimator xgboost's best error=4.1843,	best estimator xgboost's best error=4.1843
[flaml.automl: 09-18 00:49:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:49:41] {3072} INFO -  at 22.5s,	estimator xgboost's best error=3.2978,	best estimator xgboost's best error=3.2978
[flaml.automl: 09-18 00:49:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:49:42] {3072} INFO -  at 24.1s,	estimator xgboost's best error=2.9808,	best estimator xgboost's best error=2.9808
[flaml.automl: 09-18 00:49:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:49:44] {3072} INFO -  at 25.7s,	estimator xgboost's best error=2.9808,	best estimator xgboost's best error=2.9808
[flaml.automl: 09-18 00:49:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:49:46] {3072} INFO -  at 28.2s,	estimator xgboost's best error=2.9808,	best estimator xgboost's best error=2.9808
[flaml.automl: 09-18 00:49:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:49:48] {3072} INFO -  at 29.4s,	estimator xgboost's best error=2.9808,	best estimator xgboost's best error=2.9808
[flaml.automl: 09-18 00:49:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:49:50] {3072} INFO -  at 32.1s,	estimator xgboost's best error=2.9808,	best estimator xgboost's best error=2.9808
[flaml.automl: 09-18 00:49:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:49:52] {3072} INFO -  at 33.2s,	estimator xgboost's best error=2.9808,	best estimator xgboost's best error=2.9808
[flaml.automl: 09-18 00:49:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:49:53] {3072} INFO -  at 34.4s,	estimator xgboost's best error=2.9808,	best estimator xgboost's best error=2.9808
[flaml.automl: 09-18 00:49:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:49:59] {3072} INFO -  at 40.9s,	estimator xgboost's best error=2.9255,	best estimator xgboost's best error=2.9255
[flaml.automl: 09-18 00:49:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:50:11] {3072} INFO -  at 53.0s,	estimator xgboost's best error=2.8580,	best estimator xgboost's best error=2.8580
[flaml.automl: 09-18 00:50:23] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 00:50:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:50:23] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:50:23] {2637} INFO - Time taken to find the best model: 52.981566429138184
[flaml.automl: 09-18 00:50:23] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.8579673198173787
NO2(0)最好结果：{'pred_time': 9.315808614095051e-06, 'wall_clock_time': 52.981566429138184, 'metric_for_logging': {'pred_time': 9.315808614095051e-06}, 'val_loss': 2.8579673198173787, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.081521034240723}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6950753568710828
NO2(0)的mse=21.56477002516282
NO2(0)的mae=2.9526156516837796
NO2(0)的mar=0.2622194655375872
总共花费的时间为：65.76
双鸭山市
2248A
2249A
2250A
2251A
[flaml.automl: 09-18 01:02:59] {2390} INFO - task = regression
[flaml.automl: 09-18 01:02:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:02:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:02:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:02:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:02:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:03:01] {3025} INFO - Estimated sufficient time budget=88742s. Estimated necessary time budget=89s.
[flaml.automl: 09-18 01:03:01] {3072} INFO -  at 2.4s,	estimator xgboost's best error=7.6970,	best estimator xgboost's best error=7.6970
[flaml.automl: 09-18 01:03:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:03:05] {3072} INFO -  at 6.4s,	estimator xgboost's best error=4.0445,	best estimator xgboost's best error=4.0445
[flaml.automl: 09-18 01:03:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:03:07] {3072} INFO -  at 8.6s,	estimator xgboost's best error=4.0445,	best estimator xgboost's best error=4.0445
[flaml.automl: 09-18 01:03:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:03:13] {3072} INFO -  at 14.7s,	estimator xgboost's best error=4.0445,	best estimator xgboost's best error=4.0445
[flaml.automl: 09-18 01:03:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:03:15] {3072} INFO -  at 16.7s,	estimator xgboost's best error=3.1356,	best estimator xgboost's best error=3.1356
[flaml.automl: 09-18 01:03:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:03:18] {3072} INFO -  at 19.7s,	estimator xgboost's best error=2.8669,	best estimator xgboost's best error=2.8669
[flaml.automl: 09-18 01:03:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:03:21] {3072} INFO -  at 22.6s,	estimator xgboost's best error=2.8669,	best estimator xgboost's best error=2.8669
[flaml.automl: 09-18 01:03:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:03:24] {3072} INFO -  at 26.1s,	estimator xgboost's best error=2.8669,	best estimator xgboost's best error=2.8669
[flaml.automl: 09-18 01:03:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:03:27] {3072} INFO -  at 28.3s,	estimator xgboost's best error=2.8669,	best estimator xgboost's best error=2.8669
[flaml.automl: 09-18 01:03:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:03:29] {3072} INFO -  at 30.9s,	estimator xgboost's best error=2.8669,	best estimator xgboost's best error=2.8669
[flaml.automl: 09-18 01:03:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:03:32] {3072} INFO -  at 33.9s,	estimator xgboost's best error=2.8669,	best estimator xgboost's best error=2.8669
[flaml.automl: 09-18 01:03:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:03:34] {3072} INFO -  at 36.1s,	estimator xgboost's best error=2.8669,	best estimator xgboost's best error=2.8669
[flaml.automl: 09-18 01:03:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:03:47] {3072} INFO -  at 48.3s,	estimator xgboost's best error=2.8491,	best estimator xgboost's best error=2.8491
[flaml.automl: 09-18 01:03:59] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 01:03:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 01:03:59] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:03:59] {2637} INFO - Time taken to find the best model: 48.305869817733765
[flaml.automl: 09-18 01:03:59] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 40779}
NO2(0)最佳损失：-1.8491376878830152
NO2(0)最好结果：{'pred_time': 2.046538367275632e-05, 'wall_clock_time': 48.305869817733765, 'metric_for_logging': {'pred_time': 2.046538367275632e-05}, 'val_loss': 2.8491376878830152, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 40779}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 40779, 'experiment_tag': 'exp', 'time_total_s': 12.161572933197021}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.6665903249706033
NO2(0)的mse=20.87210152120937
NO2(0)的mae=2.9190913118932547
NO2(0)的mar=0.27059003902247747
总共花费的时间为：61.17
伊春市
2252A
2253A
2254A
3342A
3343A
3344A
3480A
[flaml.automl: 09-18 01:26:11] {2390} INFO - task = regression
[flaml.automl: 09-18 01:26:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:26:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:26:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:26:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:26:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:26:12] {3025} INFO - Estimated sufficient time budget=86340s. Estimated necessary time budget=86s.
[flaml.automl: 09-18 01:26:12] {3072} INFO -  at 1.7s,	estimator xgboost's best error=6.6550,	best estimator xgboost's best error=6.6550
[flaml.automl: 09-18 01:26:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:26:14] {3072} INFO -  at 3.8s,	estimator xgboost's best error=3.3635,	best estimator xgboost's best error=3.3635
[flaml.automl: 09-18 01:26:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:26:15] {3072} INFO -  at 4.9s,	estimator xgboost's best error=3.3635,	best estimator xgboost's best error=3.3635
[flaml.automl: 09-18 01:26:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:26:19] {3072} INFO -  at 8.2s,	estimator xgboost's best error=3.3635,	best estimator xgboost's best error=3.3635
[flaml.automl: 09-18 01:26:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:26:20] {3072} INFO -  at 9.3s,	estimator xgboost's best error=2.4616,	best estimator xgboost's best error=2.4616
[flaml.automl: 09-18 01:26:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:26:21] {3072} INFO -  at 10.8s,	estimator xgboost's best error=2.2513,	best estimator xgboost's best error=2.2513
[flaml.automl: 09-18 01:26:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:26:23] {3072} INFO -  at 12.4s,	estimator xgboost's best error=2.2513,	best estimator xgboost's best error=2.2513
[flaml.automl: 09-18 01:26:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:26:25] {3072} INFO -  at 14.9s,	estimator xgboost's best error=2.2513,	best estimator xgboost's best error=2.2513
[flaml.automl: 09-18 01:26:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:26:27] {3072} INFO -  at 16.1s,	estimator xgboost's best error=2.2513,	best estimator xgboost's best error=2.2513
[flaml.automl: 09-18 01:26:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:26:29] {3072} INFO -  at 18.7s,	estimator xgboost's best error=2.2513,	best estimator xgboost's best error=2.2513
[flaml.automl: 09-18 01:26:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:26:31] {3072} INFO -  at 20.3s,	estimator xgboost's best error=2.2303,	best estimator xgboost's best error=2.2303
[flaml.automl: 09-18 01:26:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:26:32] {3072} INFO -  at 21.5s,	estimator xgboost's best error=2.2303,	best estimator xgboost's best error=2.2303
[flaml.automl: 09-18 01:26:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:26:39] {3072} INFO -  at 28.1s,	estimator xgboost's best error=2.1779,	best estimator xgboost's best error=2.1779
[flaml.automl: 09-18 01:26:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:26:51] {3072} INFO -  at 40.1s,	estimator xgboost's best error=2.1330,	best estimator xgboost's best error=2.1330
[flaml.automl: 09-18 01:26:51] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:26:57] {3072} INFO -  at 46.6s,	estimator xgboost's best error=2.1330,	best estimator xgboost's best error=2.1330
[flaml.automl: 09-18 01:26:57] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 01:27:10] {3072} INFO -  at 59.2s,	estimator xgboost's best error=2.1276,	best estimator xgboost's best error=2.1276
[flaml.automl: 09-18 01:27:48] {3335} INFO - retrain xgboost for 38.5s
[flaml.automl: 09-18 01:27:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:27:48] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:27:48] {2637} INFO - Time taken to find the best model: 59.227110624313354
[flaml.automl: 09-18 01:27:48] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 72543}
NO2(0)最佳损失：-1.1276013295565597
NO2(0)最好结果：{'pred_time': 1.0230052916291719e-05, 'wall_clock_time': 59.227110624313354, 'metric_for_logging': {'pred_time': 1.0230052916291719e-05}, 'val_loss': 2.1276013295565597, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 72543}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 72543, 'experiment_tag': 'exp', 'time_total_s': 12.580252647399902}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7418913102389919
NO2(0)的mse=10.803820090706584
NO2(0)的mae=2.07204943788202
NO2(0)的mar=0.21814644632725858
总共花费的时间为：98.99
佳木斯市
2255A
2256A
2257A
2258A
2259A
[flaml.automl: 09-18 01:45:17] {2390} INFO - task = regression
[flaml.automl: 09-18 01:45:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:45:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:45:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:45:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:45:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:45:19] {3025} INFO - Estimated sufficient time budget=63179s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 01:45:19] {3072} INFO -  at 1.5s,	estimator xgboost's best error=10.8710,	best estimator xgboost's best error=10.8710
[flaml.automl: 09-18 01:45:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:45:21] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.5728,	best estimator xgboost's best error=5.5728
[flaml.automl: 09-18 01:45:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:45:22] {3072} INFO -  at 4.9s,	estimator xgboost's best error=5.5728,	best estimator xgboost's best error=5.5728
[flaml.automl: 09-18 01:45:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:45:27] {3072} INFO -  at 9.6s,	estimator xgboost's best error=5.5728,	best estimator xgboost's best error=5.5728
[flaml.automl: 09-18 01:45:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:45:28] {3072} INFO -  at 10.8s,	estimator xgboost's best error=4.1717,	best estimator xgboost's best error=4.1717
[flaml.automl: 09-18 01:45:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:45:30] {3072} INFO -  at 12.4s,	estimator xgboost's best error=3.7567,	best estimator xgboost's best error=3.7567
[flaml.automl: 09-18 01:45:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:45:31] {3072} INFO -  at 14.0s,	estimator xgboost's best error=3.7567,	best estimator xgboost's best error=3.7567
[flaml.automl: 09-18 01:45:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:45:34] {3072} INFO -  at 16.5s,	estimator xgboost's best error=3.7567,	best estimator xgboost's best error=3.7567
[flaml.automl: 09-18 01:45:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:45:35] {3072} INFO -  at 17.6s,	estimator xgboost's best error=3.7567,	best estimator xgboost's best error=3.7567
[flaml.automl: 09-18 01:45:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:45:37] {3072} INFO -  at 20.3s,	estimator xgboost's best error=3.7567,	best estimator xgboost's best error=3.7567
[flaml.automl: 09-18 01:45:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:45:39] {3072} INFO -  at 21.9s,	estimator xgboost's best error=3.7567,	best estimator xgboost's best error=3.7567
[flaml.automl: 09-18 01:45:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:45:40] {3072} INFO -  at 23.1s,	estimator xgboost's best error=3.7567,	best estimator xgboost's best error=3.7567
[flaml.automl: 09-18 01:45:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:45:47] {3072} INFO -  at 29.6s,	estimator xgboost's best error=3.6657,	best estimator xgboost's best error=3.6657
[flaml.automl: 09-18 01:45:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:45:59] {3072} INFO -  at 41.8s,	estimator xgboost's best error=3.5716,	best estimator xgboost's best error=3.5716
[flaml.automl: 09-18 01:45:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:46:05] {3072} INFO -  at 48.3s,	estimator xgboost's best error=3.5716,	best estimator xgboost's best error=3.5716
[flaml.automl: 09-18 01:46:17] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 01:46:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:46:17] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:46:17] {2637} INFO - Time taken to find the best model: 41.76317310333252
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52817}
NO2(0)最佳损失：-2.5715635605860983
NO2(0)最好结果：{'pred_time': 6.9632118291395005e-06, 'wall_clock_time': 41.76317310333252, 'metric_for_logging': {'pred_time': 6.9632118291395005e-06}, 'val_loss': 3.5715635605860983, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52817}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52817, 'experiment_tag': 'exp', 'time_total_s': 12.117175340652466}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.763445020452647
NO2(0)的mse=32.45339629784554
NO2(0)的mae=3.820828972556002
NO2(0)的mar=0.2733504630689735
总共花费的时间为：61.15
七台河市
2262A
3345A
3637A
3684A
[flaml.automl: 09-18 01:58:31] {2390} INFO - task = regression
[flaml.automl: 09-18 01:58:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:58:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:58:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:58:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:58:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:58:33] {3025} INFO - Estimated sufficient time budget=53141s. Estimated necessary time budget=53s.
[flaml.automl: 09-18 01:58:33] {3072} INFO -  at 1.5s,	estimator xgboost's best error=12.6044,	best estimator xgboost's best error=12.6044
[flaml.automl: 09-18 01:58:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:58:35] {3072} INFO -  at 3.6s,	estimator xgboost's best error=6.3139,	best estimator xgboost's best error=6.3139
[flaml.automl: 09-18 01:58:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:58:36] {3072} INFO -  at 4.8s,	estimator xgboost's best error=6.3139,	best estimator xgboost's best error=6.3139
[flaml.automl: 09-18 01:58:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:58:42] {3072} INFO -  at 11.1s,	estimator xgboost's best error=6.3139,	best estimator xgboost's best error=6.3139
[flaml.automl: 09-18 01:58:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:58:43] {3072} INFO -  at 12.2s,	estimator xgboost's best error=4.5731,	best estimator xgboost's best error=4.5731
[flaml.automl: 09-18 01:58:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:58:45] {3072} INFO -  at 13.8s,	estimator xgboost's best error=4.1730,	best estimator xgboost's best error=4.1730
[flaml.automl: 09-18 01:58:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:58:47] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.1730,	best estimator xgboost's best error=4.1730
[flaml.automl: 09-18 01:58:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:58:49] {3072} INFO -  at 17.9s,	estimator xgboost's best error=4.1730,	best estimator xgboost's best error=4.1730
[flaml.automl: 09-18 01:58:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:58:50] {3072} INFO -  at 19.0s,	estimator xgboost's best error=4.1730,	best estimator xgboost's best error=4.1730
[flaml.automl: 09-18 01:58:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:58:53] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.1730,	best estimator xgboost's best error=4.1730
[flaml.automl: 09-18 01:58:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:58:54] {3072} INFO -  at 23.3s,	estimator xgboost's best error=4.1730,	best estimator xgboost's best error=4.1730
[flaml.automl: 09-18 01:58:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:58:56] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.1730,	best estimator xgboost's best error=4.1730
[flaml.automl: 09-18 01:58:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:59:02] {3072} INFO -  at 31.0s,	estimator xgboost's best error=4.0484,	best estimator xgboost's best error=4.0484
[flaml.automl: 09-18 01:59:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:59:14] {3072} INFO -  at 43.0s,	estimator xgboost's best error=4.0074,	best estimator xgboost's best error=4.0074
[flaml.automl: 09-18 01:59:14] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:59:21] {3072} INFO -  at 49.5s,	estimator xgboost's best error=4.0074,	best estimator xgboost's best error=4.0074
[flaml.automl: 09-18 01:59:33] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-18 01:59:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:59:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:59:33] {2637} INFO - Time taken to find the best model: 43.03864097595215
[flaml.automl: 09-18 01:59:33] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41544}
NO2(0)最佳损失：-3.007397896339296
NO2(0)最好结果：{'pred_time': 9.942989419594661e-06, 'wall_clock_time': 43.03864097595215, 'metric_for_logging': {'pred_time': 9.942989419594661e-06}, 'val_loss': 4.007397896339296, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41544}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41544, 'experiment_tag': 'exp', 'time_total_s': 12.077736616134644}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7073809897905425
NO2(0)的mse=35.30915899457714
NO2(0)的mae=3.9615325766000136
NO2(0)的mar=0.2308516921626172
总共花费的时间为：62.23
黑河市
2263A
2264A
2265A
[flaml.automl: 09-18 02:09:07] {2390} INFO - task = regression
[flaml.automl: 09-18 02:09:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:09:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:09:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:09:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:09:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:09:09] {3025} INFO - Estimated sufficient time budget=22239s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 02:09:09] {3072} INFO -  at 2.4s,	estimator xgboost's best error=5.9099,	best estimator xgboost's best error=5.9099
[flaml.automl: 09-18 02:09:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:09:12] {3072} INFO -  at 4.6s,	estimator xgboost's best error=3.1767,	best estimator xgboost's best error=3.1767
[flaml.automl: 09-18 02:09:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:09:13] {3072} INFO -  at 5.8s,	estimator xgboost's best error=3.1767,	best estimator xgboost's best error=3.1767
[flaml.automl: 09-18 02:09:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:09:23] {3072} INFO -  at 15.8s,	estimator xgboost's best error=3.1767,	best estimator xgboost's best error=3.1767
[flaml.automl: 09-18 02:09:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:09:24] {3072} INFO -  at 16.9s,	estimator xgboost's best error=2.5643,	best estimator xgboost's best error=2.5643
[flaml.automl: 09-18 02:09:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:09:26] {3072} INFO -  at 18.5s,	estimator xgboost's best error=2.4229,	best estimator xgboost's best error=2.4229
[flaml.automl: 09-18 02:09:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:09:27] {3072} INFO -  at 20.2s,	estimator xgboost's best error=2.4229,	best estimator xgboost's best error=2.4229
[flaml.automl: 09-18 02:09:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:09:30] {3072} INFO -  at 22.6s,	estimator xgboost's best error=2.4229,	best estimator xgboost's best error=2.4229
[flaml.automl: 09-18 02:09:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:09:31] {3072} INFO -  at 23.8s,	estimator xgboost's best error=2.4229,	best estimator xgboost's best error=2.4229
[flaml.automl: 09-18 02:09:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:09:33] {3072} INFO -  at 26.5s,	estimator xgboost's best error=2.4229,	best estimator xgboost's best error=2.4229
[flaml.automl: 09-18 02:09:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:09:35] {3072} INFO -  at 27.6s,	estimator xgboost's best error=2.4229,	best estimator xgboost's best error=2.4229
[flaml.automl: 09-18 02:09:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:09:36] {3072} INFO -  at 28.8s,	estimator xgboost's best error=2.4229,	best estimator xgboost's best error=2.4229
[flaml.automl: 09-18 02:09:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:09:42] {3072} INFO -  at 35.3s,	estimator xgboost's best error=2.4050,	best estimator xgboost's best error=2.4050
[flaml.automl: 09-18 02:09:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:09:54] {3072} INFO -  at 47.4s,	estimator xgboost's best error=2.3756,	best estimator xgboost's best error=2.3756
[flaml.automl: 09-18 02:09:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:10:01] {3072} INFO -  at 53.9s,	estimator xgboost's best error=2.3756,	best estimator xgboost's best error=2.3756
[flaml.automl: 09-18 02:10:13] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 02:10:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:10:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:10:13] {2637} INFO - Time taken to find the best model: 47.36577558517456
[flaml.automl: 09-18 02:10:13] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.375639434553619
NO2(0)最好结果：{'pred_time': 1.2058987576737363e-05, 'wall_clock_time': 47.36577558517456, 'metric_for_logging': {'pred_time': 1.2058987576737363e-05}, 'val_loss': 2.375639434553619, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.06185793876648}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.5540887904395579
NO2(0)的mse=11.568403952966529
NO2(0)的mae=2.282343958432858
NO2(0)的mar=0.2701627319788739
总共花费的时间为：66.47
绥化市
2266A
2267A
[flaml.automl: 09-18 02:16:20] {2390} INFO - task = regression
[flaml.automl: 09-18 02:16:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:16:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:16:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:16:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:16:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:16:21] {3025} INFO - Estimated sufficient time budget=12116s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 02:16:21] {3072} INFO -  at 1.3s,	estimator xgboost's best error=9.5865,	best estimator xgboost's best error=9.5865
[flaml.automl: 09-18 02:16:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:16:23] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.6716,	best estimator xgboost's best error=4.6716
[flaml.automl: 09-18 02:16:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:16:24] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.6716,	best estimator xgboost's best error=4.6716
[flaml.automl: 09-18 02:16:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:16:34] {3072} INFO -  at 14.1s,	estimator xgboost's best error=4.6716,	best estimator xgboost's best error=4.6716
[flaml.automl: 09-18 02:16:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:16:35] {3072} INFO -  at 15.3s,	estimator xgboost's best error=3.2970,	best estimator xgboost's best error=3.2970
[flaml.automl: 09-18 02:16:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:16:36] {3072} INFO -  at 16.9s,	estimator xgboost's best error=2.9463,	best estimator xgboost's best error=2.9463
[flaml.automl: 09-18 02:16:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:16:38] {3072} INFO -  at 18.5s,	estimator xgboost's best error=2.9463,	best estimator xgboost's best error=2.9463
[flaml.automl: 09-18 02:16:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:16:40] {3072} INFO -  at 21.0s,	estimator xgboost's best error=2.9463,	best estimator xgboost's best error=2.9463
[flaml.automl: 09-18 02:16:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:16:42] {3072} INFO -  at 22.1s,	estimator xgboost's best error=2.9463,	best estimator xgboost's best error=2.9463
[flaml.automl: 09-18 02:16:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:16:44] {3072} INFO -  at 24.6s,	estimator xgboost's best error=2.9463,	best estimator xgboost's best error=2.9463
[flaml.automl: 09-18 02:16:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:16:45] {3072} INFO -  at 25.7s,	estimator xgboost's best error=2.9463,	best estimator xgboost's best error=2.9463
[flaml.automl: 09-18 02:16:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:16:46] {3072} INFO -  at 26.9s,	estimator xgboost's best error=2.9463,	best estimator xgboost's best error=2.9463
[flaml.automl: 09-18 02:16:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:16:52] {3072} INFO -  at 32.9s,	estimator xgboost's best error=2.9454,	best estimator xgboost's best error=2.9454
[flaml.automl: 09-18 02:16:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:17:05] {3072} INFO -  at 45.5s,	estimator xgboost's best error=2.8689,	best estimator xgboost's best error=2.8689
[flaml.automl: 09-18 02:17:05] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:17:16] {3072} INFO -  at 56.8s,	estimator xgboost's best error=2.8689,	best estimator xgboost's best error=2.8689
[flaml.automl: 09-18 02:17:36] {3335} INFO - retrain xgboost for 19.4s
[flaml.automl: 09-18 02:17:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:17:36] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:17:36] {2637} INFO - Time taken to find the best model: 45.53262376785278
[flaml.automl: 09-18 02:17:36] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.8688736648706352
NO2(0)最好结果：{'pred_time': 3.6547366473916766e-05, 'wall_clock_time': 45.53262376785278, 'metric_for_logging': {'pred_time': 3.6547366473916766e-05}, 'val_loss': 2.868873664870635, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.582725524902344}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7621045191747003
NO2(0)的mse=18.229487982308996
NO2(0)的mae=2.8737394345114624
NO2(0)的mar=0.22283320078614882
总共花费的时间为：76.76
大兴安岭地区
3663A
[flaml.automl: 09-18 02:21:16] {2390} INFO - task = regression
[flaml.automl: 09-18 02:21:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:21:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:21:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:21:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:21:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:21:17] {3025} INFO - Estimated sufficient time budget=12265s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 02:21:17] {3072} INFO -  at 1.3s,	estimator xgboost's best error=6.5749,	best estimator xgboost's best error=6.5749
[flaml.automl: 09-18 02:21:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:21:19] {3072} INFO -  at 3.1s,	estimator xgboost's best error=3.7399,	best estimator xgboost's best error=3.7399
[flaml.automl: 09-18 02:21:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:21:20] {3072} INFO -  at 4.3s,	estimator xgboost's best error=3.7399,	best estimator xgboost's best error=3.7399
[flaml.automl: 09-18 02:21:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:21:27] {3072} INFO -  at 11.4s,	estimator xgboost's best error=3.7399,	best estimator xgboost's best error=3.7399
[flaml.automl: 09-18 02:21:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:21:28] {3072} INFO -  at 12.6s,	estimator xgboost's best error=2.8197,	best estimator xgboost's best error=2.8197
[flaml.automl: 09-18 02:21:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:21:30] {3072} INFO -  at 14.2s,	estimator xgboost's best error=2.7587,	best estimator xgboost's best error=2.7587
[flaml.automl: 09-18 02:21:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:21:32] {3072} INFO -  at 15.8s,	estimator xgboost's best error=2.7439,	best estimator xgboost's best error=2.7439
[flaml.automl: 09-18 02:21:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:21:34] {3072} INFO -  at 18.1s,	estimator xgboost's best error=2.7439,	best estimator xgboost's best error=2.7439
[flaml.automl: 09-18 02:21:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:21:36] {3072} INFO -  at 19.7s,	estimator xgboost's best error=2.7371,	best estimator xgboost's best error=2.7371
[flaml.automl: 09-18 02:21:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:21:38] {3072} INFO -  at 22.3s,	estimator xgboost's best error=2.7371,	best estimator xgboost's best error=2.7371
[flaml.automl: 09-18 02:21:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:21:40] {3072} INFO -  at 24.0s,	estimator xgboost's best error=2.7029,	best estimator xgboost's best error=2.7029
[flaml.automl: 09-18 02:21:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:21:41] {3072} INFO -  at 25.2s,	estimator xgboost's best error=2.7029,	best estimator xgboost's best error=2.7029
[flaml.automl: 09-18 02:21:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:21:47] {3072} INFO -  at 31.0s,	estimator xgboost's best error=2.6602,	best estimator xgboost's best error=2.6602
[flaml.automl: 09-18 02:21:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:21:56] {3072} INFO -  at 39.9s,	estimator xgboost's best error=2.5912,	best estimator xgboost's best error=2.5912
[flaml.automl: 09-18 02:21:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:22:01] {3072} INFO -  at 45.1s,	estimator xgboost's best error=2.5912,	best estimator xgboost's best error=2.5912
[flaml.automl: 09-18 02:22:01] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 02:22:15] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.5709,	best estimator xgboost's best error=2.5709
[flaml.automl: 09-18 02:22:30] {3335} INFO - retrain xgboost for 14.4s
[flaml.automl: 09-18 02:22:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6733124546765852, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=11.891165198487528, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001530931541736259, reg_lambda=2.6245894006715305,
             scale_pos_weight=1, subsample=0.9453052099956202,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 02:22:30] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:22:30] {2637} INFO - Time taken to find the best model: 59.485471963882446
[flaml.automl: 09-18 02:22:30] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 25, 'max_leaves': 10, 'min_child_weight': 11.891165198487528, 'learning_rate': 0.5310029457853219, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6733124546765852, 'reg_alpha': 0.001530931541736259, 'reg_lambda': 2.6245894006715305}
NO2(0)最佳损失：-1.570935919489064
NO2(0)最好结果：{'pred_time': 3.367586596928177e-05, 'wall_clock_time': 59.485471963882446, 'metric_for_logging': {'pred_time': 3.367586596928177e-05}, 'val_loss': 2.570935919489064, 'training_iteration': 1, 'config': {'n_estimators': 25, 'max_leaves': 10, 'min_child_weight': 11.891165198487528, 'learning_rate': 0.5310029457853219, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6733124546765852, 'reg_alpha': 0.001530931541736259, 'reg_lambda': 2.6245894006715305}, 'config/n_estimators': 25, 'config/max_leaves': 10, 'config/min_child_weight': 11.891165198487528, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.9453052099956202, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6733124546765852, 'config/reg_alpha': 0.001530931541736259, 'config/reg_lambda': 2.6245894006715305, 'experiment_tag': 'exp', 'time_total_s': 14.396648168563843}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6733124546765852, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=11.891165198487528, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001530931541736259, reg_lambda=2.6245894006715305,
             scale_pos_weight=1, subsample=0.9453052099956202,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7118426234623224
NO2(0)的mse=17.884810653682603
NO2(0)的mae=2.671066372221663
NO2(0)的mar=0.27373522934310934
总共花费的时间为：74.11
蚌埠市
2270A
2271A
2274A
2275A
3715A
[flaml.automl: 09-18 02:38:37] {2390} INFO - task = regression
[flaml.automl: 09-18 02:38:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:38:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:38:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:38:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:38:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:38:39] {3025} INFO - Estimated sufficient time budget=65076s. Estimated necessary time budget=65s.
[flaml.automl: 09-18 02:38:39] {3072} INFO -  at 1.5s,	estimator xgboost's best error=14.6361,	best estimator xgboost's best error=14.6361
[flaml.automl: 09-18 02:38:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:38:41] {3072} INFO -  at 3.6s,	estimator xgboost's best error=7.4639,	best estimator xgboost's best error=7.4639
[flaml.automl: 09-18 02:38:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:38:42] {3072} INFO -  at 4.8s,	estimator xgboost's best error=7.4639,	best estimator xgboost's best error=7.4639
[flaml.automl: 09-18 02:38:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:38:47] {3072} INFO -  at 9.7s,	estimator xgboost's best error=7.4639,	best estimator xgboost's best error=7.4639
[flaml.automl: 09-18 02:38:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:38:48] {3072} INFO -  at 10.8s,	estimator xgboost's best error=5.3292,	best estimator xgboost's best error=5.3292
[flaml.automl: 09-18 02:38:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:38:50] {3072} INFO -  at 12.4s,	estimator xgboost's best error=4.6636,	best estimator xgboost's best error=4.6636
[flaml.automl: 09-18 02:38:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:38:51] {3072} INFO -  at 14.0s,	estimator xgboost's best error=4.6636,	best estimator xgboost's best error=4.6636
[flaml.automl: 09-18 02:38:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:38:54] {3072} INFO -  at 16.4s,	estimator xgboost's best error=4.6636,	best estimator xgboost's best error=4.6636
[flaml.automl: 09-18 02:38:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:38:55] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.6636,	best estimator xgboost's best error=4.6636
[flaml.automl: 09-18 02:38:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:38:57] {3072} INFO -  at 20.3s,	estimator xgboost's best error=4.6636,	best estimator xgboost's best error=4.6636
[flaml.automl: 09-18 02:38:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:38:59] {3072} INFO -  at 21.9s,	estimator xgboost's best error=4.6636,	best estimator xgboost's best error=4.6636
[flaml.automl: 09-18 02:38:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:39:00] {3072} INFO -  at 23.0s,	estimator xgboost's best error=4.6636,	best estimator xgboost's best error=4.6636
[flaml.automl: 09-18 02:39:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:39:07] {3072} INFO -  at 29.6s,	estimator xgboost's best error=4.4889,	best estimator xgboost's best error=4.4889
[flaml.automl: 09-18 02:39:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:39:19] {3072} INFO -  at 41.7s,	estimator xgboost's best error=4.3863,	best estimator xgboost's best error=4.3863
[flaml.automl: 09-18 02:39:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:39:25] {3072} INFO -  at 48.2s,	estimator xgboost's best error=4.3863,	best estimator xgboost's best error=4.3863
[flaml.automl: 09-18 02:39:38] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 02:39:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:39:38] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:39:38] {2637} INFO - Time taken to find the best model: 41.66259837150574
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54136}
NO2(0)最佳损失：-3.386299733052704
NO2(0)最好结果：{'pred_time': 7.675762506241494e-06, 'wall_clock_time': 41.66259837150574, 'metric_for_logging': {'pred_time': 7.675762506241494e-06}, 'val_loss': 4.386299733052704, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54136}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54136, 'experiment_tag': 'exp', 'time_total_s': 12.108972787857056}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7987422583038257
NO2(0)的mse=47.433868884542996
NO2(0)的mae=4.454618928212737
NO2(0)的mar=0.2650442444040363
总共花费的时间为：61.23
淮南市
2278A
2279A
2280A
2281A
[flaml.automl: 09-18 02:52:21] {2390} INFO - task = regression
[flaml.automl: 09-18 02:52:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:52:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:52:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:52:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:52:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:52:22] {3025} INFO - Estimated sufficient time budget=48889s. Estimated necessary time budget=49s.
[flaml.automl: 09-18 02:52:22] {3072} INFO -  at 1.4s,	estimator xgboost's best error=12.5402,	best estimator xgboost's best error=12.5402
[flaml.automl: 09-18 02:52:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:52:24] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.2069,	best estimator xgboost's best error=6.2069
[flaml.automl: 09-18 02:52:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:52:25] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.2069,	best estimator xgboost's best error=6.2069
[flaml.automl: 09-18 02:52:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:52:32] {3072} INFO -  at 11.1s,	estimator xgboost's best error=6.2069,	best estimator xgboost's best error=6.2069
[flaml.automl: 09-18 02:52:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:52:33] {3072} INFO -  at 12.2s,	estimator xgboost's best error=4.1399,	best estimator xgboost's best error=4.1399
[flaml.automl: 09-18 02:52:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:52:34] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.6767,	best estimator xgboost's best error=3.6767
[flaml.automl: 09-18 02:52:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:52:36] {3072} INFO -  at 15.4s,	estimator xgboost's best error=3.6767,	best estimator xgboost's best error=3.6767
[flaml.automl: 09-18 02:52:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:52:38] {3072} INFO -  at 17.9s,	estimator xgboost's best error=3.6767,	best estimator xgboost's best error=3.6767
[flaml.automl: 09-18 02:52:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:52:39] {3072} INFO -  at 19.0s,	estimator xgboost's best error=3.6767,	best estimator xgboost's best error=3.6767
[flaml.automl: 09-18 02:52:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:52:42] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.6767,	best estimator xgboost's best error=3.6767
[flaml.automl: 09-18 02:52:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:52:44] {3072} INFO -  at 23.3s,	estimator xgboost's best error=3.6163,	best estimator xgboost's best error=3.6163
[flaml.automl: 09-18 02:52:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:52:45] {3072} INFO -  at 24.5s,	estimator xgboost's best error=3.6163,	best estimator xgboost's best error=3.6163
[flaml.automl: 09-18 02:52:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:52:51] {3072} INFO -  at 31.0s,	estimator xgboost's best error=3.4793,	best estimator xgboost's best error=3.4793
[flaml.automl: 09-18 02:52:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:53:16] {3072} INFO -  at 55.2s,	estimator xgboost's best error=3.3997,	best estimator xgboost's best error=3.3997
[flaml.automl: 09-18 02:53:42] {3335} INFO - retrain xgboost for 26.6s
[flaml.automl: 09-18 02:53:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:53:42] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:53:42] {2637} INFO - Time taken to find the best model: 55.232311725616455
[flaml.automl: 09-18 02:53:42] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40933}
NO2(0)最佳损失：-2.399703941860941
NO2(0)最好结果：{'pred_time': 2.5223941429810724e-05, 'wall_clock_time': 55.232311725616455, 'metric_for_logging': {'pred_time': 2.5223941429810724e-05}, 'val_loss': 3.399703941860941, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40933}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40933, 'experiment_tag': 'exp', 'time_total_s': 24.244020700454712}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8354025124707893
NO2(0)的mse=26.728181799438524
NO2(0)的mae=3.3994686462966923
NO2(0)的mar=0.2250551116696041
总共花费的时间为：82.85
淮北市
2282A
2283A
2284A
3330A
[flaml.automl: 09-18 03:06:10] {2390} INFO - task = regression
[flaml.automl: 09-18 03:06:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:06:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:06:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:06:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:06:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:06:12] {3025} INFO - Estimated sufficient time budget=51161s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 03:06:12] {3072} INFO -  at 1.4s,	estimator xgboost's best error=12.6373,	best estimator xgboost's best error=12.6373
[flaml.automl: 09-18 03:06:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:06:14] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.2296,	best estimator xgboost's best error=6.2296
[flaml.automl: 09-18 03:06:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:06:15] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.2296,	best estimator xgboost's best error=6.2296
[flaml.automl: 09-18 03:06:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:06:21] {3072} INFO -  at 11.0s,	estimator xgboost's best error=6.2296,	best estimator xgboost's best error=6.2296
[flaml.automl: 09-18 03:06:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:06:22] {3072} INFO -  at 12.2s,	estimator xgboost's best error=4.2175,	best estimator xgboost's best error=4.2175
[flaml.automl: 09-18 03:06:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:06:24] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.7028,	best estimator xgboost's best error=3.7028
[flaml.automl: 09-18 03:06:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:06:26] {3072} INFO -  at 15.4s,	estimator xgboost's best error=3.7028,	best estimator xgboost's best error=3.7028
[flaml.automl: 09-18 03:06:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:06:28] {3072} INFO -  at 17.8s,	estimator xgboost's best error=3.7028,	best estimator xgboost's best error=3.7028
[flaml.automl: 09-18 03:06:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:06:29] {3072} INFO -  at 19.0s,	estimator xgboost's best error=3.7028,	best estimator xgboost's best error=3.7028
[flaml.automl: 09-18 03:06:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:06:32] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.7028,	best estimator xgboost's best error=3.7028
[flaml.automl: 09-18 03:06:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:06:34] {3072} INFO -  at 23.3s,	estimator xgboost's best error=3.6568,	best estimator xgboost's best error=3.6568
[flaml.automl: 09-18 03:06:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:06:35] {3072} INFO -  at 24.4s,	estimator xgboost's best error=3.6568,	best estimator xgboost's best error=3.6568
[flaml.automl: 09-18 03:06:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:06:41] {3072} INFO -  at 31.0s,	estimator xgboost's best error=3.4514,	best estimator xgboost's best error=3.4514
[flaml.automl: 09-18 03:06:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:06:53] {3072} INFO -  at 43.1s,	estimator xgboost's best error=3.4361,	best estimator xgboost's best error=3.4361
[flaml.automl: 09-18 03:06:53] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 03:07:00] {3072} INFO -  at 49.6s,	estimator xgboost's best error=3.4361,	best estimator xgboost's best error=3.4361
[flaml.automl: 09-18 03:07:12] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 03:07:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:07:12] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:07:12] {2637} INFO - Time taken to find the best model: 43.07144021987915
[flaml.automl: 09-18 03:07:12] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42914}
NO2(0)最佳损失：-2.4361378472664743
NO2(0)最好结果：{'pred_time': 8.187321002789828e-06, 'wall_clock_time': 43.07144021987915, 'metric_for_logging': {'pred_time': 8.187321002789828e-06}, 'val_loss': 3.4361378472664743, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42914}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42914, 'experiment_tag': 'exp', 'time_total_s': 12.09420895576477}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8591145479405
NO2(0)的mse=28.136638480902633
NO2(0)的mae=3.5144054221531023
NO2(0)的mar=0.2303768403275172
总共花费的时间为：62.62
铜陵市
2285A
2286A
2287A
2288A
2289A
2290A
[flaml.automl: 09-18 03:26:06] {2390} INFO - task = regression
[flaml.automl: 09-18 03:26:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:26:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:26:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:26:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:26:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:26:07] {3025} INFO - Estimated sufficient time budget=74590s. Estimated necessary time budget=75s.
[flaml.automl: 09-18 03:26:07] {3072} INFO -  at 1.5s,	estimator xgboost's best error=19.4548,	best estimator xgboost's best error=19.4548
[flaml.automl: 09-18 03:26:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:26:09] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.6576,	best estimator xgboost's best error=9.6576
[flaml.automl: 09-18 03:26:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:26:10] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.6576,	best estimator xgboost's best error=9.6576
[flaml.automl: 09-18 03:26:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:26:14] {3072} INFO -  at 9.1s,	estimator xgboost's best error=9.6576,	best estimator xgboost's best error=9.6576
[flaml.automl: 09-18 03:26:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:26:16] {3072} INFO -  at 10.3s,	estimator xgboost's best error=6.9017,	best estimator xgboost's best error=6.9017
[flaml.automl: 09-18 03:26:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:26:17] {3072} INFO -  at 11.8s,	estimator xgboost's best error=6.2936,	best estimator xgboost's best error=6.2936
[flaml.automl: 09-18 03:26:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:26:19] {3072} INFO -  at 13.5s,	estimator xgboost's best error=6.2936,	best estimator xgboost's best error=6.2936
[flaml.automl: 09-18 03:26:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:26:21] {3072} INFO -  at 15.9s,	estimator xgboost's best error=6.2936,	best estimator xgboost's best error=6.2936
[flaml.automl: 09-18 03:26:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:26:22] {3072} INFO -  at 17.1s,	estimator xgboost's best error=6.2936,	best estimator xgboost's best error=6.2936
[flaml.automl: 09-18 03:26:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:26:25] {3072} INFO -  at 19.7s,	estimator xgboost's best error=6.2936,	best estimator xgboost's best error=6.2936
[flaml.automl: 09-18 03:26:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:26:27] {3072} INFO -  at 21.4s,	estimator xgboost's best error=6.2257,	best estimator xgboost's best error=6.2257
[flaml.automl: 09-18 03:26:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:26:28] {3072} INFO -  at 22.6s,	estimator xgboost's best error=6.2257,	best estimator xgboost's best error=6.2257
[flaml.automl: 09-18 03:26:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:26:34] {3072} INFO -  at 29.1s,	estimator xgboost's best error=6.0299,	best estimator xgboost's best error=6.0299
[flaml.automl: 09-18 03:26:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:26:47] {3072} INFO -  at 41.2s,	estimator xgboost's best error=5.9463,	best estimator xgboost's best error=5.9463
[flaml.automl: 09-18 03:26:47] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 03:26:53] {3072} INFO -  at 47.8s,	estimator xgboost's best error=5.9463,	best estimator xgboost's best error=5.9463
[flaml.automl: 09-18 03:26:53] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 03:27:06] {3072} INFO -  at 60.5s,	estimator xgboost's best error=5.9459,	best estimator xgboost's best error=5.9459
[flaml.automl: 09-18 03:28:03] {3335} INFO - retrain xgboost for 56.9s
[flaml.automl: 09-18 03:28:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:28:03] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:28:03] {2637} INFO - Time taken to find the best model: 60.51134490966797
[flaml.automl: 09-18 03:28:03] {2648} WARNING - Time taken to find the best model is 101% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 62802}
NO2(0)最佳损失：-4.945937912005342
NO2(0)最好结果：{'pred_time': 1.5752126603810454e-05, 'wall_clock_time': 60.51134490966797, 'metric_for_logging': {'pred_time': 1.5752126603810454e-05}, 'val_loss': 5.945937912005342, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 62802}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 62802, 'experiment_tag': 'exp', 'time_total_s': 12.729427814483643}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7463296581136482
NO2(0)的mse=76.78697411598965
NO2(0)的mae=5.74611078742733
NO2(0)的mar=0.22431781647729118
总共花费的时间为：118.57
安庆市
2291A
2292A
3173A
[flaml.automl: 09-18 03:37:31] {2390} INFO - task = regression
[flaml.automl: 09-18 03:37:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:37:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:37:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:37:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:37:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:37:32] {3025} INFO - Estimated sufficient time budget=12244s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 03:37:32] {3072} INFO -  at 1.4s,	estimator xgboost's best error=14.8124,	best estimator xgboost's best error=14.8124
[flaml.automl: 09-18 03:37:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:37:34] {3072} INFO -  at 3.6s,	estimator xgboost's best error=7.3920,	best estimator xgboost's best error=7.3920
[flaml.automl: 09-18 03:37:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:37:36] {3072} INFO -  at 4.8s,	estimator xgboost's best error=7.3920,	best estimator xgboost's best error=7.3920
[flaml.automl: 09-18 03:37:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:37:46] {3072} INFO -  at 14.8s,	estimator xgboost's best error=7.3920,	best estimator xgboost's best error=7.3920
[flaml.automl: 09-18 03:37:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:37:47] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.2389,	best estimator xgboost's best error=5.2389
[flaml.automl: 09-18 03:37:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:37:48] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.5991,	best estimator xgboost's best error=4.5991
[flaml.automl: 09-18 03:37:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:37:50] {3072} INFO -  at 19.2s,	estimator xgboost's best error=4.5991,	best estimator xgboost's best error=4.5991
[flaml.automl: 09-18 03:37:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:37:53] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.5991,	best estimator xgboost's best error=4.5991
[flaml.automl: 09-18 03:37:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:37:54] {3072} INFO -  at 22.8s,	estimator xgboost's best error=4.5991,	best estimator xgboost's best error=4.5991
[flaml.automl: 09-18 03:37:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:37:56] {3072} INFO -  at 25.4s,	estimator xgboost's best error=4.5991,	best estimator xgboost's best error=4.5991
[flaml.automl: 09-18 03:37:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:37:57] {3072} INFO -  at 26.6s,	estimator xgboost's best error=4.5991,	best estimator xgboost's best error=4.5991
[flaml.automl: 09-18 03:37:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:37:59] {3072} INFO -  at 27.7s,	estimator xgboost's best error=4.5991,	best estimator xgboost's best error=4.5991
[flaml.automl: 09-18 03:37:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:38:05] {3072} INFO -  at 34.2s,	estimator xgboost's best error=4.5439,	best estimator xgboost's best error=4.5439
[flaml.automl: 09-18 03:38:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:38:17] {3072} INFO -  at 46.3s,	estimator xgboost's best error=4.3190,	best estimator xgboost's best error=4.3190
[flaml.automl: 09-18 03:38:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 03:38:24] {3072} INFO -  at 52.8s,	estimator xgboost's best error=4.3190,	best estimator xgboost's best error=4.3190
[flaml.automl: 09-18 03:38:36] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 03:38:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:38:36] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:38:36] {2637} INFO - Time taken to find the best model: 46.32082176208496
[flaml.automl: 09-18 03:38:36] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.3189801893382214
NO2(0)最好结果：{'pred_time': 1.1119923235162861e-05, 'wall_clock_time': 46.32082176208496, 'metric_for_logging': {'pred_time': 1.1119923235162861e-05}, 'val_loss': 4.318980189338221, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.076188325881958}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7483838189573541
NO2(0)的mse=56.15534435220547
NO2(0)的mae=4.574336069675532
NO2(0)的mar=0.22872293293678098
总共花费的时间为：65.71
黄山市
2295A
2296A
2297A
[flaml.automl: 09-18 03:48:26] {2390} INFO - task = regression
[flaml.automl: 09-18 03:48:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:48:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:48:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:48:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:48:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:48:27] {3025} INFO - Estimated sufficient time budget=12094s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 03:48:27] {3072} INFO -  at 1.4s,	estimator xgboost's best error=6.2505,	best estimator xgboost's best error=6.2505
[flaml.automl: 09-18 03:48:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:48:29] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.1947,	best estimator xgboost's best error=3.1947
[flaml.automl: 09-18 03:48:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:48:30] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.1947,	best estimator xgboost's best error=3.1947
[flaml.automl: 09-18 03:48:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:48:40] {3072} INFO -  at 14.6s,	estimator xgboost's best error=3.1947,	best estimator xgboost's best error=3.1947
[flaml.automl: 09-18 03:48:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:48:42] {3072} INFO -  at 15.8s,	estimator xgboost's best error=2.2114,	best estimator xgboost's best error=2.2114
[flaml.automl: 09-18 03:48:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:48:43] {3072} INFO -  at 17.4s,	estimator xgboost's best error=2.0043,	best estimator xgboost's best error=2.0043
[flaml.automl: 09-18 03:48:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:48:45] {3072} INFO -  at 19.0s,	estimator xgboost's best error=2.0043,	best estimator xgboost's best error=2.0043
[flaml.automl: 09-18 03:48:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:48:47] {3072} INFO -  at 21.5s,	estimator xgboost's best error=2.0043,	best estimator xgboost's best error=2.0043
[flaml.automl: 09-18 03:48:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:48:48] {3072} INFO -  at 22.6s,	estimator xgboost's best error=2.0043,	best estimator xgboost's best error=2.0043
[flaml.automl: 09-18 03:48:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:48:51] {3072} INFO -  at 25.4s,	estimator xgboost's best error=2.0043,	best estimator xgboost's best error=2.0043
[flaml.automl: 09-18 03:48:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:48:52] {3072} INFO -  at 26.5s,	estimator xgboost's best error=2.0043,	best estimator xgboost's best error=2.0043
[flaml.automl: 09-18 03:48:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:48:53] {3072} INFO -  at 27.7s,	estimator xgboost's best error=2.0043,	best estimator xgboost's best error=2.0043
[flaml.automl: 09-18 03:48:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:49:00] {3072} INFO -  at 34.2s,	estimator xgboost's best error=1.9470,	best estimator xgboost's best error=1.9470
[flaml.automl: 09-18 03:49:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:49:12] {3072} INFO -  at 46.3s,	estimator xgboost's best error=1.8706,	best estimator xgboost's best error=1.8706
[flaml.automl: 09-18 03:49:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 03:49:18] {3072} INFO -  at 52.8s,	estimator xgboost's best error=1.8706,	best estimator xgboost's best error=1.8706
[flaml.automl: 09-18 03:49:31] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 03:49:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:49:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:49:31] {2637} INFO - Time taken to find the best model: 46.25401496887207
[flaml.automl: 09-18 03:49:31] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-0.8705646032149266
NO2(0)最好结果：{'pred_time': 1.2359545822838715e-05, 'wall_clock_time': 46.25401496887207, 'metric_for_logging': {'pred_time': 1.2359545822838715e-05}, 'val_loss': 1.8705646032149266, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.08695387840271}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7898948375665457
NO2(0)的mse=8.270501712035301
NO2(0)的mae=1.861526084734439
NO2(0)的mar=0.22792501248947689
总共花费的时间为：65.36
滁州市
2298A
2299A
3331A
[flaml.automl: 09-18 03:59:17] {2390} INFO - task = regression
[flaml.automl: 09-18 03:59:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:59:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:59:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:59:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:59:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:59:18] {3025} INFO - Estimated sufficient time budget=12178s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 03:59:18] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.5149,	best estimator xgboost's best error=15.5149
[flaml.automl: 09-18 03:59:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:59:20] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.3121,	best estimator xgboost's best error=7.3121
[flaml.automl: 09-18 03:59:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:59:21] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.3121,	best estimator xgboost's best error=7.3121
[flaml.automl: 09-18 03:59:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:59:31] {3072} INFO -  at 14.7s,	estimator xgboost's best error=7.3121,	best estimator xgboost's best error=7.3121
[flaml.automl: 09-18 03:59:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:59:33] {3072} INFO -  at 15.9s,	estimator xgboost's best error=4.5294,	best estimator xgboost's best error=4.5294
[flaml.automl: 09-18 03:59:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:59:34] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.8161,	best estimator xgboost's best error=3.8161
[flaml.automl: 09-18 03:59:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:59:36] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.8161,	best estimator xgboost's best error=3.8161
[flaml.automl: 09-18 03:59:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:59:38] {3072} INFO -  at 21.5s,	estimator xgboost's best error=3.8161,	best estimator xgboost's best error=3.8161
[flaml.automl: 09-18 03:59:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:59:39] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.8161,	best estimator xgboost's best error=3.8161
[flaml.automl: 09-18 03:59:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:59:42] {3072} INFO -  at 25.4s,	estimator xgboost's best error=3.8161,	best estimator xgboost's best error=3.8161
[flaml.automl: 09-18 03:59:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:59:43] {3072} INFO -  at 26.5s,	estimator xgboost's best error=3.8161,	best estimator xgboost's best error=3.8161
[flaml.automl: 09-18 03:59:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:59:44] {3072} INFO -  at 27.7s,	estimator xgboost's best error=3.8161,	best estimator xgboost's best error=3.8161
[flaml.automl: 09-18 03:59:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:59:51] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.5839,	best estimator xgboost's best error=3.5839
[flaml.automl: 09-18 03:59:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:00:03] {3072} INFO -  at 46.3s,	estimator xgboost's best error=3.4469,	best estimator xgboost's best error=3.4469
[flaml.automl: 09-18 04:00:03] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 04:00:09] {3072} INFO -  at 52.8s,	estimator xgboost's best error=3.4469,	best estimator xgboost's best error=3.4469
[flaml.automl: 09-18 04:00:21] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 04:00:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:00:21] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:00:21] {2637} INFO - Time taken to find the best model: 46.303362131118774
[flaml.automl: 09-18 04:00:21] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.446852447793398
NO2(0)最好结果：{'pred_time': 1.2576895416098297e-05, 'wall_clock_time': 46.303362131118774, 'metric_for_logging': {'pred_time': 1.2576895416098297e-05}, 'val_loss': 3.446852447793398, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.086780309677124}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8664968836125906
NO2(0)的mse=26.994156454671106
NO2(0)的mae=3.4647919599872483
NO2(0)的mar=0.16344022851317755
总共花费的时间为：65.58
阜阳市
2301A
2875A
3468A
3469A
[flaml.automl: 09-18 04:13:27] {2390} INFO - task = regression
[flaml.automl: 09-18 04:13:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:13:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:13:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:13:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:13:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:13:28] {3025} INFO - Estimated sufficient time budget=51702s. Estimated necessary time budget=52s.
[flaml.automl: 09-18 04:13:28] {3072} INFO -  at 1.4s,	estimator xgboost's best error=12.3725,	best estimator xgboost's best error=12.3725
[flaml.automl: 09-18 04:13:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:13:30] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.0129,	best estimator xgboost's best error=6.0129
[flaml.automl: 09-18 04:13:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:13:31] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.0129,	best estimator xgboost's best error=6.0129
[flaml.automl: 09-18 04:13:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:13:37] {3072} INFO -  at 11.0s,	estimator xgboost's best error=6.0129,	best estimator xgboost's best error=6.0129
[flaml.automl: 09-18 04:13:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:13:39] {3072} INFO -  at 12.1s,	estimator xgboost's best error=3.8503,	best estimator xgboost's best error=3.8503
[flaml.automl: 09-18 04:13:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:13:40] {3072} INFO -  at 13.7s,	estimator xgboost's best error=3.4344,	best estimator xgboost's best error=3.4344
[flaml.automl: 09-18 04:13:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:13:42] {3072} INFO -  at 15.3s,	estimator xgboost's best error=3.4344,	best estimator xgboost's best error=3.4344
[flaml.automl: 09-18 04:13:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:13:44] {3072} INFO -  at 17.8s,	estimator xgboost's best error=3.4344,	best estimator xgboost's best error=3.4344
[flaml.automl: 09-18 04:13:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:13:45] {3072} INFO -  at 18.9s,	estimator xgboost's best error=3.4344,	best estimator xgboost's best error=3.4344
[flaml.automl: 09-18 04:13:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:13:48] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.4344,	best estimator xgboost's best error=3.4344
[flaml.automl: 09-18 04:13:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:13:50] {3072} INFO -  at 23.1s,	estimator xgboost's best error=3.4020,	best estimator xgboost's best error=3.4020
[flaml.automl: 09-18 04:13:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:13:51] {3072} INFO -  at 24.3s,	estimator xgboost's best error=3.4020,	best estimator xgboost's best error=3.4020
[flaml.automl: 09-18 04:13:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:13:57] {3072} INFO -  at 30.8s,	estimator xgboost's best error=3.2098,	best estimator xgboost's best error=3.2098
[flaml.automl: 09-18 04:13:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:14:09] {3072} INFO -  at 42.9s,	estimator xgboost's best error=3.1534,	best estimator xgboost's best error=3.1534
[flaml.automl: 09-18 04:14:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 04:14:16] {3072} INFO -  at 49.5s,	estimator xgboost's best error=3.1534,	best estimator xgboost's best error=3.1534
[flaml.automl: 09-18 04:14:28] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 04:14:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:14:28] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:14:28] {2637} INFO - Time taken to find the best model: 42.93543744087219
[flaml.automl: 09-18 04:14:28] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43087}
NO2(0)最佳损失：-2.15335636539268
NO2(0)最好结果：{'pred_time': 8.43458008347896e-06, 'wall_clock_time': 42.93543744087219, 'metric_for_logging': {'pred_time': 8.43458008347896e-06}, 'val_loss': 3.15335636539268, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43087}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43087, 'experiment_tag': 'exp', 'time_total_s': 12.094362497329712}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8142346991271912
NO2(0)的mse=23.730558033653708
NO2(0)的mae=3.1199908672866963
NO2(0)的mar=0.18536766676131305
总共花费的时间为：62.16
宿州市
3463A
3634A
3701A
[flaml.automl: 09-18 04:24:17] {2390} INFO - task = regression
[flaml.automl: 09-18 04:24:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:24:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:24:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:24:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:24:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:24:19] {3025} INFO - Estimated sufficient time budget=22703s. Estimated necessary time budget=23s.
[flaml.automl: 09-18 04:24:19] {3072} INFO -  at 2.5s,	estimator xgboost's best error=11.9637,	best estimator xgboost's best error=11.9637
[flaml.automl: 09-18 04:24:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:24:23] {3072} INFO -  at 6.5s,	estimator xgboost's best error=6.1004,	best estimator xgboost's best error=6.1004
[flaml.automl: 09-18 04:24:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:24:26] {3072} INFO -  at 8.7s,	estimator xgboost's best error=6.1004,	best estimator xgboost's best error=6.1004
[flaml.automl: 09-18 04:24:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:24:44] {3072} INFO -  at 27.2s,	estimator xgboost's best error=6.1004,	best estimator xgboost's best error=6.1004
[flaml.automl: 09-18 04:24:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:24:46] {3072} INFO -  at 28.7s,	estimator xgboost's best error=4.4062,	best estimator xgboost's best error=4.4062
[flaml.automl: 09-18 04:24:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:24:47] {3072} INFO -  at 30.3s,	estimator xgboost's best error=3.8275,	best estimator xgboost's best error=3.8275
[flaml.automl: 09-18 04:24:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:24:49] {3072} INFO -  at 31.9s,	estimator xgboost's best error=3.8275,	best estimator xgboost's best error=3.8275
[flaml.automl: 09-18 04:24:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:24:51] {3072} INFO -  at 34.4s,	estimator xgboost's best error=3.8275,	best estimator xgboost's best error=3.8275
[flaml.automl: 09-18 04:24:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:24:52] {3072} INFO -  at 35.5s,	estimator xgboost's best error=3.8275,	best estimator xgboost's best error=3.8275
[flaml.automl: 09-18 04:24:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:24:55] {3072} INFO -  at 38.2s,	estimator xgboost's best error=3.8275,	best estimator xgboost's best error=3.8275
[flaml.automl: 09-18 04:24:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:24:56] {3072} INFO -  at 39.4s,	estimator xgboost's best error=3.8275,	best estimator xgboost's best error=3.8275
[flaml.automl: 09-18 04:24:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:24:57] {3072} INFO -  at 40.5s,	estimator xgboost's best error=3.8275,	best estimator xgboost's best error=3.8275
[flaml.automl: 09-18 04:24:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:25:04] {3072} INFO -  at 47.0s,	estimator xgboost's best error=3.6295,	best estimator xgboost's best error=3.6295
[flaml.automl: 09-18 04:25:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:25:16] {3072} INFO -  at 59.1s,	estimator xgboost's best error=3.5197,	best estimator xgboost's best error=3.5197
[flaml.automl: 09-18 04:25:28] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 04:25:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:25:28] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:25:28] {2637} INFO - Time taken to find the best model: 59.09574222564697
[flaml.automl: 09-18 04:25:28] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.519687107035187
NO2(0)最好结果：{'pred_time': 1.1327352128664296e-05, 'wall_clock_time': 59.09574222564697, 'metric_for_logging': {'pred_time': 1.1327352128664296e-05}, 'val_loss': 3.519687107035187, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.093701124191284}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8436843429256311
NO2(0)的mse=27.73018280384307
NO2(0)的mae=3.458186727292173
NO2(0)的mar=0.25776861744837387
总共花费的时间为：71.76
六安市
2307A
2308A
2309A
2310A
[flaml.automl: 09-18 04:37:46] {2390} INFO - task = regression
[flaml.automl: 09-18 04:37:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:37:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:37:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:37:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:37:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:37:49] {3025} INFO - Estimated sufficient time budget=94561s. Estimated necessary time budget=95s.
[flaml.automl: 09-18 04:37:49] {3072} INFO -  at 2.4s,	estimator xgboost's best error=12.5744,	best estimator xgboost's best error=12.5744
[flaml.automl: 09-18 04:37:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:37:52] {3072} INFO -  at 6.2s,	estimator xgboost's best error=6.4300,	best estimator xgboost's best error=6.4300
[flaml.automl: 09-18 04:37:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:37:55] {3072} INFO -  at 8.5s,	estimator xgboost's best error=6.4300,	best estimator xgboost's best error=6.4300
[flaml.automl: 09-18 04:37:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:38:00] {3072} INFO -  at 13.5s,	estimator xgboost's best error=6.4300,	best estimator xgboost's best error=6.4300
[flaml.automl: 09-18 04:38:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:38:02] {3072} INFO -  at 15.6s,	estimator xgboost's best error=4.6142,	best estimator xgboost's best error=4.6142
[flaml.automl: 09-18 04:38:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:38:05] {3072} INFO -  at 18.3s,	estimator xgboost's best error=4.1248,	best estimator xgboost's best error=4.1248
[flaml.automl: 09-18 04:38:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:38:08] {3072} INFO -  at 21.4s,	estimator xgboost's best error=4.1248,	best estimator xgboost's best error=4.1248
[flaml.automl: 09-18 04:38:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:38:11] {3072} INFO -  at 24.9s,	estimator xgboost's best error=4.1248,	best estimator xgboost's best error=4.1248
[flaml.automl: 09-18 04:38:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:38:13] {3072} INFO -  at 27.0s,	estimator xgboost's best error=4.1248,	best estimator xgboost's best error=4.1248
[flaml.automl: 09-18 04:38:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:38:16] {3072} INFO -  at 29.6s,	estimator xgboost's best error=4.1248,	best estimator xgboost's best error=4.1248
[flaml.automl: 09-18 04:38:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:38:19] {3072} INFO -  at 32.6s,	estimator xgboost's best error=4.0417,	best estimator xgboost's best error=4.0417
[flaml.automl: 09-18 04:38:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:38:21] {3072} INFO -  at 34.7s,	estimator xgboost's best error=4.0417,	best estimator xgboost's best error=4.0417
[flaml.automl: 09-18 04:38:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:38:33] {3072} INFO -  at 46.8s,	estimator xgboost's best error=3.9623,	best estimator xgboost's best error=3.9623
[flaml.automl: 09-18 04:38:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:38:46] {3072} INFO -  at 59.8s,	estimator xgboost's best error=3.8684,	best estimator xgboost's best error=3.8684
[flaml.automl: 09-18 04:39:07] {3335} INFO - retrain xgboost for 21.1s
[flaml.automl: 09-18 04:39:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:39:07] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:39:07] {2637} INFO - Time taken to find the best model: 59.790353775024414
[flaml.automl: 09-18 04:39:07] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43366}
NO2(0)最佳损失：-2.8683515008887603
NO2(0)最好结果：{'pred_time': 1.727374516772294e-05, 'wall_clock_time': 59.790353775024414, 'metric_for_logging': {'pred_time': 1.727374516772294e-05}, 'val_loss': 3.8683515008887603, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43366}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43366, 'experiment_tag': 'exp', 'time_total_s': 12.996946811676025}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8056480500055828
NO2(0)的mse=32.81907113271566
NO2(0)的mae=3.773326310997451
NO2(0)的mar=0.26358067358410825
总共花费的时间为：81.89
亳州市
2311A
2312A
3332A
[flaml.automl: 09-18 04:49:02] {2390} INFO - task = regression
[flaml.automl: 09-18 04:49:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:49:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:49:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:49:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:49:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:49:03] {3025} INFO - Estimated sufficient time budget=12196s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 04:49:03] {3072} INFO -  at 1.4s,	estimator xgboost's best error=9.3067,	best estimator xgboost's best error=9.3067
[flaml.automl: 09-18 04:49:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:49:05] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.4802,	best estimator xgboost's best error=4.4802
[flaml.automl: 09-18 04:49:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:49:06] {3072} INFO -  at 4.8s,	estimator xgboost's best error=4.4802,	best estimator xgboost's best error=4.4802
[flaml.automl: 09-18 04:49:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:49:16] {3072} INFO -  at 14.8s,	estimator xgboost's best error=4.4802,	best estimator xgboost's best error=4.4802
[flaml.automl: 09-18 04:49:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:49:17] {3072} INFO -  at 15.9s,	estimator xgboost's best error=2.8548,	best estimator xgboost's best error=2.8548
[flaml.automl: 09-18 04:49:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:49:19] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.4729,	best estimator xgboost's best error=2.4729
[flaml.automl: 09-18 04:49:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:49:21] {3072} INFO -  at 19.2s,	estimator xgboost's best error=2.4729,	best estimator xgboost's best error=2.4729
[flaml.automl: 09-18 04:49:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:49:23] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.4729,	best estimator xgboost's best error=2.4729
[flaml.automl: 09-18 04:49:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:49:24] {3072} INFO -  at 22.8s,	estimator xgboost's best error=2.4729,	best estimator xgboost's best error=2.4729
[flaml.automl: 09-18 04:49:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:49:27] {3072} INFO -  at 25.5s,	estimator xgboost's best error=2.4729,	best estimator xgboost's best error=2.4729
[flaml.automl: 09-18 04:49:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:49:28] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.4729,	best estimator xgboost's best error=2.4729
[flaml.automl: 09-18 04:49:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:49:29] {3072} INFO -  at 27.8s,	estimator xgboost's best error=2.4729,	best estimator xgboost's best error=2.4729
[flaml.automl: 09-18 04:49:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:49:36] {3072} INFO -  at 34.3s,	estimator xgboost's best error=2.3085,	best estimator xgboost's best error=2.3085
[flaml.automl: 09-18 04:49:36] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:49:48] {3072} INFO -  at 46.3s,	estimator xgboost's best error=2.2191,	best estimator xgboost's best error=2.2191
[flaml.automl: 09-18 04:49:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 04:49:54] {3072} INFO -  at 52.9s,	estimator xgboost's best error=2.2191,	best estimator xgboost's best error=2.2191
[flaml.automl: 09-18 04:50:06] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 04:50:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:50:06] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:50:06] {2637} INFO - Time taken to find the best model: 46.32874011993408
[flaml.automl: 09-18 04:50:06] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.219123518497197
NO2(0)最好结果：{'pred_time': 1.1710875935711793e-05, 'wall_clock_time': 46.32874011993408, 'metric_for_logging': {'pred_time': 1.1710875935711793e-05}, 'val_loss': 2.219123518497197, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.036335229873657}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8709167650787538
NO2(0)的mse=11.619955344386174
NO2(0)的mae=2.2272019601055764
NO2(0)的mar=0.18197515582849486
总共花费的时间为：65.50
池州市
3237A
3333A
3334A
[flaml.automl: 09-18 04:59:39] {2390} INFO - task = regression
[flaml.automl: 09-18 04:59:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:59:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:59:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:59:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:59:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:59:40] {3025} INFO - Estimated sufficient time budget=12155s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 04:59:40] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.8780,	best estimator xgboost's best error=13.8780
[flaml.automl: 09-18 04:59:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:59:42] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.7133,	best estimator xgboost's best error=6.7133
[flaml.automl: 09-18 04:59:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:59:44] {3072} INFO -  at 4.8s,	estimator xgboost's best error=6.7133,	best estimator xgboost's best error=6.7133
[flaml.automl: 09-18 04:59:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:59:54] {3072} INFO -  at 14.8s,	estimator xgboost's best error=6.7133,	best estimator xgboost's best error=6.7133
[flaml.automl: 09-18 04:59:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:59:55] {3072} INFO -  at 15.9s,	estimator xgboost's best error=4.3623,	best estimator xgboost's best error=4.3623
[flaml.automl: 09-18 04:59:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:59:57] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.9142,	best estimator xgboost's best error=3.9142
[flaml.automl: 09-18 04:59:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:00:00] {3072} INFO -  at 21.5s,	estimator xgboost's best error=3.9142,	best estimator xgboost's best error=3.9142
[flaml.automl: 09-18 05:00:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:00:04] {3072} INFO -  at 25.6s,	estimator xgboost's best error=3.9142,	best estimator xgboost's best error=3.9142
[flaml.automl: 09-18 05:00:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:00:06] {3072} INFO -  at 27.6s,	estimator xgboost's best error=3.9142,	best estimator xgboost's best error=3.9142
[flaml.automl: 09-18 05:00:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:00:11] {3072} INFO -  at 32.5s,	estimator xgboost's best error=3.9142,	best estimator xgboost's best error=3.9142
[flaml.automl: 09-18 05:00:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:00:13] {3072} INFO -  at 34.5s,	estimator xgboost's best error=3.9142,	best estimator xgboost's best error=3.9142
[flaml.automl: 09-18 05:00:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:00:15] {3072} INFO -  at 36.5s,	estimator xgboost's best error=3.9142,	best estimator xgboost's best error=3.9142
[flaml.automl: 09-18 05:00:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:00:26] {3072} INFO -  at 47.3s,	estimator xgboost's best error=3.7939,	best estimator xgboost's best error=3.7939
[flaml.automl: 09-18 05:00:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:00:38] {3072} INFO -  at 59.5s,	estimator xgboost's best error=3.6520,	best estimator xgboost's best error=3.6520
[flaml.automl: 09-18 05:00:58] {3335} INFO - retrain xgboost for 19.5s
[flaml.automl: 09-18 05:00:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:00:58] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:00:58] {2637} INFO - Time taken to find the best model: 59.48228979110718
[flaml.automl: 09-18 05:00:58] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.6519734520489116
NO2(0)最好结果：{'pred_time': 2.479319675209607e-05, 'wall_clock_time': 59.48228979110718, 'metric_for_logging': {'pred_time': 2.479319675209607e-05}, 'val_loss': 3.6519734520489116, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.19528579711914}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.797239974616708
NO2(0)的mse=31.337360015867226
NO2(0)的mae=3.6791290162221766
NO2(0)的mar=0.19935308781788455
总共花费的时间为：79.58
宣城市
2316A
2317A
2318A
3470A
[flaml.automl: 09-18 05:13:25] {2390} INFO - task = regression
[flaml.automl: 09-18 05:13:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:13:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:13:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:13:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:13:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:13:28] {3025} INFO - Estimated sufficient time budget=141188s. Estimated necessary time budget=141s.
[flaml.automl: 09-18 05:13:28] {3072} INFO -  at 3.6s,	estimator xgboost's best error=13.9973,	best estimator xgboost's best error=13.9973
[flaml.automl: 09-18 05:13:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:13:34] {3072} INFO -  at 9.6s,	estimator xgboost's best error=6.7444,	best estimator xgboost's best error=6.7444
[flaml.automl: 09-18 05:13:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:13:38] {3072} INFO -  at 13.1s,	estimator xgboost's best error=6.7444,	best estimator xgboost's best error=6.7444
[flaml.automl: 09-18 05:13:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:13:42] {3072} INFO -  at 17.9s,	estimator xgboost's best error=6.7444,	best estimator xgboost's best error=6.7444
[flaml.automl: 09-18 05:13:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:13:46] {3072} INFO -  at 21.0s,	estimator xgboost's best error=4.6223,	best estimator xgboost's best error=4.6223
[flaml.automl: 09-18 05:13:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:13:49] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.3599,	best estimator xgboost's best error=4.3599
[flaml.automl: 09-18 05:13:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:13:53] {3072} INFO -  at 28.2s,	estimator xgboost's best error=4.3539,	best estimator xgboost's best error=4.3539
[flaml.automl: 09-18 05:13:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:13:56] {3072} INFO -  at 31.0s,	estimator xgboost's best error=4.3539,	best estimator xgboost's best error=4.3539
[flaml.automl: 09-18 05:13:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:13:58] {3072} INFO -  at 33.6s,	estimator xgboost's best error=4.3539,	best estimator xgboost's best error=4.3539
[flaml.automl: 09-18 05:13:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:14:01] {3072} INFO -  at 36.1s,	estimator xgboost's best error=4.3539,	best estimator xgboost's best error=4.3539
[flaml.automl: 09-18 05:14:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:14:03] {3072} INFO -  at 38.8s,	estimator xgboost's best error=4.3539,	best estimator xgboost's best error=4.3539
[flaml.automl: 09-18 05:14:03] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:14:08] {3072} INFO -  at 43.3s,	estimator xgboost's best error=4.2126,	best estimator xgboost's best error=4.2126
[flaml.automl: 09-18 05:14:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:14:11] {3072} INFO -  at 46.5s,	estimator xgboost's best error=4.2126,	best estimator xgboost's best error=4.2126
[flaml.automl: 09-18 05:14:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:14:20] {3072} INFO -  at 55.6s,	estimator xgboost's best error=3.9465,	best estimator xgboost's best error=3.9465
[flaml.automl: 09-18 05:14:27] {3335} INFO - retrain xgboost for 6.9s
[flaml.automl: 09-18 05:14:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 05:14:27] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:14:27] {2637} INFO - Time taken to find the best model: 55.61238479614258
[flaml.automl: 09-18 05:14:27] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 42069}
NO2(0)最佳损失：-2.946504763026926
NO2(0)最好结果：{'pred_time': 8.53462015243775e-06, 'wall_clock_time': 55.61238479614258, 'metric_for_logging': {'pred_time': 8.53462015243775e-06}, 'val_loss': 3.946504763026926, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 42069}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 42069, 'experiment_tag': 'exp', 'time_total_s': 9.10691785812378}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7556318755610008
NO2(0)的mse=32.66948860289397
NO2(0)的mae=3.9399259778963955
NO2(0)的mar=0.20860964651767627
总共花费的时间为：63.21
莆田市
2319A
2320A
2321A
2322A
2323A
[flaml.automl: 09-18 05:30:02] {2390} INFO - task = regression
[flaml.automl: 09-18 05:30:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:30:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:30:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:30:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:30:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:30:03] {3025} INFO - Estimated sufficient time budget=63120s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 05:30:03] {3072} INFO -  at 1.5s,	estimator xgboost's best error=8.5550,	best estimator xgboost's best error=8.5550
[flaml.automl: 09-18 05:30:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:30:05] {3072} INFO -  at 3.6s,	estimator xgboost's best error=4.1107,	best estimator xgboost's best error=4.1107
[flaml.automl: 09-18 05:30:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:30:06] {3072} INFO -  at 4.8s,	estimator xgboost's best error=4.1107,	best estimator xgboost's best error=4.1107
[flaml.automl: 09-18 05:30:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:30:11] {3072} INFO -  at 9.6s,	estimator xgboost's best error=4.1107,	best estimator xgboost's best error=4.1107
[flaml.automl: 09-18 05:30:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:30:12] {3072} INFO -  at 10.8s,	estimator xgboost's best error=2.5565,	best estimator xgboost's best error=2.5565
[flaml.automl: 09-18 05:30:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:30:14] {3072} INFO -  at 12.4s,	estimator xgboost's best error=2.2233,	best estimator xgboost's best error=2.2233
[flaml.automl: 09-18 05:30:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:30:15] {3072} INFO -  at 14.0s,	estimator xgboost's best error=2.2233,	best estimator xgboost's best error=2.2233
[flaml.automl: 09-18 05:30:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:30:18] {3072} INFO -  at 16.4s,	estimator xgboost's best error=2.2233,	best estimator xgboost's best error=2.2233
[flaml.automl: 09-18 05:30:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:30:19] {3072} INFO -  at 17.6s,	estimator xgboost's best error=2.2233,	best estimator xgboost's best error=2.2233
[flaml.automl: 09-18 05:30:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:30:22] {3072} INFO -  at 20.3s,	estimator xgboost's best error=2.2233,	best estimator xgboost's best error=2.2233
[flaml.automl: 09-18 05:30:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:30:23] {3072} INFO -  at 21.9s,	estimator xgboost's best error=2.1996,	best estimator xgboost's best error=2.1996
[flaml.automl: 09-18 05:30:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:30:24] {3072} INFO -  at 23.0s,	estimator xgboost's best error=2.1996,	best estimator xgboost's best error=2.1996
[flaml.automl: 09-18 05:30:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:30:31] {3072} INFO -  at 29.6s,	estimator xgboost's best error=2.0907,	best estimator xgboost's best error=2.0907
[flaml.automl: 09-18 05:30:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:30:43] {3072} INFO -  at 41.7s,	estimator xgboost's best error=2.0515,	best estimator xgboost's best error=2.0515
[flaml.automl: 09-18 05:30:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:30:50] {3072} INFO -  at 48.2s,	estimator xgboost's best error=2.0515,	best estimator xgboost's best error=2.0515
[flaml.automl: 09-18 05:31:02] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 05:31:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:31:02] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:31:02] {2637} INFO - Time taken to find the best model: 41.727742433547974
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52592}
NO2(0)最佳损失：-1.051473446205656
NO2(0)最好结果：{'pred_time': 7.426591869252419e-06, 'wall_clock_time': 41.727742433547974, 'metric_for_logging': {'pred_time': 7.426591869252419e-06}, 'val_loss': 2.051473446205656, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52592}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52592, 'experiment_tag': 'exp', 'time_total_s': 12.128512620925903}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.838973508893683
NO2(0)的mse=8.95555077221669
NO2(0)的mae=2.0116876473728653
NO2(0)的mar=0.172887915293476
总共花费的时间为：61.41
三明市
2324A
2325A
2326A
2327A
[flaml.automl: 09-18 05:43:30] {2390} INFO - task = regression
[flaml.automl: 09-18 05:43:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:43:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:43:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:43:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:43:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:43:31] {3025} INFO - Estimated sufficient time budget=49810s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 05:43:31] {3072} INFO -  at 1.4s,	estimator xgboost's best error=11.6867,	best estimator xgboost's best error=11.6867
[flaml.automl: 09-18 05:43:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:43:33] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.7201,	best estimator xgboost's best error=5.7201
[flaml.automl: 09-18 05:43:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:43:34] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.7201,	best estimator xgboost's best error=5.7201
[flaml.automl: 09-18 05:43:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:43:41] {3072} INFO -  at 11.0s,	estimator xgboost's best error=5.7201,	best estimator xgboost's best error=5.7201
[flaml.automl: 09-18 05:43:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:43:42] {3072} INFO -  at 12.1s,	estimator xgboost's best error=3.7688,	best estimator xgboost's best error=3.7688
[flaml.automl: 09-18 05:43:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:43:43] {3072} INFO -  at 13.7s,	estimator xgboost's best error=3.3825,	best estimator xgboost's best error=3.3825
[flaml.automl: 09-18 05:43:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:43:45] {3072} INFO -  at 15.3s,	estimator xgboost's best error=3.3825,	best estimator xgboost's best error=3.3825
[flaml.automl: 09-18 05:43:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:43:48] {3072} INFO -  at 17.8s,	estimator xgboost's best error=3.3825,	best estimator xgboost's best error=3.3825
[flaml.automl: 09-18 05:43:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:43:49] {3072} INFO -  at 18.9s,	estimator xgboost's best error=3.3825,	best estimator xgboost's best error=3.3825
[flaml.automl: 09-18 05:43:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:43:51] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.3825,	best estimator xgboost's best error=3.3825
[flaml.automl: 09-18 05:43:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:43:53] {3072} INFO -  at 23.2s,	estimator xgboost's best error=3.3728,	best estimator xgboost's best error=3.3728
[flaml.automl: 09-18 05:43:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:43:54] {3072} INFO -  at 24.4s,	estimator xgboost's best error=3.3728,	best estimator xgboost's best error=3.3728
[flaml.automl: 09-18 05:43:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:44:01] {3072} INFO -  at 30.9s,	estimator xgboost's best error=3.2242,	best estimator xgboost's best error=3.2242
[flaml.automl: 09-18 05:44:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:44:13] {3072} INFO -  at 43.0s,	estimator xgboost's best error=3.1264,	best estimator xgboost's best error=3.1264
[flaml.automl: 09-18 05:44:13] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:44:19] {3072} INFO -  at 49.6s,	estimator xgboost's best error=3.1264,	best estimator xgboost's best error=3.1264
[flaml.automl: 09-18 05:44:31] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 05:44:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:44:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:44:31] {2637} INFO - Time taken to find the best model: 43.014037132263184
[flaml.automl: 09-18 05:44:31] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41878}
NO2(0)最佳损失：-2.1264111313842426
NO2(0)最好结果：{'pred_time': 8.55002630714905e-06, 'wall_clock_time': 43.014037132263184, 'metric_for_logging': {'pred_time': 8.55002630714905e-06}, 'val_loss': 3.1264111313842426, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41878}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41878, 'experiment_tag': 'exp', 'time_total_s': 12.12417721748352}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7446104134932423
NO2(0)的mse=20.531545841535234
NO2(0)的mae=3.110684210547302
NO2(0)的mar=0.19738586120299645
总共花费的时间为：62.32
南平市
2331A
2332A
2333A
2334A
[flaml.automl: 09-18 05:56:55] {2390} INFO - task = regression
[flaml.automl: 09-18 05:56:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:56:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:56:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:56:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:56:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:56:58] {3025} INFO - Estimated sufficient time budget=91208s. Estimated necessary time budget=91s.
[flaml.automl: 09-18 05:56:58] {3072} INFO -  at 2.4s,	estimator xgboost's best error=6.3069,	best estimator xgboost's best error=6.3069
[flaml.automl: 09-18 05:56:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:57:01] {3072} INFO -  at 5.9s,	estimator xgboost's best error=3.0707,	best estimator xgboost's best error=3.0707
[flaml.automl: 09-18 05:57:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:57:03] {3072} INFO -  at 7.5s,	estimator xgboost's best error=3.0707,	best estimator xgboost's best error=3.0707
[flaml.automl: 09-18 05:57:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:57:09] {3072} INFO -  at 13.5s,	estimator xgboost's best error=3.0707,	best estimator xgboost's best error=3.0707
[flaml.automl: 09-18 05:57:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:57:10] {3072} INFO -  at 15.2s,	estimator xgboost's best error=2.1563,	best estimator xgboost's best error=2.1563
[flaml.automl: 09-18 05:57:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:57:13] {3072} INFO -  at 17.8s,	estimator xgboost's best error=1.9374,	best estimator xgboost's best error=1.9374
[flaml.automl: 09-18 05:57:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:57:15] {3072} INFO -  at 19.5s,	estimator xgboost's best error=1.9374,	best estimator xgboost's best error=1.9374
[flaml.automl: 09-18 05:57:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:57:17] {3072} INFO -  at 21.9s,	estimator xgboost's best error=1.9374,	best estimator xgboost's best error=1.9374
[flaml.automl: 09-18 05:57:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:57:18] {3072} INFO -  at 23.0s,	estimator xgboost's best error=1.9374,	best estimator xgboost's best error=1.9374
[flaml.automl: 09-18 05:57:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:57:21] {3072} INFO -  at 25.7s,	estimator xgboost's best error=1.9374,	best estimator xgboost's best error=1.9374
[flaml.automl: 09-18 05:57:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:57:22] {3072} INFO -  at 27.3s,	estimator xgboost's best error=1.9007,	best estimator xgboost's best error=1.9007
[flaml.automl: 09-18 05:57:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:57:24] {3072} INFO -  at 28.4s,	estimator xgboost's best error=1.9007,	best estimator xgboost's best error=1.9007
[flaml.automl: 09-18 05:57:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:57:30] {3072} INFO -  at 34.9s,	estimator xgboost's best error=1.8234,	best estimator xgboost's best error=1.8234
[flaml.automl: 09-18 05:57:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:57:42] {3072} INFO -  at 47.0s,	estimator xgboost's best error=1.7844,	best estimator xgboost's best error=1.7844
[flaml.automl: 09-18 05:57:42] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:57:49] {3072} INFO -  at 53.5s,	estimator xgboost's best error=1.7844,	best estimator xgboost's best error=1.7844
[flaml.automl: 09-18 05:58:01] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 05:58:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:58:01] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:58:01] {2637} INFO - Time taken to find the best model: 46.97431802749634
[flaml.automl: 09-18 05:58:01] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42131}
NO2(0)最佳损失：-0.7844466034391404
NO2(0)最好结果：{'pred_time': 8.713430138237199e-06, 'wall_clock_time': 46.97431802749634, 'metric_for_logging': {'pred_time': 8.713430138237199e-06}, 'val_loss': 1.7844466034391404, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42131}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42131, 'experiment_tag': 'exp', 'time_total_s': 12.043738842010498}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8476008945255403
NO2(0)的mse=7.886223868885296
NO2(0)的mae=1.812957671113951
NO2(0)的mar=0.21078841844124654
总共花费的时间为：66.36
龙岩市
2335A
2336A
2337A
2338A
[flaml.automl: 09-18 06:10:42] {2390} INFO - task = regression
[flaml.automl: 09-18 06:10:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:10:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:10:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:10:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:10:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:10:43] {3025} INFO - Estimated sufficient time budget=55770s. Estimated necessary time budget=56s.
[flaml.automl: 09-18 06:10:43] {3072} INFO -  at 1.5s,	estimator xgboost's best error=11.8700,	best estimator xgboost's best error=11.8700
[flaml.automl: 09-18 06:10:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:10:45] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.7603,	best estimator xgboost's best error=5.7603
[flaml.automl: 09-18 06:10:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:10:46] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.7603,	best estimator xgboost's best error=5.7603
[flaml.automl: 09-18 06:10:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:10:52] {3072} INFO -  at 10.7s,	estimator xgboost's best error=5.7603,	best estimator xgboost's best error=5.7603
[flaml.automl: 09-18 06:10:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:10:53] {3072} INFO -  at 11.8s,	estimator xgboost's best error=3.7885,	best estimator xgboost's best error=3.7885
[flaml.automl: 09-18 06:10:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:10:55] {3072} INFO -  at 13.4s,	estimator xgboost's best error=3.3545,	best estimator xgboost's best error=3.3545
[flaml.automl: 09-18 06:10:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:10:56] {3072} INFO -  at 15.0s,	estimator xgboost's best error=3.3545,	best estimator xgboost's best error=3.3545
[flaml.automl: 09-18 06:10:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:10:59] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.3545,	best estimator xgboost's best error=3.3545
[flaml.automl: 09-18 06:10:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:11:00] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.3545,	best estimator xgboost's best error=3.3545
[flaml.automl: 09-18 06:11:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:11:02] {3072} INFO -  at 21.1s,	estimator xgboost's best error=3.3545,	best estimator xgboost's best error=3.3545
[flaml.automl: 09-18 06:11:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:11:04] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.3330,	best estimator xgboost's best error=3.3330
[flaml.automl: 09-18 06:11:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:11:05] {3072} INFO -  at 23.8s,	estimator xgboost's best error=3.3330,	best estimator xgboost's best error=3.3330
[flaml.automl: 09-18 06:11:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:11:12] {3072} INFO -  at 30.3s,	estimator xgboost's best error=3.1488,	best estimator xgboost's best error=3.1488
[flaml.automl: 09-18 06:11:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:11:24] {3072} INFO -  at 42.3s,	estimator xgboost's best error=3.1147,	best estimator xgboost's best error=3.1147
[flaml.automl: 09-18 06:11:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:11:30] {3072} INFO -  at 48.8s,	estimator xgboost's best error=3.1147,	best estimator xgboost's best error=3.1147
[flaml.automl: 09-18 06:11:42] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 06:11:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:11:42] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:11:42] {2637} INFO - Time taken to find the best model: 42.29767203330994
[flaml.automl: 09-18 06:11:42] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42895}
NO2(0)最佳损失：-2.1147234170772458
NO2(0)最好结果：{'pred_time': 9.597710700862334e-06, 'wall_clock_time': 42.29767203330994, 'metric_for_logging': {'pred_time': 9.597710700862334e-06}, 'val_loss': 3.1147234170772458, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42895}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42895, 'experiment_tag': 'exp', 'time_total_s': 12.028974294662476}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7820027133825198
NO2(0)的mse=22.565057469463337
NO2(0)的mae=3.1503283203188386
NO2(0)的mar=0.19800596231734063
总共花费的时间为：61.63
宁德市
2339A
3209A
[flaml.automl: 09-18 06:17:47] {2390} INFO - task = regression
[flaml.automl: 09-18 06:17:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:17:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:17:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:17:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:17:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:17:49] {3025} INFO - Estimated sufficient time budget=11781s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 06:17:49] {3072} INFO -  at 1.3s,	estimator xgboost's best error=9.0702,	best estimator xgboost's best error=9.0702
[flaml.automl: 09-18 06:17:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:17:50] {3072} INFO -  at 3.1s,	estimator xgboost's best error=4.9224,	best estimator xgboost's best error=4.9224
[flaml.automl: 09-18 06:17:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:17:52] {3072} INFO -  at 4.3s,	estimator xgboost's best error=4.9224,	best estimator xgboost's best error=4.9224
[flaml.automl: 09-18 06:17:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:18:00] {3072} INFO -  at 12.6s,	estimator xgboost's best error=4.9224,	best estimator xgboost's best error=4.9224
[flaml.automl: 09-18 06:18:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:18:01] {3072} INFO -  at 13.8s,	estimator xgboost's best error=2.8599,	best estimator xgboost's best error=2.8599
[flaml.automl: 09-18 06:18:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:18:03] {3072} INFO -  at 15.3s,	estimator xgboost's best error=2.5986,	best estimator xgboost's best error=2.5986
[flaml.automl: 09-18 06:18:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:18:04] {3072} INFO -  at 16.9s,	estimator xgboost's best error=2.5986,	best estimator xgboost's best error=2.5986
[flaml.automl: 09-18 06:18:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:18:07] {3072} INFO -  at 19.4s,	estimator xgboost's best error=2.5986,	best estimator xgboost's best error=2.5986
[flaml.automl: 09-18 06:18:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:18:08] {3072} INFO -  at 20.5s,	estimator xgboost's best error=2.5986,	best estimator xgboost's best error=2.5986
[flaml.automl: 09-18 06:18:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:18:10] {3072} INFO -  at 23.0s,	estimator xgboost's best error=2.5986,	best estimator xgboost's best error=2.5986
[flaml.automl: 09-18 06:18:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:18:11] {3072} INFO -  at 24.1s,	estimator xgboost's best error=2.5986,	best estimator xgboost's best error=2.5986
[flaml.automl: 09-18 06:18:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:18:12] {3072} INFO -  at 25.3s,	estimator xgboost's best error=2.5986,	best estimator xgboost's best error=2.5986
[flaml.automl: 09-18 06:18:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:18:19] {3072} INFO -  at 31.4s,	estimator xgboost's best error=2.5612,	best estimator xgboost's best error=2.5612
[flaml.automl: 09-18 06:18:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:18:29] {3072} INFO -  at 41.8s,	estimator xgboost's best error=2.5247,	best estimator xgboost's best error=2.5247
[flaml.automl: 09-18 06:18:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:18:35] {3072} INFO -  at 47.9s,	estimator xgboost's best error=2.5247,	best estimator xgboost's best error=2.5247
[flaml.automl: 09-18 06:18:35] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 06:18:47] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.5093,	best estimator xgboost's best error=2.5093
[flaml.automl: 09-18 06:19:04] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-18 06:19:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:19:04] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:19:04] {2637} INFO - Time taken to find the best model: 59.54067802429199
[flaml.automl: 09-18 06:19:04] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-1.5093041346161136
NO2(0)最好结果：{'pred_time': 1.8325075171854345e-05, 'wall_clock_time': 59.54067802429199, 'metric_for_logging': {'pred_time': 1.8325075171854345e-05}, 'val_loss': 2.5093041346161136, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 11.649553775787354}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.735099760763495
NO2(0)的mse=11.47279688531506
NO2(0)的mae=2.4547088860306676
NO2(0)的mar=0.2047504681030962
总共花费的时间为：77.27
景德镇市
2342A
2343A
2344A
2345A
2346A
[flaml.automl: 09-18 06:35:01] {2390} INFO - task = regression
[flaml.automl: 09-18 06:35:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:35:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:35:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:35:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:35:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:35:03] {3025} INFO - Estimated sufficient time budget=96600s. Estimated necessary time budget=97s.
[flaml.automl: 09-18 06:35:03] {3072} INFO -  at 2.2s,	estimator xgboost's best error=10.3681,	best estimator xgboost's best error=10.3681
[flaml.automl: 09-18 06:35:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:35:06] {3072} INFO -  at 5.4s,	estimator xgboost's best error=5.1967,	best estimator xgboost's best error=5.1967
[flaml.automl: 09-18 06:35:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:35:08] {3072} INFO -  at 7.1s,	estimator xgboost's best error=5.1967,	best estimator xgboost's best error=5.1967
[flaml.automl: 09-18 06:35:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:35:13] {3072} INFO -  at 11.9s,	estimator xgboost's best error=5.1967,	best estimator xgboost's best error=5.1967
[flaml.automl: 09-18 06:35:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:35:15] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.6908,	best estimator xgboost's best error=3.6908
[flaml.automl: 09-18 06:35:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:35:17] {3072} INFO -  at 16.3s,	estimator xgboost's best error=3.3565,	best estimator xgboost's best error=3.3565
[flaml.automl: 09-18 06:35:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:35:20] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.3565,	best estimator xgboost's best error=3.3565
[flaml.automl: 09-18 06:35:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:35:23] {3072} INFO -  at 22.4s,	estimator xgboost's best error=3.3565,	best estimator xgboost's best error=3.3565
[flaml.automl: 09-18 06:35:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:35:25] {3072} INFO -  at 24.6s,	estimator xgboost's best error=3.3565,	best estimator xgboost's best error=3.3565
[flaml.automl: 09-18 06:35:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:35:28] {3072} INFO -  at 27.0s,	estimator xgboost's best error=3.3565,	best estimator xgboost's best error=3.3565
[flaml.automl: 09-18 06:35:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:35:31] {3072} INFO -  at 29.9s,	estimator xgboost's best error=3.3521,	best estimator xgboost's best error=3.3521
[flaml.automl: 09-18 06:35:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:35:33] {3072} INFO -  at 32.1s,	estimator xgboost's best error=3.3521,	best estimator xgboost's best error=3.3521
[flaml.automl: 09-18 06:35:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:35:40] {3072} INFO -  at 39.4s,	estimator xgboost's best error=3.2388,	best estimator xgboost's best error=3.2388
[flaml.automl: 09-18 06:35:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:35:52] {3072} INFO -  at 51.5s,	estimator xgboost's best error=3.1527,	best estimator xgboost's best error=3.1527
[flaml.automl: 09-18 06:36:04] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 06:36:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:36:04] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:36:04] {2637} INFO - Time taken to find the best model: 51.5215106010437
[flaml.automl: 09-18 06:36:04] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51528}
NO2(0)最佳损失：-2.152712665818348
NO2(0)最好结果：{'pred_time': 7.169837098916094e-06, 'wall_clock_time': 51.5215106010437, 'metric_for_logging': {'pred_time': 7.169837098916094e-06}, 'val_loss': 3.152712665818348, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51528}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51528, 'experiment_tag': 'exp', 'time_total_s': 12.100620031356812}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7849950150277858
NO2(0)的mse=21.259430512244613
NO2(0)的mae=3.0619444975272847
NO2(0)的mar=0.22511082911437993
总共花费的时间为：64.66
萍乡市
2347A
2348A
2349A
2350A
2351A
[flaml.automl: 09-18 06:52:35] {2390} INFO - task = regression
[flaml.automl: 09-18 06:52:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:52:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:52:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:52:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:52:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:52:36] {3025} INFO - Estimated sufficient time budget=61974s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 06:52:36] {3072} INFO -  at 1.5s,	estimator xgboost's best error=12.2881,	best estimator xgboost's best error=12.2881
[flaml.automl: 09-18 06:52:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:52:38] {3072} INFO -  at 3.6s,	estimator xgboost's best error=6.0146,	best estimator xgboost's best error=6.0146
[flaml.automl: 09-18 06:52:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:52:39] {3072} INFO -  at 4.8s,	estimator xgboost's best error=6.0146,	best estimator xgboost's best error=6.0146
[flaml.automl: 09-18 06:52:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:52:44] {3072} INFO -  at 9.6s,	estimator xgboost's best error=6.0146,	best estimator xgboost's best error=6.0146
[flaml.automl: 09-18 06:52:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:52:45] {3072} INFO -  at 10.7s,	estimator xgboost's best error=3.8579,	best estimator xgboost's best error=3.8579
[flaml.automl: 09-18 06:52:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:52:47] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.4718,	best estimator xgboost's best error=3.4718
[flaml.automl: 09-18 06:52:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:52:48] {3072} INFO -  at 13.9s,	estimator xgboost's best error=3.4718,	best estimator xgboost's best error=3.4718
[flaml.automl: 09-18 06:52:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:52:51] {3072} INFO -  at 16.4s,	estimator xgboost's best error=3.4718,	best estimator xgboost's best error=3.4718
[flaml.automl: 09-18 06:52:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:52:52] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.4718,	best estimator xgboost's best error=3.4718
[flaml.automl: 09-18 06:52:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:52:54] {3072} INFO -  at 20.2s,	estimator xgboost's best error=3.4718,	best estimator xgboost's best error=3.4718
[flaml.automl: 09-18 06:52:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:52:56] {3072} INFO -  at 21.8s,	estimator xgboost's best error=3.4470,	best estimator xgboost's best error=3.4470
[flaml.automl: 09-18 06:52:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:52:57] {3072} INFO -  at 23.0s,	estimator xgboost's best error=3.4470,	best estimator xgboost's best error=3.4470
[flaml.automl: 09-18 06:52:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:53:04] {3072} INFO -  at 29.5s,	estimator xgboost's best error=3.2960,	best estimator xgboost's best error=3.2960
[flaml.automl: 09-18 06:53:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:53:16] {3072} INFO -  at 41.6s,	estimator xgboost's best error=3.2285,	best estimator xgboost's best error=3.2285
[flaml.automl: 09-18 06:53:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:53:22] {3072} INFO -  at 48.1s,	estimator xgboost's best error=3.2285,	best estimator xgboost's best error=3.2285
[flaml.automl: 09-18 06:53:34] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 06:53:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:53:34] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:53:34] {2637} INFO - Time taken to find the best model: 41.58460831642151
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51670}
NO2(0)最佳损失：-2.2285466593800836
NO2(0)最好结果：{'pred_time': 7.062454914556734e-06, 'wall_clock_time': 41.58460831642151, 'metric_for_logging': {'pred_time': 7.062454914556734e-06}, 'val_loss': 3.2285466593800836, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51670}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51670, 'experiment_tag': 'exp', 'time_total_s': 12.05774474143982}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8250874695740856
NO2(0)的mse=22.56471208292439
NO2(0)的mae=3.194748243399922
NO2(0)的mar=0.2119631158769502
总共花费的时间为：60.98
新余市
2352A
2353A
2354A
2355A
[flaml.automl: 09-18 07:05:41] {2390} INFO - task = regression
[flaml.automl: 09-18 07:05:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:05:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:05:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:05:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:05:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:05:43] {3025} INFO - Estimated sufficient time budget=12066s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 07:05:43] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.3663,	best estimator xgboost's best error=13.3663
[flaml.automl: 09-18 07:05:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:05:45] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.6515,	best estimator xgboost's best error=6.6515
[flaml.automl: 09-18 07:05:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:05:46] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.6515,	best estimator xgboost's best error=6.6515
[flaml.automl: 09-18 07:05:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:05:56] {3072} INFO -  at 14.7s,	estimator xgboost's best error=6.6515,	best estimator xgboost's best error=6.6515
[flaml.automl: 09-18 07:05:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:05:57] {3072} INFO -  at 15.9s,	estimator xgboost's best error=4.4981,	best estimator xgboost's best error=4.4981
[flaml.automl: 09-18 07:05:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:05:59] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.9960,	best estimator xgboost's best error=3.9960
[flaml.automl: 09-18 07:05:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:06:00] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.9960,	best estimator xgboost's best error=3.9960
[flaml.automl: 09-18 07:06:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:06:03] {3072} INFO -  at 21.5s,	estimator xgboost's best error=3.9960,	best estimator xgboost's best error=3.9960
[flaml.automl: 09-18 07:06:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:06:04] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.9960,	best estimator xgboost's best error=3.9960
[flaml.automl: 09-18 07:06:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:06:06] {3072} INFO -  at 25.4s,	estimator xgboost's best error=3.9960,	best estimator xgboost's best error=3.9960
[flaml.automl: 09-18 07:06:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:06:08] {3072} INFO -  at 26.6s,	estimator xgboost's best error=3.9960,	best estimator xgboost's best error=3.9960
[flaml.automl: 09-18 07:06:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:06:09] {3072} INFO -  at 27.7s,	estimator xgboost's best error=3.9960,	best estimator xgboost's best error=3.9960
[flaml.automl: 09-18 07:06:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:06:15] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.8196,	best estimator xgboost's best error=3.8196
[flaml.automl: 09-18 07:06:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:06:27] {3072} INFO -  at 46.3s,	estimator xgboost's best error=3.7606,	best estimator xgboost's best error=3.7606
[flaml.automl: 09-18 07:06:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:06:34] {3072} INFO -  at 52.8s,	estimator xgboost's best error=3.7606,	best estimator xgboost's best error=3.7606
[flaml.automl: 09-18 07:06:46] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 07:06:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:06:46] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:06:46] {2637} INFO - Time taken to find the best model: 46.30610251426697
[flaml.automl: 09-18 07:06:46] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.7605676278471947
NO2(0)最好结果：{'pred_time': 8.965872685401448e-06, 'wall_clock_time': 46.30610251426697, 'metric_for_logging': {'pred_time': 8.965872685401448e-06}, 'val_loss': 3.7605676278471947, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.063117980957031}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7645023849166384
NO2(0)的mse=33.449199539247104
NO2(0)的mae=3.796440342646565
NO2(0)的mar=0.23261667306001987
总共花费的时间为：65.54
鹰潭市
2357A
2358A
2359A
2360A
2361A
[flaml.automl: 09-18 07:22:23] {2390} INFO - task = regression
[flaml.automl: 09-18 07:22:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:22:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:22:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:22:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:22:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:22:25] {3025} INFO - Estimated sufficient time budget=119955s. Estimated necessary time budget=120s.
[flaml.automl: 09-18 07:22:25] {3072} INFO -  at 2.6s,	estimator xgboost's best error=9.7233,	best estimator xgboost's best error=9.7233
[flaml.automl: 09-18 07:22:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:22:29] {3072} INFO -  at 6.5s,	estimator xgboost's best error=5.1215,	best estimator xgboost's best error=5.1215
[flaml.automl: 09-18 07:22:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:22:31] {3072} INFO -  at 8.7s,	estimator xgboost's best error=5.1215,	best estimator xgboost's best error=5.1215
[flaml.automl: 09-18 07:22:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:22:35] {3072} INFO -  at 12.8s,	estimator xgboost's best error=5.1215,	best estimator xgboost's best error=5.1215
[flaml.automl: 09-18 07:22:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:22:38] {3072} INFO -  at 14.9s,	estimator xgboost's best error=3.7835,	best estimator xgboost's best error=3.7835
[flaml.automl: 09-18 07:22:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:22:40] {3072} INFO -  at 17.9s,	estimator xgboost's best error=3.4064,	best estimator xgboost's best error=3.4064
[flaml.automl: 09-18 07:22:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:22:43] {3072} INFO -  at 20.8s,	estimator xgboost's best error=3.4064,	best estimator xgboost's best error=3.4064
[flaml.automl: 09-18 07:22:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:22:47] {3072} INFO -  at 24.3s,	estimator xgboost's best error=3.4064,	best estimator xgboost's best error=3.4064
[flaml.automl: 09-18 07:22:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:22:49] {3072} INFO -  at 26.4s,	estimator xgboost's best error=3.4064,	best estimator xgboost's best error=3.4064
[flaml.automl: 09-18 07:22:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:22:52] {3072} INFO -  at 29.1s,	estimator xgboost's best error=3.4064,	best estimator xgboost's best error=3.4064
[flaml.automl: 09-18 07:22:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:22:55] {3072} INFO -  at 32.0s,	estimator xgboost's best error=3.4034,	best estimator xgboost's best error=3.4034
[flaml.automl: 09-18 07:22:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:22:57] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.4034,	best estimator xgboost's best error=3.4034
[flaml.automl: 09-18 07:22:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:23:07] {3072} INFO -  at 44.8s,	estimator xgboost's best error=3.2848,	best estimator xgboost's best error=3.2848
[flaml.automl: 09-18 07:23:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:23:20] {3072} INFO -  at 56.9s,	estimator xgboost's best error=3.1826,	best estimator xgboost's best error=3.1826
[flaml.automl: 09-18 07:23:32] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 07:23:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:23:32] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:23:32] {2637} INFO - Time taken to find the best model: 56.926307678222656
[flaml.automl: 09-18 07:23:32] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51544}
NO2(0)最佳损失：-2.182646478146481
NO2(0)最好结果：{'pred_time': 7.07967487793395e-06, 'wall_clock_time': 56.926307678222656, 'metric_for_logging': {'pred_time': 7.07967487793395e-06}, 'val_loss': 3.182646478146481, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51544}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51544, 'experiment_tag': 'exp', 'time_total_s': 12.107383728027344}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7759103680539494
NO2(0)的mse=23.33206418999938
NO2(0)的mae=3.1751933219221535
NO2(0)的mar=0.3199385746290489
总共花费的时间为：69.95
赣州市
2362A
2363A
2364A
2365A
2366A
3109A
[flaml.automl: 09-18 07:42:09] {2390} INFO - task = regression
[flaml.automl: 09-18 07:42:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:42:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:42:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:42:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:42:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:42:10] {3025} INFO - Estimated sufficient time budget=71919s. Estimated necessary time budget=72s.
[flaml.automl: 09-18 07:42:10] {3072} INFO -  at 1.5s,	estimator xgboost's best error=10.0332,	best estimator xgboost's best error=10.0332
[flaml.automl: 09-18 07:42:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:42:12] {3072} INFO -  at 3.6s,	estimator xgboost's best error=4.8787,	best estimator xgboost's best error=4.8787
[flaml.automl: 09-18 07:42:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:42:13] {3072} INFO -  at 4.8s,	estimator xgboost's best error=4.8787,	best estimator xgboost's best error=4.8787
[flaml.automl: 09-18 07:42:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:42:17] {3072} INFO -  at 9.0s,	estimator xgboost's best error=4.8787,	best estimator xgboost's best error=4.8787
[flaml.automl: 09-18 07:42:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:42:19] {3072} INFO -  at 10.2s,	estimator xgboost's best error=3.3216,	best estimator xgboost's best error=3.3216
[flaml.automl: 09-18 07:42:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:42:20] {3072} INFO -  at 11.8s,	estimator xgboost's best error=2.9847,	best estimator xgboost's best error=2.9847
[flaml.automl: 09-18 07:42:20] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:42:22] {3072} INFO -  at 13.4s,	estimator xgboost's best error=2.9847,	best estimator xgboost's best error=2.9847
[flaml.automl: 09-18 07:42:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:42:24] {3072} INFO -  at 15.8s,	estimator xgboost's best error=2.9847,	best estimator xgboost's best error=2.9847
[flaml.automl: 09-18 07:42:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:42:25] {3072} INFO -  at 17.0s,	estimator xgboost's best error=2.9847,	best estimator xgboost's best error=2.9847
[flaml.automl: 09-18 07:42:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:42:28] {3072} INFO -  at 19.7s,	estimator xgboost's best error=2.9847,	best estimator xgboost's best error=2.9847
[flaml.automl: 09-18 07:42:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:42:30] {3072} INFO -  at 21.3s,	estimator xgboost's best error=2.9686,	best estimator xgboost's best error=2.9686
[flaml.automl: 09-18 07:42:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:42:31] {3072} INFO -  at 22.5s,	estimator xgboost's best error=2.9686,	best estimator xgboost's best error=2.9686
[flaml.automl: 09-18 07:42:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:42:37] {3072} INFO -  at 29.0s,	estimator xgboost's best error=2.8855,	best estimator xgboost's best error=2.8855
[flaml.automl: 09-18 07:42:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:42:50] {3072} INFO -  at 41.1s,	estimator xgboost's best error=2.8343,	best estimator xgboost's best error=2.8343
[flaml.automl: 09-18 07:42:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:42:56] {3072} INFO -  at 47.7s,	estimator xgboost's best error=2.8343,	best estimator xgboost's best error=2.8343
[flaml.automl: 09-18 07:42:56] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 07:43:08] {3072} INFO -  at 59.1s,	estimator xgboost's best error=2.8343,	best estimator xgboost's best error=2.8343
[flaml.automl: 09-18 07:43:20] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 07:43:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:43:20] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:43:20] {2637} INFO - Time taken to find the best model: 41.126952171325684
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 59773}
NO2(0)最佳损失：-1.8342616821907614
NO2(0)最好结果：{'pred_time': 6.379150762790587e-06, 'wall_clock_time': 41.126952171325684, 'metric_for_logging': {'pred_time': 6.379150762790587e-06}, 'val_loss': 2.8342616821907614, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 59773}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 59773, 'experiment_tag': 'exp', 'time_total_s': 12.08956503868103}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7828680252314195
NO2(0)的mse=17.50266565459948
NO2(0)的mae=2.729588130851425
NO2(0)的mar=0.19430673395830542
总共花费的时间为：72.35
吉安市
2367A
2368A
2369A
2370A
[flaml.automl: 09-18 07:55:43] {2390} INFO - task = regression
[flaml.automl: 09-18 07:55:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:55:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:55:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:55:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:55:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:55:45] {3025} INFO - Estimated sufficient time budget=83248s. Estimated necessary time budget=83s.
[flaml.automl: 09-18 07:55:45] {3072} INFO -  at 2.3s,	estimator xgboost's best error=9.9220,	best estimator xgboost's best error=9.9220
[flaml.automl: 09-18 07:55:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:55:49] {3072} INFO -  at 6.2s,	estimator xgboost's best error=4.8369,	best estimator xgboost's best error=4.8369
[flaml.automl: 09-18 07:55:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:55:51] {3072} INFO -  at 8.4s,	estimator xgboost's best error=4.8369,	best estimator xgboost's best error=4.8369
[flaml.automl: 09-18 07:55:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:55:56] {3072} INFO -  at 13.7s,	estimator xgboost's best error=4.8369,	best estimator xgboost's best error=4.8369
[flaml.automl: 09-18 07:55:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:55:58] {3072} INFO -  at 15.5s,	estimator xgboost's best error=3.0367,	best estimator xgboost's best error=3.0367
[flaml.automl: 09-18 07:55:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:56:00] {3072} INFO -  at 17.9s,	estimator xgboost's best error=2.7087,	best estimator xgboost's best error=2.7087
[flaml.automl: 09-18 07:56:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:56:03] {3072} INFO -  at 20.8s,	estimator xgboost's best error=2.7087,	best estimator xgboost's best error=2.7087
[flaml.automl: 09-18 07:56:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:56:07] {3072} INFO -  at 25.1s,	estimator xgboost's best error=2.7087,	best estimator xgboost's best error=2.7087
[flaml.automl: 09-18 07:56:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:56:09] {3072} INFO -  at 26.8s,	estimator xgboost's best error=2.7087,	best estimator xgboost's best error=2.7087
[flaml.automl: 09-18 07:56:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:56:12] {3072} INFO -  at 29.5s,	estimator xgboost's best error=2.7087,	best estimator xgboost's best error=2.7087
[flaml.automl: 09-18 07:56:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:56:15] {3072} INFO -  at 32.3s,	estimator xgboost's best error=2.7087,	best estimator xgboost's best error=2.7087
[flaml.automl: 09-18 07:56:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:56:17] {3072} INFO -  at 34.4s,	estimator xgboost's best error=2.7087,	best estimator xgboost's best error=2.7087
[flaml.automl: 09-18 07:56:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:56:23] {3072} INFO -  at 41.1s,	estimator xgboost's best error=2.5406,	best estimator xgboost's best error=2.5406
[flaml.automl: 09-18 07:56:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:56:36] {3072} INFO -  at 53.2s,	estimator xgboost's best error=2.4687,	best estimator xgboost's best error=2.4687
[flaml.automl: 09-18 07:56:48] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 07:56:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:56:48] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:56:48] {2637} INFO - Time taken to find the best model: 53.21312069892883
[flaml.automl: 09-18 07:56:48] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42733}
NO2(0)最佳损失：-1.4686840837642907
NO2(0)最好结果：{'pred_time': 8.34234088665562e-06, 'wall_clock_time': 53.21312069892883, 'metric_for_logging': {'pred_time': 8.34234088665562e-06}, 'val_loss': 2.4686840837642907, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42733}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42733, 'experiment_tag': 'exp', 'time_total_s': 12.086537837982178}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8585925400821742
NO2(0)的mse=13.17557179455524
NO2(0)的mae=2.4380818487930878
NO2(0)的mar=0.19647339420566198
总共花费的时间为：66.05
宜春市
2371A
2374A
2375A
3151A
3415A
[flaml.automl: 09-18 08:12:41] {2390} INFO - task = regression
[flaml.automl: 09-18 08:12:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:12:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:12:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:12:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:12:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:12:45] {3025} INFO - Estimated sufficient time budget=177913s. Estimated necessary time budget=178s.
[flaml.automl: 09-18 08:12:45] {3072} INFO -  at 3.9s,	estimator xgboost's best error=12.5077,	best estimator xgboost's best error=12.5077
[flaml.automl: 09-18 08:12:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:12:49] {3072} INFO -  at 8.5s,	estimator xgboost's best error=7.8024,	best estimator xgboost's best error=7.8024
[flaml.automl: 09-18 08:12:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:12:53] {3072} INFO -  at 11.9s,	estimator xgboost's best error=7.8024,	best estimator xgboost's best error=7.8024
[flaml.automl: 09-18 08:12:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:12:56] {3072} INFO -  at 15.4s,	estimator xgboost's best error=7.8024,	best estimator xgboost's best error=7.8024
[flaml.automl: 09-18 08:12:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:13:00] {3072} INFO -  at 18.7s,	estimator xgboost's best error=3.9941,	best estimator xgboost's best error=3.9941
[flaml.automl: 09-18 08:13:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:13:03] {3072} INFO -  at 22.3s,	estimator xgboost's best error=3.8305,	best estimator xgboost's best error=3.8305
[flaml.automl: 09-18 08:13:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:13:06] {3072} INFO -  at 25.3s,	estimator xgboost's best error=3.8305,	best estimator xgboost's best error=3.8305
[flaml.automl: 09-18 08:13:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:13:09] {3072} INFO -  at 27.5s,	estimator xgboost's best error=3.8305,	best estimator xgboost's best error=3.8305
[flaml.automl: 09-18 08:13:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:13:11] {3072} INFO -  at 29.5s,	estimator xgboost's best error=3.7950,	best estimator xgboost's best error=3.7950
[flaml.automl: 09-18 08:13:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:13:13] {3072} INFO -  at 31.9s,	estimator xgboost's best error=3.7950,	best estimator xgboost's best error=3.7950
[flaml.automl: 09-18 08:13:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:13:15] {3072} INFO -  at 33.9s,	estimator xgboost's best error=3.7950,	best estimator xgboost's best error=3.7950
[flaml.automl: 09-18 08:13:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:13:17] {3072} INFO -  at 35.7s,	estimator xgboost's best error=3.7950,	best estimator xgboost's best error=3.7950
[flaml.automl: 09-18 08:13:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:13:19] {3072} INFO -  at 37.8s,	estimator xgboost's best error=3.6701,	best estimator xgboost's best error=3.6701
[flaml.automl: 09-18 08:13:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:13:20] {3072} INFO -  at 39.4s,	estimator xgboost's best error=3.6701,	best estimator xgboost's best error=3.6701
[flaml.automl: 09-18 08:13:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:13:22] {3072} INFO -  at 40.6s,	estimator xgboost's best error=3.6701,	best estimator xgboost's best error=3.6701
[flaml.automl: 09-18 08:13:22] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 08:13:23] {3072} INFO -  at 41.8s,	estimator xgboost's best error=3.6701,	best estimator xgboost's best error=3.6701
[flaml.automl: 09-18 08:13:23] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 08:13:24] {3072} INFO -  at 43.0s,	estimator xgboost's best error=3.6701,	best estimator xgboost's best error=3.6701
[flaml.automl: 09-18 08:13:24] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 08:13:30] {3072} INFO -  at 49.3s,	estimator xgboost's best error=3.4493,	best estimator xgboost's best error=3.4493
[flaml.automl: 09-18 08:13:30] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 08:13:32] {3072} INFO -  at 51.2s,	estimator xgboost's best error=3.4493,	best estimator xgboost's best error=3.4493
[flaml.automl: 09-18 08:13:32] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 08:13:40] {3072} INFO -  at 59.2s,	estimator xgboost's best error=3.4493,	best estimator xgboost's best error=3.4493
[flaml.automl: 09-18 08:13:44] {3335} INFO - retrain xgboost for 4.3s
[flaml.automl: 09-18 08:13:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8001981686029529, colsample_bynode=1,
             colsample_bytree=0.4886990821367252, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.127746696828244,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.017060012345834385, reg_lambda=23.923563879604686,
             scale_pos_weight=1, subsample=0.8477820686885189,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 08:13:44] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:13:44] {2637} INFO - Time taken to find the best model: 49.33472943305969
[flaml.automl: 09-18 08:13:44] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.127746696828244, 'learning_rate': 1.0, 'subsample': 0.8477820686885189, 'colsample_bylevel': 0.8001981686029529, 'colsample_bytree': 0.4886990821367252, 'reg_alpha': 0.017060012345834385, 'reg_lambda': 23.923563879604686, 'FLAML_sample_size': 50994}
NO2(0)最佳损失：-2.4492650976125496
NO2(0)最好结果：{'pred_time': 7.122055613765309e-06, 'wall_clock_time': 49.33472943305969, 'metric_for_logging': {'pred_time': 7.122055613765309e-06}, 'val_loss': 3.4492650976125496, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.127746696828244, 'learning_rate': 1.0, 'subsample': 0.8477820686885189, 'colsample_bylevel': 0.8001981686029529, 'colsample_bytree': 0.4886990821367252, 'reg_alpha': 0.017060012345834385, 'reg_lambda': 23.923563879604686, 'FLAML_sample_size': 50994}, 'config/n_estimators': 8, 'config/max_leaves': 9, 'config/min_child_weight': 0.127746696828244, 'config/learning_rate': 1.0, 'config/subsample': 0.8477820686885189, 'config/colsample_bylevel': 0.8001981686029529, 'config/colsample_bytree': 0.4886990821367252, 'config/reg_alpha': 0.017060012345834385, 'config/reg_lambda': 23.923563879604686, 'config/FLAML_sample_size': 50994, 'experiment_tag': 'exp', 'time_total_s': 6.315384864807129}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8001981686029529, colsample_bynode=1,
             colsample_bytree=0.4886990821367252, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.127746696828244,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.017060012345834385, reg_lambda=23.923563879604686,
             scale_pos_weight=1, subsample=0.8477820686885189,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7761299827726802
NO2(0)的mse=26.297419881680877
NO2(0)的mae=3.53933743702714
NO2(0)的mar=0.23081370232755616
总共花费的时间为：64.39
抚州市
2376A
2377A
2378A
2379A
2380A
[flaml.automl: 09-18 08:29:56] {2390} INFO - task = regression
[flaml.automl: 09-18 08:29:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:29:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:29:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:29:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:29:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:29:57] {3025} INFO - Estimated sufficient time budget=61175s. Estimated necessary time budget=61s.
[flaml.automl: 09-18 08:29:57] {3072} INFO -  at 1.5s,	estimator xgboost's best error=8.1845,	best estimator xgboost's best error=8.1845
[flaml.automl: 09-18 08:29:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:30:00] {3072} INFO -  at 3.6s,	estimator xgboost's best error=4.0230,	best estimator xgboost's best error=4.0230
[flaml.automl: 09-18 08:30:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:30:01] {3072} INFO -  at 4.8s,	estimator xgboost's best error=4.0230,	best estimator xgboost's best error=4.0230
[flaml.automl: 09-18 08:30:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:30:06] {3072} INFO -  at 9.6s,	estimator xgboost's best error=4.0230,	best estimator xgboost's best error=4.0230
[flaml.automl: 09-18 08:30:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:30:07] {3072} INFO -  at 10.7s,	estimator xgboost's best error=2.6703,	best estimator xgboost's best error=2.6703
[flaml.automl: 09-18 08:30:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:30:08] {3072} INFO -  at 12.3s,	estimator xgboost's best error=2.3837,	best estimator xgboost's best error=2.3837
[flaml.automl: 09-18 08:30:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:30:10] {3072} INFO -  at 13.9s,	estimator xgboost's best error=2.3837,	best estimator xgboost's best error=2.3837
[flaml.automl: 09-18 08:30:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:30:12] {3072} INFO -  at 16.4s,	estimator xgboost's best error=2.3837,	best estimator xgboost's best error=2.3837
[flaml.automl: 09-18 08:30:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:30:13] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.3837,	best estimator xgboost's best error=2.3837
[flaml.automl: 09-18 08:30:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:30:16] {3072} INFO -  at 20.2s,	estimator xgboost's best error=2.3837,	best estimator xgboost's best error=2.3837
[flaml.automl: 09-18 08:30:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:30:18] {3072} INFO -  at 21.8s,	estimator xgboost's best error=2.3819,	best estimator xgboost's best error=2.3819
[flaml.automl: 09-18 08:30:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:30:19] {3072} INFO -  at 23.0s,	estimator xgboost's best error=2.3819,	best estimator xgboost's best error=2.3819
[flaml.automl: 09-18 08:30:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:30:25] {3072} INFO -  at 29.6s,	estimator xgboost's best error=2.2532,	best estimator xgboost's best error=2.2532
[flaml.automl: 09-18 08:30:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:30:38] {3072} INFO -  at 41.7s,	estimator xgboost's best error=2.2013,	best estimator xgboost's best error=2.2013
[flaml.automl: 09-18 08:30:38] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:30:47] {3072} INFO -  at 50.9s,	estimator xgboost's best error=2.2013,	best estimator xgboost's best error=2.2013
[flaml.automl: 09-18 08:31:09] {3335} INFO - retrain xgboost for 21.9s
[flaml.automl: 09-18 08:31:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:31:09] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:31:09] {2637} INFO - Time taken to find the best model: 41.69351649284363
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51553}
NO2(0)最佳损失：-1.2013410765747268
NO2(0)最好结果：{'pred_time': 7.790740111835924e-06, 'wall_clock_time': 41.69351649284363, 'metric_for_logging': {'pred_time': 7.790740111835924e-06}, 'val_loss': 2.2013410765747268, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51553}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51553, 'experiment_tag': 'exp', 'time_total_s': 12.11752963066101}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8145448670723472
NO2(0)的mse=10.663899491669715
NO2(0)的mae=2.1649828478565203
NO2(0)的mar=0.20222245964808044
总共花费的时间为：73.72
上饶市
2381A
2382A
2383A
3685A
[flaml.automl: 09-18 08:43:11] {2390} INFO - task = regression
[flaml.automl: 09-18 08:43:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:43:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:43:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:43:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:43:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:43:12] {3025} INFO - Estimated sufficient time budget=50258s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 08:43:12] {3072} INFO -  at 1.4s,	estimator xgboost's best error=12.0313,	best estimator xgboost's best error=12.0313
[flaml.automl: 09-18 08:43:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:43:14] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.0049,	best estimator xgboost's best error=6.0049
[flaml.automl: 09-18 08:43:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:43:16] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.0049,	best estimator xgboost's best error=6.0049
[flaml.automl: 09-18 08:43:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:43:22] {3072} INFO -  at 11.1s,	estimator xgboost's best error=6.0049,	best estimator xgboost's best error=6.0049
[flaml.automl: 09-18 08:43:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:43:23] {3072} INFO -  at 12.2s,	estimator xgboost's best error=3.9740,	best estimator xgboost's best error=3.9740
[flaml.automl: 09-18 08:43:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:43:25] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.4874,	best estimator xgboost's best error=3.4874
[flaml.automl: 09-18 08:43:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:43:26] {3072} INFO -  at 15.4s,	estimator xgboost's best error=3.4874,	best estimator xgboost's best error=3.4874
[flaml.automl: 09-18 08:43:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:43:29] {3072} INFO -  at 17.9s,	estimator xgboost's best error=3.4874,	best estimator xgboost's best error=3.4874
[flaml.automl: 09-18 08:43:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:43:30] {3072} INFO -  at 19.0s,	estimator xgboost's best error=3.4874,	best estimator xgboost's best error=3.4874
[flaml.automl: 09-18 08:43:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:43:32] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.4874,	best estimator xgboost's best error=3.4874
[flaml.automl: 09-18 08:43:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:43:34] {3072} INFO -  at 23.3s,	estimator xgboost's best error=3.4874,	best estimator xgboost's best error=3.4874
[flaml.automl: 09-18 08:43:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:43:35] {3072} INFO -  at 24.5s,	estimator xgboost's best error=3.4874,	best estimator xgboost's best error=3.4874
[flaml.automl: 09-18 08:43:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:43:42] {3072} INFO -  at 31.0s,	estimator xgboost's best error=3.3237,	best estimator xgboost's best error=3.3237
[flaml.automl: 09-18 08:43:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:43:54] {3072} INFO -  at 43.1s,	estimator xgboost's best error=3.2536,	best estimator xgboost's best error=3.2536
[flaml.automl: 09-18 08:43:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:44:00] {3072} INFO -  at 49.6s,	estimator xgboost's best error=3.2536,	best estimator xgboost's best error=3.2536
[flaml.automl: 09-18 08:44:12] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 08:44:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:44:12] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:44:12] {2637} INFO - Time taken to find the best model: 43.06613898277283
[flaml.automl: 09-18 08:44:12] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40635}
NO2(0)最佳损失：-2.253592156888639
NO2(0)最好结果：{'pred_time': 8.821223397323592e-06, 'wall_clock_time': 43.06613898277283, 'metric_for_logging': {'pred_time': 8.821223397323592e-06}, 'val_loss': 3.253592156888639, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40635}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40635, 'experiment_tag': 'exp', 'time_total_s': 12.092800378799438}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8116671305544259
NO2(0)的mse=23.16792881807615
NO2(0)的mae=3.332621423115121
NO2(0)的mar=0.24032952322607223
总共花费的时间为：62.39
鹤壁市
2385A
2386A
2387A
3474A
3596A
[flaml.automl: 09-18 09:00:13] {2390} INFO - task = regression
[flaml.automl: 09-18 09:00:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:00:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:00:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:00:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:00:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:00:14] {3025} INFO - Estimated sufficient time budget=65259s. Estimated necessary time budget=65s.
[flaml.automl: 09-18 09:00:14] {3072} INFO -  at 1.5s,	estimator xgboost's best error=18.6852,	best estimator xgboost's best error=18.6852
[flaml.automl: 09-18 09:00:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:00:16] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.0396,	best estimator xgboost's best error=9.0396
[flaml.automl: 09-18 09:00:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:00:17] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.0396,	best estimator xgboost's best error=9.0396
[flaml.automl: 09-18 09:00:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:00:22] {3072} INFO -  at 9.6s,	estimator xgboost's best error=9.0396,	best estimator xgboost's best error=9.0396
[flaml.automl: 09-18 09:00:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:00:23] {3072} INFO -  at 10.8s,	estimator xgboost's best error=5.7463,	best estimator xgboost's best error=5.7463
[flaml.automl: 09-18 09:00:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:00:25] {3072} INFO -  at 12.3s,	estimator xgboost's best error=4.9149,	best estimator xgboost's best error=4.9149
[flaml.automl: 09-18 09:00:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:00:27] {3072} INFO -  at 14.0s,	estimator xgboost's best error=4.9149,	best estimator xgboost's best error=4.9149
[flaml.automl: 09-18 09:00:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:00:29] {3072} INFO -  at 16.4s,	estimator xgboost's best error=4.9149,	best estimator xgboost's best error=4.9149
[flaml.automl: 09-18 09:00:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:00:30] {3072} INFO -  at 17.6s,	estimator xgboost's best error=4.9149,	best estimator xgboost's best error=4.9149
[flaml.automl: 09-18 09:00:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:00:33] {3072} INFO -  at 20.2s,	estimator xgboost's best error=4.9149,	best estimator xgboost's best error=4.9149
[flaml.automl: 09-18 09:00:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:00:34] {3072} INFO -  at 21.9s,	estimator xgboost's best error=4.8881,	best estimator xgboost's best error=4.8881
[flaml.automl: 09-18 09:00:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:00:36] {3072} INFO -  at 23.0s,	estimator xgboost's best error=4.8881,	best estimator xgboost's best error=4.8881
[flaml.automl: 09-18 09:00:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:00:42] {3072} INFO -  at 29.5s,	estimator xgboost's best error=4.5519,	best estimator xgboost's best error=4.5519
[flaml.automl: 09-18 09:00:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:00:54] {3072} INFO -  at 41.5s,	estimator xgboost's best error=4.4918,	best estimator xgboost's best error=4.4918
[flaml.automl: 09-18 09:00:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 09:01:01] {3072} INFO -  at 48.1s,	estimator xgboost's best error=4.4918,	best estimator xgboost's best error=4.4918
[flaml.automl: 09-18 09:01:13] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 09:01:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 09:01:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:01:13] {2637} INFO - Time taken to find the best model: 41.53450345993042
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54015}
NO2(0)最佳损失：-3.4918393566385184
NO2(0)最好结果：{'pred_time': 6.627694879599867e-06, 'wall_clock_time': 41.53450345993042, 'metric_for_logging': {'pred_time': 6.627694879599867e-06}, 'val_loss': 4.491839356638518, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54015}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54015, 'experiment_tag': 'exp', 'time_total_s': 12.008631229400635}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8358236821660596
NO2(0)的mse=45.108275284074026
NO2(0)的mae=4.556820143429231
NO2(0)的mar=0.20813393550400255
总共花费的时间为：61.08
新乡市
2390A
2391A
3054A
3475A
3476A
[flaml.automl: 09-18 09:17:27] {2390} INFO - task = regression
[flaml.automl: 09-18 09:17:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:17:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:17:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:17:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:17:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:17:28] {3025} INFO - Estimated sufficient time budget=60962s. Estimated necessary time budget=61s.
[flaml.automl: 09-18 09:17:28] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.5548,	best estimator xgboost's best error=17.5548
[flaml.automl: 09-18 09:17:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:17:30] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.3446,	best estimator xgboost's best error=8.3446
[flaml.automl: 09-18 09:17:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:17:32] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.3446,	best estimator xgboost's best error=8.3446
[flaml.automl: 09-18 09:17:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:17:36] {3072} INFO -  at 9.5s,	estimator xgboost's best error=8.3446,	best estimator xgboost's best error=8.3446
[flaml.automl: 09-18 09:17:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:17:38] {3072} INFO -  at 10.6s,	estimator xgboost's best error=5.3618,	best estimator xgboost's best error=5.3618
[flaml.automl: 09-18 09:17:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:17:39] {3072} INFO -  at 12.2s,	estimator xgboost's best error=4.5489,	best estimator xgboost's best error=4.5489
[flaml.automl: 09-18 09:17:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:17:41] {3072} INFO -  at 13.8s,	estimator xgboost's best error=4.5489,	best estimator xgboost's best error=4.5489
[flaml.automl: 09-18 09:17:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:17:43] {3072} INFO -  at 16.3s,	estimator xgboost's best error=4.5489,	best estimator xgboost's best error=4.5489
[flaml.automl: 09-18 09:17:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:17:44] {3072} INFO -  at 17.4s,	estimator xgboost's best error=4.5489,	best estimator xgboost's best error=4.5489
[flaml.automl: 09-18 09:17:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:17:47] {3072} INFO -  at 20.1s,	estimator xgboost's best error=4.5489,	best estimator xgboost's best error=4.5489
[flaml.automl: 09-18 09:17:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:17:49] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.5145,	best estimator xgboost's best error=4.5145
[flaml.automl: 09-18 09:17:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:17:50] {3072} INFO -  at 22.9s,	estimator xgboost's best error=4.5145,	best estimator xgboost's best error=4.5145
[flaml.automl: 09-18 09:17:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:17:56] {3072} INFO -  at 29.4s,	estimator xgboost's best error=4.2962,	best estimator xgboost's best error=4.2962
[flaml.automl: 09-18 09:17:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:18:08] {3072} INFO -  at 41.5s,	estimator xgboost's best error=4.1858,	best estimator xgboost's best error=4.1858
[flaml.automl: 09-18 09:18:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 09:18:15] {3072} INFO -  at 48.0s,	estimator xgboost's best error=4.1858,	best estimator xgboost's best error=4.1858
[flaml.automl: 09-18 09:18:27] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 09:18:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 09:18:27] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:18:27] {2637} INFO - Time taken to find the best model: 41.49871373176575
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51672}
NO2(0)最佳损失：-3.185822122855205
NO2(0)最好结果：{'pred_time': 7.005736039765244e-06, 'wall_clock_time': 41.49871373176575, 'metric_for_logging': {'pred_time': 7.005736039765244e-06}, 'val_loss': 4.185822122855205, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51672}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51672, 'experiment_tag': 'exp', 'time_total_s': 12.109411239624023}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8566391711855029
NO2(0)的mse=38.465319730185534
NO2(0)的mae=4.195556806845351
NO2(0)的mar=0.1921716259540082
总共花费的时间为：60.96
濮阳市
2392A
2395A
3021A
[flaml.automl: 09-18 09:28:43] {2390} INFO - task = regression
[flaml.automl: 09-18 09:28:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:28:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:28:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:28:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:28:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:28:46] {3025} INFO - Estimated sufficient time budget=33612s. Estimated necessary time budget=34s.
[flaml.automl: 09-18 09:28:46] {3072} INFO -  at 3.6s,	estimator xgboost's best error=14.9165,	best estimator xgboost's best error=14.9165
[flaml.automl: 09-18 09:28:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:28:52] {3072} INFO -  at 9.5s,	estimator xgboost's best error=7.0192,	best estimator xgboost's best error=7.0192
[flaml.automl: 09-18 09:28:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:28:55] {3072} INFO -  at 13.0s,	estimator xgboost's best error=7.0192,	best estimator xgboost's best error=7.0192
[flaml.automl: 09-18 09:28:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:29:20] {3072} INFO -  at 37.4s,	estimator xgboost's best error=7.0192,	best estimator xgboost's best error=7.0192
[flaml.automl: 09-18 09:29:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:29:21] {3072} INFO -  at 39.1s,	estimator xgboost's best error=4.1764,	best estimator xgboost's best error=4.1764
[flaml.automl: 09-18 09:29:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:29:24] {3072} INFO -  at 41.8s,	estimator xgboost's best error=3.5494,	best estimator xgboost's best error=3.5494
[flaml.automl: 09-18 09:29:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:29:27] {3072} INFO -  at 44.6s,	estimator xgboost's best error=3.5494,	best estimator xgboost's best error=3.5494
[flaml.automl: 09-18 09:29:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:29:31] {3072} INFO -  at 48.7s,	estimator xgboost's best error=3.5494,	best estimator xgboost's best error=3.5494
[flaml.automl: 09-18 09:29:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:29:32] {3072} INFO -  at 49.9s,	estimator xgboost's best error=3.5494,	best estimator xgboost's best error=3.5494
[flaml.automl: 09-18 09:29:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:29:35] {3072} INFO -  at 52.5s,	estimator xgboost's best error=3.5494,	best estimator xgboost's best error=3.5494
[flaml.automl: 09-18 09:29:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:29:36] {3072} INFO -  at 53.7s,	estimator xgboost's best error=3.5494,	best estimator xgboost's best error=3.5494
[flaml.automl: 09-18 09:29:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:29:37] {3072} INFO -  at 54.9s,	estimator xgboost's best error=3.5494,	best estimator xgboost's best error=3.5494
[flaml.automl: 09-18 09:29:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:29:42] {3072} INFO -  at 59.8s,	estimator xgboost's best error=3.2991,	best estimator xgboost's best error=3.2991
[flaml.automl: 09-18 09:29:49] {3335} INFO - retrain xgboost for 6.4s
[flaml.automl: 09-18 09:29:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 09:29:49] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:29:49] {2637} INFO - Time taken to find the best model: 59.78541898727417
[flaml.automl: 09-18 09:29:49] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-2.2990658524418257
NO2(0)最好结果：{'pred_time': 1.2919534446832887e-05, 'wall_clock_time': 59.78541898727417, 'metric_for_logging': {'pred_time': 1.2919534446832887e-05}, 'val_loss': 3.2990658524418257, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 4.923544406890869}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8818478611471628
NO2(0)的mse=24.629124819386725
NO2(0)的mae=3.33682896771482
NO2(0)的mar=0.1619456664222156
总共花费的时间为：66.93
许昌市
2396A
3134A
3337A
3338A
3597A
[flaml.automl: 09-18 09:46:07] {2390} INFO - task = regression
[flaml.automl: 09-18 09:46:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:46:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:46:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:46:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:46:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:46:09] {3025} INFO - Estimated sufficient time budget=113933s. Estimated necessary time budget=114s.
[flaml.automl: 09-18 09:46:09] {3072} INFO -  at 2.4s,	estimator xgboost's best error=14.0295,	best estimator xgboost's best error=14.0295
[flaml.automl: 09-18 09:46:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:46:12] {3072} INFO -  at 6.0s,	estimator xgboost's best error=6.6164,	best estimator xgboost's best error=6.6164
[flaml.automl: 09-18 09:46:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:46:14] {3072} INFO -  at 7.6s,	estimator xgboost's best error=6.6164,	best estimator xgboost's best error=6.6164
[flaml.automl: 09-18 09:46:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:46:18] {3072} INFO -  at 11.9s,	estimator xgboost's best error=6.6164,	best estimator xgboost's best error=6.6164
[flaml.automl: 09-18 09:46:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:46:19] {3072} INFO -  at 13.0s,	estimator xgboost's best error=4.1320,	best estimator xgboost's best error=4.1320
[flaml.automl: 09-18 09:46:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:46:21] {3072} INFO -  at 14.6s,	estimator xgboost's best error=3.4750,	best estimator xgboost's best error=3.4750
[flaml.automl: 09-18 09:46:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:46:22] {3072} INFO -  at 16.2s,	estimator xgboost's best error=3.4750,	best estimator xgboost's best error=3.4750
[flaml.automl: 09-18 09:46:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:46:25] {3072} INFO -  at 18.7s,	estimator xgboost's best error=3.4750,	best estimator xgboost's best error=3.4750
[flaml.automl: 09-18 09:46:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:46:26] {3072} INFO -  at 19.8s,	estimator xgboost's best error=3.4750,	best estimator xgboost's best error=3.4750
[flaml.automl: 09-18 09:46:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:46:29] {3072} INFO -  at 22.4s,	estimator xgboost's best error=3.4750,	best estimator xgboost's best error=3.4750
[flaml.automl: 09-18 09:46:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:46:30] {3072} INFO -  at 24.0s,	estimator xgboost's best error=3.4709,	best estimator xgboost's best error=3.4709
[flaml.automl: 09-18 09:46:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:46:31] {3072} INFO -  at 25.2s,	estimator xgboost's best error=3.4709,	best estimator xgboost's best error=3.4709
[flaml.automl: 09-18 09:46:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:46:38] {3072} INFO -  at 31.7s,	estimator xgboost's best error=3.2439,	best estimator xgboost's best error=3.2439
[flaml.automl: 09-18 09:46:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:46:50] {3072} INFO -  at 43.7s,	estimator xgboost's best error=3.1579,	best estimator xgboost's best error=3.1579
[flaml.automl: 09-18 09:46:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 09:46:56] {3072} INFO -  at 50.2s,	estimator xgboost's best error=3.1579,	best estimator xgboost's best error=3.1579
[flaml.automl: 09-18 09:47:08] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 09:47:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 09:47:08] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:47:08] {2637} INFO - Time taken to find the best model: 43.66155219078064
[flaml.automl: 09-18 09:47:08] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54272}
NO2(0)最佳损失：-2.1579159209827465
NO2(0)最好结果：{'pred_time': 6.758500600292046e-06, 'wall_clock_time': 43.66155219078064, 'metric_for_logging': {'pred_time': 6.758500600292046e-06}, 'val_loss': 3.1579159209827465, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54272}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54272, 'experiment_tag': 'exp', 'time_total_s': 11.980805158615112}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8688128801354944
NO2(0)的mse=22.93943989217283
NO2(0)的mae=3.1923072966794934
NO2(0)的mar=0.16896431939040096
总共花费的时间为：63.37
漯河市
2399A
2400A
2401A
2402A
3478A
3479A
[flaml.automl: 09-18 10:05:43] {2390} INFO - task = regression
[flaml.automl: 09-18 10:05:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:05:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:05:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:05:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:05:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:05:44] {3025} INFO - Estimated sufficient time budget=75745s. Estimated necessary time budget=76s.
[flaml.automl: 09-18 10:05:44] {3072} INFO -  at 1.5s,	estimator xgboost's best error=11.4991,	best estimator xgboost's best error=11.4991
[flaml.automl: 09-18 10:05:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:05:46] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.7014,	best estimator xgboost's best error=5.7014
[flaml.automl: 09-18 10:05:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:05:47] {3072} INFO -  at 4.9s,	estimator xgboost's best error=5.7014,	best estimator xgboost's best error=5.7014
[flaml.automl: 09-18 10:05:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:05:51] {3072} INFO -  at 8.7s,	estimator xgboost's best error=5.7014,	best estimator xgboost's best error=5.7014
[flaml.automl: 09-18 10:05:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:05:52] {3072} INFO -  at 9.8s,	estimator xgboost's best error=3.8581,	best estimator xgboost's best error=3.8581
[flaml.automl: 09-18 10:05:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:05:54] {3072} INFO -  at 11.4s,	estimator xgboost's best error=3.3783,	best estimator xgboost's best error=3.3783
[flaml.automl: 09-18 10:05:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:05:56] {3072} INFO -  at 13.0s,	estimator xgboost's best error=3.3783,	best estimator xgboost's best error=3.3783
[flaml.automl: 09-18 10:05:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:05:58] {3072} INFO -  at 15.4s,	estimator xgboost's best error=3.3783,	best estimator xgboost's best error=3.3783
[flaml.automl: 09-18 10:05:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:05:59] {3072} INFO -  at 16.6s,	estimator xgboost's best error=3.3783,	best estimator xgboost's best error=3.3783
[flaml.automl: 09-18 10:05:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:06:02] {3072} INFO -  at 19.3s,	estimator xgboost's best error=3.3783,	best estimator xgboost's best error=3.3783
[flaml.automl: 09-18 10:06:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:06:04] {3072} INFO -  at 20.9s,	estimator xgboost's best error=3.3783,	best estimator xgboost's best error=3.3783
[flaml.automl: 09-18 10:06:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:06:05] {3072} INFO -  at 22.1s,	estimator xgboost's best error=3.3783,	best estimator xgboost's best error=3.3783
[flaml.automl: 09-18 10:06:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:06:11] {3072} INFO -  at 28.6s,	estimator xgboost's best error=3.2396,	best estimator xgboost's best error=3.2396
[flaml.automl: 09-18 10:06:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:06:23] {3072} INFO -  at 40.7s,	estimator xgboost's best error=3.1508,	best estimator xgboost's best error=3.1508
[flaml.automl: 09-18 10:06:23] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 10:06:30] {3072} INFO -  at 47.3s,	estimator xgboost's best error=3.1508,	best estimator xgboost's best error=3.1508
[flaml.automl: 09-18 10:06:30] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 10:06:42] {3072} INFO -  at 59.7s,	estimator xgboost's best error=3.1166,	best estimator xgboost's best error=3.1166
[flaml.automl: 09-18 10:07:18] {3335} INFO - retrain xgboost for 35.2s
[flaml.automl: 09-18 10:07:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:07:18] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:07:18] {2637} INFO - Time taken to find the best model: 59.73013424873352
[flaml.automl: 09-18 10:07:18] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 63510}
NO2(0)最佳损失：-2.1165895380615023
NO2(0)最好结果：{'pred_time': 5.804210272020467e-06, 'wall_clock_time': 59.73013424873352, 'metric_for_logging': {'pred_time': 5.804210272020467e-06}, 'val_loss': 3.1165895380615023, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 63510}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 63510, 'experiment_tag': 'exp', 'time_total_s': 12.477619886398315}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8293511138007619
NO2(0)的mse=24.033185089444824
NO2(0)的mae=3.1709760779127936
NO2(0)的mar=0.2223197741668286
总共花费的时间为：95.97
南阳市
2403A
2404A
2405A
2406A
2407A
[flaml.automl: 09-18 10:23:05] {2390} INFO - task = regression
[flaml.automl: 09-18 10:23:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:23:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:23:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:23:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:23:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:23:07] {3025} INFO - Estimated sufficient time budget=62728s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 10:23:07] {3072} INFO -  at 1.5s,	estimator xgboost's best error=13.3609,	best estimator xgboost's best error=13.3609
[flaml.automl: 09-18 10:23:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:23:09] {3072} INFO -  at 3.6s,	estimator xgboost's best error=6.5484,	best estimator xgboost's best error=6.5484
[flaml.automl: 09-18 10:23:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:23:10] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.5484,	best estimator xgboost's best error=6.5484
[flaml.automl: 09-18 10:23:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:23:15] {3072} INFO -  at 9.5s,	estimator xgboost's best error=6.5484,	best estimator xgboost's best error=6.5484
[flaml.automl: 09-18 10:23:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:23:16] {3072} INFO -  at 10.6s,	estimator xgboost's best error=4.3461,	best estimator xgboost's best error=4.3461
[flaml.automl: 09-18 10:23:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:23:17] {3072} INFO -  at 12.2s,	estimator xgboost's best error=3.8108,	best estimator xgboost's best error=3.8108
[flaml.automl: 09-18 10:23:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:23:19] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.8108,	best estimator xgboost's best error=3.8108
[flaml.automl: 09-18 10:23:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:23:21] {3072} INFO -  at 16.3s,	estimator xgboost's best error=3.8108,	best estimator xgboost's best error=3.8108
[flaml.automl: 09-18 10:23:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:23:23] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.8108,	best estimator xgboost's best error=3.8108
[flaml.automl: 09-18 10:23:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:23:25] {3072} INFO -  at 20.1s,	estimator xgboost's best error=3.8108,	best estimator xgboost's best error=3.8108
[flaml.automl: 09-18 10:23:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:23:27] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.7909,	best estimator xgboost's best error=3.7909
[flaml.automl: 09-18 10:23:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:23:28] {3072} INFO -  at 22.9s,	estimator xgboost's best error=3.7909,	best estimator xgboost's best error=3.7909
[flaml.automl: 09-18 10:23:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:23:35] {3072} INFO -  at 29.4s,	estimator xgboost's best error=3.6303,	best estimator xgboost's best error=3.6303
[flaml.automl: 09-18 10:23:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:23:47] {3072} INFO -  at 41.5s,	estimator xgboost's best error=3.5110,	best estimator xgboost's best error=3.5110
[flaml.automl: 09-18 10:23:47] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 10:23:53] {3072} INFO -  at 48.0s,	estimator xgboost's best error=3.5110,	best estimator xgboost's best error=3.5110
[flaml.automl: 09-18 10:24:05] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 10:24:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:24:05] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:24:05] {2637} INFO - Time taken to find the best model: 41.524250984191895
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53433}
NO2(0)最佳损失：-2.5110249843770864
NO2(0)最好结果：{'pred_time': 6.761844811033183e-06, 'wall_clock_time': 41.524250984191895, 'metric_for_logging': {'pred_time': 6.761844811033183e-06}, 'val_loss': 3.5110249843770864, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53433}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53433, 'experiment_tag': 'exp', 'time_total_s': 12.097209215164185}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8166294894255051
NO2(0)的mse=28.72563243350207
NO2(0)的mae=3.4747994164002383
NO2(0)的mar=0.20205941777834674
总共花费的时间为：61.00
商丘市
2408A
2409A
2410A
[flaml.automl: 09-18 10:33:54] {2390} INFO - task = regression
[flaml.automl: 09-18 10:33:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:33:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:33:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:33:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:33:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:33:56] {3025} INFO - Estimated sufficient time budget=22402s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 10:33:56] {3072} INFO -  at 2.5s,	estimator xgboost's best error=13.0473,	best estimator xgboost's best error=13.0473
[flaml.automl: 09-18 10:33:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:34:00] {3072} INFO -  at 6.4s,	estimator xgboost's best error=6.0986,	best estimator xgboost's best error=6.0986
[flaml.automl: 09-18 10:34:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:34:02] {3072} INFO -  at 8.6s,	estimator xgboost's best error=6.0986,	best estimator xgboost's best error=6.0986
[flaml.automl: 09-18 10:34:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:34:21] {3072} INFO -  at 27.0s,	estimator xgboost's best error=6.0986,	best estimator xgboost's best error=6.0986
[flaml.automl: 09-18 10:34:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:34:23] {3072} INFO -  at 29.0s,	estimator xgboost's best error=3.6480,	best estimator xgboost's best error=3.6480
[flaml.automl: 09-18 10:34:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:34:26] {3072} INFO -  at 32.0s,	estimator xgboost's best error=3.0529,	best estimator xgboost's best error=3.0529
[flaml.automl: 09-18 10:34:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:34:29] {3072} INFO -  at 35.0s,	estimator xgboost's best error=3.0529,	best estimator xgboost's best error=3.0529
[flaml.automl: 09-18 10:34:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:34:33] {3072} INFO -  at 39.5s,	estimator xgboost's best error=3.0529,	best estimator xgboost's best error=3.0529
[flaml.automl: 09-18 10:34:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:34:35] {3072} INFO -  at 41.4s,	estimator xgboost's best error=3.0529,	best estimator xgboost's best error=3.0529
[flaml.automl: 09-18 10:34:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:34:38] {3072} INFO -  at 44.1s,	estimator xgboost's best error=3.0529,	best estimator xgboost's best error=3.0529
[flaml.automl: 09-18 10:34:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:34:39] {3072} INFO -  at 45.2s,	estimator xgboost's best error=3.0529,	best estimator xgboost's best error=3.0529
[flaml.automl: 09-18 10:34:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:34:40] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.0529,	best estimator xgboost's best error=3.0529
[flaml.automl: 09-18 10:34:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:34:46] {3072} INFO -  at 52.8s,	estimator xgboost's best error=2.9598,	best estimator xgboost's best error=2.9598
[flaml.automl: 09-18 10:34:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:34:53] {3072} INFO -  at 59.8s,	estimator xgboost's best error=2.8074,	best estimator xgboost's best error=2.8074
[flaml.automl: 09-18 10:35:05] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-18 10:35:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:35:05] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:35:05] {2637} INFO - Time taken to find the best model: 59.76844024658203
[flaml.automl: 09-18 10:35:05] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.8074144848658444
NO2(0)最好结果：{'pred_time': 1.2587509828710882e-05, 'wall_clock_time': 59.76844024658203, 'metric_for_logging': {'pred_time': 1.2587509828710882e-05}, 'val_loss': 2.8074144848658444, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 6.9696431159973145}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8940314810390462
NO2(0)的mse=17.485905871214175
NO2(0)的mae=2.6799089412454578
NO2(0)的mar=0.155183875671492
总共花费的时间为：72.59
驻马店市
2420A
2421A
2422A
3339A
[flaml.automl: 09-18 10:48:32] {2390} INFO - task = regression
[flaml.automl: 09-18 10:48:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:48:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:48:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:48:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:48:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:48:33] {3025} INFO - Estimated sufficient time budget=50322s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 10:48:33] {3072} INFO -  at 1.4s,	estimator xgboost's best error=11.2898,	best estimator xgboost's best error=11.2898
[flaml.automl: 09-18 10:48:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:48:35] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.3671,	best estimator xgboost's best error=5.3671
[flaml.automl: 09-18 10:48:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:48:36] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.3671,	best estimator xgboost's best error=5.3671
[flaml.automl: 09-18 10:48:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:48:43] {3072} INFO -  at 11.1s,	estimator xgboost's best error=5.3671,	best estimator xgboost's best error=5.3671
[flaml.automl: 09-18 10:48:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:48:44] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.4785,	best estimator xgboost's best error=3.4785
[flaml.automl: 09-18 10:48:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:48:45] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.0060,	best estimator xgboost's best error=3.0060
[flaml.automl: 09-18 10:48:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:48:47] {3072} INFO -  at 15.5s,	estimator xgboost's best error=3.0060,	best estimator xgboost's best error=3.0060
[flaml.automl: 09-18 10:48:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:48:49] {3072} INFO -  at 17.9s,	estimator xgboost's best error=3.0060,	best estimator xgboost's best error=3.0060
[flaml.automl: 09-18 10:48:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:48:51] {3072} INFO -  at 19.0s,	estimator xgboost's best error=3.0060,	best estimator xgboost's best error=3.0060
[flaml.automl: 09-18 10:48:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:48:53] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.0060,	best estimator xgboost's best error=3.0060
[flaml.automl: 09-18 10:48:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:48:55] {3072} INFO -  at 23.3s,	estimator xgboost's best error=2.9906,	best estimator xgboost's best error=2.9906
[flaml.automl: 09-18 10:48:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:48:56] {3072} INFO -  at 24.5s,	estimator xgboost's best error=2.9906,	best estimator xgboost's best error=2.9906
[flaml.automl: 09-18 10:48:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:49:02] {3072} INFO -  at 31.0s,	estimator xgboost's best error=2.9233,	best estimator xgboost's best error=2.9233
[flaml.automl: 09-18 10:49:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:49:15] {3072} INFO -  at 43.1s,	estimator xgboost's best error=2.7807,	best estimator xgboost's best error=2.7807
[flaml.automl: 09-18 10:49:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 10:49:21] {3072} INFO -  at 49.6s,	estimator xgboost's best error=2.7807,	best estimator xgboost's best error=2.7807
[flaml.automl: 09-18 10:49:33] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 10:49:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:49:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:49:33] {2637} INFO - Time taken to find the best model: 43.055991888046265
[flaml.automl: 09-18 10:49:33] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42246}
NO2(0)最佳损失：-1.780740969070334
NO2(0)最好结果：{'pred_time': 8.516886613193247e-06, 'wall_clock_time': 43.055991888046265, 'metric_for_logging': {'pred_time': 8.516886613193247e-06}, 'val_loss': 2.780740969070334, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42246}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42246, 'experiment_tag': 'exp', 'time_total_s': 12.056690692901611}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8395773247751415
NO2(0)的mse=18.179079887165884
NO2(0)的mae=2.7424093382947285
NO2(0)的mar=0.17952668900796034
总共花费的时间为：62.33
黄石市
2423A
2424A
2427A
3149A
[flaml.automl: 09-18 11:02:02] {2390} INFO - task = regression
[flaml.automl: 09-18 11:02:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:02:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:02:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:02:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:02:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:02:04] {3025} INFO - Estimated sufficient time budget=94655s. Estimated necessary time budget=95s.
[flaml.automl: 09-18 11:02:04] {3072} INFO -  at 2.5s,	estimator xgboost's best error=16.4308,	best estimator xgboost's best error=16.4308
[flaml.automl: 09-18 11:02:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:02:08] {3072} INFO -  at 6.3s,	estimator xgboost's best error=8.3263,	best estimator xgboost's best error=8.3263
[flaml.automl: 09-18 11:02:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:02:10] {3072} INFO -  at 8.5s,	estimator xgboost's best error=8.3263,	best estimator xgboost's best error=8.3263
[flaml.automl: 09-18 11:02:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:02:15] {3072} INFO -  at 13.5s,	estimator xgboost's best error=8.3263,	best estimator xgboost's best error=8.3263
[flaml.automl: 09-18 11:02:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:02:17] {3072} INFO -  at 15.7s,	estimator xgboost's best error=5.5866,	best estimator xgboost's best error=5.5866
[flaml.automl: 09-18 11:02:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:02:20] {3072} INFO -  at 18.6s,	estimator xgboost's best error=5.0052,	best estimator xgboost's best error=5.0052
[flaml.automl: 09-18 11:02:20] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:02:23] {3072} INFO -  at 21.7s,	estimator xgboost's best error=5.0052,	best estimator xgboost's best error=5.0052
[flaml.automl: 09-18 11:02:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:02:27] {3072} INFO -  at 25.1s,	estimator xgboost's best error=5.0052,	best estimator xgboost's best error=5.0052
[flaml.automl: 09-18 11:02:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:02:29] {3072} INFO -  at 27.3s,	estimator xgboost's best error=5.0052,	best estimator xgboost's best error=5.0052
[flaml.automl: 09-18 11:02:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:02:31] {3072} INFO -  at 29.9s,	estimator xgboost's best error=5.0052,	best estimator xgboost's best error=5.0052
[flaml.automl: 09-18 11:02:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:02:34] {3072} INFO -  at 32.4s,	estimator xgboost's best error=4.9773,	best estimator xgboost's best error=4.9773
[flaml.automl: 09-18 11:02:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:02:35] {3072} INFO -  at 33.6s,	estimator xgboost's best error=4.9773,	best estimator xgboost's best error=4.9773
[flaml.automl: 09-18 11:02:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:02:42] {3072} INFO -  at 40.1s,	estimator xgboost's best error=4.7490,	best estimator xgboost's best error=4.7490
[flaml.automl: 09-18 11:02:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:02:54] {3072} INFO -  at 52.2s,	estimator xgboost's best error=4.7258,	best estimator xgboost's best error=4.7258
[flaml.automl: 09-18 11:03:06] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 11:03:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:03:06] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:03:06] {2637} INFO - Time taken to find the best model: 52.21329975128174
[flaml.automl: 09-18 11:03:06] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43686}
NO2(0)最佳损失：-3.725787509212042
NO2(0)最好结果：{'pred_time': 8.370873856863696e-06, 'wall_clock_time': 52.21329975128174, 'metric_for_logging': {'pred_time': 8.370873856863696e-06}, 'val_loss': 4.725787509212042, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43686}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43686, 'experiment_tag': 'exp', 'time_total_s': 12.083719253540039}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7751897592185022
NO2(0)的mse=45.590990170870676
NO2(0)的mae=4.668418302574024
NO2(0)的mar=0.24832902599487564
总共花费的时间为：65.10
十堰市
2428A
2429A
2430A
2431A
3545A
[flaml.automl: 09-18 11:19:12] {2390} INFO - task = regression
[flaml.automl: 09-18 11:19:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:19:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:19:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:19:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:19:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:19:14] {3025} INFO - Estimated sufficient time budget=64033s. Estimated necessary time budget=64s.
[flaml.automl: 09-18 11:19:14] {3072} INFO -  at 1.5s,	estimator xgboost's best error=9.6971,	best estimator xgboost's best error=9.6971
[flaml.automl: 09-18 11:19:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:19:16] {3072} INFO -  at 3.6s,	estimator xgboost's best error=4.6566,	best estimator xgboost's best error=4.6566
[flaml.automl: 09-18 11:19:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:19:17] {3072} INFO -  at 4.8s,	estimator xgboost's best error=4.6566,	best estimator xgboost's best error=4.6566
[flaml.automl: 09-18 11:19:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:19:22] {3072} INFO -  at 10.0s,	estimator xgboost's best error=4.6566,	best estimator xgboost's best error=4.6566
[flaml.automl: 09-18 11:19:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:19:24] {3072} INFO -  at 12.2s,	estimator xgboost's best error=3.1475,	best estimator xgboost's best error=3.1475
[flaml.automl: 09-18 11:19:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:19:27] {3072} INFO -  at 15.1s,	estimator xgboost's best error=2.7860,	best estimator xgboost's best error=2.7860
[flaml.automl: 09-18 11:19:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:19:30] {3072} INFO -  at 18.1s,	estimator xgboost's best error=2.7860,	best estimator xgboost's best error=2.7860
[flaml.automl: 09-18 11:19:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:19:34] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.7860,	best estimator xgboost's best error=2.7860
[flaml.automl: 09-18 11:19:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:19:36] {3072} INFO -  at 23.8s,	estimator xgboost's best error=2.7860,	best estimator xgboost's best error=2.7860
[flaml.automl: 09-18 11:19:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:19:38] {3072} INFO -  at 26.5s,	estimator xgboost's best error=2.7860,	best estimator xgboost's best error=2.7860
[flaml.automl: 09-18 11:19:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:19:41] {3072} INFO -  at 29.4s,	estimator xgboost's best error=2.7860,	best estimator xgboost's best error=2.7860
[flaml.automl: 09-18 11:19:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:19:44] {3072} INFO -  at 31.6s,	estimator xgboost's best error=2.7860,	best estimator xgboost's best error=2.7860
[flaml.automl: 09-18 11:19:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:19:56] {3072} INFO -  at 43.7s,	estimator xgboost's best error=2.6766,	best estimator xgboost's best error=2.6766
[flaml.automl: 09-18 11:19:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:20:11] {3072} INFO -  at 58.8s,	estimator xgboost's best error=2.6452,	best estimator xgboost's best error=2.6452
[flaml.automl: 09-18 11:20:28] {3335} INFO - retrain xgboost for 17.5s
[flaml.automl: 09-18 11:20:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:20:28] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:20:28] {2637} INFO - Time taken to find the best model: 58.84157419204712
[flaml.automl: 09-18 11:20:28] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52866}
NO2(0)最佳损失：-1.645207989447207
NO2(0)最好结果：{'pred_time': 1.604410105917297e-05, 'wall_clock_time': 58.84157419204712, 'metric_for_logging': {'pred_time': 1.604410105917297e-05}, 'val_loss': 2.645207989447207, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52866}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52866, 'experiment_tag': 'exp', 'time_total_s': 15.180415153503418}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8090370629875585
NO2(0)的mse=17.159933190399734
NO2(0)的mae=2.56348887212393
NO2(0)的mar=0.17685480680373925
总共花费的时间为：77.32
襄阳市
2432A
2433A
2434A
2435A
3396A
3397A
[flaml.automl: 09-18 11:40:07] {2390} INFO - task = regression
[flaml.automl: 09-18 11:40:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:40:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:40:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:40:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:40:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:40:08] {3025} INFO - Estimated sufficient time budget=77464s. Estimated necessary time budget=77s.
[flaml.automl: 09-18 11:40:08] {3072} INFO -  at 1.5s,	estimator xgboost's best error=14.1022,	best estimator xgboost's best error=14.1022
[flaml.automl: 09-18 11:40:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:40:11] {3072} INFO -  at 3.7s,	estimator xgboost's best error=6.8287,	best estimator xgboost's best error=6.8287
[flaml.automl: 09-18 11:40:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:40:12] {3072} INFO -  at 4.9s,	estimator xgboost's best error=6.8287,	best estimator xgboost's best error=6.8287
[flaml.automl: 09-18 11:40:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:40:16] {3072} INFO -  at 8.6s,	estimator xgboost's best error=6.8287,	best estimator xgboost's best error=6.8287
[flaml.automl: 09-18 11:40:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:40:17] {3072} INFO -  at 9.8s,	estimator xgboost's best error=4.3881,	best estimator xgboost's best error=4.3881
[flaml.automl: 09-18 11:40:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:40:18] {3072} INFO -  at 11.3s,	estimator xgboost's best error=3.8873,	best estimator xgboost's best error=3.8873
[flaml.automl: 09-18 11:40:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:40:20] {3072} INFO -  at 13.0s,	estimator xgboost's best error=3.8873,	best estimator xgboost's best error=3.8873
[flaml.automl: 09-18 11:40:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:40:22] {3072} INFO -  at 15.4s,	estimator xgboost's best error=3.8873,	best estimator xgboost's best error=3.8873
[flaml.automl: 09-18 11:40:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:40:23] {3072} INFO -  at 16.6s,	estimator xgboost's best error=3.8873,	best estimator xgboost's best error=3.8873
[flaml.automl: 09-18 11:40:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:40:26] {3072} INFO -  at 19.2s,	estimator xgboost's best error=3.8873,	best estimator xgboost's best error=3.8873
[flaml.automl: 09-18 11:40:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:40:28] {3072} INFO -  at 20.8s,	estimator xgboost's best error=3.8644,	best estimator xgboost's best error=3.8644
[flaml.automl: 09-18 11:40:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:40:29] {3072} INFO -  at 22.0s,	estimator xgboost's best error=3.8644,	best estimator xgboost's best error=3.8644
[flaml.automl: 09-18 11:40:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:40:35] {3072} INFO -  at 28.6s,	estimator xgboost's best error=3.7213,	best estimator xgboost's best error=3.7213
[flaml.automl: 09-18 11:40:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:40:48] {3072} INFO -  at 40.8s,	estimator xgboost's best error=3.6329,	best estimator xgboost's best error=3.6329
[flaml.automl: 09-18 11:40:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 11:40:54] {3072} INFO -  at 47.3s,	estimator xgboost's best error=3.6329,	best estimator xgboost's best error=3.6329
[flaml.automl: 09-18 11:40:54] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 11:41:07] {3072} INFO -  at 59.7s,	estimator xgboost's best error=3.6172,	best estimator xgboost's best error=3.6172
[flaml.automl: 09-18 11:41:28] {3335} INFO - retrain xgboost for 21.4s
[flaml.automl: 09-18 11:41:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:41:28] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:41:28] {2637} INFO - Time taken to find the best model: 59.749016523361206
[flaml.automl: 09-18 11:41:28] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 65224}
NO2(0)最佳损失：-2.617168234838029
NO2(0)最好结果：{'pred_time': 5.531422876101193e-06, 'wall_clock_time': 59.749016523361206, 'metric_for_logging': {'pred_time': 5.531422876101193e-06}, 'val_loss': 3.617168234838029, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 65224}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 65224, 'experiment_tag': 'exp', 'time_total_s': 12.463813781738281}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8175557506949302
NO2(0)的mse=29.199426524319776
NO2(0)的mae=3.5877249220150915
NO2(0)的mar=0.18769693948779176
总共花费的时间为：82.19
鄂州市
2436A
2437A
[flaml.automl: 09-18 11:47:31] {2390} INFO - task = regression
[flaml.automl: 09-18 11:47:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:47:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:47:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:47:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:47:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:47:33] {3025} INFO - Estimated sufficient time budget=28658s. Estimated necessary time budget=29s.
[flaml.automl: 09-18 11:47:33] {3072} INFO -  at 3.0s,	estimator xgboost's best error=17.1725,	best estimator xgboost's best error=17.1725
[flaml.automl: 09-18 11:47:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:47:38] {3072} INFO -  at 7.9s,	estimator xgboost's best error=8.4381,	best estimator xgboost's best error=8.4381
[flaml.automl: 09-18 11:47:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:47:41] {3072} INFO -  at 10.5s,	estimator xgboost's best error=8.4381,	best estimator xgboost's best error=8.4381
[flaml.automl: 09-18 11:47:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:48:03] {3072} INFO -  at 32.7s,	estimator xgboost's best error=8.4381,	best estimator xgboost's best error=8.4381
[flaml.automl: 09-18 11:48:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:48:05] {3072} INFO -  at 34.7s,	estimator xgboost's best error=5.5200,	best estimator xgboost's best error=5.5200
[flaml.automl: 09-18 11:48:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:48:07] {3072} INFO -  at 36.3s,	estimator xgboost's best error=4.7127,	best estimator xgboost's best error=4.7127
[flaml.automl: 09-18 11:48:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:48:08] {3072} INFO -  at 37.9s,	estimator xgboost's best error=4.7127,	best estimator xgboost's best error=4.7127
[flaml.automl: 09-18 11:48:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:48:11] {3072} INFO -  at 40.4s,	estimator xgboost's best error=4.7127,	best estimator xgboost's best error=4.7127
[flaml.automl: 09-18 11:48:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:48:12] {3072} INFO -  at 41.5s,	estimator xgboost's best error=4.7127,	best estimator xgboost's best error=4.7127
[flaml.automl: 09-18 11:48:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:48:14] {3072} INFO -  at 44.0s,	estimator xgboost's best error=4.7127,	best estimator xgboost's best error=4.7127
[flaml.automl: 09-18 11:48:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:48:16] {3072} INFO -  at 45.5s,	estimator xgboost's best error=4.7127,	best estimator xgboost's best error=4.7127
[flaml.automl: 09-18 11:48:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:48:18] {3072} INFO -  at 47.6s,	estimator xgboost's best error=4.7127,	best estimator xgboost's best error=4.7127
[flaml.automl: 09-18 11:48:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:48:28] {3072} INFO -  at 58.0s,	estimator xgboost's best error=4.6025,	best estimator xgboost's best error=4.6025
[flaml.automl: 09-18 11:48:39] {3335} INFO - retrain xgboost for 10.9s
[flaml.automl: 09-18 11:48:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 11:48:39] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:48:39] {2637} INFO - Time taken to find the best model: 57.950688123703
[flaml.automl: 09-18 11:48:39] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-3.602513169638593
NO2(0)最好结果：{'pred_time': 2.3593740277193355e-05, 'wall_clock_time': 57.950688123703, 'metric_for_logging': {'pred_time': 2.3593740277193355e-05}, 'val_loss': 4.602513169638593, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 10.395753383636475}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7941145331527676
NO2(0)的mse=52.43331792074009
NO2(0)的mae=4.8398724088754355
NO2(0)的mar=0.22652542341852597
总共花费的时间为：69.48
荆门市
2439A
2440A
2441A
3547A
[flaml.automl: 09-18 12:01:21] {2390} INFO - task = regression
[flaml.automl: 09-18 12:01:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:01:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:01:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:01:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:01:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:01:23] {3025} INFO - Estimated sufficient time budget=50854s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 12:01:23] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.1132,	best estimator xgboost's best error=13.1132
[flaml.automl: 09-18 12:01:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:01:25] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.5670,	best estimator xgboost's best error=6.5670
[flaml.automl: 09-18 12:01:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:01:26] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.5670,	best estimator xgboost's best error=6.5670
[flaml.automl: 09-18 12:01:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:01:32] {3072} INFO -  at 11.1s,	estimator xgboost's best error=6.5670,	best estimator xgboost's best error=6.5670
[flaml.automl: 09-18 12:01:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:01:34] {3072} INFO -  at 12.2s,	estimator xgboost's best error=4.7309,	best estimator xgboost's best error=4.7309
[flaml.automl: 09-18 12:01:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:01:35] {3072} INFO -  at 13.8s,	estimator xgboost's best error=4.3801,	best estimator xgboost's best error=4.3801
[flaml.automl: 09-18 12:01:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:01:37] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.3801,	best estimator xgboost's best error=4.3801
[flaml.automl: 09-18 12:01:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:01:39] {3072} INFO -  at 17.9s,	estimator xgboost's best error=4.3801,	best estimator xgboost's best error=4.3801
[flaml.automl: 09-18 12:01:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:01:40] {3072} INFO -  at 19.0s,	estimator xgboost's best error=4.3801,	best estimator xgboost's best error=4.3801
[flaml.automl: 09-18 12:01:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:01:43] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.3801,	best estimator xgboost's best error=4.3801
[flaml.automl: 09-18 12:01:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:01:45] {3072} INFO -  at 23.3s,	estimator xgboost's best error=4.3458,	best estimator xgboost's best error=4.3458
[flaml.automl: 09-18 12:01:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:01:46] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.3458,	best estimator xgboost's best error=4.3458
[flaml.automl: 09-18 12:01:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:01:52] {3072} INFO -  at 31.0s,	estimator xgboost's best error=4.2126,	best estimator xgboost's best error=4.2126
[flaml.automl: 09-18 12:01:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:02:04] {3072} INFO -  at 43.1s,	estimator xgboost's best error=4.1254,	best estimator xgboost's best error=4.1254
[flaml.automl: 09-18 12:02:04] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 12:02:11] {3072} INFO -  at 49.6s,	estimator xgboost's best error=4.1254,	best estimator xgboost's best error=4.1254
[flaml.automl: 09-18 12:02:23] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 12:02:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 12:02:23] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:02:23] {2637} INFO - Time taken to find the best model: 43.0777268409729
[flaml.automl: 09-18 12:02:23] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42721}
NO2(0)最佳损失：-3.1253965178213043
NO2(0)最好结果：{'pred_time': 8.432343303968913e-06, 'wall_clock_time': 43.0777268409729, 'metric_for_logging': {'pred_time': 8.432343303968913e-06}, 'val_loss': 4.125396517821304, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42721}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42721, 'experiment_tag': 'exp', 'time_total_s': 12.088243007659912}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7809731011897436
NO2(0)的mse=40.4081646536823
NO2(0)的mae=4.150843416051277
NO2(0)的mar=0.2638897751037802
总共花费的时间为：62.43
孝感市
2443A
2444A
[flaml.automl: 09-18 12:08:52] {2390} INFO - task = regression
[flaml.automl: 09-18 12:08:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:08:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:08:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:08:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:08:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:08:54] {3025} INFO - Estimated sufficient time budget=22183s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 12:08:54] {3072} INFO -  at 2.3s,	estimator xgboost's best error=11.9684,	best estimator xgboost's best error=11.9684
[flaml.automl: 09-18 12:08:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:08:58] {3072} INFO -  at 6.3s,	estimator xgboost's best error=5.7432,	best estimator xgboost's best error=5.7432
[flaml.automl: 09-18 12:08:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:09:00] {3072} INFO -  at 8.5s,	estimator xgboost's best error=5.7432,	best estimator xgboost's best error=5.7432
[flaml.automl: 09-18 12:09:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:09:23] {3072} INFO -  at 31.6s,	estimator xgboost's best error=5.7432,	best estimator xgboost's best error=5.7432
[flaml.automl: 09-18 12:09:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:09:27] {3072} INFO -  at 35.3s,	estimator xgboost's best error=3.6418,	best estimator xgboost's best error=3.6418
[flaml.automl: 09-18 12:09:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:09:33] {3072} INFO -  at 40.9s,	estimator xgboost's best error=2.9728,	best estimator xgboost's best error=2.9728
[flaml.automl: 09-18 12:09:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:09:39] {3072} INFO -  at 47.0s,	estimator xgboost's best error=2.9728,	best estimator xgboost's best error=2.9728
[flaml.automl: 09-18 12:09:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:09:48] {3072} INFO -  at 56.2s,	estimator xgboost's best error=2.9728,	best estimator xgboost's best error=2.9728
[flaml.automl: 09-18 12:09:54] {3335} INFO - retrain xgboost for 5.8s
[flaml.automl: 09-18 12:09:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 12:09:54] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:09:54] {2637} INFO - Time taken to find the best model: 40.870431661605835
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-1.9728325847768189
NO2(0)最好结果：{'pred_time': 6.783414183810539e-05, 'wall_clock_time': 40.870431661605835, 'metric_for_logging': {'pred_time': 6.783414183810539e-05}, 'val_loss': 2.972832584776819, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 5.592733144760132}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.854769307918565
NO2(0)的mse=21.532575686436996
NO2(0)的mae=3.028314799315187
NO2(0)的mar=0.20225741464823357
总共花费的时间为：62.69
黄冈市
2929A
3398A
[flaml.automl: 09-18 12:17:23] {2390} INFO - task = regression
[flaml.automl: 09-18 12:17:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:17:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:17:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:17:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:17:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:17:24] {3025} INFO - Estimated sufficient time budget=11998s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 12:17:24] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.9686,	best estimator xgboost's best error=11.9686
[flaml.automl: 09-18 12:17:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:17:27] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.3070,	best estimator xgboost's best error=6.3070
[flaml.automl: 09-18 12:17:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:17:28] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.3070,	best estimator xgboost's best error=6.3070
[flaml.automl: 09-18 12:17:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:17:37] {3072} INFO -  at 14.1s,	estimator xgboost's best error=6.3070,	best estimator xgboost's best error=6.3070
[flaml.automl: 09-18 12:17:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:17:38] {3072} INFO -  at 15.3s,	estimator xgboost's best error=4.7460,	best estimator xgboost's best error=4.7460
[flaml.automl: 09-18 12:17:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:17:40] {3072} INFO -  at 16.9s,	estimator xgboost's best error=4.3072,	best estimator xgboost's best error=4.3072
[flaml.automl: 09-18 12:17:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:17:42] {3072} INFO -  at 18.5s,	estimator xgboost's best error=4.3072,	best estimator xgboost's best error=4.3072
[flaml.automl: 09-18 12:17:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:17:45] {3072} INFO -  at 22.4s,	estimator xgboost's best error=4.3072,	best estimator xgboost's best error=4.3072
[flaml.automl: 09-18 12:17:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:17:48] {3072} INFO -  at 24.6s,	estimator xgboost's best error=4.3072,	best estimator xgboost's best error=4.3072
[flaml.automl: 09-18 12:17:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:17:52] {3072} INFO -  at 29.0s,	estimator xgboost's best error=4.3072,	best estimator xgboost's best error=4.3072
[flaml.automl: 09-18 12:17:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:17:54] {3072} INFO -  at 31.1s,	estimator xgboost's best error=4.3072,	best estimator xgboost's best error=4.3072
[flaml.automl: 09-18 12:17:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:17:56] {3072} INFO -  at 33.2s,	estimator xgboost's best error=4.3072,	best estimator xgboost's best error=4.3072
[flaml.automl: 09-18 12:17:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:18:07] {3072} INFO -  at 44.4s,	estimator xgboost's best error=4.2194,	best estimator xgboost's best error=4.2194
[flaml.automl: 09-18 12:18:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:18:22] {3072} INFO -  at 59.4s,	estimator xgboost's best error=4.0936,	best estimator xgboost's best error=4.0936
[flaml.automl: 09-18 12:18:42] {3335} INFO - retrain xgboost for 19.1s
[flaml.automl: 09-18 12:18:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 12:18:42] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:18:42] {2637} INFO - Time taken to find the best model: 59.422656536102295
[flaml.automl: 09-18 12:18:42] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.0935554795638796
NO2(0)最好结果：{'pred_time': 2.946568419176084e-05, 'wall_clock_time': 59.422656536102295, 'metric_for_logging': {'pred_time': 2.946568419176084e-05}, 'val_loss': 4.0935554795638796, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 15.014068126678467}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7410174040585208
NO2(0)的mse=39.22265580179444
NO2(0)的mae=4.164507056001102
NO2(0)的mar=0.34893545853682617
总共花费的时间为：79.03
咸宁市
2447A
2448A
2449A
[flaml.automl: 09-18 12:27:50] {2390} INFO - task = regression
[flaml.automl: 09-18 12:27:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:27:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:27:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:27:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:27:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:27:53] {3025} INFO - Estimated sufficient time budget=21754s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 12:27:53] {3072} INFO -  at 2.3s,	estimator xgboost's best error=7.2187,	best estimator xgboost's best error=7.2187
[flaml.automl: 09-18 12:27:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:27:57] {3072} INFO -  at 6.4s,	estimator xgboost's best error=3.8069,	best estimator xgboost's best error=3.8069
[flaml.automl: 09-18 12:27:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:27:59] {3072} INFO -  at 8.6s,	estimator xgboost's best error=3.8069,	best estimator xgboost's best error=3.8069
[flaml.automl: 09-18 12:27:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:28:17] {3072} INFO -  at 27.3s,	estimator xgboost's best error=3.8069,	best estimator xgboost's best error=3.8069
[flaml.automl: 09-18 12:28:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:28:20] {3072} INFO -  at 29.5s,	estimator xgboost's best error=2.7992,	best estimator xgboost's best error=2.7992
[flaml.automl: 09-18 12:28:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:28:23] {3072} INFO -  at 32.4s,	estimator xgboost's best error=2.5496,	best estimator xgboost's best error=2.5496
[flaml.automl: 09-18 12:28:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:28:24] {3072} INFO -  at 34.2s,	estimator xgboost's best error=2.5496,	best estimator xgboost's best error=2.5496
[flaml.automl: 09-18 12:28:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:28:27] {3072} INFO -  at 36.7s,	estimator xgboost's best error=2.5496,	best estimator xgboost's best error=2.5496
[flaml.automl: 09-18 12:28:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:28:28] {3072} INFO -  at 37.9s,	estimator xgboost's best error=2.5496,	best estimator xgboost's best error=2.5496
[flaml.automl: 09-18 12:28:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:28:31] {3072} INFO -  at 40.6s,	estimator xgboost's best error=2.5496,	best estimator xgboost's best error=2.5496
[flaml.automl: 09-18 12:28:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:28:32] {3072} INFO -  at 41.7s,	estimator xgboost's best error=2.5496,	best estimator xgboost's best error=2.5496
[flaml.automl: 09-18 12:28:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:28:33] {3072} INFO -  at 42.9s,	estimator xgboost's best error=2.5496,	best estimator xgboost's best error=2.5496
[flaml.automl: 09-18 12:28:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:28:40] {3072} INFO -  at 49.4s,	estimator xgboost's best error=2.4588,	best estimator xgboost's best error=2.4588
[flaml.automl: 09-18 12:28:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:28:50] {3072} INFO -  at 59.6s,	estimator xgboost's best error=2.4132,	best estimator xgboost's best error=2.4132
[flaml.automl: 09-18 12:29:02] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 12:29:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 12:29:02] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:29:02] {2637} INFO - Time taken to find the best model: 59.60072112083435
[flaml.automl: 09-18 12:29:02] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.4132142983527278
NO2(0)最好结果：{'pred_time': 1.1715490467610208e-05, 'wall_clock_time': 59.60072112083435, 'metric_for_logging': {'pred_time': 1.1715490467610208e-05}, 'val_loss': 2.413214298352728, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.194471597671509}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8125580756983828
NO2(0)的mse=13.014725689382988
NO2(0)的mae=2.359641600230175
NO2(0)的mar=0.27694018597539977
总共花费的时间为：72.23
随州市
2451A
2452A
2453A
[flaml.automl: 09-18 12:38:44] {2390} INFO - task = regression
[flaml.automl: 09-18 12:38:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:38:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:38:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:38:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:38:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:38:46] {3025} INFO - Estimated sufficient time budget=22116s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 12:38:46] {3072} INFO -  at 2.4s,	estimator xgboost's best error=10.5629,	best estimator xgboost's best error=10.5629
[flaml.automl: 09-18 12:38:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:38:50] {3072} INFO -  at 6.3s,	estimator xgboost's best error=5.2133,	best estimator xgboost's best error=5.2133
[flaml.automl: 09-18 12:38:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:38:53] {3072} INFO -  at 8.5s,	estimator xgboost's best error=5.2133,	best estimator xgboost's best error=5.2133
[flaml.automl: 09-18 12:38:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:39:11] {3072} INFO -  at 27.1s,	estimator xgboost's best error=5.2133,	best estimator xgboost's best error=5.2133
[flaml.automl: 09-18 12:39:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:39:13] {3072} INFO -  at 29.3s,	estimator xgboost's best error=3.5374,	best estimator xgboost's best error=3.5374
[flaml.automl: 09-18 12:39:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:39:16] {3072} INFO -  at 32.3s,	estimator xgboost's best error=3.1697,	best estimator xgboost's best error=3.1697
[flaml.automl: 09-18 12:39:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:39:19] {3072} INFO -  at 35.4s,	estimator xgboost's best error=3.1697,	best estimator xgboost's best error=3.1697
[flaml.automl: 09-18 12:39:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:39:24] {3072} INFO -  at 40.1s,	estimator xgboost's best error=3.1697,	best estimator xgboost's best error=3.1697
[flaml.automl: 09-18 12:39:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:39:26] {3072} INFO -  at 42.2s,	estimator xgboost's best error=3.1697,	best estimator xgboost's best error=3.1697
[flaml.automl: 09-18 12:39:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:39:31] {3072} INFO -  at 47.2s,	estimator xgboost's best error=3.1697,	best estimator xgboost's best error=3.1697
[flaml.automl: 09-18 12:39:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:39:33] {3072} INFO -  at 49.3s,	estimator xgboost's best error=3.1697,	best estimator xgboost's best error=3.1697
[flaml.automl: 09-18 12:39:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:39:36] {3072} INFO -  at 51.6s,	estimator xgboost's best error=3.1697,	best estimator xgboost's best error=3.1697
[flaml.automl: 09-18 12:39:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:39:43] {3072} INFO -  at 59.2s,	estimator xgboost's best error=3.1437,	best estimator xgboost's best error=3.1437
[flaml.automl: 09-18 12:39:51] {3335} INFO - retrain xgboost for 7.8s
[flaml.automl: 09-18 12:39:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 12:39:51] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:39:51] {2637} INFO - Time taken to find the best model: 59.21843433380127
[flaml.automl: 09-18 12:39:51] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-2.143742099214459
NO2(0)最好结果：{'pred_time': 1.829912127550289e-05, 'wall_clock_time': 59.21843433380127, 'metric_for_logging': {'pred_time': 1.829912127550289e-05}, 'val_loss': 3.143742099214459, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 7.652942895889282}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8107643519893128
NO2(0)的mse=23.39810250002281
NO2(0)的mae=3.1820838717979196
NO2(0)的mar=0.2519276917445538
总共花费的时间为：67.61
恩施土家族苗族自治州
2454A
2455A
3549A
[flaml.automl: 09-18 12:49:55] {2390} INFO - task = regression
[flaml.automl: 09-18 12:49:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:49:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:49:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:49:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:49:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:49:57] {3025} INFO - Estimated sufficient time budget=19230s. Estimated necessary time budget=19s.
[flaml.automl: 09-18 12:49:57] {3072} INFO -  at 2.1s,	estimator xgboost's best error=7.1876,	best estimator xgboost's best error=7.1876
[flaml.automl: 09-18 12:49:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:50:01] {3072} INFO -  at 5.9s,	estimator xgboost's best error=3.7103,	best estimator xgboost's best error=3.7103
[flaml.automl: 09-18 12:50:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:50:03] {3072} INFO -  at 8.1s,	estimator xgboost's best error=3.7103,	best estimator xgboost's best error=3.7103
[flaml.automl: 09-18 12:50:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:50:21] {3072} INFO -  at 25.8s,	estimator xgboost's best error=3.7103,	best estimator xgboost's best error=3.7103
[flaml.automl: 09-18 12:50:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:50:24] {3072} INFO -  at 28.5s,	estimator xgboost's best error=2.7358,	best estimator xgboost's best error=2.7358
[flaml.automl: 09-18 12:50:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:50:27] {3072} INFO -  at 32.1s,	estimator xgboost's best error=2.4481,	best estimator xgboost's best error=2.4481
[flaml.automl: 09-18 12:50:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:50:32] {3072} INFO -  at 36.7s,	estimator xgboost's best error=2.4481,	best estimator xgboost's best error=2.4481
[flaml.automl: 09-18 12:50:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:50:38] {3072} INFO -  at 43.3s,	estimator xgboost's best error=2.4481,	best estimator xgboost's best error=2.4481
[flaml.automl: 09-18 12:50:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:50:42] {3072} INFO -  at 46.4s,	estimator xgboost's best error=2.4481,	best estimator xgboost's best error=2.4481
[flaml.automl: 09-18 12:50:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:50:48] {3072} INFO -  at 52.9s,	estimator xgboost's best error=2.4481,	best estimator xgboost's best error=2.4481
[flaml.automl: 09-18 12:50:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:50:51] {3072} INFO -  at 55.7s,	estimator xgboost's best error=2.4481,	best estimator xgboost's best error=2.4481
[flaml.automl: 09-18 12:50:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:50:54] {3072} INFO -  at 58.4s,	estimator xgboost's best error=2.4481,	best estimator xgboost's best error=2.4481
[flaml.automl: 09-18 12:50:57] {3335} INFO - retrain xgboost for 3.7s
[flaml.automl: 09-18 12:50:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 12:50:57] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:50:57] {2637} INFO - Time taken to find the best model: 32.14405655860901
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-1.4480579675722827
NO2(0)最好结果：{'pred_time': 2.991910561699039e-05, 'wall_clock_time': 32.14405655860901, 'metric_for_logging': {'pred_time': 2.991910561699039e-05}, 'val_loss': 2.4480579675722827, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 3.624088764190674}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7159367749109073
NO2(0)的mse=14.750657858425445
NO2(0)的mae=2.535186467790388
NO2(0)的mar=0.3186924564909124
总共花费的时间为：62.87
衡阳市
2456A
2457A
2458A
2459A
2460A
2461A
3399A
[flaml.automl: 09-18 13:12:13] {2390} INFO - task = regression
[flaml.automl: 09-18 13:12:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:12:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:12:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:12:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:12:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:12:14] {3025} INFO - Estimated sufficient time budget=88068s. Estimated necessary time budget=88s.
[flaml.automl: 09-18 13:12:14] {3072} INFO -  at 1.6s,	estimator xgboost's best error=11.1791,	best estimator xgboost's best error=11.1791
[flaml.automl: 09-18 13:12:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:12:16] {3072} INFO -  at 3.7s,	estimator xgboost's best error=5.3410,	best estimator xgboost's best error=5.3410
[flaml.automl: 09-18 13:12:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:12:17] {3072} INFO -  at 4.9s,	estimator xgboost's best error=5.3410,	best estimator xgboost's best error=5.3410
[flaml.automl: 09-18 13:12:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:12:20] {3072} INFO -  at 8.2s,	estimator xgboost's best error=5.3410,	best estimator xgboost's best error=5.3410
[flaml.automl: 09-18 13:12:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:12:22] {3072} INFO -  at 9.3s,	estimator xgboost's best error=3.5434,	best estimator xgboost's best error=3.5434
[flaml.automl: 09-18 13:12:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:12:23] {3072} INFO -  at 10.9s,	estimator xgboost's best error=3.1447,	best estimator xgboost's best error=3.1447
[flaml.automl: 09-18 13:12:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:12:25] {3072} INFO -  at 12.5s,	estimator xgboost's best error=3.1447,	best estimator xgboost's best error=3.1447
[flaml.automl: 09-18 13:12:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:12:27] {3072} INFO -  at 14.9s,	estimator xgboost's best error=3.1447,	best estimator xgboost's best error=3.1447
[flaml.automl: 09-18 13:12:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:12:28] {3072} INFO -  at 16.1s,	estimator xgboost's best error=3.1447,	best estimator xgboost's best error=3.1447
[flaml.automl: 09-18 13:12:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:12:31] {3072} INFO -  at 18.7s,	estimator xgboost's best error=3.1447,	best estimator xgboost's best error=3.1447
[flaml.automl: 09-18 13:12:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:12:33] {3072} INFO -  at 20.3s,	estimator xgboost's best error=3.1276,	best estimator xgboost's best error=3.1276
[flaml.automl: 09-18 13:12:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:12:34] {3072} INFO -  at 21.5s,	estimator xgboost's best error=3.1276,	best estimator xgboost's best error=3.1276
[flaml.automl: 09-18 13:12:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:12:40] {3072} INFO -  at 28.0s,	estimator xgboost's best error=2.9780,	best estimator xgboost's best error=2.9780
[flaml.automl: 09-18 13:12:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:12:52] {3072} INFO -  at 40.2s,	estimator xgboost's best error=2.9306,	best estimator xgboost's best error=2.9306
[flaml.automl: 09-18 13:12:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:12:59] {3072} INFO -  at 46.8s,	estimator xgboost's best error=2.9306,	best estimator xgboost's best error=2.9306
[flaml.automl: 09-18 13:12:59] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 13:13:12] {3072} INFO -  at 59.3s,	estimator xgboost's best error=2.9200,	best estimator xgboost's best error=2.9200
[flaml.automl: 09-18 13:13:33] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-18 13:13:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:13:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:13:33] {2637} INFO - Time taken to find the best model: 59.32576560974121
[flaml.automl: 09-18 13:13:33] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 73244}
NO2(0)最佳损失：-1.9199801251495452
NO2(0)最好结果：{'pred_time': 4.908130156904521e-06, 'wall_clock_time': 59.32576560974121, 'metric_for_logging': {'pred_time': 4.908130156904521e-06}, 'val_loss': 2.919980125149545, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 73244}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 73244, 'experiment_tag': 'exp', 'time_total_s': 12.559743165969849}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8297311542064719
NO2(0)的mse=18.46081092092532
NO2(0)的mae=2.8759182828276555
NO2(0)的mar=0.19121446784008847
总共花费的时间为：81.91
邵阳市
2462A
2463A
2464A
2465A
2466A
[flaml.automl: 09-18 13:28:50] {2390} INFO - task = regression
[flaml.automl: 09-18 13:28:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:28:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:28:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:28:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:28:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:28:52] {3025} INFO - Estimated sufficient time budget=61788s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 13:28:52] {3072} INFO -  at 1.5s,	estimator xgboost's best error=9.5346,	best estimator xgboost's best error=9.5346
[flaml.automl: 09-18 13:28:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:28:54] {3072} INFO -  at 3.6s,	estimator xgboost's best error=4.5848,	best estimator xgboost's best error=4.5848
[flaml.automl: 09-18 13:28:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:28:55] {3072} INFO -  at 4.8s,	estimator xgboost's best error=4.5848,	best estimator xgboost's best error=4.5848
[flaml.automl: 09-18 13:28:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:29:00] {3072} INFO -  at 9.6s,	estimator xgboost's best error=4.5848,	best estimator xgboost's best error=4.5848
[flaml.automl: 09-18 13:29:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:29:01] {3072} INFO -  at 10.7s,	estimator xgboost's best error=2.9525,	best estimator xgboost's best error=2.9525
[flaml.automl: 09-18 13:29:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:29:03] {3072} INFO -  at 12.3s,	estimator xgboost's best error=2.6052,	best estimator xgboost's best error=2.6052
[flaml.automl: 09-18 13:29:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:29:04] {3072} INFO -  at 13.9s,	estimator xgboost's best error=2.6052,	best estimator xgboost's best error=2.6052
[flaml.automl: 09-18 13:29:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:29:07] {3072} INFO -  at 16.3s,	estimator xgboost's best error=2.6052,	best estimator xgboost's best error=2.6052
[flaml.automl: 09-18 13:29:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:29:08] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.6052,	best estimator xgboost's best error=2.6052
[flaml.automl: 09-18 13:29:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:29:10] {3072} INFO -  at 20.1s,	estimator xgboost's best error=2.6052,	best estimator xgboost's best error=2.6052
[flaml.automl: 09-18 13:29:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:29:12] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.6040,	best estimator xgboost's best error=2.6040
[flaml.automl: 09-18 13:29:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:29:13] {3072} INFO -  at 22.9s,	estimator xgboost's best error=2.6040,	best estimator xgboost's best error=2.6040
[flaml.automl: 09-18 13:29:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:29:20] {3072} INFO -  at 29.4s,	estimator xgboost's best error=2.4876,	best estimator xgboost's best error=2.4876
[flaml.automl: 09-18 13:29:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:29:32] {3072} INFO -  at 41.5s,	estimator xgboost's best error=2.4433,	best estimator xgboost's best error=2.4433
[flaml.automl: 09-18 13:29:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:29:38] {3072} INFO -  at 48.0s,	estimator xgboost's best error=2.4433,	best estimator xgboost's best error=2.4433
[flaml.automl: 09-18 13:29:50] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 13:29:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:29:50] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:29:50] {2637} INFO - Time taken to find the best model: 41.5043249130249
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51396}
NO2(0)最佳损失：-1.4432900693783552
NO2(0)最好结果：{'pred_time': 7.144040932819819e-06, 'wall_clock_time': 41.5043249130249, 'metric_for_logging': {'pred_time': 7.144040932819819e-06}, 'val_loss': 2.4432900693783552, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51396}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51396, 'experiment_tag': 'exp', 'time_total_s': 12.101330757141113}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.826451560698435
NO2(0)的mse=13.716007199606794
NO2(0)的mae=2.501542982302214
NO2(0)的mar=0.18114659621204762
总共花费的时间为：60.98
益阳市
2467A
2468A
2469A
2470A
2471A
[flaml.automl: 09-18 13:45:58] {2390} INFO - task = regression
[flaml.automl: 09-18 13:45:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:45:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:45:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:45:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:45:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:46:00] {3025} INFO - Estimated sufficient time budget=110298s. Estimated necessary time budget=110s.
[flaml.automl: 09-18 13:46:00] {3072} INFO -  at 2.5s,	estimator xgboost's best error=10.3992,	best estimator xgboost's best error=10.3992
[flaml.automl: 09-18 13:46:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:46:04] {3072} INFO -  at 6.4s,	estimator xgboost's best error=5.0779,	best estimator xgboost's best error=5.0779
[flaml.automl: 09-18 13:46:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:46:06] {3072} INFO -  at 8.5s,	estimator xgboost's best error=5.0779,	best estimator xgboost's best error=5.0779
[flaml.automl: 09-18 13:46:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:46:12] {3072} INFO -  at 13.5s,	estimator xgboost's best error=5.0779,	best estimator xgboost's best error=5.0779
[flaml.automl: 09-18 13:46:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:46:14] {3072} INFO -  at 15.7s,	estimator xgboost's best error=3.6325,	best estimator xgboost's best error=3.6325
[flaml.automl: 09-18 13:46:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:46:17] {3072} INFO -  at 18.5s,	estimator xgboost's best error=3.2552,	best estimator xgboost's best error=3.2552
[flaml.automl: 09-18 13:46:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:46:20] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.2552,	best estimator xgboost's best error=3.2552
[flaml.automl: 09-18 13:46:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:46:23] {3072} INFO -  at 25.1s,	estimator xgboost's best error=3.2552,	best estimator xgboost's best error=3.2552
[flaml.automl: 09-18 13:46:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:46:25] {3072} INFO -  at 27.3s,	estimator xgboost's best error=3.2552,	best estimator xgboost's best error=3.2552
[flaml.automl: 09-18 13:46:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:46:28] {3072} INFO -  at 30.0s,	estimator xgboost's best error=3.2552,	best estimator xgboost's best error=3.2552
[flaml.automl: 09-18 13:46:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:46:31] {3072} INFO -  at 32.9s,	estimator xgboost's best error=3.2489,	best estimator xgboost's best error=3.2489
[flaml.automl: 09-18 13:46:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:46:33] {3072} INFO -  at 35.1s,	estimator xgboost's best error=3.2489,	best estimator xgboost's best error=3.2489
[flaml.automl: 09-18 13:46:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:46:40] {3072} INFO -  at 42.3s,	estimator xgboost's best error=3.1366,	best estimator xgboost's best error=3.1366
[flaml.automl: 09-18 13:46:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:46:52] {3072} INFO -  at 54.4s,	estimator xgboost's best error=3.0680,	best estimator xgboost's best error=3.0680
[flaml.automl: 09-18 13:47:04] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 13:47:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:47:04] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:47:04] {2637} INFO - Time taken to find the best model: 54.42337727546692
[flaml.automl: 09-18 13:47:04] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 50544}
NO2(0)最佳损失：-2.068047042416032
NO2(0)最好结果：{'pred_time': 7.954715323923659e-06, 'wall_clock_time': 54.42337727546692, 'metric_for_logging': {'pred_time': 7.954715323923659e-06}, 'val_loss': 3.068047042416032, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 50544}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 50544, 'experiment_tag': 'exp', 'time_total_s': 12.096034288406372}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7730351972665486
NO2(0)的mse=23.143297431764744
NO2(0)的mae=3.0818759091598373
NO2(0)的mar=0.20961301717471786
总共花费的时间为：67.47
郴州市
2472A
2473A
2474A
2475A
2476A
[flaml.automl: 09-18 14:02:31] {2390} INFO - task = regression
[flaml.automl: 09-18 14:02:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:02:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:02:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:02:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:02:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:02:32] {3025} INFO - Estimated sufficient time budget=61997s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 14:02:32] {3072} INFO -  at 1.5s,	estimator xgboost's best error=11.4964,	best estimator xgboost's best error=11.4964
[flaml.automl: 09-18 14:02:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:02:34] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.5961,	best estimator xgboost's best error=5.5961
[flaml.automl: 09-18 14:02:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:02:35] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.5961,	best estimator xgboost's best error=5.5961
[flaml.automl: 09-18 14:02:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:02:40] {3072} INFO -  at 9.6s,	estimator xgboost's best error=5.5961,	best estimator xgboost's best error=5.5961
[flaml.automl: 09-18 14:02:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:02:41] {3072} INFO -  at 10.7s,	estimator xgboost's best error=3.5922,	best estimator xgboost's best error=3.5922
[flaml.automl: 09-18 14:02:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:02:43] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.1221,	best estimator xgboost's best error=3.1221
[flaml.automl: 09-18 14:02:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:02:44] {3072} INFO -  at 13.9s,	estimator xgboost's best error=3.1221,	best estimator xgboost's best error=3.1221
[flaml.automl: 09-18 14:02:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:02:47] {3072} INFO -  at 16.4s,	estimator xgboost's best error=3.1221,	best estimator xgboost's best error=3.1221
[flaml.automl: 09-18 14:02:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:02:48] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.1221,	best estimator xgboost's best error=3.1221
[flaml.automl: 09-18 14:02:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:02:51] {3072} INFO -  at 20.2s,	estimator xgboost's best error=3.1221,	best estimator xgboost's best error=3.1221
[flaml.automl: 09-18 14:02:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:02:52] {3072} INFO -  at 21.8s,	estimator xgboost's best error=3.0884,	best estimator xgboost's best error=3.0884
[flaml.automl: 09-18 14:02:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:02:53] {3072} INFO -  at 23.0s,	estimator xgboost's best error=3.0884,	best estimator xgboost's best error=3.0884
[flaml.automl: 09-18 14:02:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:03:00] {3072} INFO -  at 29.5s,	estimator xgboost's best error=2.9109,	best estimator xgboost's best error=2.9109
[flaml.automl: 09-18 14:03:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:03:12] {3072} INFO -  at 41.7s,	estimator xgboost's best error=2.8408,	best estimator xgboost's best error=2.8408
[flaml.automl: 09-18 14:03:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 14:03:19] {3072} INFO -  at 48.2s,	estimator xgboost's best error=2.8408,	best estimator xgboost's best error=2.8408
[flaml.automl: 09-18 14:03:31] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 14:03:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:03:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:03:31] {2637} INFO - Time taken to find the best model: 41.66889262199402
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52158}
NO2(0)最佳损失：-1.8407884195313278
NO2(0)最好结果：{'pred_time': 7.0945569940728926e-06, 'wall_clock_time': 41.66889262199402, 'metric_for_logging': {'pred_time': 7.0945569940728926e-06}, 'val_loss': 2.8407884195313278, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52158}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52158, 'experiment_tag': 'exp', 'time_total_s': 12.126052618026733}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8230039224911886
NO2(0)的mse=18.35159231072864
NO2(0)的mae=2.8794308641312285
NO2(0)的mar=0.20369394051040238
总共花费的时间为：61.64
永州市
2477A
2478A
2479A
2480A
2481A
[flaml.automl: 09-18 14:19:02] {2390} INFO - task = regression
[flaml.automl: 09-18 14:19:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:19:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:19:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:19:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:19:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:19:03] {3025} INFO - Estimated sufficient time budget=60653s. Estimated necessary time budget=61s.
[flaml.automl: 09-18 14:19:03] {3072} INFO -  at 1.4s,	estimator xgboost's best error=9.1435,	best estimator xgboost's best error=9.1435
[flaml.automl: 09-18 14:19:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:19:05] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.4459,	best estimator xgboost's best error=4.4459
[flaml.automl: 09-18 14:19:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:19:06] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.4459,	best estimator xgboost's best error=4.4459
[flaml.automl: 09-18 14:19:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:19:12] {3072} INFO -  at 10.0s,	estimator xgboost's best error=4.4459,	best estimator xgboost's best error=4.4459
[flaml.automl: 09-18 14:19:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:19:13] {3072} INFO -  at 11.1s,	estimator xgboost's best error=2.8069,	best estimator xgboost's best error=2.8069
[flaml.automl: 09-18 14:19:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:19:14] {3072} INFO -  at 12.7s,	estimator xgboost's best error=2.4645,	best estimator xgboost's best error=2.4645
[flaml.automl: 09-18 14:19:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:19:16] {3072} INFO -  at 14.3s,	estimator xgboost's best error=2.4645,	best estimator xgboost's best error=2.4645
[flaml.automl: 09-18 14:19:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:19:18] {3072} INFO -  at 16.8s,	estimator xgboost's best error=2.4645,	best estimator xgboost's best error=2.4645
[flaml.automl: 09-18 14:19:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:19:20] {3072} INFO -  at 17.9s,	estimator xgboost's best error=2.4645,	best estimator xgboost's best error=2.4645
[flaml.automl: 09-18 14:19:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:19:22] {3072} INFO -  at 20.5s,	estimator xgboost's best error=2.4645,	best estimator xgboost's best error=2.4645
[flaml.automl: 09-18 14:19:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:19:24] {3072} INFO -  at 22.1s,	estimator xgboost's best error=2.4431,	best estimator xgboost's best error=2.4431
[flaml.automl: 09-18 14:19:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:19:25] {3072} INFO -  at 23.3s,	estimator xgboost's best error=2.4431,	best estimator xgboost's best error=2.4431
[flaml.automl: 09-18 14:19:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:19:32] {3072} INFO -  at 29.8s,	estimator xgboost's best error=2.3433,	best estimator xgboost's best error=2.3433
[flaml.automl: 09-18 14:19:32] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:19:44] {3072} INFO -  at 42.0s,	estimator xgboost's best error=2.2786,	best estimator xgboost's best error=2.2786
[flaml.automl: 09-18 14:19:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 14:19:50] {3072} INFO -  at 48.6s,	estimator xgboost's best error=2.2786,	best estimator xgboost's best error=2.2786
[flaml.automl: 09-18 14:20:02] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 14:20:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:20:02] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:20:02] {2637} INFO - Time taken to find the best model: 41.99696969985962
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51320}
NO2(0)最佳损失：-1.2785879153233757
NO2(0)最好结果：{'pred_time': 6.946245662041052e-06, 'wall_clock_time': 41.99696969985962, 'metric_for_logging': {'pred_time': 6.946245662041052e-06}, 'val_loss': 2.2785879153233757, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51320}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51320, 'experiment_tag': 'exp', 'time_total_s': 12.163694143295288}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8647023364257835
NO2(0)的mse=12.16176610802461
NO2(0)的mae=2.288204202070983
NO2(0)的mar=0.19609102479089044
总共花费的时间为：61.51
怀化市
2482A
2483A
2484A
2485A
2486A
[flaml.automl: 09-18 14:36:05] {2390} INFO - task = regression
[flaml.automl: 09-18 14:36:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:36:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:36:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:36:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:36:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:36:07] {3025} INFO - Estimated sufficient time budget=108753s. Estimated necessary time budget=109s.
[flaml.automl: 09-18 14:36:07] {3072} INFO -  at 2.5s,	estimator xgboost's best error=7.4669,	best estimator xgboost's best error=7.4669
[flaml.automl: 09-18 14:36:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:36:10] {3072} INFO -  at 5.9s,	estimator xgboost's best error=3.9060,	best estimator xgboost's best error=3.9060
[flaml.automl: 09-18 14:36:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:36:12] {3072} INFO -  at 8.1s,	estimator xgboost's best error=3.9060,	best estimator xgboost's best error=3.9060
[flaml.automl: 09-18 14:36:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:36:17] {3072} INFO -  at 12.9s,	estimator xgboost's best error=3.9060,	best estimator xgboost's best error=3.9060
[flaml.automl: 09-18 14:36:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:36:19] {3072} INFO -  at 15.1s,	estimator xgboost's best error=2.8746,	best estimator xgboost's best error=2.8746
[flaml.automl: 09-18 14:36:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:36:22] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.5890,	best estimator xgboost's best error=2.5890
[flaml.automl: 09-18 14:36:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:36:24] {3072} INFO -  at 20.0s,	estimator xgboost's best error=2.5890,	best estimator xgboost's best error=2.5890
[flaml.automl: 09-18 14:36:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:36:27] {3072} INFO -  at 23.2s,	estimator xgboost's best error=2.5890,	best estimator xgboost's best error=2.5890
[flaml.automl: 09-18 14:36:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:36:30] {3072} INFO -  at 25.2s,	estimator xgboost's best error=2.5890,	best estimator xgboost's best error=2.5890
[flaml.automl: 09-18 14:36:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:36:32] {3072} INFO -  at 27.6s,	estimator xgboost's best error=2.5890,	best estimator xgboost's best error=2.5890
[flaml.automl: 09-18 14:36:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:36:35] {3072} INFO -  at 30.5s,	estimator xgboost's best error=2.5441,	best estimator xgboost's best error=2.5441
[flaml.automl: 09-18 14:36:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:36:37] {3072} INFO -  at 32.7s,	estimator xgboost's best error=2.5441,	best estimator xgboost's best error=2.5441
[flaml.automl: 09-18 14:36:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:36:48] {3072} INFO -  at 43.3s,	estimator xgboost's best error=2.4398,	best estimator xgboost's best error=2.4398
[flaml.automl: 09-18 14:36:48] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:37:03] {3072} INFO -  at 58.9s,	estimator xgboost's best error=2.3885,	best estimator xgboost's best error=2.3885
[flaml.automl: 09-18 14:37:24] {3335} INFO - retrain xgboost for 21.2s
[flaml.automl: 09-18 14:37:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:37:24] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:37:24] {2637} INFO - Time taken to find the best model: 58.860050201416016
[flaml.automl: 09-18 14:37:24] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 50742}
NO2(0)最佳损失：-1.3884892618245965
NO2(0)最好结果：{'pred_time': 1.2252803557559335e-05, 'wall_clock_time': 58.860050201416016, 'metric_for_logging': {'pred_time': 1.2252803557559335e-05}, 'val_loss': 2.3884892618245965, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 50742}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 50742, 'experiment_tag': 'exp', 'time_total_s': 15.542267322540283}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7830467182423222
NO2(0)的mse=14.676342130802741
NO2(0)的mae=2.429346469984184
NO2(0)的mar=0.2809437386814847
总共花费的时间为：81.12
娄底市
2487A
2488A
2489A
2490A
2491A
[flaml.automl: 09-18 14:52:42] {2390} INFO - task = regression
[flaml.automl: 09-18 14:52:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:52:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:52:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:52:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:52:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:52:44] {3025} INFO - Estimated sufficient time budget=61443s. Estimated necessary time budget=61s.
[flaml.automl: 09-18 14:52:44] {3072} INFO -  at 1.4s,	estimator xgboost's best error=10.9066,	best estimator xgboost's best error=10.9066
[flaml.automl: 09-18 14:52:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:52:46] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.3889,	best estimator xgboost's best error=5.3889
[flaml.automl: 09-18 14:52:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:52:47] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.3889,	best estimator xgboost's best error=5.3889
[flaml.automl: 09-18 14:52:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:52:52] {3072} INFO -  at 9.6s,	estimator xgboost's best error=5.3889,	best estimator xgboost's best error=5.3889
[flaml.automl: 09-18 14:52:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:52:53] {3072} INFO -  at 10.7s,	estimator xgboost's best error=3.7858,	best estimator xgboost's best error=3.7858
[flaml.automl: 09-18 14:52:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:52:54] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.4393,	best estimator xgboost's best error=3.4393
[flaml.automl: 09-18 14:52:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:52:56] {3072} INFO -  at 13.9s,	estimator xgboost's best error=3.4393,	best estimator xgboost's best error=3.4393
[flaml.automl: 09-18 14:52:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:52:59] {3072} INFO -  at 16.4s,	estimator xgboost's best error=3.4393,	best estimator xgboost's best error=3.4393
[flaml.automl: 09-18 14:52:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:53:00] {3072} INFO -  at 17.6s,	estimator xgboost's best error=3.4393,	best estimator xgboost's best error=3.4393
[flaml.automl: 09-18 14:53:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:53:02] {3072} INFO -  at 20.2s,	estimator xgboost's best error=3.4393,	best estimator xgboost's best error=3.4393
[flaml.automl: 09-18 14:53:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:53:04] {3072} INFO -  at 21.9s,	estimator xgboost's best error=3.4393,	best estimator xgboost's best error=3.4393
[flaml.automl: 09-18 14:53:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:53:05] {3072} INFO -  at 23.0s,	estimator xgboost's best error=3.4393,	best estimator xgboost's best error=3.4393
[flaml.automl: 09-18 14:53:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:53:12] {3072} INFO -  at 29.6s,	estimator xgboost's best error=3.3328,	best estimator xgboost's best error=3.3328
[flaml.automl: 09-18 14:53:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:53:24] {3072} INFO -  at 41.7s,	estimator xgboost's best error=3.2463,	best estimator xgboost's best error=3.2463
[flaml.automl: 09-18 14:53:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 14:53:30] {3072} INFO -  at 48.2s,	estimator xgboost's best error=3.2463,	best estimator xgboost's best error=3.2463
[flaml.automl: 09-18 14:53:42] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 14:53:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:53:42] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:53:42] {2637} INFO - Time taken to find the best model: 41.71349859237671
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51357}
NO2(0)最佳损失：-2.2463075062304094
NO2(0)最好结果：{'pred_time': 7.426068225925433e-06, 'wall_clock_time': 41.71349859237671, 'metric_for_logging': {'pred_time': 7.426068225925433e-06}, 'val_loss': 3.2463075062304094, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51357}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51357, 'experiment_tag': 'exp', 'time_total_s': 12.121228218078613}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7955795274824757
NO2(0)的mse=25.675125823275458
NO2(0)的mae=3.1851160417989006
NO2(0)的mar=0.22098546421525242
总共花费的时间为：61.15
湘西州
2492A
2493A
2494A
[flaml.automl: 09-18 15:03:40] {2390} INFO - task = regression
[flaml.automl: 09-18 15:03:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:03:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:03:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:03:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:03:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:03:41] {3025} INFO - Estimated sufficient time budget=12217s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 15:03:41] {3072} INFO -  at 1.4s,	estimator xgboost's best error=6.0265,	best estimator xgboost's best error=6.0265
[flaml.automl: 09-18 15:03:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:03:43] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.1924,	best estimator xgboost's best error=3.1924
[flaml.automl: 09-18 15:03:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:03:44] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.1924,	best estimator xgboost's best error=3.1924
[flaml.automl: 09-18 15:03:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:03:54] {3072} INFO -  at 14.8s,	estimator xgboost's best error=3.1924,	best estimator xgboost's best error=3.1924
[flaml.automl: 09-18 15:03:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:03:55] {3072} INFO -  at 15.9s,	estimator xgboost's best error=2.4086,	best estimator xgboost's best error=2.4086
[flaml.automl: 09-18 15:03:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:03:57] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.2697,	best estimator xgboost's best error=2.2697
[flaml.automl: 09-18 15:03:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:03:59] {3072} INFO -  at 19.2s,	estimator xgboost's best error=2.2659,	best estimator xgboost's best error=2.2659
[flaml.automl: 09-18 15:03:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:04:01] {3072} INFO -  at 21.9s,	estimator xgboost's best error=2.2659,	best estimator xgboost's best error=2.2659
[flaml.automl: 09-18 15:04:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:04:03] {3072} INFO -  at 23.5s,	estimator xgboost's best error=2.2659,	best estimator xgboost's best error=2.2659
[flaml.automl: 09-18 15:04:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:04:06] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.2230,	best estimator xgboost's best error=2.2230
[flaml.automl: 09-18 15:04:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:04:08] {3072} INFO -  at 28.2s,	estimator xgboost's best error=2.2230,	best estimator xgboost's best error=2.2230
[flaml.automl: 09-18 15:04:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:04:09] {3072} INFO -  at 29.4s,	estimator xgboost's best error=2.2230,	best estimator xgboost's best error=2.2230
[flaml.automl: 09-18 15:04:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:04:23] {3072} INFO -  at 43.0s,	estimator xgboost's best error=2.1517,	best estimator xgboost's best error=2.1517
[flaml.automl: 09-18 15:04:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:04:39] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.1517,	best estimator xgboost's best error=2.1517
[flaml.automl: 09-18 15:04:53] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-18 15:04:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 15:04:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:04:53] {2637} INFO - Time taken to find the best model: 43.04191017150879
[flaml.automl: 09-18 15:04:53] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
NO2(0)最佳损失：-1.1517384084801305
NO2(0)最好结果：{'pred_time': 1.255208769583508e-05, 'wall_clock_time': 43.04191017150879, 'metric_for_logging': {'pred_time': 1.255208769583508e-05}, 'val_loss': 2.1517384084801305, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 13.671735048294067}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7916425266490268
NO2(0)的mse=10.978484648212824
NO2(0)的mae=2.1324005530356067
NO2(0)的mar=0.3085203001698037
总共花费的时间为：73.82
梧州市
2495A
2496A
2497A
2498A
[flaml.automl: 09-18 15:17:24] {2390} INFO - task = regression
[flaml.automl: 09-18 15:17:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:17:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:17:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:17:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:17:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:17:25] {3025} INFO - Estimated sufficient time budget=51358s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 15:17:25] {3072} INFO -  at 1.5s,	estimator xgboost's best error=15.5585,	best estimator xgboost's best error=15.5585
[flaml.automl: 09-18 15:17:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:17:27] {3072} INFO -  at 3.6s,	estimator xgboost's best error=7.9231,	best estimator xgboost's best error=7.9231
[flaml.automl: 09-18 15:17:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:17:29] {3072} INFO -  at 4.8s,	estimator xgboost's best error=7.9231,	best estimator xgboost's best error=7.9231
[flaml.automl: 09-18 15:17:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:17:34] {3072} INFO -  at 10.6s,	estimator xgboost's best error=7.9231,	best estimator xgboost's best error=7.9231
[flaml.automl: 09-18 15:17:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:17:36] {3072} INFO -  at 11.8s,	estimator xgboost's best error=5.5652,	best estimator xgboost's best error=5.5652
[flaml.automl: 09-18 15:17:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:17:37] {3072} INFO -  at 13.3s,	estimator xgboost's best error=5.0540,	best estimator xgboost's best error=5.0540
[flaml.automl: 09-18 15:17:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:17:39] {3072} INFO -  at 14.9s,	estimator xgboost's best error=5.0540,	best estimator xgboost's best error=5.0540
[flaml.automl: 09-18 15:17:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:17:41] {3072} INFO -  at 17.4s,	estimator xgboost's best error=5.0540,	best estimator xgboost's best error=5.0540
[flaml.automl: 09-18 15:17:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:17:42] {3072} INFO -  at 18.5s,	estimator xgboost's best error=5.0540,	best estimator xgboost's best error=5.0540
[flaml.automl: 09-18 15:17:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:17:45] {3072} INFO -  at 21.2s,	estimator xgboost's best error=5.0540,	best estimator xgboost's best error=5.0540
[flaml.automl: 09-18 15:17:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:17:47] {3072} INFO -  at 22.8s,	estimator xgboost's best error=5.0439,	best estimator xgboost's best error=5.0439
[flaml.automl: 09-18 15:17:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:17:48] {3072} INFO -  at 24.0s,	estimator xgboost's best error=5.0439,	best estimator xgboost's best error=5.0439
[flaml.automl: 09-18 15:17:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:17:57] {3072} INFO -  at 33.1s,	estimator xgboost's best error=4.9506,	best estimator xgboost's best error=4.9506
[flaml.automl: 09-18 15:17:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:18:18] {3072} INFO -  at 53.7s,	estimator xgboost's best error=4.7070,	best estimator xgboost's best error=4.7070
[flaml.automl: 09-18 15:18:37] {3335} INFO - retrain xgboost for 19.7s
[flaml.automl: 09-18 15:18:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:18:37] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:18:37] {2637} INFO - Time taken to find the best model: 53.747827768325806
[flaml.automl: 09-18 15:18:37] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42934}
NO2(0)最佳损失：-3.7070242416131576
NO2(0)最好结果：{'pred_time': 1.6915725727197236e-05, 'wall_clock_time': 53.747827768325806, 'metric_for_logging': {'pred_time': 1.6915725727197236e-05}, 'val_loss': 4.707024241613158, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42934}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42934, 'experiment_tag': 'exp', 'time_total_s': 20.677561044692993}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7558574469152334
NO2(0)的mse=47.63665394608052
NO2(0)的mae=4.626951015258685
NO2(0)的mar=0.2704442608881379
总共花费的时间为：74.21
防城港市
2499A
2500A
2501A
[flaml.automl: 09-18 15:27:47] {2390} INFO - task = regression
[flaml.automl: 09-18 15:27:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:27:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:27:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:27:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:27:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:27:48] {3025} INFO - Estimated sufficient time budget=12164s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 15:27:48] {3072} INFO -  at 1.4s,	estimator xgboost's best error=9.9737,	best estimator xgboost's best error=9.9737
[flaml.automl: 09-18 15:27:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:27:50] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.0040,	best estimator xgboost's best error=5.0040
[flaml.automl: 09-18 15:27:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:27:51] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.0040,	best estimator xgboost's best error=5.0040
[flaml.automl: 09-18 15:27:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:28:01] {3072} INFO -  at 14.8s,	estimator xgboost's best error=5.0040,	best estimator xgboost's best error=5.0040
[flaml.automl: 09-18 15:28:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:28:03] {3072} INFO -  at 15.9s,	estimator xgboost's best error=3.6491,	best estimator xgboost's best error=3.6491
[flaml.automl: 09-18 15:28:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:28:04] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.4127,	best estimator xgboost's best error=3.4127
[flaml.automl: 09-18 15:28:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:28:06] {3072} INFO -  at 19.2s,	estimator xgboost's best error=3.4127,	best estimator xgboost's best error=3.4127
[flaml.automl: 09-18 15:28:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:28:08] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.4127,	best estimator xgboost's best error=3.4127
[flaml.automl: 09-18 15:28:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:28:09] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.4127,	best estimator xgboost's best error=3.4127
[flaml.automl: 09-18 15:28:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:28:12] {3072} INFO -  at 25.5s,	estimator xgboost's best error=3.4127,	best estimator xgboost's best error=3.4127
[flaml.automl: 09-18 15:28:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:28:13] {3072} INFO -  at 26.6s,	estimator xgboost's best error=3.4127,	best estimator xgboost's best error=3.4127
[flaml.automl: 09-18 15:28:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:28:14] {3072} INFO -  at 27.8s,	estimator xgboost's best error=3.4127,	best estimator xgboost's best error=3.4127
[flaml.automl: 09-18 15:28:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:28:21] {3072} INFO -  at 34.3s,	estimator xgboost's best error=3.3695,	best estimator xgboost's best error=3.3695
[flaml.automl: 09-18 15:28:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:28:33] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.2729,	best estimator xgboost's best error=3.2729
[flaml.automl: 09-18 15:28:33] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:28:39] {3072} INFO -  at 52.8s,	estimator xgboost's best error=3.2729,	best estimator xgboost's best error=3.2729
[flaml.automl: 09-18 15:28:51] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 15:28:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:28:51] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:28:51] {2637} INFO - Time taken to find the best model: 46.3546838760376
[flaml.automl: 09-18 15:28:51] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.2729228642341255
NO2(0)最好结果：{'pred_time': 1.1526048183441162e-05, 'wall_clock_time': 46.3546838760376, 'metric_for_logging': {'pred_time': 1.1526048183441162e-05}, 'val_loss': 3.2729228642341255, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.073959350585938}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.618488839394925
NO2(0)的mse=24.685388294055215
NO2(0)的mae=3.1796874915432984
NO2(0)的mar=0.22450188981194438
总共花费的时间为：65.41
钦州市
2502A
2503A
2504A
3404A
[flaml.automl: 09-18 15:41:55] {2390} INFO - task = regression
[flaml.automl: 09-18 15:41:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:41:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:41:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:41:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:41:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:41:58] {3025} INFO - Estimated sufficient time budget=93645s. Estimated necessary time budget=94s.
[flaml.automl: 09-18 15:41:58] {3072} INFO -  at 2.5s,	estimator xgboost's best error=10.2920,	best estimator xgboost's best error=10.2920
[flaml.automl: 09-18 15:41:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:42:01] {3072} INFO -  at 6.1s,	estimator xgboost's best error=5.0572,	best estimator xgboost's best error=5.0572
[flaml.automl: 09-18 15:42:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:42:03] {3072} INFO -  at 8.1s,	estimator xgboost's best error=5.0572,	best estimator xgboost's best error=5.0572
[flaml.automl: 09-18 15:42:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:42:09] {3072} INFO -  at 13.4s,	estimator xgboost's best error=5.0572,	best estimator xgboost's best error=5.0572
[flaml.automl: 09-18 15:42:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:42:10] {3072} INFO -  at 15.0s,	estimator xgboost's best error=3.5128,	best estimator xgboost's best error=3.5128
[flaml.automl: 09-18 15:42:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:42:13] {3072} INFO -  at 17.8s,	estimator xgboost's best error=3.1817,	best estimator xgboost's best error=3.1817
[flaml.automl: 09-18 15:42:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:42:15] {3072} INFO -  at 20.1s,	estimator xgboost's best error=3.1817,	best estimator xgboost's best error=3.1817
[flaml.automl: 09-18 15:42:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:42:19] {3072} INFO -  at 24.0s,	estimator xgboost's best error=3.1817,	best estimator xgboost's best error=3.1817
[flaml.automl: 09-18 15:42:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:42:21] {3072} INFO -  at 25.7s,	estimator xgboost's best error=3.1817,	best estimator xgboost's best error=3.1817
[flaml.automl: 09-18 15:42:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:42:24] {3072} INFO -  at 29.1s,	estimator xgboost's best error=3.1817,	best estimator xgboost's best error=3.1817
[flaml.automl: 09-18 15:42:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:42:27] {3072} INFO -  at 31.5s,	estimator xgboost's best error=3.1672,	best estimator xgboost's best error=3.1672
[flaml.automl: 09-18 15:42:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:42:28] {3072} INFO -  at 33.3s,	estimator xgboost's best error=3.1672,	best estimator xgboost's best error=3.1672
[flaml.automl: 09-18 15:42:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:42:38] {3072} INFO -  at 43.1s,	estimator xgboost's best error=3.1037,	best estimator xgboost's best error=3.1037
[flaml.automl: 09-18 15:42:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:42:52] {3072} INFO -  at 57.1s,	estimator xgboost's best error=3.0521,	best estimator xgboost's best error=3.0521
[flaml.automl: 09-18 15:43:04] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 15:43:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:43:04] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:43:04] {2637} INFO - Time taken to find the best model: 57.05997586250305
[flaml.automl: 09-18 15:43:04] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42911}
NO2(0)最佳损失：-2.052113784339604
NO2(0)最好结果：{'pred_time': 8.257193453359923e-06, 'wall_clock_time': 57.05997586250305, 'metric_for_logging': {'pred_time': 8.257193453359923e-06}, 'val_loss': 3.052113784339604, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42911}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42911, 'experiment_tag': 'exp', 'time_total_s': 13.929351091384888}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7160845604788115
NO2(0)的mse=22.030234124971603
NO2(0)的mae=3.054039172155986
NO2(0)的mar=0.20607228629742894
总共花费的时间为：69.90
贵港市
2505A
2506A
2507A
2508A
3405A
[flaml.automl: 09-18 15:59:08] {2390} INFO - task = regression
[flaml.automl: 09-18 15:59:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:59:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:59:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:59:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:59:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:59:09] {3025} INFO - Estimated sufficient time budget=61492s. Estimated necessary time budget=61s.
[flaml.automl: 09-18 15:59:10] {3072} INFO -  at 1.5s,	estimator xgboost's best error=11.1960,	best estimator xgboost's best error=11.1960
[flaml.automl: 09-18 15:59:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:59:12] {3072} INFO -  at 3.6s,	estimator xgboost's best error=5.4693,	best estimator xgboost's best error=5.4693
[flaml.automl: 09-18 15:59:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:59:13] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.4693,	best estimator xgboost's best error=5.4693
[flaml.automl: 09-18 15:59:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:59:18] {3072} INFO -  at 9.6s,	estimator xgboost's best error=5.4693,	best estimator xgboost's best error=5.4693
[flaml.automl: 09-18 15:59:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:59:19] {3072} INFO -  at 10.7s,	estimator xgboost's best error=3.8087,	best estimator xgboost's best error=3.8087
[flaml.automl: 09-18 15:59:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:59:20] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.4344,	best estimator xgboost's best error=3.4344
[flaml.automl: 09-18 15:59:20] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:59:22] {3072} INFO -  at 13.9s,	estimator xgboost's best error=3.4344,	best estimator xgboost's best error=3.4344
[flaml.automl: 09-18 15:59:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:59:24] {3072} INFO -  at 16.3s,	estimator xgboost's best error=3.4344,	best estimator xgboost's best error=3.4344
[flaml.automl: 09-18 15:59:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:59:25] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.4344,	best estimator xgboost's best error=3.4344
[flaml.automl: 09-18 15:59:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:59:28] {3072} INFO -  at 20.1s,	estimator xgboost's best error=3.4344,	best estimator xgboost's best error=3.4344
[flaml.automl: 09-18 15:59:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:59:30] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.4112,	best estimator xgboost's best error=3.4112
[flaml.automl: 09-18 15:59:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:59:31] {3072} INFO -  at 22.9s,	estimator xgboost's best error=3.4112,	best estimator xgboost's best error=3.4112
[flaml.automl: 09-18 15:59:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:59:37] {3072} INFO -  at 29.4s,	estimator xgboost's best error=3.3661,	best estimator xgboost's best error=3.3661
[flaml.automl: 09-18 15:59:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:59:49] {3072} INFO -  at 41.3s,	estimator xgboost's best error=3.2928,	best estimator xgboost's best error=3.2928
[flaml.automl: 09-18 15:59:49] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:59:56] {3072} INFO -  at 47.9s,	estimator xgboost's best error=3.2928,	best estimator xgboost's best error=3.2928
[flaml.automl: 09-18 15:59:56] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 16:00:07] {3072} INFO -  at 59.2s,	estimator xgboost's best error=3.2543,	best estimator xgboost's best error=3.2543
[flaml.automl: 09-18 16:00:29] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-18 16:00:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:00:29] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:00:29] {2637} INFO - Time taken to find the best model: 59.23719239234924
[flaml.automl: 09-18 16:00:29] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 51759}
NO2(0)最佳损失：-2.2543424060762507
NO2(0)最好结果：{'pred_time': 7.281382989153909e-06, 'wall_clock_time': 59.23719239234924, 'metric_for_logging': {'pred_time': 7.281382989153909e-06}, 'val_loss': 3.2543424060762507, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 51759}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 51759, 'experiment_tag': 'exp', 'time_total_s': 11.38426160812378}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7174162314355608
NO2(0)的mse=24.735224442874415
NO2(0)的mae=3.2929129111451583
NO2(0)的mar=0.2054667427968485
总共花费的时间为：81.67
玉林市
2509A
2510A
2511A
3532A
3533A
[flaml.automl: 09-18 16:16:33] {2390} INFO - task = regression
[flaml.automl: 09-18 16:16:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:16:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:16:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:16:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:16:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:16:34] {3025} INFO - Estimated sufficient time budget=86159s. Estimated necessary time budget=86s.
[flaml.automl: 09-18 16:16:34] {3072} INFO -  at 2.0s,	estimator xgboost's best error=9.3510,	best estimator xgboost's best error=9.3510
[flaml.automl: 09-18 16:16:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:16:38] {3072} INFO -  at 5.2s,	estimator xgboost's best error=4.5869,	best estimator xgboost's best error=4.5869
[flaml.automl: 09-18 16:16:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:16:40] {3072} INFO -  at 7.3s,	estimator xgboost's best error=4.5869,	best estimator xgboost's best error=4.5869
[flaml.automl: 09-18 16:16:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:16:44] {3072} INFO -  at 12.0s,	estimator xgboost's best error=4.5869,	best estimator xgboost's best error=4.5869
[flaml.automl: 09-18 16:16:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:16:46] {3072} INFO -  at 14.1s,	estimator xgboost's best error=3.2315,	best estimator xgboost's best error=3.2315
[flaml.automl: 09-18 16:16:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:16:49] {3072} INFO -  at 16.5s,	estimator xgboost's best error=2.8856,	best estimator xgboost's best error=2.8856
[flaml.automl: 09-18 16:16:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:16:51] {3072} INFO -  at 18.2s,	estimator xgboost's best error=2.8856,	best estimator xgboost's best error=2.8856
[flaml.automl: 09-18 16:16:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:16:53] {3072} INFO -  at 20.6s,	estimator xgboost's best error=2.8856,	best estimator xgboost's best error=2.8856
[flaml.automl: 09-18 16:16:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:16:54] {3072} INFO -  at 21.8s,	estimator xgboost's best error=2.8856,	best estimator xgboost's best error=2.8856
[flaml.automl: 09-18 16:16:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:16:57] {3072} INFO -  at 24.5s,	estimator xgboost's best error=2.8856,	best estimator xgboost's best error=2.8856
[flaml.automl: 09-18 16:16:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:16:58] {3072} INFO -  at 26.1s,	estimator xgboost's best error=2.8856,	best estimator xgboost's best error=2.8856
[flaml.automl: 09-18 16:16:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:17:00] {3072} INFO -  at 27.2s,	estimator xgboost's best error=2.8856,	best estimator xgboost's best error=2.8856
[flaml.automl: 09-18 16:17:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:17:06] {3072} INFO -  at 33.8s,	estimator xgboost's best error=2.7704,	best estimator xgboost's best error=2.7704
[flaml.automl: 09-18 16:17:06] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:17:18] {3072} INFO -  at 45.8s,	estimator xgboost's best error=2.7187,	best estimator xgboost's best error=2.7187
[flaml.automl: 09-18 16:17:18] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 16:17:25] {3072} INFO -  at 52.4s,	estimator xgboost's best error=2.7187,	best estimator xgboost's best error=2.7187
[flaml.automl: 09-18 16:17:37] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 16:17:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:17:37] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:17:37] {2637} INFO - Time taken to find the best model: 45.83185577392578
[flaml.automl: 09-18 16:17:37] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51981}
NO2(0)最佳损失：-1.7186991231428288
NO2(0)最好结果：{'pred_time': 7.167706198969706e-06, 'wall_clock_time': 45.83185577392578, 'metric_for_logging': {'pred_time': 7.167706198969706e-06}, 'val_loss': 2.718699123142829, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51981}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51981, 'experiment_tag': 'exp', 'time_total_s': 12.077337265014648}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7495799146025959
NO2(0)的mse=15.709469592577237
NO2(0)的mae=2.70554353612054
NO2(0)的mar=0.20079583270687745
总共花费的时间为：65.47
百色市
2512A
2513A
3406A
[flaml.automl: 09-18 16:27:21] {2390} INFO - task = regression
[flaml.automl: 09-18 16:27:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:27:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:27:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:27:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:27:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:27:23] {3025} INFO - Estimated sufficient time budget=12161s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:27:23] {3072} INFO -  at 1.4s,	estimator xgboost's best error=9.7621,	best estimator xgboost's best error=9.7621
[flaml.automl: 09-18 16:27:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:27:25] {3072} INFO -  at 3.6s,	estimator xgboost's best error=4.6920,	best estimator xgboost's best error=4.6920
[flaml.automl: 09-18 16:27:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:27:26] {3072} INFO -  at 4.8s,	estimator xgboost's best error=4.6920,	best estimator xgboost's best error=4.6920
[flaml.automl: 09-18 16:27:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:27:36] {3072} INFO -  at 14.8s,	estimator xgboost's best error=4.6920,	best estimator xgboost's best error=4.6920
[flaml.automl: 09-18 16:27:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:27:37] {3072} INFO -  at 16.0s,	estimator xgboost's best error=3.1298,	best estimator xgboost's best error=3.1298
[flaml.automl: 09-18 16:27:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:27:39] {3072} INFO -  at 17.6s,	estimator xgboost's best error=2.7725,	best estimator xgboost's best error=2.7725
[flaml.automl: 09-18 16:27:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:27:40] {3072} INFO -  at 19.2s,	estimator xgboost's best error=2.7725,	best estimator xgboost's best error=2.7725
[flaml.automl: 09-18 16:27:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:27:43] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.7725,	best estimator xgboost's best error=2.7725
[flaml.automl: 09-18 16:27:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:27:44] {3072} INFO -  at 22.9s,	estimator xgboost's best error=2.7725,	best estimator xgboost's best error=2.7725
[flaml.automl: 09-18 16:27:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:27:47] {3072} INFO -  at 25.6s,	estimator xgboost's best error=2.7725,	best estimator xgboost's best error=2.7725
[flaml.automl: 09-18 16:27:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:27:48] {3072} INFO -  at 26.7s,	estimator xgboost's best error=2.7725,	best estimator xgboost's best error=2.7725
[flaml.automl: 09-18 16:27:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:27:49] {3072} INFO -  at 27.9s,	estimator xgboost's best error=2.7725,	best estimator xgboost's best error=2.7725
[flaml.automl: 09-18 16:27:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:27:55] {3072} INFO -  at 34.4s,	estimator xgboost's best error=2.7173,	best estimator xgboost's best error=2.7173
[flaml.automl: 09-18 16:27:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:28:07] {3072} INFO -  at 46.4s,	estimator xgboost's best error=2.6048,	best estimator xgboost's best error=2.6048
[flaml.automl: 09-18 16:28:07] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 16:28:14] {3072} INFO -  at 52.8s,	estimator xgboost's best error=2.6048,	best estimator xgboost's best error=2.6048
[flaml.automl: 09-18 16:28:26] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 16:28:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:28:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:28:26] {2637} INFO - Time taken to find the best model: 46.35939335823059
[flaml.automl: 09-18 16:28:26] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.604793212866448
NO2(0)最好结果：{'pred_time': 1.0976320983096239e-05, 'wall_clock_time': 46.35939335823059, 'metric_for_logging': {'pred_time': 1.0976320983096239e-05}, 'val_loss': 2.604793212866448, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.99284052848816}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7696541837047888
NO2(0)的mse=14.475114340281005
NO2(0)的mae=2.6198945206013464
NO2(0)的mar=0.19801946144196875
总共花费的时间为：65.45
贺州市
2514A
2515A
3534A
[flaml.automl: 09-18 16:38:16] {2390} INFO - task = regression
[flaml.automl: 09-18 16:38:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:38:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:38:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:38:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:38:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:38:18] {3025} INFO - Estimated sufficient time budget=12124s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:38:18] {3072} INFO -  at 1.4s,	estimator xgboost's best error=10.8183,	best estimator xgboost's best error=10.8183
[flaml.automl: 09-18 16:38:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:38:20] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.0115,	best estimator xgboost's best error=5.0115
[flaml.automl: 09-18 16:38:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:38:21] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.0115,	best estimator xgboost's best error=5.0115
[flaml.automl: 09-18 16:38:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:38:31] {3072} INFO -  at 14.7s,	estimator xgboost's best error=5.0115,	best estimator xgboost's best error=5.0115
[flaml.automl: 09-18 16:38:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:38:32] {3072} INFO -  at 15.9s,	estimator xgboost's best error=3.1073,	best estimator xgboost's best error=3.1073
[flaml.automl: 09-18 16:38:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:38:34] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.8140,	best estimator xgboost's best error=2.8140
[flaml.automl: 09-18 16:38:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:38:35] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.8140,	best estimator xgboost's best error=2.8140
[flaml.automl: 09-18 16:38:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:38:38] {3072} INFO -  at 21.6s,	estimator xgboost's best error=2.8140,	best estimator xgboost's best error=2.8140
[flaml.automl: 09-18 16:38:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:38:39] {3072} INFO -  at 22.8s,	estimator xgboost's best error=2.8140,	best estimator xgboost's best error=2.8140
[flaml.automl: 09-18 16:38:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:38:42] {3072} INFO -  at 25.5s,	estimator xgboost's best error=2.8140,	best estimator xgboost's best error=2.8140
[flaml.automl: 09-18 16:38:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:38:43] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.8140,	best estimator xgboost's best error=2.8140
[flaml.automl: 09-18 16:38:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:38:44] {3072} INFO -  at 27.8s,	estimator xgboost's best error=2.8140,	best estimator xgboost's best error=2.8140
[flaml.automl: 09-18 16:38:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:38:50] {3072} INFO -  at 34.3s,	estimator xgboost's best error=2.8140,	best estimator xgboost's best error=2.8140
[flaml.automl: 09-18 16:38:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:38:53] {3072} INFO -  at 37.1s,	estimator xgboost's best error=2.7702,	best estimator xgboost's best error=2.7702
[flaml.automl: 09-18 16:38:53] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 16:38:55] {3072} INFO -  at 38.7s,	estimator xgboost's best error=2.7702,	best estimator xgboost's best error=2.7702
[flaml.automl: 09-18 16:38:55] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 16:39:00] {3072} INFO -  at 43.5s,	estimator xgboost's best error=2.7551,	best estimator xgboost's best error=2.7551
[flaml.automl: 09-18 16:39:00] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 16:39:03] {3072} INFO -  at 46.4s,	estimator xgboost's best error=2.7551,	best estimator xgboost's best error=2.7551
[flaml.automl: 09-18 16:39:03] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 16:39:05] {3072} INFO -  at 48.5s,	estimator xgboost's best error=2.7551,	best estimator xgboost's best error=2.7551
[flaml.automl: 09-18 16:39:05] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 16:39:16] {3072} INFO -  at 59.8s,	estimator xgboost's best error=2.7551,	best estimator xgboost's best error=2.7551
[flaml.automl: 09-18 16:39:21] {3335} INFO - retrain xgboost for 4.8s
[flaml.automl: 09-18 16:39:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:39:21] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:39:21] {2637} INFO - Time taken to find the best model: 43.52938199043274
[flaml.automl: 09-18 16:39:21] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}
NO2(0)最佳损失：-1.7551056072011342
NO2(0)最好结果：{'pred_time': 1.0853900422281741e-05, 'wall_clock_time': 43.52938199043274, 'metric_for_logging': {'pred_time': 1.0853900422281741e-05}, 'val_loss': 2.755105607201134, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}, 'config/n_estimators': 9, 'config/max_leaves': 9, 'config/min_child_weight': 3.029166996333365, 'config/learning_rate': 0.28782051713764356, 'config/subsample': 0.8051874633193139, 'config/colsample_bylevel': 0.656637827405616, 'config/colsample_bytree': 0.7193335366419935, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 2.3848121277079324, 'experiment_tag': 'exp', 'time_total_s': 4.8032450675964355}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6011538145146889
NO2(0)的mse=17.677859544219597
NO2(0)的mae=2.7387838935755697
NO2(0)的mar=0.16557722875226305
总共花费的时间为：65.24
河池市
2516A
2517A
2518A
[flaml.automl: 09-18 16:48:49] {2390} INFO - task = regression
[flaml.automl: 09-18 16:48:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:48:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:48:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:48:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:48:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:48:50] {3025} INFO - Estimated sufficient time budget=12137s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:48:50] {3072} INFO -  at 1.4s,	estimator xgboost's best error=10.0637,	best estimator xgboost's best error=10.0637
[flaml.automl: 09-18 16:48:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:48:53] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.7443,	best estimator xgboost's best error=4.7443
[flaml.automl: 09-18 16:48:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:48:54] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.7443,	best estimator xgboost's best error=4.7443
[flaml.automl: 09-18 16:48:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:49:04] {3072} INFO -  at 14.7s,	estimator xgboost's best error=4.7443,	best estimator xgboost's best error=4.7443
[flaml.automl: 09-18 16:49:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:49:05] {3072} INFO -  at 15.8s,	estimator xgboost's best error=3.0528,	best estimator xgboost's best error=3.0528
[flaml.automl: 09-18 16:49:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:49:07] {3072} INFO -  at 17.4s,	estimator xgboost's best error=2.6769,	best estimator xgboost's best error=2.6769
[flaml.automl: 09-18 16:49:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:49:08] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.6769,	best estimator xgboost's best error=2.6769
[flaml.automl: 09-18 16:49:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:49:11] {3072} INFO -  at 21.5s,	estimator xgboost's best error=2.6769,	best estimator xgboost's best error=2.6769
[flaml.automl: 09-18 16:49:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:49:12] {3072} INFO -  at 22.7s,	estimator xgboost's best error=2.6769,	best estimator xgboost's best error=2.6769
[flaml.automl: 09-18 16:49:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:49:15] {3072} INFO -  at 25.4s,	estimator xgboost's best error=2.6769,	best estimator xgboost's best error=2.6769
[flaml.automl: 09-18 16:49:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:49:16] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.6769,	best estimator xgboost's best error=2.6769
[flaml.automl: 09-18 16:49:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:49:17] {3072} INFO -  at 27.7s,	estimator xgboost's best error=2.6769,	best estimator xgboost's best error=2.6769
[flaml.automl: 09-18 16:49:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:49:23] {3072} INFO -  at 34.2s,	estimator xgboost's best error=2.5964,	best estimator xgboost's best error=2.5964
[flaml.automl: 09-18 16:49:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:49:35] {3072} INFO -  at 46.2s,	estimator xgboost's best error=2.5495,	best estimator xgboost's best error=2.5495
[flaml.automl: 09-18 16:49:35] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 16:49:42] {3072} INFO -  at 52.7s,	estimator xgboost's best error=2.5495,	best estimator xgboost's best error=2.5495
[flaml.automl: 09-18 16:49:54] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 16:49:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:49:54] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:49:54] {2637} INFO - Time taken to find the best model: 46.22436261177063
[flaml.automl: 09-18 16:49:54] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.5495309082122213
NO2(0)最好结果：{'pred_time': 1.3326710276028083e-05, 'wall_clock_time': 46.22436261177063, 'metric_for_logging': {'pred_time': 1.3326710276028083e-05}, 'val_loss': 2.5495309082122213, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.020828485488892}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7846456359236429
NO2(0)的mse=16.353145194015255
NO2(0)的mae=2.6368503799510106
NO2(0)的mar=0.17842520004544124
总共花费的时间为：65.27
来宾市
2519A
2520A
3535A
[flaml.automl: 09-18 16:59:22] {2390} INFO - task = regression
[flaml.automl: 09-18 16:59:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:59:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:59:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:59:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:59:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:59:24] {3025} INFO - Estimated sufficient time budget=22456s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 16:59:24] {3072} INFO -  at 2.4s,	estimator xgboost's best error=10.0224,	best estimator xgboost's best error=10.0224
[flaml.automl: 09-18 16:59:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:59:28] {3072} INFO -  at 6.5s,	estimator xgboost's best error=4.9712,	best estimator xgboost's best error=4.9712
[flaml.automl: 09-18 16:59:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:59:31] {3072} INFO -  at 8.9s,	estimator xgboost's best error=4.9712,	best estimator xgboost's best error=4.9712
[flaml.automl: 09-18 16:59:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:59:48] {3072} INFO -  at 25.8s,	estimator xgboost's best error=4.9712,	best estimator xgboost's best error=4.9712
[flaml.automl: 09-18 16:59:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:59:50] {3072} INFO -  at 27.8s,	estimator xgboost's best error=3.3911,	best estimator xgboost's best error=3.3911
[flaml.automl: 09-18 16:59:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:59:52] {3072} INFO -  at 30.5s,	estimator xgboost's best error=3.0104,	best estimator xgboost's best error=3.0104
[flaml.automl: 09-18 16:59:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:59:55] {3072} INFO -  at 33.5s,	estimator xgboost's best error=3.0104,	best estimator xgboost's best error=3.0104
[flaml.automl: 09-18 16:59:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:59:59] {3072} INFO -  at 37.6s,	estimator xgboost's best error=3.0104,	best estimator xgboost's best error=3.0104
[flaml.automl: 09-18 16:59:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:00:01] {3072} INFO -  at 39.7s,	estimator xgboost's best error=3.0104,	best estimator xgboost's best error=3.0104
[flaml.automl: 09-18 17:00:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:00:04] {3072} INFO -  at 42.4s,	estimator xgboost's best error=3.0104,	best estimator xgboost's best error=3.0104
[flaml.automl: 09-18 17:00:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:00:05] {3072} INFO -  at 43.6s,	estimator xgboost's best error=3.0104,	best estimator xgboost's best error=3.0104
[flaml.automl: 09-18 17:00:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:00:07] {3072} INFO -  at 44.7s,	estimator xgboost's best error=3.0104,	best estimator xgboost's best error=3.0104
[flaml.automl: 09-18 17:00:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:00:13] {3072} INFO -  at 51.3s,	estimator xgboost's best error=2.8915,	best estimator xgboost's best error=2.8915
[flaml.automl: 09-18 17:00:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:00:21] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.8268,	best estimator xgboost's best error=2.8268
[flaml.automl: 09-18 17:00:33] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 17:00:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:00:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:00:33] {2637} INFO - Time taken to find the best model: 59.54050135612488
[flaml.automl: 09-18 17:00:33] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.8268131884692056
NO2(0)最好结果：{'pred_time': 1.1243946203060757e-05, 'wall_clock_time': 59.54050135612488, 'metric_for_logging': {'pred_time': 1.1243946203060757e-05}, 'val_loss': 2.8268131884692056, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 8.287198066711426}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7585834092948656
NO2(0)的mse=20.17935814169258
NO2(0)的mae=2.8674701120448014
NO2(0)的mar=0.21228136903807787
总共花费的时间为：72.26
崇左市
2521A
2522A
[flaml.automl: 09-18 17:07:12] {2390} INFO - task = regression
[flaml.automl: 09-18 17:07:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:07:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:07:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:07:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:07:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:07:14] {3025} INFO - Estimated sufficient time budget=23083s. Estimated necessary time budget=23s.
[flaml.automl: 09-18 17:07:14] {3072} INFO -  at 2.4s,	estimator xgboost's best error=8.4969,	best estimator xgboost's best error=8.4969
[flaml.automl: 09-18 17:07:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:07:18] {3072} INFO -  at 6.4s,	estimator xgboost's best error=4.1562,	best estimator xgboost's best error=4.1562
[flaml.automl: 09-18 17:07:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:07:20] {3072} INFO -  at 8.7s,	estimator xgboost's best error=4.1562,	best estimator xgboost's best error=4.1562
[flaml.automl: 09-18 17:07:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:07:38] {3072} INFO -  at 26.4s,	estimator xgboost's best error=4.1562,	best estimator xgboost's best error=4.1562
[flaml.automl: 09-18 17:07:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:07:40] {3072} INFO -  at 28.5s,	estimator xgboost's best error=2.8285,	best estimator xgboost's best error=2.8285
[flaml.automl: 09-18 17:07:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:07:43] {3072} INFO -  at 31.4s,	estimator xgboost's best error=2.5592,	best estimator xgboost's best error=2.5592
[flaml.automl: 09-18 17:07:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:07:46] {3072} INFO -  at 33.8s,	estimator xgboost's best error=2.5592,	best estimator xgboost's best error=2.5592
[flaml.automl: 09-18 17:07:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:07:48] {3072} INFO -  at 36.3s,	estimator xgboost's best error=2.5592,	best estimator xgboost's best error=2.5592
[flaml.automl: 09-18 17:07:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:07:49] {3072} INFO -  at 37.5s,	estimator xgboost's best error=2.5592,	best estimator xgboost's best error=2.5592
[flaml.automl: 09-18 17:07:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:07:52] {3072} INFO -  at 39.9s,	estimator xgboost's best error=2.5592,	best estimator xgboost's best error=2.5592
[flaml.automl: 09-18 17:07:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:07:53] {3072} INFO -  at 41.1s,	estimator xgboost's best error=2.5592,	best estimator xgboost's best error=2.5592
[flaml.automl: 09-18 17:07:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:07:54] {3072} INFO -  at 42.2s,	estimator xgboost's best error=2.5592,	best estimator xgboost's best error=2.5592
[flaml.automl: 09-18 17:07:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:08:00] {3072} INFO -  at 48.3s,	estimator xgboost's best error=2.4553,	best estimator xgboost's best error=2.4553
[flaml.automl: 09-18 17:08:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:08:10] {3072} INFO -  at 58.7s,	estimator xgboost's best error=2.4102,	best estimator xgboost's best error=2.4102
[flaml.automl: 09-18 17:08:21] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 17:08:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:08:21] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:08:21] {2637} INFO - Time taken to find the best model: 58.665517807006836
[flaml.automl: 09-18 17:08:21] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.4101918788517223
NO2(0)最好结果：{'pred_time': 1.6597539436917346e-05, 'wall_clock_time': 58.665517807006836, 'metric_for_logging': {'pred_time': 1.6597539436917346e-05}, 'val_loss': 2.4101918788517223, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.407348394393921}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7492866591630294
NO2(0)的mse=14.691897191238802
NO2(0)的mae=2.5644430118156967
NO2(0)的mar=0.21166868192720878
总共花费的时间为：69.49
广元市
2523A
2524A
3617A
[flaml.automl: 09-18 17:17:44] {2390} INFO - task = regression
[flaml.automl: 09-18 17:17:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:17:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:17:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:17:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:17:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:17:46] {3025} INFO - Estimated sufficient time budget=12298s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 17:17:46] {3072} INFO -  at 1.4s,	estimator xgboost's best error=15.4564,	best estimator xgboost's best error=15.4564
[flaml.automl: 09-18 17:17:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:17:48] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.7588,	best estimator xgboost's best error=7.7588
[flaml.automl: 09-18 17:17:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:17:49] {3072} INFO -  at 4.8s,	estimator xgboost's best error=7.7588,	best estimator xgboost's best error=7.7588
[flaml.automl: 09-18 17:17:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:17:59] {3072} INFO -  at 14.8s,	estimator xgboost's best error=7.7588,	best estimator xgboost's best error=7.7588
[flaml.automl: 09-18 17:17:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:18:00] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.3506,	best estimator xgboost's best error=5.3506
[flaml.automl: 09-18 17:18:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:18:02] {3072} INFO -  at 17.5s,	estimator xgboost's best error=4.5770,	best estimator xgboost's best error=4.5770
[flaml.automl: 09-18 17:18:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:18:03] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.5770,	best estimator xgboost's best error=4.5770
[flaml.automl: 09-18 17:18:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:18:06] {3072} INFO -  at 21.6s,	estimator xgboost's best error=4.5770,	best estimator xgboost's best error=4.5770
[flaml.automl: 09-18 17:18:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:18:07] {3072} INFO -  at 22.8s,	estimator xgboost's best error=4.5770,	best estimator xgboost's best error=4.5770
[flaml.automl: 09-18 17:18:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:18:10] {3072} INFO -  at 25.4s,	estimator xgboost's best error=4.5770,	best estimator xgboost's best error=4.5770
[flaml.automl: 09-18 17:18:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:18:11] {3072} INFO -  at 26.6s,	estimator xgboost's best error=4.5770,	best estimator xgboost's best error=4.5770
[flaml.automl: 09-18 17:18:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:18:12] {3072} INFO -  at 27.7s,	estimator xgboost's best error=4.5770,	best estimator xgboost's best error=4.5770
[flaml.automl: 09-18 17:18:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:18:18] {3072} INFO -  at 34.1s,	estimator xgboost's best error=4.4506,	best estimator xgboost's best error=4.4506
[flaml.automl: 09-18 17:18:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:18:30] {3072} INFO -  at 46.0s,	estimator xgboost's best error=4.3253,	best estimator xgboost's best error=4.3253
[flaml.automl: 09-18 17:18:30] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 17:18:37] {3072} INFO -  at 52.6s,	estimator xgboost's best error=4.3253,	best estimator xgboost's best error=4.3253
[flaml.automl: 09-18 17:18:58] {3335} INFO - retrain xgboost for 20.7s
[flaml.automl: 09-18 17:18:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:18:58] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:18:58] {2637} INFO - Time taken to find the best model: 46.045016050338745
[flaml.automl: 09-18 17:18:58] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.325329743866101
NO2(0)最好结果：{'pred_time': 1.2959295650811144e-05, 'wall_clock_time': 46.045016050338745, 'metric_for_logging': {'pred_time': 1.2959295650811144e-05}, 'val_loss': 4.325329743866101, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.931617259979248}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8118045639881536
NO2(0)的mse=38.90345081924168
NO2(0)的mae=4.172295922578622
NO2(0)的mar=0.24512532078938706
总共花费的时间为：73.87
遂宁市
2527A
2528A
2529A
2530A
[flaml.automl: 09-18 17:31:24] {2390} INFO - task = regression
[flaml.automl: 09-18 17:31:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:31:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:31:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:31:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:31:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:31:25] {3025} INFO - Estimated sufficient time budget=49505s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 17:31:25] {3072} INFO -  at 1.4s,	estimator xgboost's best error=11.8879,	best estimator xgboost's best error=11.8879
[flaml.automl: 09-18 17:31:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:31:27] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.0673,	best estimator xgboost's best error=6.0673
[flaml.automl: 09-18 17:31:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:31:28] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.0673,	best estimator xgboost's best error=6.0673
[flaml.automl: 09-18 17:31:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:31:35] {3072} INFO -  at 11.1s,	estimator xgboost's best error=6.0673,	best estimator xgboost's best error=6.0673
[flaml.automl: 09-18 17:31:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:31:36] {3072} INFO -  at 12.2s,	estimator xgboost's best error=4.4865,	best estimator xgboost's best error=4.4865
[flaml.automl: 09-18 17:31:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:31:37] {3072} INFO -  at 13.8s,	estimator xgboost's best error=4.1009,	best estimator xgboost's best error=4.1009
[flaml.automl: 09-18 17:31:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:31:39] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.1009,	best estimator xgboost's best error=4.1009
[flaml.automl: 09-18 17:31:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:31:41] {3072} INFO -  at 17.8s,	estimator xgboost's best error=4.1009,	best estimator xgboost's best error=4.1009
[flaml.automl: 09-18 17:31:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:31:43] {3072} INFO -  at 19.0s,	estimator xgboost's best error=4.1009,	best estimator xgboost's best error=4.1009
[flaml.automl: 09-18 17:31:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:31:45] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.1009,	best estimator xgboost's best error=4.1009
[flaml.automl: 09-18 17:31:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:31:47] {3072} INFO -  at 23.3s,	estimator xgboost's best error=4.0931,	best estimator xgboost's best error=4.0931
[flaml.automl: 09-18 17:31:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:31:48] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.0931,	best estimator xgboost's best error=4.0931
[flaml.automl: 09-18 17:31:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:31:55] {3072} INFO -  at 31.0s,	estimator xgboost's best error=3.9757,	best estimator xgboost's best error=3.9757
[flaml.automl: 09-18 17:31:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:32:07] {3072} INFO -  at 43.1s,	estimator xgboost's best error=3.8838,	best estimator xgboost's best error=3.8838
[flaml.automl: 09-18 17:32:07] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 17:32:13] {3072} INFO -  at 49.6s,	estimator xgboost's best error=3.8838,	best estimator xgboost's best error=3.8838
[flaml.automl: 09-18 17:32:33] {3335} INFO - retrain xgboost for 20.2s
[flaml.automl: 09-18 17:32:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:32:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:32:33] {2637} INFO - Time taken to find the best model: 43.07664179801941
[flaml.automl: 09-18 17:32:33] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41463}
NO2(0)最佳损失：-2.883814137480739
NO2(0)最好结果：{'pred_time': 8.639672564135657e-06, 'wall_clock_time': 43.07664179801941, 'metric_for_logging': {'pred_time': 8.639672564135657e-06}, 'val_loss': 3.883814137480739, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41463}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41463, 'experiment_tag': 'exp', 'time_total_s': 12.096181154251099}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7684127603867448
NO2(0)的mse=33.45071984910817
NO2(0)的mae=3.789493267879423
NO2(0)的mar=0.29232029960333195
总共花费的时间为：70.53
内江市
2531A
2532A
2533A
2534A
[flaml.automl: 09-18 17:45:11] {2390} INFO - task = regression
[flaml.automl: 09-18 17:45:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:45:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:45:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:45:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:45:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:45:13] {3025} INFO - Estimated sufficient time budget=84466s. Estimated necessary time budget=84s.
[flaml.automl: 09-18 17:45:13] {3072} INFO -  at 2.3s,	estimator xgboost's best error=14.5044,	best estimator xgboost's best error=14.5044
[flaml.automl: 09-18 17:45:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:45:16] {3072} INFO -  at 5.2s,	estimator xgboost's best error=7.0559,	best estimator xgboost's best error=7.0559
[flaml.automl: 09-18 17:45:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:45:17] {3072} INFO -  at 6.4s,	estimator xgboost's best error=7.0559,	best estimator xgboost's best error=7.0559
[flaml.automl: 09-18 17:45:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:45:23] {3072} INFO -  at 12.3s,	estimator xgboost's best error=7.0559,	best estimator xgboost's best error=7.0559
[flaml.automl: 09-18 17:45:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:45:24] {3072} INFO -  at 13.4s,	estimator xgboost's best error=4.6973,	best estimator xgboost's best error=4.6973
[flaml.automl: 09-18 17:45:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:45:25] {3072} INFO -  at 15.0s,	estimator xgboost's best error=4.1920,	best estimator xgboost's best error=4.1920
[flaml.automl: 09-18 17:45:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:45:27] {3072} INFO -  at 16.6s,	estimator xgboost's best error=4.1920,	best estimator xgboost's best error=4.1920
[flaml.automl: 09-18 17:45:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:45:29] {3072} INFO -  at 19.1s,	estimator xgboost's best error=4.1920,	best estimator xgboost's best error=4.1920
[flaml.automl: 09-18 17:45:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:45:31] {3072} INFO -  at 20.2s,	estimator xgboost's best error=4.1920,	best estimator xgboost's best error=4.1920
[flaml.automl: 09-18 17:45:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:45:33] {3072} INFO -  at 22.9s,	estimator xgboost's best error=4.1920,	best estimator xgboost's best error=4.1920
[flaml.automl: 09-18 17:45:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:45:35] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.1920,	best estimator xgboost's best error=4.1920
[flaml.automl: 09-18 17:45:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:45:36] {3072} INFO -  at 25.6s,	estimator xgboost's best error=4.1920,	best estimator xgboost's best error=4.1920
[flaml.automl: 09-18 17:45:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:45:42] {3072} INFO -  at 32.1s,	estimator xgboost's best error=4.0848,	best estimator xgboost's best error=4.0848
[flaml.automl: 09-18 17:45:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:45:55] {3072} INFO -  at 44.2s,	estimator xgboost's best error=3.9864,	best estimator xgboost's best error=3.9864
[flaml.automl: 09-18 17:45:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 17:46:01] {3072} INFO -  at 50.8s,	estimator xgboost's best error=3.9864,	best estimator xgboost's best error=3.9864
[flaml.automl: 09-18 17:46:13] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 17:46:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:46:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:46:13] {2637} INFO - Time taken to find the best model: 44.219587087631226
[flaml.automl: 09-18 17:46:13] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42695}
NO2(0)最佳损失：-2.986358597723871
NO2(0)最好结果：{'pred_time': 8.568042044293821e-06, 'wall_clock_time': 44.219587087631226, 'metric_for_logging': {'pred_time': 8.568042044293821e-06}, 'val_loss': 3.986358597723871, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42695}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42695, 'experiment_tag': 'exp', 'time_total_s': 12.08924412727356}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7261801434384825
NO2(0)的mse=36.78958627275842
NO2(0)的mae=4.0799463758475865
NO2(0)的mar=0.21537572469337382
总共花费的时间为：63.66
眉山市
2539A
2540A
3137A
3148A
[flaml.automl: 09-18 17:58:48] {2390} INFO - task = regression
[flaml.automl: 09-18 17:58:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:58:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:58:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:58:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:58:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:58:49] {3025} INFO - Estimated sufficient time budget=50194s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 17:58:49] {3072} INFO -  at 1.4s,	estimator xgboost's best error=17.0472,	best estimator xgboost's best error=17.0472
[flaml.automl: 09-18 17:58:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:58:51] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.4208,	best estimator xgboost's best error=8.4208
[flaml.automl: 09-18 17:58:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:58:52] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.4208,	best estimator xgboost's best error=8.4208
[flaml.automl: 09-18 17:58:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:58:59] {3072} INFO -  at 11.1s,	estimator xgboost's best error=8.4208,	best estimator xgboost's best error=8.4208
[flaml.automl: 09-18 17:58:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:59:00] {3072} INFO -  at 12.2s,	estimator xgboost's best error=5.5964,	best estimator xgboost's best error=5.5964
[flaml.automl: 09-18 17:59:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:59:01] {3072} INFO -  at 13.8s,	estimator xgboost's best error=4.8574,	best estimator xgboost's best error=4.8574
[flaml.automl: 09-18 17:59:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:59:03] {3072} INFO -  at 15.5s,	estimator xgboost's best error=4.8574,	best estimator xgboost's best error=4.8574
[flaml.automl: 09-18 17:59:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:59:05] {3072} INFO -  at 17.9s,	estimator xgboost's best error=4.8574,	best estimator xgboost's best error=4.8574
[flaml.automl: 09-18 17:59:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:59:06] {3072} INFO -  at 19.0s,	estimator xgboost's best error=4.8574,	best estimator xgboost's best error=4.8574
[flaml.automl: 09-18 17:59:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:59:09] {3072} INFO -  at 21.7s,	estimator xgboost's best error=4.8574,	best estimator xgboost's best error=4.8574
[flaml.automl: 09-18 17:59:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:59:11] {3072} INFO -  at 23.3s,	estimator xgboost's best error=4.8497,	best estimator xgboost's best error=4.8497
[flaml.automl: 09-18 17:59:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:59:12] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.8497,	best estimator xgboost's best error=4.8497
[flaml.automl: 09-18 17:59:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:59:18] {3072} INFO -  at 31.0s,	estimator xgboost's best error=4.6423,	best estimator xgboost's best error=4.6423
[flaml.automl: 09-18 17:59:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:59:31] {3072} INFO -  at 43.1s,	estimator xgboost's best error=4.5747,	best estimator xgboost's best error=4.5747
[flaml.automl: 09-18 17:59:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 17:59:37] {3072} INFO -  at 49.6s,	estimator xgboost's best error=4.5747,	best estimator xgboost's best error=4.5747
[flaml.automl: 09-18 17:59:49] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 17:59:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:59:49] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:59:49] {2637} INFO - Time taken to find the best model: 43.09102535247803
[flaml.automl: 09-18 17:59:49] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41598}
NO2(0)最佳损失：-3.574670245456355
NO2(0)最好结果：{'pred_time': 9.06222675776497e-06, 'wall_clock_time': 43.09102535247803, 'metric_for_logging': {'pred_time': 9.06222675776497e-06}, 'val_loss': 4.574670245456355, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41598}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41598, 'experiment_tag': 'exp', 'time_total_s': 12.083690881729126}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8123346229047568
NO2(0)的mse=43.294055173013504
NO2(0)的mae=4.441535392027592
NO2(0)的mar=0.20744437002560898
总共花费的时间为：62.35
广安市
2543A
2544A
2545A
2902A
[flaml.automl: 09-18 18:11:53] {2390} INFO - task = regression
[flaml.automl: 09-18 18:11:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:11:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:11:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:11:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:11:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:11:54] {3025} INFO - Estimated sufficient time budget=47754s. Estimated necessary time budget=48s.
[flaml.automl: 09-18 18:11:54] {3072} INFO -  at 1.4s,	estimator xgboost's best error=9.7218,	best estimator xgboost's best error=9.7218
[flaml.automl: 09-18 18:11:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:11:56] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.6870,	best estimator xgboost's best error=4.6870
[flaml.automl: 09-18 18:11:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:11:57] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.6870,	best estimator xgboost's best error=4.6870
[flaml.automl: 09-18 18:11:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:12:03] {3072} INFO -  at 11.0s,	estimator xgboost's best error=4.6870,	best estimator xgboost's best error=4.6870
[flaml.automl: 09-18 18:12:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:12:05] {3072} INFO -  at 12.1s,	estimator xgboost's best error=3.0926,	best estimator xgboost's best error=3.0926
[flaml.automl: 09-18 18:12:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:12:06] {3072} INFO -  at 13.7s,	estimator xgboost's best error=2.7592,	best estimator xgboost's best error=2.7592
[flaml.automl: 09-18 18:12:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:12:08] {3072} INFO -  at 15.3s,	estimator xgboost's best error=2.7592,	best estimator xgboost's best error=2.7592
[flaml.automl: 09-18 18:12:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:12:10] {3072} INFO -  at 17.8s,	estimator xgboost's best error=2.7592,	best estimator xgboost's best error=2.7592
[flaml.automl: 09-18 18:12:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:12:11] {3072} INFO -  at 18.9s,	estimator xgboost's best error=2.7592,	best estimator xgboost's best error=2.7592
[flaml.automl: 09-18 18:12:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:12:14] {3072} INFO -  at 21.6s,	estimator xgboost's best error=2.7592,	best estimator xgboost's best error=2.7592
[flaml.automl: 09-18 18:12:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:12:16] {3072} INFO -  at 23.2s,	estimator xgboost's best error=2.7592,	best estimator xgboost's best error=2.7592
[flaml.automl: 09-18 18:12:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:12:17] {3072} INFO -  at 24.3s,	estimator xgboost's best error=2.7592,	best estimator xgboost's best error=2.7592
[flaml.automl: 09-18 18:12:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:12:23] {3072} INFO -  at 30.8s,	estimator xgboost's best error=2.6465,	best estimator xgboost's best error=2.6465
[flaml.automl: 09-18 18:12:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:12:35] {3072} INFO -  at 42.9s,	estimator xgboost's best error=2.5893,	best estimator xgboost's best error=2.5893
[flaml.automl: 09-18 18:12:35] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 18:12:42] {3072} INFO -  at 49.5s,	estimator xgboost's best error=2.5893,	best estimator xgboost's best error=2.5893
[flaml.automl: 09-18 18:12:54] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 18:12:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:12:54] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:12:54] {2637} INFO - Time taken to find the best model: 42.92667078971863
[flaml.automl: 09-18 18:12:54] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40070}
NO2(0)最佳损失：-1.5892798017711285
NO2(0)最好结果：{'pred_time': 1.0111399936911607e-05, 'wall_clock_time': 42.92667078971863, 'metric_for_logging': {'pred_time': 1.0111399936911607e-05}, 'val_loss': 2.5892798017711285, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40070}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40070, 'experiment_tag': 'exp', 'time_total_s': 12.1007821559906}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7951308599141251
NO2(0)的mse=15.20731791976691
NO2(0)的mae=2.621549095412517
NO2(0)的mar=0.1971812940814903
总共花费的时间为：62.30
达州市
2548A
2549A
2550A
2551A
2552A
[flaml.automl: 09-18 18:28:14] {2390} INFO - task = regression
[flaml.automl: 09-18 18:28:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:28:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:28:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:28:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:28:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:28:15] {3025} INFO - Estimated sufficient time budget=62012s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 18:28:15] {3072} INFO -  at 1.5s,	estimator xgboost's best error=20.4567,	best estimator xgboost's best error=20.4567
[flaml.automl: 09-18 18:28:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:28:17] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.8914,	best estimator xgboost's best error=9.8914
[flaml.automl: 09-18 18:28:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:28:18] {3072} INFO -  at 4.8s,	estimator xgboost's best error=9.8914,	best estimator xgboost's best error=9.8914
[flaml.automl: 09-18 18:28:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:28:23] {3072} INFO -  at 9.6s,	estimator xgboost's best error=9.8914,	best estimator xgboost's best error=9.8914
[flaml.automl: 09-18 18:28:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:28:24] {3072} INFO -  at 10.8s,	estimator xgboost's best error=5.9088,	best estimator xgboost's best error=5.9088
[flaml.automl: 09-18 18:28:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:28:26] {3072} INFO -  at 12.4s,	estimator xgboost's best error=5.2589,	best estimator xgboost's best error=5.2589
[flaml.automl: 09-18 18:28:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:28:27] {3072} INFO -  at 14.0s,	estimator xgboost's best error=5.2589,	best estimator xgboost's best error=5.2589
[flaml.automl: 09-18 18:28:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:28:30] {3072} INFO -  at 16.5s,	estimator xgboost's best error=5.2589,	best estimator xgboost's best error=5.2589
[flaml.automl: 09-18 18:28:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:28:31] {3072} INFO -  at 17.6s,	estimator xgboost's best error=5.2589,	best estimator xgboost's best error=5.2589
[flaml.automl: 09-18 18:28:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:28:34] {3072} INFO -  at 20.3s,	estimator xgboost's best error=5.2589,	best estimator xgboost's best error=5.2589
[flaml.automl: 09-18 18:28:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:28:35] {3072} INFO -  at 21.9s,	estimator xgboost's best error=5.1973,	best estimator xgboost's best error=5.1973
[flaml.automl: 09-18 18:28:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:28:36] {3072} INFO -  at 23.0s,	estimator xgboost's best error=5.1973,	best estimator xgboost's best error=5.1973
[flaml.automl: 09-18 18:28:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:28:43] {3072} INFO -  at 29.6s,	estimator xgboost's best error=4.9302,	best estimator xgboost's best error=4.9302
[flaml.automl: 09-18 18:28:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:28:55] {3072} INFO -  at 41.7s,	estimator xgboost's best error=4.7998,	best estimator xgboost's best error=4.7998
[flaml.automl: 09-18 18:28:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 18:29:02] {3072} INFO -  at 48.2s,	estimator xgboost's best error=4.7998,	best estimator xgboost's best error=4.7998
[flaml.automl: 09-18 18:29:14] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 18:29:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:29:14] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:29:14] {2637} INFO - Time taken to find the best model: 41.71353554725647
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52143}
NO2(0)最佳损失：-3.7998122343491962
NO2(0)最好结果：{'pred_time': 7.95537864993184e-06, 'wall_clock_time': 41.71353554725647, 'metric_for_logging': {'pred_time': 7.95537864993184e-06}, 'val_loss': 4.799812234349196, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52143}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52143, 'experiment_tag': 'exp', 'time_total_s': 12.136488914489746}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7880327984093214
NO2(0)的mse=49.90900115432212
NO2(0)的mae=4.841653433783746
NO2(0)的mar=0.2017540769817714
总共花费的时间为：61.22
雅安市
2555A
2556A
[flaml.automl: 09-18 18:36:01] {2390} INFO - task = regression
[flaml.automl: 09-18 18:36:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:36:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:36:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:36:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:36:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:36:04] {3025} INFO - Estimated sufficient time budget=30604s. Estimated necessary time budget=31s.
[flaml.automl: 09-18 18:36:04] {3072} INFO -  at 3.2s,	estimator xgboost's best error=9.5519,	best estimator xgboost's best error=9.5519
[flaml.automl: 09-18 18:36:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:36:10] {3072} INFO -  at 9.2s,	estimator xgboost's best error=4.6107,	best estimator xgboost's best error=4.6107
[flaml.automl: 09-18 18:36:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:36:13] {3072} INFO -  at 12.7s,	estimator xgboost's best error=4.6107,	best estimator xgboost's best error=4.6107
[flaml.automl: 09-18 18:36:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:36:42] {3072} INFO -  at 41.5s,	estimator xgboost's best error=4.6107,	best estimator xgboost's best error=4.6107
[flaml.automl: 09-18 18:36:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:36:46] {3072} INFO -  at 45.7s,	estimator xgboost's best error=3.3034,	best estimator xgboost's best error=3.3034
[flaml.automl: 09-18 18:36:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:36:55] {3072} INFO -  at 54.1s,	estimator xgboost's best error=3.0166,	best estimator xgboost's best error=3.0166
[flaml.automl: 09-18 18:37:02] {3335} INFO - retrain xgboost for 7.4s
[flaml.automl: 09-18 18:37:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 18:37:02] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:37:02] {2637} INFO - Time taken to find the best model: 54.14742565155029
[flaml.automl: 09-18 18:37:02] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-2.0165677917972893
NO2(0)最好结果：{'pred_time': 8.662568353747282e-05, 'wall_clock_time': 54.14742565155029, 'metric_for_logging': {'pred_time': 8.662568353747282e-05}, 'val_loss': 3.0165677917972893, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 8.443105459213257}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.5933600772924357
NO2(0)的mse=19.64138004908752
NO2(0)的mae=3.001593991613802
NO2(0)的mar=0.2326125343432621
总共花费的时间为：62.07
巴中市
2914A
3183A
3616A
[flaml.automl: 09-18 18:47:18] {2390} INFO - task = regression
[flaml.automl: 09-18 18:47:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:47:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:47:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:47:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:47:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:47:19] {3025} INFO - Estimated sufficient time budget=12073s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 18:47:19] {3072} INFO -  at 1.4s,	estimator xgboost's best error=11.6577,	best estimator xgboost's best error=11.6577
[flaml.automl: 09-18 18:47:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:47:21] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.5449,	best estimator xgboost's best error=5.5449
[flaml.automl: 09-18 18:47:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:47:22] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.5449,	best estimator xgboost's best error=5.5449
[flaml.automl: 09-18 18:47:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:47:32] {3072} INFO -  at 14.6s,	estimator xgboost's best error=5.5449,	best estimator xgboost's best error=5.5449
[flaml.automl: 09-18 18:47:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:47:33] {3072} INFO -  at 15.8s,	estimator xgboost's best error=3.6812,	best estimator xgboost's best error=3.6812
[flaml.automl: 09-18 18:47:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:47:35] {3072} INFO -  at 17.4s,	estimator xgboost's best error=3.2564,	best estimator xgboost's best error=3.2564
[flaml.automl: 09-18 18:47:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:47:37] {3072} INFO -  at 19.0s,	estimator xgboost's best error=3.2564,	best estimator xgboost's best error=3.2564
[flaml.automl: 09-18 18:47:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:47:39] {3072} INFO -  at 21.5s,	estimator xgboost's best error=3.2564,	best estimator xgboost's best error=3.2564
[flaml.automl: 09-18 18:47:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:47:40] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.2564,	best estimator xgboost's best error=3.2564
[flaml.automl: 09-18 18:47:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:47:43] {3072} INFO -  at 25.3s,	estimator xgboost's best error=3.2564,	best estimator xgboost's best error=3.2564
[flaml.automl: 09-18 18:47:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:47:44] {3072} INFO -  at 26.5s,	estimator xgboost's best error=3.2564,	best estimator xgboost's best error=3.2564
[flaml.automl: 09-18 18:47:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:47:45] {3072} INFO -  at 27.7s,	estimator xgboost's best error=3.2564,	best estimator xgboost's best error=3.2564
[flaml.automl: 09-18 18:47:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:47:52] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.1511,	best estimator xgboost's best error=3.1511
[flaml.automl: 09-18 18:47:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:48:04] {3072} INFO -  at 46.2s,	estimator xgboost's best error=3.0818,	best estimator xgboost's best error=3.0818
[flaml.automl: 09-18 18:48:04] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 18:48:10] {3072} INFO -  at 52.7s,	estimator xgboost's best error=3.0818,	best estimator xgboost's best error=3.0818
[flaml.automl: 09-18 18:48:22] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 18:48:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:48:22] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:48:22] {2637} INFO - Time taken to find the best model: 46.24637746810913
[flaml.automl: 09-18 18:48:22] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.0818410704090553
NO2(0)最好结果：{'pred_time': 1.1229494773221565e-05, 'wall_clock_time': 46.24637746810913, 'metric_for_logging': {'pred_time': 1.1229494773221565e-05}, 'val_loss': 3.0818410704090553, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.084365606307983}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8089225246433549
NO2(0)的mse=24.649301090976998
NO2(0)的mae=3.1413372381036844
NO2(0)的mar=0.19782204446745244
总共花费的时间为：65.37
资阳市
2561A
2562A
2563A
2564A
2565A
[flaml.automl: 09-18 19:03:41] {2390} INFO - task = regression
[flaml.automl: 09-18 19:03:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:03:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:03:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:03:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:03:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:03:42] {3025} INFO - Estimated sufficient time budget=61594s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 19:03:42] {3072} INFO -  at 1.5s,	estimator xgboost's best error=13.0018,	best estimator xgboost's best error=13.0018
[flaml.automl: 09-18 19:03:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:03:44] {3072} INFO -  at 3.6s,	estimator xgboost's best error=6.4083,	best estimator xgboost's best error=6.4083
[flaml.automl: 09-18 19:03:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:03:45] {3072} INFO -  at 4.8s,	estimator xgboost's best error=6.4083,	best estimator xgboost's best error=6.4083
[flaml.automl: 09-18 19:03:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:03:50] {3072} INFO -  at 10.0s,	estimator xgboost's best error=6.4083,	best estimator xgboost's best error=6.4083
[flaml.automl: 09-18 19:03:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:03:51] {3072} INFO -  at 11.1s,	estimator xgboost's best error=4.3638,	best estimator xgboost's best error=4.3638
[flaml.automl: 09-18 19:03:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:03:53] {3072} INFO -  at 12.7s,	estimator xgboost's best error=3.9427,	best estimator xgboost's best error=3.9427
[flaml.automl: 09-18 19:03:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:03:55] {3072} INFO -  at 14.3s,	estimator xgboost's best error=3.9427,	best estimator xgboost's best error=3.9427
[flaml.automl: 09-18 19:03:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:03:57] {3072} INFO -  at 16.7s,	estimator xgboost's best error=3.9427,	best estimator xgboost's best error=3.9427
[flaml.automl: 09-18 19:03:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:03:58] {3072} INFO -  at 17.9s,	estimator xgboost's best error=3.9427,	best estimator xgboost's best error=3.9427
[flaml.automl: 09-18 19:03:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:04:01] {3072} INFO -  at 20.5s,	estimator xgboost's best error=3.9427,	best estimator xgboost's best error=3.9427
[flaml.automl: 09-18 19:04:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:04:02] {3072} INFO -  at 22.2s,	estimator xgboost's best error=3.9290,	best estimator xgboost's best error=3.9290
[flaml.automl: 09-18 19:04:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:04:04] {3072} INFO -  at 23.3s,	estimator xgboost's best error=3.9290,	best estimator xgboost's best error=3.9290
[flaml.automl: 09-18 19:04:04] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:04:10] {3072} INFO -  at 29.9s,	estimator xgboost's best error=3.7533,	best estimator xgboost's best error=3.7533
[flaml.automl: 09-18 19:04:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:04:22] {3072} INFO -  at 42.0s,	estimator xgboost's best error=3.7050,	best estimator xgboost's best error=3.7050
[flaml.automl: 09-18 19:04:22] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 19:04:29] {3072} INFO -  at 48.5s,	estimator xgboost's best error=3.7050,	best estimator xgboost's best error=3.7050
[flaml.automl: 09-18 19:04:41] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 19:04:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:04:41] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:04:41] {2637} INFO - Time taken to find the best model: 41.97183084487915
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51691}
NO2(0)最佳损失：-2.704992790408121
NO2(0)最好结果：{'pred_time': 6.935722648599354e-06, 'wall_clock_time': 41.97183084487915, 'metric_for_logging': {'pred_time': 6.935722648599354e-06}, 'val_loss': 3.704992790408121, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51691}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51691, 'experiment_tag': 'exp', 'time_total_s': 12.112112522125244}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7959334247528935
NO2(0)的mse=27.880070111244038
NO2(0)的mae=3.6180208669452973
NO2(0)的mar=0.22142190457321761
总共花费的时间为：61.33
阿坝藏族羌族自治州
2566A
2567A
2568A
[flaml.automl: 09-18 19:14:25] {2390} INFO - task = regression
[flaml.automl: 09-18 19:14:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:14:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:14:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:14:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:14:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:14:27] {3025} INFO - Estimated sufficient time budget=12272s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 19:14:27] {3072} INFO -  at 1.4s,	estimator xgboost's best error=6.3505,	best estimator xgboost's best error=6.3505
[flaml.automl: 09-18 19:14:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:14:29] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.1114,	best estimator xgboost's best error=3.1114
[flaml.automl: 09-18 19:14:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:14:30] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.1114,	best estimator xgboost's best error=3.1114
[flaml.automl: 09-18 19:14:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:14:40] {3072} INFO -  at 14.6s,	estimator xgboost's best error=3.1114,	best estimator xgboost's best error=3.1114
[flaml.automl: 09-18 19:14:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:14:41] {3072} INFO -  at 15.8s,	estimator xgboost's best error=2.1153,	best estimator xgboost's best error=2.1153
[flaml.automl: 09-18 19:14:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:14:43] {3072} INFO -  at 17.4s,	estimator xgboost's best error=2.0119,	best estimator xgboost's best error=2.0119
[flaml.automl: 09-18 19:14:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:14:44] {3072} INFO -  at 19.0s,	estimator xgboost's best error=2.0119,	best estimator xgboost's best error=2.0119
[flaml.automl: 09-18 19:14:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:14:47] {3072} INFO -  at 21.5s,	estimator xgboost's best error=2.0119,	best estimator xgboost's best error=2.0119
[flaml.automl: 09-18 19:14:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:14:48] {3072} INFO -  at 22.7s,	estimator xgboost's best error=2.0119,	best estimator xgboost's best error=2.0119
[flaml.automl: 09-18 19:14:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:14:51] {3072} INFO -  at 25.4s,	estimator xgboost's best error=2.0119,	best estimator xgboost's best error=2.0119
[flaml.automl: 09-18 19:14:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:14:52] {3072} INFO -  at 26.5s,	estimator xgboost's best error=2.0119,	best estimator xgboost's best error=2.0119
[flaml.automl: 09-18 19:14:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:14:53] {3072} INFO -  at 27.7s,	estimator xgboost's best error=2.0119,	best estimator xgboost's best error=2.0119
[flaml.automl: 09-18 19:14:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:15:00] {3072} INFO -  at 34.2s,	estimator xgboost's best error=1.9853,	best estimator xgboost's best error=1.9853
[flaml.automl: 09-18 19:15:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:15:12] {3072} INFO -  at 46.2s,	estimator xgboost's best error=1.9368,	best estimator xgboost's best error=1.9368
[flaml.automl: 09-18 19:15:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 19:15:18] {3072} INFO -  at 52.7s,	estimator xgboost's best error=1.9368,	best estimator xgboost's best error=1.9368
[flaml.automl: 09-18 19:15:30] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 19:15:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:15:30] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:15:30] {2637} INFO - Time taken to find the best model: 46.23147654533386
[flaml.automl: 09-18 19:15:30] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-0.9368008474846086
NO2(0)最好结果：{'pred_time': 1.2261453361946725e-05, 'wall_clock_time': 46.23147654533386, 'metric_for_logging': {'pred_time': 1.2261453361946725e-05}, 'val_loss': 1.9368008474846086, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.048109292984009}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6137033379968149
NO2(0)的mse=7.65239748689331
NO2(0)的mae=1.8948814471673052
NO2(0)的mar=0.20747173103893404
总共花费的时间为：65.35
甘孜藏族自治州
3065A
[flaml.automl: 09-18 19:18:42] {2390} INFO - task = regression
[flaml.automl: 09-18 19:18:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:18:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:18:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:18:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:18:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:18:44] {3025} INFO - Estimated sufficient time budget=22332s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 19:18:44] {3072} INFO -  at 2.3s,	estimator xgboost's best error=12.3531,	best estimator xgboost's best error=12.3531
[flaml.automl: 09-18 19:18:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:18:48] {3072} INFO -  at 5.7s,	estimator xgboost's best error=6.8245,	best estimator xgboost's best error=6.8245
[flaml.automl: 09-18 19:18:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:18:50] {3072} INFO -  at 7.9s,	estimator xgboost's best error=6.8245,	best estimator xgboost's best error=6.8245
[flaml.automl: 09-18 19:18:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:19:03] {3072} INFO -  at 21.1s,	estimator xgboost's best error=6.8245,	best estimator xgboost's best error=6.8245
[flaml.automl: 09-18 19:19:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:19:05] {3072} INFO -  at 23.2s,	estimator xgboost's best error=4.6094,	best estimator xgboost's best error=4.6094
[flaml.automl: 09-18 19:19:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:19:08] {3072} INFO -  at 25.9s,	estimator xgboost's best error=4.2395,	best estimator xgboost's best error=4.2395
[flaml.automl: 09-18 19:19:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:19:10] {3072} INFO -  at 28.7s,	estimator xgboost's best error=4.2395,	best estimator xgboost's best error=4.2395
[flaml.automl: 09-18 19:19:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:19:14] {3072} INFO -  at 32.6s,	estimator xgboost's best error=4.2395,	best estimator xgboost's best error=4.2395
[flaml.automl: 09-18 19:19:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:19:16] {3072} INFO -  at 34.4s,	estimator xgboost's best error=4.2395,	best estimator xgboost's best error=4.2395
[flaml.automl: 09-18 19:19:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:19:20] {3072} INFO -  at 38.6s,	estimator xgboost's best error=4.2395,	best estimator xgboost's best error=4.2395
[flaml.automl: 09-18 19:19:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:19:22] {3072} INFO -  at 40.6s,	estimator xgboost's best error=4.2395,	best estimator xgboost's best error=4.2395
[flaml.automl: 09-18 19:19:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:19:24] {3072} INFO -  at 42.5s,	estimator xgboost's best error=4.2395,	best estimator xgboost's best error=4.2395
[flaml.automl: 09-18 19:19:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:19:33] {3072} INFO -  at 51.6s,	estimator xgboost's best error=4.2395,	best estimator xgboost's best error=4.2395
[flaml.automl: 09-18 19:19:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:19:36] {3072} INFO -  at 54.1s,	estimator xgboost's best error=4.2395,	best estimator xgboost's best error=4.2395
[flaml.automl: 09-18 19:19:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 19:19:38] {3072} INFO -  at 55.9s,	estimator xgboost's best error=4.2395,	best estimator xgboost's best error=4.2395
[flaml.automl: 09-18 19:19:38] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 19:19:40] {3072} INFO -  at 58.4s,	estimator xgboost's best error=4.2395,	best estimator xgboost's best error=4.2395
[flaml.automl: 09-18 19:19:42] {3335} INFO - retrain xgboost for 1.5s
[flaml.automl: 09-18 19:19:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 19:19:42] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:19:42] {2637} INFO - Time taken to find the best model: 25.909725427627563
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-3.2394989873333184
NO2(0)最好结果：{'pred_time': 5.747839721322757e-05, 'wall_clock_time': 25.909725427627563, 'metric_for_logging': {'pred_time': 5.747839721322757e-05}, 'val_loss': 4.239498987333318, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 2.6698625087738037}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.33936108688822664
NO2(0)的mse=38.36891899433476
NO2(0)的mae=4.330267919095835
NO2(0)的mar=0.24351361119721893
总共花费的时间为：60.11
凉山彝族自治州
2571A
2572A
2573A
2574A
2575A
[flaml.automl: 09-18 19:35:35] {2390} INFO - task = regression
[flaml.automl: 09-18 19:35:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:35:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:35:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:35:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:35:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:35:36] {3025} INFO - Estimated sufficient time budget=61107s. Estimated necessary time budget=61s.
[flaml.automl: 09-18 19:35:36] {3072} INFO -  at 1.4s,	estimator xgboost's best error=8.6695,	best estimator xgboost's best error=8.6695
[flaml.automl: 09-18 19:35:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:35:38] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.3753,	best estimator xgboost's best error=4.3753
[flaml.automl: 09-18 19:35:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:35:39] {3072} INFO -  at 4.6s,	estimator xgboost's best error=4.3753,	best estimator xgboost's best error=4.3753
[flaml.automl: 09-18 19:35:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:35:44] {3072} INFO -  at 9.8s,	estimator xgboost's best error=4.3753,	best estimator xgboost's best error=4.3753
[flaml.automl: 09-18 19:35:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:35:45] {3072} INFO -  at 11.0s,	estimator xgboost's best error=3.1521,	best estimator xgboost's best error=3.1521
[flaml.automl: 09-18 19:35:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:35:47] {3072} INFO -  at 12.5s,	estimator xgboost's best error=2.9094,	best estimator xgboost's best error=2.9094
[flaml.automl: 09-18 19:35:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:35:49] {3072} INFO -  at 14.1s,	estimator xgboost's best error=2.9094,	best estimator xgboost's best error=2.9094
[flaml.automl: 09-18 19:35:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:35:51] {3072} INFO -  at 16.6s,	estimator xgboost's best error=2.9094,	best estimator xgboost's best error=2.9094
[flaml.automl: 09-18 19:35:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:35:52] {3072} INFO -  at 17.7s,	estimator xgboost's best error=2.9094,	best estimator xgboost's best error=2.9094
[flaml.automl: 09-18 19:35:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:35:55] {3072} INFO -  at 20.4s,	estimator xgboost's best error=2.9094,	best estimator xgboost's best error=2.9094
[flaml.automl: 09-18 19:35:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:35:56] {3072} INFO -  at 22.0s,	estimator xgboost's best error=2.9094,	best estimator xgboost's best error=2.9094
[flaml.automl: 09-18 19:35:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:35:58] {3072} INFO -  at 23.1s,	estimator xgboost's best error=2.9094,	best estimator xgboost's best error=2.9094
[flaml.automl: 09-18 19:35:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:36:04] {3072} INFO -  at 29.7s,	estimator xgboost's best error=2.8292,	best estimator xgboost's best error=2.8292
[flaml.automl: 09-18 19:36:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:36:16] {3072} INFO -  at 41.8s,	estimator xgboost's best error=2.7547,	best estimator xgboost's best error=2.7547
[flaml.automl: 09-18 19:36:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 19:36:23] {3072} INFO -  at 48.4s,	estimator xgboost's best error=2.7547,	best estimator xgboost's best error=2.7547
[flaml.automl: 09-18 19:36:35] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 19:36:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:36:35] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:36:35] {2637} INFO - Time taken to find the best model: 41.816691160202026
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51839}
NO2(0)最佳损失：-1.7546579904854296
NO2(0)最好结果：{'pred_time': 7.150032454066807e-06, 'wall_clock_time': 41.816691160202026, 'metric_for_logging': {'pred_time': 7.150032454066807e-06}, 'val_loss': 2.7546579904854296, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51839}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51839, 'experiment_tag': 'exp', 'time_total_s': 12.137739658355713}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6700583545678531
NO2(0)的mse=16.6382029706739
NO2(0)的mae=2.7338473388552664
NO2(0)的mar=0.22248486437459314
总共花费的时间为：61.39
六盘水市
2576A
2577A
2578A
2579A
2580A
[flaml.automl: 09-18 19:52:33] {2390} INFO - task = regression
[flaml.automl: 09-18 19:52:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:52:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:52:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:52:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:52:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:52:34] {3025} INFO - Estimated sufficient time budget=62934s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 19:52:34] {3072} INFO -  at 1.5s,	estimator xgboost's best error=6.6032,	best estimator xgboost's best error=6.6032
[flaml.automl: 09-18 19:52:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:52:36] {3072} INFO -  at 3.6s,	estimator xgboost's best error=3.4775,	best estimator xgboost's best error=3.4775
[flaml.automl: 09-18 19:52:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:52:37] {3072} INFO -  at 4.8s,	estimator xgboost's best error=3.4775,	best estimator xgboost's best error=3.4775
[flaml.automl: 09-18 19:52:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:52:42] {3072} INFO -  at 9.6s,	estimator xgboost's best error=3.4775,	best estimator xgboost's best error=3.4775
[flaml.automl: 09-18 19:52:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:52:43] {3072} INFO -  at 10.7s,	estimator xgboost's best error=2.7211,	best estimator xgboost's best error=2.7211
[flaml.automl: 09-18 19:52:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:52:45] {3072} INFO -  at 12.3s,	estimator xgboost's best error=2.5214,	best estimator xgboost's best error=2.5214
[flaml.automl: 09-18 19:52:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:52:46] {3072} INFO -  at 13.9s,	estimator xgboost's best error=2.5214,	best estimator xgboost's best error=2.5214
[flaml.automl: 09-18 19:52:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:52:49] {3072} INFO -  at 16.3s,	estimator xgboost's best error=2.5214,	best estimator xgboost's best error=2.5214
[flaml.automl: 09-18 19:52:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:52:50] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.5214,	best estimator xgboost's best error=2.5214
[flaml.automl: 09-18 19:52:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:52:53] {3072} INFO -  at 20.1s,	estimator xgboost's best error=2.5214,	best estimator xgboost's best error=2.5214
[flaml.automl: 09-18 19:52:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:52:54] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.5214,	best estimator xgboost's best error=2.5214
[flaml.automl: 09-18 19:52:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:52:55] {3072} INFO -  at 22.9s,	estimator xgboost's best error=2.5214,	best estimator xgboost's best error=2.5214
[flaml.automl: 09-18 19:52:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:53:02] {3072} INFO -  at 29.5s,	estimator xgboost's best error=2.4945,	best estimator xgboost's best error=2.4945
[flaml.automl: 09-18 19:53:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:53:14] {3072} INFO -  at 41.5s,	estimator xgboost's best error=2.4483,	best estimator xgboost's best error=2.4483
[flaml.automl: 09-18 19:53:14] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 19:53:24] {3072} INFO -  at 51.7s,	estimator xgboost's best error=2.4483,	best estimator xgboost's best error=2.4483
[flaml.automl: 09-18 19:53:57] {3335} INFO - retrain xgboost for 32.5s
[flaml.automl: 09-18 19:53:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:53:57] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:53:57] {2637} INFO - Time taken to find the best model: 41.54432821273804
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52650}
NO2(0)最佳损失：-1.4483039760182046
NO2(0)最好结果：{'pred_time': 7.5084327632545405e-06, 'wall_clock_time': 41.54432821273804, 'metric_for_logging': {'pred_time': 7.5084327632545405e-06}, 'val_loss': 2.4483039760182046, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52650}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52650, 'experiment_tag': 'exp', 'time_total_s': 12.091968774795532}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.5535422968616261
NO2(0)的mse=13.93018559399208
NO2(0)的mae=2.4715410828223594
NO2(0)的mar=0.2636947017427588
总共花费的时间为：85.26
安顺市
3122A
3123A
3124A
3125A
[flaml.automl: 09-18 20:07:11] {2390} INFO - task = regression
[flaml.automl: 09-18 20:07:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:07:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:07:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:07:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:07:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:07:12] {3025} INFO - Estimated sufficient time budget=48608s. Estimated necessary time budget=49s.
[flaml.automl: 09-18 20:07:12] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.3115,	best estimator xgboost's best error=5.3115
[flaml.automl: 09-18 20:07:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:07:14] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.6298,	best estimator xgboost's best error=2.6298
[flaml.automl: 09-18 20:07:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:07:15] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.6298,	best estimator xgboost's best error=2.6298
[flaml.automl: 09-18 20:07:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:07:21] {3072} INFO -  at 10.8s,	estimator xgboost's best error=2.6298,	best estimator xgboost's best error=2.6298
[flaml.automl: 09-18 20:07:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:07:23] {3072} INFO -  at 11.9s,	estimator xgboost's best error=1.7012,	best estimator xgboost's best error=1.7012
[flaml.automl: 09-18 20:07:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:07:24] {3072} INFO -  at 13.5s,	estimator xgboost's best error=1.5330,	best estimator xgboost's best error=1.5330
[flaml.automl: 09-18 20:07:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:07:26] {3072} INFO -  at 15.1s,	estimator xgboost's best error=1.5330,	best estimator xgboost's best error=1.5330
[flaml.automl: 09-18 20:07:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:07:28] {3072} INFO -  at 17.5s,	estimator xgboost's best error=1.5330,	best estimator xgboost's best error=1.5330
[flaml.automl: 09-18 20:07:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:07:29] {3072} INFO -  at 18.6s,	estimator xgboost's best error=1.5330,	best estimator xgboost's best error=1.5330
[flaml.automl: 09-18 20:07:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:07:32] {3072} INFO -  at 21.2s,	estimator xgboost's best error=1.5330,	best estimator xgboost's best error=1.5330
[flaml.automl: 09-18 20:07:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:07:33] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.5221,	best estimator xgboost's best error=1.5221
[flaml.automl: 09-18 20:07:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:07:35] {3072} INFO -  at 23.9s,	estimator xgboost's best error=1.5221,	best estimator xgboost's best error=1.5221
[flaml.automl: 09-18 20:07:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:07:41] {3072} INFO -  at 30.3s,	estimator xgboost's best error=1.4690,	best estimator xgboost's best error=1.4690
[flaml.automl: 09-18 20:07:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:07:53] {3072} INFO -  at 42.2s,	estimator xgboost's best error=1.4294,	best estimator xgboost's best error=1.4294
[flaml.automl: 09-18 20:07:53] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 20:07:59] {3072} INFO -  at 48.7s,	estimator xgboost's best error=1.4294,	best estimator xgboost's best error=1.4294
[flaml.automl: 09-18 20:08:11] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 20:08:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:08:11] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:08:11] {2637} INFO - Time taken to find the best model: 42.197348833084106
[flaml.automl: 09-18 20:08:11] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41834}
NO2(0)最佳损失：-0.42936053345808034
NO2(0)最好结果：{'pred_time': 8.9641590737917e-06, 'wall_clock_time': 42.197348833084106, 'metric_for_logging': {'pred_time': 8.9641590737917e-06}, 'val_loss': 1.4293605334580803, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41834}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41834, 'experiment_tag': 'exp', 'time_total_s': 11.887151956558228}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8191368810848499
NO2(0)的mse=4.558130072139638
NO2(0)的mae=1.4295025946440794
NO2(0)的mar=0.19560393429763862
总共花费的时间为：61.68
铜仁地区
2585A
2586A
[flaml.automl: 09-18 20:14:22] {2390} INFO - task = regression
[flaml.automl: 09-18 20:14:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:14:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:14:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:14:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:14:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:14:23] {3025} INFO - Estimated sufficient time budget=12218s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:14:23] {3072} INFO -  at 1.3s,	estimator xgboost's best error=8.7249,	best estimator xgboost's best error=8.7249
[flaml.automl: 09-18 20:14:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:14:25] {3072} INFO -  at 4.0s,	estimator xgboost's best error=4.4876,	best estimator xgboost's best error=4.4876
[flaml.automl: 09-18 20:14:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:14:28] {3072} INFO -  at 6.1s,	estimator xgboost's best error=4.4876,	best estimator xgboost's best error=4.4876
[flaml.automl: 09-18 20:14:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:14:53] {3072} INFO -  at 31.1s,	estimator xgboost's best error=4.4876,	best estimator xgboost's best error=4.4876
[flaml.automl: 09-18 20:14:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:14:56] {3072} INFO -  at 34.3s,	estimator xgboost's best error=3.2388,	best estimator xgboost's best error=3.2388
[flaml.automl: 09-18 20:14:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:15:01] {3072} INFO -  at 39.3s,	estimator xgboost's best error=2.9214,	best estimator xgboost's best error=2.9214
[flaml.automl: 09-18 20:15:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:15:06] {3072} INFO -  at 44.4s,	estimator xgboost's best error=2.9214,	best estimator xgboost's best error=2.9214
[flaml.automl: 09-18 20:15:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:15:12] {3072} INFO -  at 50.9s,	estimator xgboost's best error=2.9214,	best estimator xgboost's best error=2.9214
[flaml.automl: 09-18 20:15:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:15:16] {3072} INFO -  at 54.2s,	estimator xgboost's best error=2.9214,	best estimator xgboost's best error=2.9214
[flaml.automl: 09-18 20:15:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:15:20] {3072} INFO -  at 58.9s,	estimator xgboost's best error=2.9214,	best estimator xgboost's best error=2.9214
[flaml.automl: 09-18 20:15:25] {3335} INFO - retrain xgboost for 4.1s
[flaml.automl: 09-18 20:15:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 20:15:25] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:15:25] {2637} INFO - Time taken to find the best model: 39.34430289268494
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-1.9214248881998288
NO2(0)最好结果：{'pred_time': 9.499184210292418e-05, 'wall_clock_time': 39.34430289268494, 'metric_for_logging': {'pred_time': 9.499184210292418e-05}, 'val_loss': 2.921424888199829, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 5.046422004699707}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.6499171198601912
NO2(0)的mse=20.906647198882755
NO2(0)的mae=3.0108811526587513
NO2(0)的mar=0.2796115748460653
总共花费的时间为：63.58
毕节市
2587A
2588A
3537A
[flaml.automl: 09-18 20:25:11] {2390} INFO - task = regression
[flaml.automl: 09-18 20:25:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:25:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:25:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:25:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:25:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:25:12] {3025} INFO - Estimated sufficient time budget=11986s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:25:12] {3072} INFO -  at 1.4s,	estimator xgboost's best error=7.5828,	best estimator xgboost's best error=7.5828
[flaml.automl: 09-18 20:25:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:25:14] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.6067,	best estimator xgboost's best error=3.6067
[flaml.automl: 09-18 20:25:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:25:16] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.6067,	best estimator xgboost's best error=3.6067
[flaml.automl: 09-18 20:25:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:25:26] {3072} INFO -  at 14.7s,	estimator xgboost's best error=3.6067,	best estimator xgboost's best error=3.6067
[flaml.automl: 09-18 20:25:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:25:27] {3072} INFO -  at 15.9s,	estimator xgboost's best error=2.4082,	best estimator xgboost's best error=2.4082
[flaml.automl: 09-18 20:25:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:25:28] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.1982,	best estimator xgboost's best error=2.1982
[flaml.automl: 09-18 20:25:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:25:30] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.1982,	best estimator xgboost's best error=2.1982
[flaml.automl: 09-18 20:25:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:25:33] {3072} INFO -  at 21.6s,	estimator xgboost's best error=2.1982,	best estimator xgboost's best error=2.1982
[flaml.automl: 09-18 20:25:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:25:34] {3072} INFO -  at 22.8s,	estimator xgboost's best error=2.1982,	best estimator xgboost's best error=2.1982
[flaml.automl: 09-18 20:25:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:25:36] {3072} INFO -  at 25.4s,	estimator xgboost's best error=2.1982,	best estimator xgboost's best error=2.1982
[flaml.automl: 09-18 20:25:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:25:38] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.1982,	best estimator xgboost's best error=2.1982
[flaml.automl: 09-18 20:25:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:25:39] {3072} INFO -  at 27.8s,	estimator xgboost's best error=2.1982,	best estimator xgboost's best error=2.1982
[flaml.automl: 09-18 20:25:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:25:45] {3072} INFO -  at 34.3s,	estimator xgboost's best error=2.1723,	best estimator xgboost's best error=2.1723
[flaml.automl: 09-18 20:25:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:25:57] {3072} INFO -  at 46.4s,	estimator xgboost's best error=2.1259,	best estimator xgboost's best error=2.1259
[flaml.automl: 09-18 20:25:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 20:26:04] {3072} INFO -  at 52.9s,	estimator xgboost's best error=2.1259,	best estimator xgboost's best error=2.1259
[flaml.automl: 09-18 20:26:16] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 20:26:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:26:16] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:26:16] {2637} INFO - Time taken to find the best model: 46.39859580993652
[flaml.automl: 09-18 20:26:16] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.1259466624448398
NO2(0)最好结果：{'pred_time': 1.1028012712114219e-05, 'wall_clock_time': 46.39859580993652, 'metric_for_logging': {'pred_time': 1.1028012712114219e-05}, 'val_loss': 2.1259466624448398, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.102321147918701}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7077443562893393
NO2(0)的mse=8.636680392910826
NO2(0)的mae=2.025208789186225
NO2(0)的mar=0.18518426414331127
总共花费的时间为：65.54
黔西南布依族苗族自治州
2589A
2590A
[flaml.automl: 09-18 20:31:56] {2390} INFO - task = regression
[flaml.automl: 09-18 20:31:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:31:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:31:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:31:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:31:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:31:57] {3025} INFO - Estimated sufficient time budget=12083s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:31:57] {3072} INFO -  at 1.3s,	estimator xgboost's best error=8.3646,	best estimator xgboost's best error=8.3646
[flaml.automl: 09-18 20:31:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:32:00] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.1699,	best estimator xgboost's best error=4.1699
[flaml.automl: 09-18 20:32:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:32:01] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.1699,	best estimator xgboost's best error=4.1699
[flaml.automl: 09-18 20:32:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:32:10] {3072} INFO -  at 14.2s,	estimator xgboost's best error=4.1699,	best estimator xgboost's best error=4.1699
[flaml.automl: 09-18 20:32:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:32:11] {3072} INFO -  at 15.4s,	estimator xgboost's best error=3.0187,	best estimator xgboost's best error=3.0187
[flaml.automl: 09-18 20:32:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:32:13] {3072} INFO -  at 17.0s,	estimator xgboost's best error=2.7201,	best estimator xgboost's best error=2.7201
[flaml.automl: 09-18 20:32:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:32:15] {3072} INFO -  at 18.6s,	estimator xgboost's best error=2.7201,	best estimator xgboost's best error=2.7201
[flaml.automl: 09-18 20:32:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:32:17] {3072} INFO -  at 21.1s,	estimator xgboost's best error=2.7201,	best estimator xgboost's best error=2.7201
[flaml.automl: 09-18 20:32:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:32:18] {3072} INFO -  at 22.2s,	estimator xgboost's best error=2.7201,	best estimator xgboost's best error=2.7201
[flaml.automl: 09-18 20:32:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:32:21] {3072} INFO -  at 24.7s,	estimator xgboost's best error=2.7201,	best estimator xgboost's best error=2.7201
[flaml.automl: 09-18 20:32:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:32:22] {3072} INFO -  at 25.8s,	estimator xgboost's best error=2.7201,	best estimator xgboost's best error=2.7201
[flaml.automl: 09-18 20:32:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:32:23] {3072} INFO -  at 27.0s,	estimator xgboost's best error=2.7201,	best estimator xgboost's best error=2.7201
[flaml.automl: 09-18 20:32:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:32:29] {3072} INFO -  at 33.1s,	estimator xgboost's best error=2.7038,	best estimator xgboost's best error=2.7038
[flaml.automl: 09-18 20:32:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:32:40] {3072} INFO -  at 43.5s,	estimator xgboost's best error=2.6628,	best estimator xgboost's best error=2.6628
[flaml.automl: 09-18 20:32:40] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 20:32:46] {3072} INFO -  at 49.6s,	estimator xgboost's best error=2.6628,	best estimator xgboost's best error=2.6628
[flaml.automl: 09-18 20:32:56] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 20:32:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:32:56] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:32:56] {2637} INFO - Time taken to find the best model: 43.544201612472534
[flaml.automl: 09-18 20:32:56] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.6627534366626926
NO2(0)最好结果：{'pred_time': 1.8857134256134224e-05, 'wall_clock_time': 43.544201612472534, 'metric_for_logging': {'pred_time': 1.8857134256134224e-05}, 'val_loss': 2.6627534366626926, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.489530086517334}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7113853688126442
NO2(0)的mse=15.473989360314814
NO2(0)的mae=2.6029317083907433
NO2(0)的mar=0.22394146023122874
总共花费的时间为：60.46
黔东南苗族侗族自治州
2591A
[flaml.automl: 09-18 20:36:34] {2390} INFO - task = regression
[flaml.automl: 09-18 20:36:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:36:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:36:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:36:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:36:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:36:35] {3025} INFO - Estimated sufficient time budget=12038s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:36:35] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.9384,	best estimator xgboost's best error=11.9384
[flaml.automl: 09-18 20:36:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:36:37] {3072} INFO -  at 3.1s,	estimator xgboost's best error=6.7682,	best estimator xgboost's best error=6.7682
[flaml.automl: 09-18 20:36:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:36:38] {3072} INFO -  at 4.3s,	estimator xgboost's best error=6.7682,	best estimator xgboost's best error=6.7682
[flaml.automl: 09-18 20:36:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:36:45] {3072} INFO -  at 11.4s,	estimator xgboost's best error=6.7682,	best estimator xgboost's best error=6.7682
[flaml.automl: 09-18 20:36:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:36:46] {3072} INFO -  at 12.5s,	estimator xgboost's best error=4.3679,	best estimator xgboost's best error=4.3679
[flaml.automl: 09-18 20:36:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:36:48] {3072} INFO -  at 14.1s,	estimator xgboost's best error=4.2056,	best estimator xgboost's best error=4.2056
[flaml.automl: 09-18 20:36:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:36:50] {3072} INFO -  at 15.7s,	estimator xgboost's best error=4.0826,	best estimator xgboost's best error=4.0826
[flaml.automl: 09-18 20:36:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:36:52] {3072} INFO -  at 18.1s,	estimator xgboost's best error=4.0826,	best estimator xgboost's best error=4.0826
[flaml.automl: 09-18 20:36:52] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:36:54] {3072} INFO -  at 19.7s,	estimator xgboost's best error=4.0826,	best estimator xgboost's best error=4.0826
[flaml.automl: 09-18 20:36:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:36:56] {3072} INFO -  at 22.3s,	estimator xgboost's best error=4.0031,	best estimator xgboost's best error=4.0031
[flaml.automl: 09-18 20:36:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:36:58] {3072} INFO -  at 24.0s,	estimator xgboost's best error=4.0031,	best estimator xgboost's best error=4.0031
[flaml.automl: 09-18 20:36:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:36:59] {3072} INFO -  at 25.1s,	estimator xgboost's best error=4.0031,	best estimator xgboost's best error=4.0031
[flaml.automl: 09-18 20:36:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:37:09] {3072} INFO -  at 34.8s,	estimator xgboost's best error=3.9321,	best estimator xgboost's best error=3.9321
[flaml.automl: 09-18 20:37:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:37:24] {3072} INFO -  at 50.6s,	estimator xgboost's best error=3.9321,	best estimator xgboost's best error=3.9321
[flaml.automl: 09-18 20:37:34] {3335} INFO - retrain xgboost for 9.6s
[flaml.automl: 09-18 20:37:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 20:37:34] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:37:34] {2637} INFO - Time taken to find the best model: 34.79709076881409
NO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
NO2(0)最佳损失：-2.9321467393873544
NO2(0)最好结果：{'pred_time': 3.315616917136489e-05, 'wall_clock_time': 34.79709076881409, 'metric_for_logging': {'pred_time': 3.315616917136489e-05}, 'val_loss': 3.9321467393873544, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 12, 'config/max_leaves': 14, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 9.692845582962036}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.6683539144081946
NO2(0)的mse=37.300364166698664
NO2(0)的mae=4.173135310966225
NO2(0)的mar=0.24512329551215994
总共花费的时间为：60.46
黔南布依族苗族自治州
2593A
3538A
[flaml.automl: 09-18 20:44:13] {2390} INFO - task = regression
[flaml.automl: 09-18 20:44:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:44:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:44:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:44:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:44:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:44:15] {3025} INFO - Estimated sufficient time budget=12449s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:44:15] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.3860,	best estimator xgboost's best error=4.3860
[flaml.automl: 09-18 20:44:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:44:17] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.2770,	best estimator xgboost's best error=2.2770
[flaml.automl: 09-18 20:44:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:44:18] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.2770,	best estimator xgboost's best error=2.2770
[flaml.automl: 09-18 20:44:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:44:27] {3072} INFO -  at 14.1s,	estimator xgboost's best error=2.2770,	best estimator xgboost's best error=2.2770
[flaml.automl: 09-18 20:44:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:44:29] {3072} INFO -  at 15.3s,	estimator xgboost's best error=1.7556,	best estimator xgboost's best error=1.7556
[flaml.automl: 09-18 20:44:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:44:30] {3072} INFO -  at 16.9s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:44:32] {3072} INFO -  at 18.5s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:44:34] {3072} INFO -  at 21.0s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:44:35] {3072} INFO -  at 22.1s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:44:38] {3072} INFO -  at 24.6s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:44:39] {3072} INFO -  at 25.8s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:44:40] {3072} INFO -  at 26.9s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:44:46] {3072} INFO -  at 33.0s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:44:49] {3072} INFO -  at 35.8s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:49] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 20:44:51] {3072} INFO -  at 37.9s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:51] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 20:44:54] {3072} INFO -  at 40.3s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:54] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 20:44:55] {3072} INFO -  at 41.5s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:55] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 20:44:56] {3072} INFO -  at 42.6s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:44:56] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 20:45:00] {3072} INFO -  at 46.5s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:45:00] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 20:45:01] {3072} INFO -  at 47.6s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:45:01] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 20:45:05] {3072} INFO -  at 51.6s,	estimator xgboost's best error=1.5693,	best estimator xgboost's best error=1.5693
[flaml.automl: 09-18 20:45:05] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 20:45:11] {3072} INFO -  at 58.0s,	estimator xgboost's best error=1.5572,	best estimator xgboost's best error=1.5572
[flaml.automl: 09-18 20:45:18] {3335} INFO - retrain xgboost for 6.4s
[flaml.automl: 09-18 20:45:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8492588522931204, colsample_bynode=1,
             colsample_bytree=0.6537360078914272, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.32648040091234304,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=1.728347470404329, missing=nan,
             monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3980257413064903, scale_pos_weight=1,
             subsample=0.8883759987059676, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:45:18] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:45:18] {2637} INFO - Time taken to find the best model: 58.007070541381836
[flaml.automl: 09-18 20:45:18] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 10, 'max_leaves': 11, 'min_child_weight': 1.728347470404329, 'learning_rate': 0.32648040091234304, 'subsample': 0.8883759987059676, 'colsample_bylevel': 0.8492588522931204, 'colsample_bytree': 0.6537360078914272, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3980257413064903}
NO2(0)最佳损失：-0.5571756745610974
NO2(0)最好结果：{'pred_time': 1.614502187291402e-05, 'wall_clock_time': 58.007070541381836, 'metric_for_logging': {'pred_time': 1.614502187291402e-05}, 'val_loss': 1.5571756745610974, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 11, 'min_child_weight': 1.728347470404329, 'learning_rate': 0.32648040091234304, 'subsample': 0.8883759987059676, 'colsample_bylevel': 0.8492588522931204, 'colsample_bytree': 0.6537360078914272, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3980257413064903}, 'config/n_estimators': 10, 'config/max_leaves': 11, 'config/min_child_weight': 1.728347470404329, 'config/learning_rate': 0.32648040091234304, 'config/subsample': 0.8883759987059676, 'config/colsample_bylevel': 0.8492588522931204, 'config/colsample_bytree': 0.6537360078914272, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.3980257413064903, 'experiment_tag': 'exp', 'time_total_s': 6.431474447250366}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8492588522931204, colsample_bynode=1,
             colsample_bytree=0.6537360078914272, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.32648040091234304,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=1.728347470404329, missing=nan,
             monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3980257413064903, scale_pos_weight=1,
             subsample=0.8883759987059676, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6927441206013487
NO2(0)的mse=5.122710480317955
NO2(0)的mae=1.5083848050656092
NO2(0)的mar=0.2762770736288793
总共花费的时间为：64.81
保山市
2594A
2595A
[flaml.automl: 09-18 20:51:49] {2390} INFO - task = regression
[flaml.automl: 09-18 20:51:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:51:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:51:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:51:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:51:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:51:51] {3025} INFO - Estimated sufficient time budget=12058s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:51:51] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.9569,	best estimator xgboost's best error=4.9569
[flaml.automl: 09-18 20:51:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:51:53] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.4399,	best estimator xgboost's best error=2.4399
[flaml.automl: 09-18 20:51:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:51:54] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.4399,	best estimator xgboost's best error=2.4399
[flaml.automl: 09-18 20:51:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:52:03] {3072} INFO -  at 14.0s,	estimator xgboost's best error=2.4399,	best estimator xgboost's best error=2.4399
[flaml.automl: 09-18 20:52:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:52:04] {3072} INFO -  at 15.1s,	estimator xgboost's best error=1.8098,	best estimator xgboost's best error=1.8098
[flaml.automl: 09-18 20:52:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:52:06] {3072} INFO -  at 16.7s,	estimator xgboost's best error=1.7372,	best estimator xgboost's best error=1.7372
[flaml.automl: 09-18 20:52:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:52:08] {3072} INFO -  at 18.4s,	estimator xgboost's best error=1.7372,	best estimator xgboost's best error=1.7372
[flaml.automl: 09-18 20:52:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:52:10] {3072} INFO -  at 20.8s,	estimator xgboost's best error=1.7372,	best estimator xgboost's best error=1.7372
[flaml.automl: 09-18 20:52:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:52:11] {3072} INFO -  at 21.9s,	estimator xgboost's best error=1.7372,	best estimator xgboost's best error=1.7372
[flaml.automl: 09-18 20:52:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:52:14] {3072} INFO -  at 24.4s,	estimator xgboost's best error=1.7372,	best estimator xgboost's best error=1.7372
[flaml.automl: 09-18 20:52:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:52:15] {3072} INFO -  at 25.5s,	estimator xgboost's best error=1.7372,	best estimator xgboost's best error=1.7372
[flaml.automl: 09-18 20:52:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:52:16] {3072} INFO -  at 26.7s,	estimator xgboost's best error=1.7372,	best estimator xgboost's best error=1.7372
[flaml.automl: 09-18 20:52:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:52:22] {3072} INFO -  at 32.7s,	estimator xgboost's best error=1.7372,	best estimator xgboost's best error=1.7372
[flaml.automl: 09-18 20:52:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:52:25] {3072} INFO -  at 35.5s,	estimator xgboost's best error=1.7201,	best estimator xgboost's best error=1.7201
[flaml.automl: 09-18 20:52:25] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 20:52:26] {3072} INFO -  at 37.1s,	estimator xgboost's best error=1.7201,	best estimator xgboost's best error=1.7201
[flaml.automl: 09-18 20:52:26] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 20:52:31] {3072} INFO -  at 41.9s,	estimator xgboost's best error=1.6844,	best estimator xgboost's best error=1.6844
[flaml.automl: 09-18 20:52:31] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 20:52:34] {3072} INFO -  at 44.7s,	estimator xgboost's best error=1.6844,	best estimator xgboost's best error=1.6844
[flaml.automl: 09-18 20:52:34] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 20:52:36] {3072} INFO -  at 46.9s,	estimator xgboost's best error=1.6844,	best estimator xgboost's best error=1.6844
[flaml.automl: 09-18 20:52:36] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 20:52:48] {3072} INFO -  at 58.8s,	estimator xgboost's best error=1.6844,	best estimator xgboost's best error=1.6844
[flaml.automl: 09-18 20:52:53] {3335} INFO - retrain xgboost for 4.8s
[flaml.automl: 09-18 20:52:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:52:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:52:53] {2637} INFO - Time taken to find the best model: 41.88897633552551
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}
NO2(0)最佳损失：-0.6843921643082951
NO2(0)最好结果：{'pred_time': 1.6466292666832143e-05, 'wall_clock_time': 41.88897633552551, 'metric_for_logging': {'pred_time': 1.6466292666832143e-05}, 'val_loss': 1.684392164308295, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}, 'config/n_estimators': 9, 'config/max_leaves': 9, 'config/min_child_weight': 3.029166996333365, 'config/learning_rate': 0.28782051713764356, 'config/subsample': 0.8051874633193139, 'config/colsample_bylevel': 0.656637827405616, 'config/colsample_bytree': 0.7193335366419935, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 2.3848121277079324, 'experiment_tag': 'exp', 'time_total_s': 4.7833685874938965}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.22991619940195274
NO2(0)的mse=6.413331888326734
NO2(0)的mae=1.5837714591541805
NO2(0)的mar=0.18855392255861933
总共花费的时间为：63.95
昭通市
2596A
2597A
[flaml.automl: 09-18 20:59:19] {2390} INFO - task = regression
[flaml.automl: 09-18 20:59:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:59:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:59:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:59:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:59:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:59:21] {3025} INFO - Estimated sufficient time budget=19556s. Estimated necessary time budget=20s.
[flaml.automl: 09-18 20:59:21] {3072} INFO -  at 2.1s,	estimator xgboost's best error=8.0522,	best estimator xgboost's best error=8.0522
[flaml.automl: 09-18 20:59:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:59:24] {3072} INFO -  at 5.9s,	estimator xgboost's best error=3.8958,	best estimator xgboost's best error=3.8958
[flaml.automl: 09-18 20:59:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:59:26] {3072} INFO -  at 7.5s,	estimator xgboost's best error=3.8958,	best estimator xgboost's best error=3.8958
[flaml.automl: 09-18 20:59:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:59:35] {3072} INFO -  at 16.8s,	estimator xgboost's best error=3.8958,	best estimator xgboost's best error=3.8958
[flaml.automl: 09-18 20:59:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:59:36] {3072} INFO -  at 17.9s,	estimator xgboost's best error=2.7470,	best estimator xgboost's best error=2.7470
[flaml.automl: 09-18 20:59:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:59:38] {3072} INFO -  at 19.5s,	estimator xgboost's best error=2.5581,	best estimator xgboost's best error=2.5581
[flaml.automl: 09-18 20:59:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:59:40] {3072} INFO -  at 21.1s,	estimator xgboost's best error=2.5581,	best estimator xgboost's best error=2.5581
[flaml.automl: 09-18 20:59:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:59:42] {3072} INFO -  at 23.5s,	estimator xgboost's best error=2.5581,	best estimator xgboost's best error=2.5581
[flaml.automl: 09-18 20:59:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:59:43] {3072} INFO -  at 24.6s,	estimator xgboost's best error=2.5581,	best estimator xgboost's best error=2.5581
[flaml.automl: 09-18 20:59:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:59:45] {3072} INFO -  at 27.0s,	estimator xgboost's best error=2.5581,	best estimator xgboost's best error=2.5581
[flaml.automl: 09-18 20:59:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:59:47] {3072} INFO -  at 28.1s,	estimator xgboost's best error=2.5581,	best estimator xgboost's best error=2.5581
[flaml.automl: 09-18 20:59:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:59:48] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.5581,	best estimator xgboost's best error=2.5581
[flaml.automl: 09-18 20:59:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:59:54] {3072} INFO -  at 35.1s,	estimator xgboost's best error=2.5179,	best estimator xgboost's best error=2.5179
[flaml.automl: 09-18 20:59:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 21:00:04] {3072} INFO -  at 45.2s,	estimator xgboost's best error=2.4890,	best estimator xgboost's best error=2.4890
[flaml.automl: 09-18 21:00:04] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 21:00:10] {3072} INFO -  at 51.2s,	estimator xgboost's best error=2.4890,	best estimator xgboost's best error=2.4890
[flaml.automl: 09-18 21:00:20] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 21:00:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:00:20] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:00:20] {2637} INFO - Time taken to find the best model: 45.18881416320801
[flaml.automl: 09-18 21:00:20] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.4889948854398964
NO2(0)最好结果：{'pred_time': 1.623341890907604e-05, 'wall_clock_time': 45.18881416320801, 'metric_for_logging': {'pred_time': 1.623341890907604e-05}, 'val_loss': 2.4889948854398964, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.062502384185791}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6720668063332729
NO2(0)的mse=12.491126384747872
NO2(0)的mae=2.4610422969754064
NO2(0)的mar=0.20759336782835408
总共花费的时间为：61.99
丽江市
2598A
2599A
2600A
[flaml.automl: 09-18 21:10:06] {2390} INFO - task = regression
[flaml.automl: 09-18 21:10:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:10:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:10:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:10:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:10:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:10:07] {3025} INFO - Estimated sufficient time budget=12300s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:10:07] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.2686,	best estimator xgboost's best error=5.2686
[flaml.automl: 09-18 21:10:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:10:09] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.5822,	best estimator xgboost's best error=2.5822
[flaml.automl: 09-18 21:10:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:10:11] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.5822,	best estimator xgboost's best error=2.5822
[flaml.automl: 09-18 21:10:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:10:21] {3072} INFO -  at 14.7s,	estimator xgboost's best error=2.5822,	best estimator xgboost's best error=2.5822
[flaml.automl: 09-18 21:10:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:10:22] {3072} INFO -  at 15.9s,	estimator xgboost's best error=1.7867,	best estimator xgboost's best error=1.7867
[flaml.automl: 09-18 21:10:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:10:23] {3072} INFO -  at 17.5s,	estimator xgboost's best error=1.6969,	best estimator xgboost's best error=1.6969
[flaml.automl: 09-18 21:10:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:10:25] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.6937,	best estimator xgboost's best error=1.6937
[flaml.automl: 09-18 21:10:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:10:28] {3072} INFO -  at 21.8s,	estimator xgboost's best error=1.6937,	best estimator xgboost's best error=1.6937
[flaml.automl: 09-18 21:10:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:10:29] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.6937,	best estimator xgboost's best error=1.6937
[flaml.automl: 09-18 21:10:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:10:32] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.6761,	best estimator xgboost's best error=1.6761
[flaml.automl: 09-18 21:10:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:10:34] {3072} INFO -  at 28.1s,	estimator xgboost's best error=1.6761,	best estimator xgboost's best error=1.6761
[flaml.automl: 09-18 21:10:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:10:35] {3072} INFO -  at 29.3s,	estimator xgboost's best error=1.6761,	best estimator xgboost's best error=1.6761
[flaml.automl: 09-18 21:10:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:10:49] {3072} INFO -  at 42.9s,	estimator xgboost's best error=1.6605,	best estimator xgboost's best error=1.6605
[flaml.automl: 09-18 21:10:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 21:11:05] {3072} INFO -  at 59.4s,	estimator xgboost's best error=1.6102,	best estimator xgboost's best error=1.6102
[flaml.automl: 09-18 21:11:29] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-18 21:11:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:11:29] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:11:29] {2637} INFO - Time taken to find the best model: 59.448214054107666
[flaml.automl: 09-18 21:11:29] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
NO2(0)最佳损失：-0.6102425645046037
NO2(0)最好结果：{'pred_time': 1.2045700163933356e-05, 'wall_clock_time': 59.448214054107666, 'metric_for_logging': {'pred_time': 1.2045700163933356e-05}, 'val_loss': 1.6102425645046037, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.49353337287903}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6113149756829699
NO2(0)的mse=5.347638923891094
NO2(0)的mae=1.6195925438458283
NO2(0)的mar=0.20729674906220263
总共花费的时间为：83.99
普洱市
2601A
2602A
[flaml.automl: 09-18 21:18:05] {2390} INFO - task = regression
[flaml.automl: 09-18 21:18:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:18:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:18:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:18:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:18:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:18:06] {3025} INFO - Estimated sufficient time budget=12104s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:18:06] {3072} INFO -  at 1.3s,	estimator xgboost's best error=8.6063,	best estimator xgboost's best error=8.6063
[flaml.automl: 09-18 21:18:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:18:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.1125,	best estimator xgboost's best error=4.1125
[flaml.automl: 09-18 21:18:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:18:10] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.1125,	best estimator xgboost's best error=4.1125
[flaml.automl: 09-18 21:18:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:18:19] {3072} INFO -  at 14.1s,	estimator xgboost's best error=4.1125,	best estimator xgboost's best error=4.1125
[flaml.automl: 09-18 21:18:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:18:20] {3072} INFO -  at 15.3s,	estimator xgboost's best error=2.7647,	best estimator xgboost's best error=2.7647
[flaml.automl: 09-18 21:18:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:18:22] {3072} INFO -  at 16.9s,	estimator xgboost's best error=2.5178,	best estimator xgboost's best error=2.5178
[flaml.automl: 09-18 21:18:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:18:23] {3072} INFO -  at 18.5s,	estimator xgboost's best error=2.5178,	best estimator xgboost's best error=2.5178
[flaml.automl: 09-18 21:18:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:18:26] {3072} INFO -  at 21.0s,	estimator xgboost's best error=2.5178,	best estimator xgboost's best error=2.5178
[flaml.automl: 09-18 21:18:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:18:27] {3072} INFO -  at 22.1s,	estimator xgboost's best error=2.5178,	best estimator xgboost's best error=2.5178
[flaml.automl: 09-18 21:18:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:18:30] {3072} INFO -  at 24.6s,	estimator xgboost's best error=2.5178,	best estimator xgboost's best error=2.5178
[flaml.automl: 09-18 21:18:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:18:31] {3072} INFO -  at 25.8s,	estimator xgboost's best error=2.5178,	best estimator xgboost's best error=2.5178
[flaml.automl: 09-18 21:18:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:18:32] {3072} INFO -  at 26.9s,	estimator xgboost's best error=2.5178,	best estimator xgboost's best error=2.5178
[flaml.automl: 09-18 21:18:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:18:38] {3072} INFO -  at 33.0s,	estimator xgboost's best error=2.4923,	best estimator xgboost's best error=2.4923
[flaml.automl: 09-18 21:18:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 21:18:48] {3072} INFO -  at 43.4s,	estimator xgboost's best error=2.3813,	best estimator xgboost's best error=2.3813
[flaml.automl: 09-18 21:18:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 21:18:54] {3072} INFO -  at 49.5s,	estimator xgboost's best error=2.3813,	best estimator xgboost's best error=2.3813
[flaml.automl: 09-18 21:18:54] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 21:19:04] {3072} INFO -  at 59.2s,	estimator xgboost's best error=2.3465,	best estimator xgboost's best error=2.3465
[flaml.automl: 09-18 21:19:21] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-18 21:19:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:19:21] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:19:21] {2637} INFO - Time taken to find the best model: 59.214685678482056
[flaml.automl: 09-18 21:19:21] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-1.3464786010998582
NO2(0)最好结果：{'pred_time': 1.7875538258550498e-05, 'wall_clock_time': 59.214685678482056, 'metric_for_logging': {'pred_time': 1.7875538258550498e-05}, 'val_loss': 2.346478601099858, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.726684331893921}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7701423775338677
NO2(0)的mse=12.162259632770743
NO2(0)的mae=2.359715753953186
NO2(0)的mar=0.18172238786129796
总共花费的时间为：76.89
临沧市
2603A
2604A
[flaml.automl: 09-18 21:25:38] {2390} INFO - task = regression
[flaml.automl: 09-18 21:25:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:25:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:25:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:25:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:25:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:25:39] {3025} INFO - Estimated sufficient time budget=12298s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:25:39] {3072} INFO -  at 1.3s,	estimator xgboost's best error=8.2275,	best estimator xgboost's best error=8.2275
[flaml.automl: 09-18 21:25:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:25:41] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.7859,	best estimator xgboost's best error=3.7859
[flaml.automl: 09-18 21:25:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:25:42] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.7859,	best estimator xgboost's best error=3.7859
[flaml.automl: 09-18 21:25:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:25:52] {3072} INFO -  at 14.2s,	estimator xgboost's best error=3.7859,	best estimator xgboost's best error=3.7859
[flaml.automl: 09-18 21:25:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:25:53] {3072} INFO -  at 15.4s,	estimator xgboost's best error=2.2273,	best estimator xgboost's best error=2.2273
[flaml.automl: 09-18 21:25:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:25:55] {3072} INFO -  at 17.0s,	estimator xgboost's best error=2.0191,	best estimator xgboost's best error=2.0191
[flaml.automl: 09-18 21:25:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:25:56] {3072} INFO -  at 18.6s,	estimator xgboost's best error=2.0191,	best estimator xgboost's best error=2.0191
[flaml.automl: 09-18 21:25:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:25:59] {3072} INFO -  at 21.1s,	estimator xgboost's best error=2.0191,	best estimator xgboost's best error=2.0191
[flaml.automl: 09-18 21:25:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:26:00] {3072} INFO -  at 22.3s,	estimator xgboost's best error=2.0191,	best estimator xgboost's best error=2.0191
[flaml.automl: 09-18 21:26:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:26:02] {3072} INFO -  at 24.7s,	estimator xgboost's best error=2.0191,	best estimator xgboost's best error=2.0191
[flaml.automl: 09-18 21:26:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:26:04] {3072} INFO -  at 25.9s,	estimator xgboost's best error=2.0191,	best estimator xgboost's best error=2.0191
[flaml.automl: 09-18 21:26:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:26:05] {3072} INFO -  at 27.0s,	estimator xgboost's best error=2.0191,	best estimator xgboost's best error=2.0191
[flaml.automl: 09-18 21:26:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:26:11] {3072} INFO -  at 33.1s,	estimator xgboost's best error=1.9884,	best estimator xgboost's best error=1.9884
[flaml.automl: 09-18 21:26:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 21:26:21] {3072} INFO -  at 43.6s,	estimator xgboost's best error=1.9168,	best estimator xgboost's best error=1.9168
[flaml.automl: 09-18 21:26:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 21:26:27] {3072} INFO -  at 49.7s,	estimator xgboost's best error=1.9168,	best estimator xgboost's best error=1.9168
[flaml.automl: 09-18 21:26:38] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 21:26:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:26:38] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:26:38] {2637} INFO - Time taken to find the best model: 43.60885286331177
[flaml.automl: 09-18 21:26:38] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-0.9168056664898871
NO2(0)最好结果：{'pred_time': 1.7825718485817204e-05, 'wall_clock_time': 43.60885286331177, 'metric_for_logging': {'pred_time': 1.7825718485817204e-05}, 'val_loss': 1.9168056664898871, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.500995635986328}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6402779205266937
NO2(0)的mse=7.442583347661247
NO2(0)的mae=1.9481818895891074
NO2(0)的mar=0.1554086895004846
总共花费的时间为：60.53
楚雄州
2605A
2606A
[flaml.automl: 09-18 21:33:03] {2390} INFO - task = regression
[flaml.automl: 09-18 21:33:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:33:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:33:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:33:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:33:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:33:04] {3025} INFO - Estimated sufficient time budget=11918s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:33:04] {3072} INFO -  at 1.3s,	estimator xgboost's best error=8.3611,	best estimator xgboost's best error=8.3611
[flaml.automl: 09-18 21:33:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:33:06] {3072} INFO -  at 3.4s,	estimator xgboost's best error=4.5090,	best estimator xgboost's best error=4.5090
[flaml.automl: 09-18 21:33:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:33:07] {3072} INFO -  at 4.6s,	estimator xgboost's best error=4.5090,	best estimator xgboost's best error=4.5090
[flaml.automl: 09-18 21:33:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:33:17] {3072} INFO -  at 14.0s,	estimator xgboost's best error=4.5090,	best estimator xgboost's best error=4.5090
[flaml.automl: 09-18 21:33:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:33:18] {3072} INFO -  at 15.2s,	estimator xgboost's best error=3.6082,	best estimator xgboost's best error=3.6082
[flaml.automl: 09-18 21:33:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:33:19] {3072} INFO -  at 16.8s,	estimator xgboost's best error=3.3430,	best estimator xgboost's best error=3.3430
[flaml.automl: 09-18 21:33:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:33:21] {3072} INFO -  at 18.4s,	estimator xgboost's best error=3.3430,	best estimator xgboost's best error=3.3430
[flaml.automl: 09-18 21:33:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:33:23] {3072} INFO -  at 20.9s,	estimator xgboost's best error=3.3430,	best estimator xgboost's best error=3.3430
[flaml.automl: 09-18 21:33:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:33:25] {3072} INFO -  at 22.1s,	estimator xgboost's best error=3.3430,	best estimator xgboost's best error=3.3430
[flaml.automl: 09-18 21:33:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:33:27] {3072} INFO -  at 24.5s,	estimator xgboost's best error=3.3430,	best estimator xgboost's best error=3.3430
[flaml.automl: 09-18 21:33:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:33:28] {3072} INFO -  at 25.7s,	estimator xgboost's best error=3.3430,	best estimator xgboost's best error=3.3430
[flaml.automl: 09-18 21:33:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:33:29] {3072} INFO -  at 26.8s,	estimator xgboost's best error=3.3430,	best estimator xgboost's best error=3.3430
[flaml.automl: 09-18 21:33:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:33:35] {3072} INFO -  at 32.9s,	estimator xgboost's best error=3.3428,	best estimator xgboost's best error=3.3428
[flaml.automl: 09-18 21:33:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 21:33:46] {3072} INFO -  at 43.4s,	estimator xgboost's best error=3.2479,	best estimator xgboost's best error=3.2479
[flaml.automl: 09-18 21:33:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 21:33:53] {3072} INFO -  at 50.1s,	estimator xgboost's best error=3.2479,	best estimator xgboost's best error=3.2479
[flaml.automl: 09-18 21:34:12] {3335} INFO - retrain xgboost for 19.2s
[flaml.automl: 09-18 21:34:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:34:12] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:34:12] {2637} INFO - Time taken to find the best model: 43.38012766838074
[flaml.automl: 09-18 21:34:12] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.2478642387666565
NO2(0)最好结果：{'pred_time': 1.6523246686157478e-05, 'wall_clock_time': 43.38012766838074, 'metric_for_logging': {'pred_time': 1.6523246686157478e-05}, 'val_loss': 3.2478642387666565, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.486959457397461}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7574589172814876
NO2(0)的mse=24.0321218970531
NO2(0)的mae=3.0011466598048657
NO2(0)的mar=0.26297979998731386
总共花费的时间为：69.78
红河州
2609A
3038A
[flaml.automl: 09-18 21:41:11] {2390} INFO - task = regression
[flaml.automl: 09-18 21:41:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:41:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:41:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:41:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:41:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:41:12] {3025} INFO - Estimated sufficient time budget=11863s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:41:12] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.9641,	best estimator xgboost's best error=4.9641
[flaml.automl: 09-18 21:41:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:41:14] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.5603,	best estimator xgboost's best error=2.5603
[flaml.automl: 09-18 21:41:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:41:15] {3072} INFO -  at 4.5s,	estimator xgboost's best error=2.5603,	best estimator xgboost's best error=2.5603
[flaml.automl: 09-18 21:41:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:41:24] {3072} INFO -  at 13.8s,	estimator xgboost's best error=2.5603,	best estimator xgboost's best error=2.5603
[flaml.automl: 09-18 21:41:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:41:26] {3072} INFO -  at 15.0s,	estimator xgboost's best error=1.9906,	best estimator xgboost's best error=1.9906
[flaml.automl: 09-18 21:41:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:41:27] {3072} INFO -  at 16.6s,	estimator xgboost's best error=1.8855,	best estimator xgboost's best error=1.8855
[flaml.automl: 09-18 21:41:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:41:29] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.8855,	best estimator xgboost's best error=1.8855
[flaml.automl: 09-18 21:41:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:41:31] {3072} INFO -  at 20.6s,	estimator xgboost's best error=1.8855,	best estimator xgboost's best error=1.8855
[flaml.automl: 09-18 21:41:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:41:32] {3072} INFO -  at 21.8s,	estimator xgboost's best error=1.8855,	best estimator xgboost's best error=1.8855
[flaml.automl: 09-18 21:41:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:41:35] {3072} INFO -  at 24.2s,	estimator xgboost's best error=1.8855,	best estimator xgboost's best error=1.8855
[flaml.automl: 09-18 21:41:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:41:36] {3072} INFO -  at 25.4s,	estimator xgboost's best error=1.8855,	best estimator xgboost's best error=1.8855
[flaml.automl: 09-18 21:41:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:41:37] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.8855,	best estimator xgboost's best error=1.8855
[flaml.automl: 09-18 21:41:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:41:43] {3072} INFO -  at 32.6s,	estimator xgboost's best error=1.8855,	best estimator xgboost's best error=1.8855
[flaml.automl: 09-18 21:41:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 21:41:46] {3072} INFO -  at 35.5s,	estimator xgboost's best error=1.8793,	best estimator xgboost's best error=1.8793
[flaml.automl: 09-18 21:41:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 21:41:48] {3072} INFO -  at 37.0s,	estimator xgboost's best error=1.8793,	best estimator xgboost's best error=1.8793
[flaml.automl: 09-18 21:41:48] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 21:41:52] {3072} INFO -  at 41.9s,	estimator xgboost's best error=1.8677,	best estimator xgboost's best error=1.8677
[flaml.automl: 09-18 21:41:52] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 21:41:55] {3072} INFO -  at 44.7s,	estimator xgboost's best error=1.8677,	best estimator xgboost's best error=1.8677
[flaml.automl: 09-18 21:41:55] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 21:41:57] {3072} INFO -  at 46.9s,	estimator xgboost's best error=1.8677,	best estimator xgboost's best error=1.8677
[flaml.automl: 09-18 21:41:57] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 21:42:09] {3072} INFO -  at 58.8s,	estimator xgboost's best error=1.8677,	best estimator xgboost's best error=1.8677
[flaml.automl: 09-18 21:42:14] {3335} INFO - retrain xgboost for 4.8s
[flaml.automl: 09-18 21:42:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:42:14] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:42:14] {2637} INFO - Time taken to find the best model: 41.87175393104553
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}
NO2(0)最佳损失：-0.8676557791808281
NO2(0)最好结果：{'pred_time': 1.6449791024409594e-05, 'wall_clock_time': 41.87175393104553, 'metric_for_logging': {'pred_time': 1.6449791024409594e-05}, 'val_loss': 1.8676557791808281, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}, 'config/n_estimators': 9, 'config/max_leaves': 9, 'config/min_child_weight': 3.029166996333365, 'config/learning_rate': 0.28782051713764356, 'config/subsample': 0.8051874633193139, 'config/colsample_bylevel': 0.656637827405616, 'config/colsample_bytree': 0.7193335366419935, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 2.3848121277079324, 'experiment_tag': 'exp', 'time_total_s': 4.833569288253784}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.21413702030862702
NO2(0)的mse=7.832026613885484
NO2(0)的mae=1.9852158323471487
NO2(0)的mar=0.25399921893656513
总共花费的时间为：64.06
文山州
2610A
2611A
[flaml.automl: 09-18 21:49:32] {2390} INFO - task = regression
[flaml.automl: 09-18 21:49:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:49:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:49:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:49:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:49:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:49:35] {3025} INFO - Estimated sufficient time budget=32119s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 21:49:35] {3072} INFO -  at 3.4s,	estimator xgboost's best error=6.2784,	best estimator xgboost's best error=6.2784
[flaml.automl: 09-18 21:49:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:49:41] {3072} INFO -  at 9.5s,	estimator xgboost's best error=2.8433,	best estimator xgboost's best error=2.8433
[flaml.automl: 09-18 21:49:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:49:45] {3072} INFO -  at 12.8s,	estimator xgboost's best error=2.8433,	best estimator xgboost's best error=2.8433
[flaml.automl: 09-18 21:49:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:50:15] {3072} INFO -  at 42.7s,	estimator xgboost's best error=2.8433,	best estimator xgboost's best error=2.8433
[flaml.automl: 09-18 21:50:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:50:18] {3072} INFO -  at 46.1s,	estimator xgboost's best error=1.6690,	best estimator xgboost's best error=1.6690
[flaml.automl: 09-18 21:50:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:50:22] {3072} INFO -  at 50.6s,	estimator xgboost's best error=1.5184,	best estimator xgboost's best error=1.5184
[flaml.automl: 09-18 21:50:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:50:27] {3072} INFO -  at 55.4s,	estimator xgboost's best error=1.5184,	best estimator xgboost's best error=1.5184
[flaml.automl: 09-18 21:50:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:50:31] {3072} INFO -  at 59.2s,	estimator xgboost's best error=1.5184,	best estimator xgboost's best error=1.5184
[flaml.automl: 09-18 21:50:34] {3335} INFO - retrain xgboost for 3.1s
[flaml.automl: 09-18 21:50:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 21:50:34] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:50:34] {2637} INFO - Time taken to find the best model: 50.55100607872009
[flaml.automl: 09-18 21:50:34] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-0.5183926365916569
NO2(0)最好结果：{'pred_time': 6.219256647606245e-05, 'wall_clock_time': 50.55100607872009, 'metric_for_logging': {'pred_time': 6.219256647606245e-05}, 'val_loss': 1.518392636591657, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 4.484049558639526}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.5135542593184176
NO2(0)的mse=4.616331154661055
NO2(0)的mae=1.52851451081013
NO2(0)的mar=0.15212511301200193
总共花费的时间为：63.20
西双版纳州
2612A
2613A
[flaml.automl: 09-18 21:57:38] {2390} INFO - task = regression
[flaml.automl: 09-18 21:57:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:57:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:57:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:57:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:57:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:57:41] {3025} INFO - Estimated sufficient time budget=31290s. Estimated necessary time budget=31s.
[flaml.automl: 09-18 21:57:41] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.8601,	best estimator xgboost's best error=8.8601
[flaml.automl: 09-18 21:57:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:57:47] {3072} INFO -  at 9.0s,	estimator xgboost's best error=4.7492,	best estimator xgboost's best error=4.7492
[flaml.automl: 09-18 21:57:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:57:50] {3072} INFO -  at 12.5s,	estimator xgboost's best error=4.7492,	best estimator xgboost's best error=4.7492
[flaml.automl: 09-18 21:57:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:58:12] {3072} INFO -  at 33.9s,	estimator xgboost's best error=4.7492,	best estimator xgboost's best error=4.7492
[flaml.automl: 09-18 21:58:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:58:14] {3072} INFO -  at 36.5s,	estimator xgboost's best error=2.7267,	best estimator xgboost's best error=2.7267
[flaml.automl: 09-18 21:58:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:58:17] {3072} INFO -  at 39.0s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-18 21:58:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:58:19] {3072} INFO -  at 41.7s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-18 21:58:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:58:23] {3072} INFO -  at 45.4s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-18 21:58:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:58:25] {3072} INFO -  at 47.0s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-18 21:58:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:58:29] {3072} INFO -  at 50.8s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-18 21:58:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:58:31] {3072} INFO -  at 52.7s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-18 21:58:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:58:32] {3072} INFO -  at 54.7s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-18 21:58:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:58:37] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-18 21:58:39] {3335} INFO - retrain xgboost for 1.5s
[flaml.automl: 09-18 21:58:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 21:58:39] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:58:39] {2637} INFO - Time taken to find the best model: 39.01767706871033
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-1.4748846333141103
NO2(0)最好结果：{'pred_time': 4.0384526390632165e-05, 'wall_clock_time': 39.01767706871033, 'metric_for_logging': {'pred_time': 4.0384526390632165e-05}, 'val_loss': 2.4748846333141103, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 2.5558648109436035}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7414783479147813
NO2(0)的mse=12.843176581021279
NO2(0)的mae=2.386343971554772
NO2(0)的mar=0.18214746694777215
总共花费的时间为：61.55
大理州
2614A
2615A
[flaml.automl: 09-18 22:05:33] {2390} INFO - task = regression
[flaml.automl: 09-18 22:05:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:05:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:05:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:05:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:05:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:05:34] {3025} INFO - Estimated sufficient time budget=12132s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 22:05:34] {3072} INFO -  at 1.3s,	estimator xgboost's best error=6.2400,	best estimator xgboost's best error=6.2400
[flaml.automl: 09-18 22:05:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:05:36] {3072} INFO -  at 3.4s,	estimator xgboost's best error=3.1141,	best estimator xgboost's best error=3.1141
[flaml.automl: 09-18 22:05:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:05:37] {3072} INFO -  at 4.6s,	estimator xgboost's best error=3.1141,	best estimator xgboost's best error=3.1141
[flaml.automl: 09-18 22:05:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:05:47] {3072} INFO -  at 14.1s,	estimator xgboost's best error=3.1141,	best estimator xgboost's best error=3.1141
[flaml.automl: 09-18 22:05:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:05:48] {3072} INFO -  at 15.3s,	estimator xgboost's best error=2.4148,	best estimator xgboost's best error=2.4148
[flaml.automl: 09-18 22:05:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:05:50] {3072} INFO -  at 16.9s,	estimator xgboost's best error=2.3530,	best estimator xgboost's best error=2.3530
[flaml.automl: 09-18 22:05:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:05:51] {3072} INFO -  at 18.5s,	estimator xgboost's best error=2.3530,	best estimator xgboost's best error=2.3530
[flaml.automl: 09-18 22:05:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:05:54] {3072} INFO -  at 21.0s,	estimator xgboost's best error=2.3530,	best estimator xgboost's best error=2.3530
[flaml.automl: 09-18 22:05:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:05:55] {3072} INFO -  at 22.1s,	estimator xgboost's best error=2.3530,	best estimator xgboost's best error=2.3530
[flaml.automl: 09-18 22:05:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:05:57] {3072} INFO -  at 24.6s,	estimator xgboost's best error=2.3530,	best estimator xgboost's best error=2.3530
[flaml.automl: 09-18 22:05:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:05:59] {3072} INFO -  at 25.7s,	estimator xgboost's best error=2.3530,	best estimator xgboost's best error=2.3530
[flaml.automl: 09-18 22:05:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:06:00] {3072} INFO -  at 26.9s,	estimator xgboost's best error=2.3530,	best estimator xgboost's best error=2.3530
[flaml.automl: 09-18 22:06:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:06:12] {3072} INFO -  at 39.5s,	estimator xgboost's best error=2.3530,	best estimator xgboost's best error=2.3530
[flaml.automl: 09-18 22:06:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:06:19] {3072} INFO -  at 46.1s,	estimator xgboost's best error=2.3451,	best estimator xgboost's best error=2.3451
[flaml.automl: 09-18 22:06:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 22:06:23] {3072} INFO -  at 49.8s,	estimator xgboost's best error=2.3451,	best estimator xgboost's best error=2.3451
[flaml.automl: 09-18 22:06:23] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 22:06:32] {3072} INFO -  at 58.8s,	estimator xgboost's best error=2.3190,	best estimator xgboost's best error=2.3190
[flaml.automl: 09-18 22:06:42] {3335} INFO - retrain xgboost for 10.5s
[flaml.automl: 09-18 22:06:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:06:42] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:06:42] {2637} INFO - Time taken to find the best model: 58.76140475273132
[flaml.automl: 09-18 22:06:42] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}
NO2(0)最佳损失：-1.3189905574486689
NO2(0)最好结果：{'pred_time': 6.089040568289767e-05, 'wall_clock_time': 58.76140475273132, 'metric_for_logging': {'pred_time': 6.089040568289767e-05}, 'val_loss': 2.318990557448669, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}, 'config/n_estimators': 9, 'config/max_leaves': 9, 'config/min_child_weight': 3.029166996333365, 'config/learning_rate': 0.28782051713764356, 'config/subsample': 0.8051874633193139, 'config/colsample_bylevel': 0.656637827405616, 'config/colsample_bytree': 0.7193335366419935, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 2.3848121277079324, 'experiment_tag': 'exp', 'time_total_s': 8.951236963272095}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.23625683611008053
NO2(0)的mse=13.043243070812823
NO2(0)的mae=2.3224292003807148
NO2(0)的mar=0.2222049443884354
总共花费的时间为：69.69
德宏州
2616A
[flaml.automl: 09-18 22:10:21] {2390} INFO - task = regression
[flaml.automl: 09-18 22:10:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:10:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:10:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:10:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:10:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:10:23] {3025} INFO - Estimated sufficient time budget=21808s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 22:10:23] {3072} INFO -  at 2.2s,	estimator xgboost's best error=12.9294,	best estimator xgboost's best error=12.9294
[flaml.automl: 09-18 22:10:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:10:26] {3072} INFO -  at 5.8s,	estimator xgboost's best error=7.2704,	best estimator xgboost's best error=7.2704
[flaml.automl: 09-18 22:10:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:10:29] {3072} INFO -  at 8.0s,	estimator xgboost's best error=7.2704,	best estimator xgboost's best error=7.2704
[flaml.automl: 09-18 22:10:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:10:42] {3072} INFO -  at 21.3s,	estimator xgboost's best error=7.2704,	best estimator xgboost's best error=7.2704
[flaml.automl: 09-18 22:10:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:10:44] {3072} INFO -  at 23.3s,	estimator xgboost's best error=4.3098,	best estimator xgboost's best error=4.3098
[flaml.automl: 09-18 22:10:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:10:47] {3072} INFO -  at 26.3s,	estimator xgboost's best error=4.0249,	best estimator xgboost's best error=4.0249
[flaml.automl: 09-18 22:10:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:10:50] {3072} INFO -  at 29.2s,	estimator xgboost's best error=4.0249,	best estimator xgboost's best error=4.0249
[flaml.automl: 09-18 22:10:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:10:54] {3072} INFO -  at 33.2s,	estimator xgboost's best error=4.0249,	best estimator xgboost's best error=4.0249
[flaml.automl: 09-18 22:10:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:10:57] {3072} INFO -  at 36.6s,	estimator xgboost's best error=4.0249,	best estimator xgboost's best error=4.0249
[flaml.automl: 09-18 22:10:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:11:04] {3072} INFO -  at 43.7s,	estimator xgboost's best error=4.0249,	best estimator xgboost's best error=4.0249
[flaml.automl: 09-18 22:11:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:11:08] {3072} INFO -  at 47.2s,	estimator xgboost's best error=4.0249,	best estimator xgboost's best error=4.0249
[flaml.automl: 09-18 22:11:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:11:12] {3072} INFO -  at 51.3s,	estimator xgboost's best error=4.0249,	best estimator xgboost's best error=4.0249
[flaml.automl: 09-18 22:11:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:11:18] {3072} INFO -  at 57.6s,	estimator xgboost's best error=4.0249,	best estimator xgboost's best error=4.0249
[flaml.automl: 09-18 22:11:24] {3335} INFO - retrain xgboost for 5.4s
[flaml.automl: 09-18 22:11:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:11:24] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:11:24] {2637} INFO - Time taken to find the best model: 26.281471252441406
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-3.0248624542360654
NO2(0)最好结果：{'pred_time': 6.521013956740626e-05, 'wall_clock_time': 26.281471252441406, 'metric_for_logging': {'pred_time': 6.521013956740626e-05}, 'val_loss': 4.024862454236065, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 2.9485974311828613}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7000661826890816
NO2(0)的mse=33.00953376814297
NO2(0)的mae=3.984562335143218
NO2(0)的mar=0.26356121149098166
总共花费的时间为：63.31
怒江州
2618A
2619A
[flaml.automl: 09-18 22:17:46] {2390} INFO - task = regression
[flaml.automl: 09-18 22:17:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:17:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:17:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:17:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:17:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:17:47] {3025} INFO - Estimated sufficient time budget=12141s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 22:17:47] {3072} INFO -  at 1.4s,	estimator xgboost's best error=8.1187,	best estimator xgboost's best error=8.1187
[flaml.automl: 09-18 22:17:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:17:49] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.8543,	best estimator xgboost's best error=3.8543
[flaml.automl: 09-18 22:17:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:17:50] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.8543,	best estimator xgboost's best error=3.8543
[flaml.automl: 09-18 22:17:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:18:06] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.8543,	best estimator xgboost's best error=3.8543
[flaml.automl: 09-18 22:18:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:18:09] {3072} INFO -  at 23.2s,	estimator xgboost's best error=2.5873,	best estimator xgboost's best error=2.5873
[flaml.automl: 09-18 22:18:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:18:12] {3072} INFO -  at 26.3s,	estimator xgboost's best error=2.3916,	best estimator xgboost's best error=2.3916
[flaml.automl: 09-18 22:18:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:18:15] {3072} INFO -  at 29.3s,	estimator xgboost's best error=2.3916,	best estimator xgboost's best error=2.3916
[flaml.automl: 09-18 22:18:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:18:19] {3072} INFO -  at 33.9s,	estimator xgboost's best error=2.3916,	best estimator xgboost's best error=2.3916
[flaml.automl: 09-18 22:18:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:18:21] {3072} INFO -  at 36.1s,	estimator xgboost's best error=2.3916,	best estimator xgboost's best error=2.3916
[flaml.automl: 09-18 22:18:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:18:26] {3072} INFO -  at 40.6s,	estimator xgboost's best error=2.3916,	best estimator xgboost's best error=2.3916
[flaml.automl: 09-18 22:18:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:18:28] {3072} INFO -  at 42.7s,	estimator xgboost's best error=2.3916,	best estimator xgboost's best error=2.3916
[flaml.automl: 09-18 22:18:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:18:31] {3072} INFO -  at 45.3s,	estimator xgboost's best error=2.3916,	best estimator xgboost's best error=2.3916
[flaml.automl: 09-18 22:18:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:18:43] {3072} INFO -  at 57.8s,	estimator xgboost's best error=2.3916,	best estimator xgboost's best error=2.3916
[flaml.automl: 09-18 22:18:49] {3335} INFO - retrain xgboost for 5.5s
[flaml.automl: 09-18 22:18:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:18:49] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:18:49] {2637} INFO - Time taken to find the best model: 26.255685329437256
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-1.3916304202712313
NO2(0)最好结果：{'pred_time': 3.4991879852450626e-05, 'wall_clock_time': 26.255685329437256, 'metric_for_logging': {'pred_time': 3.4991879852450626e-05}, 'val_loss': 2.3916304202712313, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 3.061615467071533}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7040314936864471
NO2(0)的mse=13.538949522657218
NO2(0)的mae=2.4089930941672604
NO2(0)的mar=0.18588299109194814
总共花费的时间为：63.79
迪庆州
迪庆州没有数据
昌都市
2622A
[flaml.automl: 09-18 22:22:08] {2390} INFO - task = regression
[flaml.automl: 09-18 22:22:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:22:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:22:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:22:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:22:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:22:09] {3025} INFO - Estimated sufficient time budget=11984s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 22:22:09] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.1691,	best estimator xgboost's best error=4.1691
[flaml.automl: 09-18 22:22:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:22:11] {3072} INFO -  at 3.1s,	estimator xgboost's best error=2.9336,	best estimator xgboost's best error=2.9336
[flaml.automl: 09-18 22:22:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:22:12] {3072} INFO -  at 4.3s,	estimator xgboost's best error=2.9336,	best estimator xgboost's best error=2.9336
[flaml.automl: 09-18 22:22:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:22:19] {3072} INFO -  at 11.4s,	estimator xgboost's best error=2.9336,	best estimator xgboost's best error=2.9336
[flaml.automl: 09-18 22:22:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:22:20] {3072} INFO -  at 12.5s,	estimator xgboost's best error=2.7002,	best estimator xgboost's best error=2.7002
[flaml.automl: 09-18 22:22:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:22:22] {3072} INFO -  at 14.1s,	estimator xgboost's best error=2.6599,	best estimator xgboost's best error=2.6599
[flaml.automl: 09-18 22:22:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:22:23] {3072} INFO -  at 15.7s,	estimator xgboost's best error=2.6599,	best estimator xgboost's best error=2.6599
[flaml.automl: 09-18 22:22:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:22:26] {3072} INFO -  at 17.9s,	estimator xgboost's best error=2.6599,	best estimator xgboost's best error=2.6599
[flaml.automl: 09-18 22:22:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:22:27] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.6599,	best estimator xgboost's best error=2.6599
[flaml.automl: 09-18 22:22:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:22:29] {3072} INFO -  at 21.5s,	estimator xgboost's best error=2.6599,	best estimator xgboost's best error=2.6599
[flaml.automl: 09-18 22:22:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:22:30] {3072} INFO -  at 22.7s,	estimator xgboost's best error=2.6599,	best estimator xgboost's best error=2.6599
[flaml.automl: 09-18 22:22:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:22:32] {3072} INFO -  at 24.3s,	estimator xgboost's best error=2.6599,	best estimator xgboost's best error=2.6599
[flaml.automl: 09-18 22:22:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:22:42] {3072} INFO -  at 34.7s,	estimator xgboost's best error=2.6597,	best estimator xgboost's best error=2.6597
[flaml.automl: 09-18 22:22:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:23:00] {3072} INFO -  at 51.9s,	estimator xgboost's best error=2.5595,	best estimator xgboost's best error=2.5595
[flaml.automl: 09-18 22:23:16] {3335} INFO - retrain xgboost for 16.4s
[flaml.automl: 09-18 22:23:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:23:16] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:23:16] {2637} INFO - Time taken to find the best model: 51.94580674171448
[flaml.automl: 09-18 22:23:16] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.5595385640020103
NO2(0)最好结果：{'pred_time': 6.1004144057026555e-05, 'wall_clock_time': 51.94580674171448, 'metric_for_logging': {'pred_time': 6.1004144057026555e-05}, 'val_loss': 2.5595385640020103, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 17, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 17.22681450843811}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.5427183823699526
NO2(0)的mse=19.385825988661924
NO2(0)的mae=2.742465907077676
NO2(0)的mar=0.6320614329821849
总共花费的时间为：68.61
山南市
2624A
2625A
[flaml.automl: 09-18 22:30:01] {2390} INFO - task = regression
[flaml.automl: 09-18 22:30:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:30:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:30:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:30:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:30:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:30:02] {3025} INFO - Estimated sufficient time budget=11959s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 22:30:02] {3072} INFO -  at 1.3s,	estimator xgboost's best error=8.3363,	best estimator xgboost's best error=8.3363
[flaml.automl: 09-18 22:30:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:30:04] {3072} INFO -  at 3.4s,	estimator xgboost's best error=4.1220,	best estimator xgboost's best error=4.1220
[flaml.automl: 09-18 22:30:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:30:05] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.1220,	best estimator xgboost's best error=4.1220
[flaml.automl: 09-18 22:30:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:30:18] {3072} INFO -  at 17.1s,	estimator xgboost's best error=4.1220,	best estimator xgboost's best error=4.1220
[flaml.automl: 09-18 22:30:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:30:20] {3072} INFO -  at 19.2s,	estimator xgboost's best error=2.5051,	best estimator xgboost's best error=2.5051
[flaml.automl: 09-18 22:30:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:30:23] {3072} INFO -  at 22.2s,	estimator xgboost's best error=2.3594,	best estimator xgboost's best error=2.3594
[flaml.automl: 09-18 22:30:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:30:26] {3072} INFO -  at 25.2s,	estimator xgboost's best error=2.2483,	best estimator xgboost's best error=2.2483
[flaml.automl: 09-18 22:30:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:30:30] {3072} INFO -  at 29.7s,	estimator xgboost's best error=2.2483,	best estimator xgboost's best error=2.2483
[flaml.automl: 09-18 22:30:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:30:33] {3072} INFO -  at 32.7s,	estimator xgboost's best error=2.2483,	best estimator xgboost's best error=2.2483
[flaml.automl: 09-18 22:30:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:30:39] {3072} INFO -  at 38.3s,	estimator xgboost's best error=2.2189,	best estimator xgboost's best error=2.2189
[flaml.automl: 09-18 22:30:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:30:42] {3072} INFO -  at 41.4s,	estimator xgboost's best error=2.2189,	best estimator xgboost's best error=2.2189
[flaml.automl: 09-18 22:30:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:30:44] {3072} INFO -  at 43.4s,	estimator xgboost's best error=2.2189,	best estimator xgboost's best error=2.2189
[flaml.automl: 09-18 22:30:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:31:00] {3072} INFO -  at 59.2s,	estimator xgboost's best error=2.2180,	best estimator xgboost's best error=2.2180
[flaml.automl: 09-18 22:31:22] {3335} INFO - retrain xgboost for 22.0s
[flaml.automl: 09-18 22:31:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:31:22] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:31:22] {2637} INFO - Time taken to find the best model: 59.16248869895935
[flaml.automl: 09-18 22:31:22] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
NO2(0)最佳损失：-1.2179519765523308
NO2(0)最好结果：{'pred_time': 2.7418648005043268e-05, 'wall_clock_time': 59.16248869895935, 'metric_for_logging': {'pred_time': 2.7418648005043268e-05}, 'val_loss': 2.217951976552331, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 15.723732948303223}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8402970076008117
NO2(0)的mse=10.830102356931238
NO2(0)的mae=2.23501156089849
NO2(0)的mar=0.21684201770445816
总共花费的时间为：81.57
日喀则市
2626A
[flaml.automl: 09-18 22:35:07] {2390} INFO - task = regression
[flaml.automl: 09-18 22:35:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:35:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:35:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:35:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:35:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:35:09] {3025} INFO - Estimated sufficient time budget=21902s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 22:35:09] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.3828,	best estimator xgboost's best error=5.3828
[flaml.automl: 09-18 22:35:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:35:13] {3072} INFO -  at 5.7s,	estimator xgboost's best error=3.1687,	best estimator xgboost's best error=3.1687
[flaml.automl: 09-18 22:35:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:35:15] {3072} INFO -  at 7.9s,	estimator xgboost's best error=3.1687,	best estimator xgboost's best error=3.1687
[flaml.automl: 09-18 22:35:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:35:28] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.1687,	best estimator xgboost's best error=3.1687
[flaml.automl: 09-18 22:35:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:35:30] {3072} INFO -  at 23.1s,	estimator xgboost's best error=2.5921,	best estimator xgboost's best error=2.5921
[flaml.automl: 09-18 22:35:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:35:33] {3072} INFO -  at 26.2s,	estimator xgboost's best error=2.5921,	best estimator xgboost's best error=2.5921
[flaml.automl: 09-18 22:35:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:35:36] {3072} INFO -  at 29.4s,	estimator xgboost's best error=2.5723,	best estimator xgboost's best error=2.5723
[flaml.automl: 09-18 22:35:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:35:41] {3072} INFO -  at 33.7s,	estimator xgboost's best error=2.5723,	best estimator xgboost's best error=2.5723
[flaml.automl: 09-18 22:35:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:35:44] {3072} INFO -  at 36.8s,	estimator xgboost's best error=2.5723,	best estimator xgboost's best error=2.5723
[flaml.automl: 09-18 22:35:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:35:49] {3072} INFO -  at 41.6s,	estimator xgboost's best error=2.5704,	best estimator xgboost's best error=2.5704
[flaml.automl: 09-18 22:35:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:35:52] {3072} INFO -  at 44.8s,	estimator xgboost's best error=2.5704,	best estimator xgboost's best error=2.5704
[flaml.automl: 09-18 22:35:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:35:54] {3072} INFO -  at 46.8s,	estimator xgboost's best error=2.5704,	best estimator xgboost's best error=2.5704
[flaml.automl: 09-18 22:35:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:36:06] {3072} INFO -  at 58.8s,	estimator xgboost's best error=2.5704,	best estimator xgboost's best error=2.5704
[flaml.automl: 09-18 22:36:10] {3335} INFO - retrain xgboost for 4.8s
[flaml.automl: 09-18 22:36:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:36:10] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:36:10] {2637} INFO - Time taken to find the best model: 41.640373945236206
NO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}
NO2(0)最佳损失：-1.5704296230768535
NO2(0)最好结果：{'pred_time': 6.794147475413458e-05, 'wall_clock_time': 41.640373945236206, 'metric_for_logging': {'pred_time': 6.794147475413458e-05}, 'val_loss': 2.5704296230768535, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}, 'config/n_estimators': 6, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'experiment_tag': 'exp', 'time_total_s': 4.858513116836548}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.1740176298407835
NO2(0)的mse=19.36131359812174
NO2(0)的mae=2.4653342284549824
NO2(0)的mar=0.2418882376231283
总共花费的时间为：63.85
那曲地区
2628A
[flaml.automl: 09-18 22:39:08] {2390} INFO - task = regression
[flaml.automl: 09-18 22:39:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:39:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:39:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:39:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:39:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:39:10] {3025} INFO - Estimated sufficient time budget=12097s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 22:39:10] {3072} INFO -  at 1.3s,	estimator xgboost's best error=6.8434,	best estimator xgboost's best error=6.8434
[flaml.automl: 09-18 22:39:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:39:11] {3072} INFO -  at 3.1s,	estimator xgboost's best error=4.0767,	best estimator xgboost's best error=4.0767
[flaml.automl: 09-18 22:39:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:39:13] {3072} INFO -  at 4.3s,	estimator xgboost's best error=4.0767,	best estimator xgboost's best error=4.0767
[flaml.automl: 09-18 22:39:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:39:20] {3072} INFO -  at 11.4s,	estimator xgboost's best error=4.0767,	best estimator xgboost's best error=4.0767
[flaml.automl: 09-18 22:39:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:39:21] {3072} INFO -  at 12.5s,	estimator xgboost's best error=3.0830,	best estimator xgboost's best error=3.0830
[flaml.automl: 09-18 22:39:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:39:22] {3072} INFO -  at 14.1s,	estimator xgboost's best error=3.0830,	best estimator xgboost's best error=3.0830
[flaml.automl: 09-18 22:39:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:39:24] {3072} INFO -  at 15.8s,	estimator xgboost's best error=3.0597,	best estimator xgboost's best error=3.0597
[flaml.automl: 09-18 22:39:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:39:26] {3072} INFO -  at 18.1s,	estimator xgboost's best error=3.0597,	best estimator xgboost's best error=3.0597
[flaml.automl: 09-18 22:39:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:39:28] {3072} INFO -  at 19.7s,	estimator xgboost's best error=3.0597,	best estimator xgboost's best error=3.0597
[flaml.automl: 09-18 22:39:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:39:31] {3072} INFO -  at 22.4s,	estimator xgboost's best error=3.0597,	best estimator xgboost's best error=3.0597
[flaml.automl: 09-18 22:39:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:39:32] {3072} INFO -  at 24.0s,	estimator xgboost's best error=3.0572,	best estimator xgboost's best error=3.0572
[flaml.automl: 09-18 22:39:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:39:33] {3072} INFO -  at 25.2s,	estimator xgboost's best error=3.0572,	best estimator xgboost's best error=3.0572
[flaml.automl: 09-18 22:39:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:39:39] {3072} INFO -  at 31.0s,	estimator xgboost's best error=3.0572,	best estimator xgboost's best error=3.0572
[flaml.automl: 09-18 22:39:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:39:43] {3072} INFO -  at 34.3s,	estimator xgboost's best error=2.9934,	best estimator xgboost's best error=2.9934
[flaml.automl: 09-18 22:39:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 22:39:45] {3072} INFO -  at 36.4s,	estimator xgboost's best error=2.9934,	best estimator xgboost's best error=2.9934
[flaml.automl: 09-18 22:39:45] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 22:39:50] {3072} INFO -  at 41.8s,	estimator xgboost's best error=2.9901,	best estimator xgboost's best error=2.9901
[flaml.automl: 09-18 22:39:50] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 22:39:53] {3072} INFO -  at 45.1s,	estimator xgboost's best error=2.9901,	best estimator xgboost's best error=2.9901
[flaml.automl: 09-18 22:39:53] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 22:39:56] {3072} INFO -  at 47.9s,	estimator xgboost's best error=2.9901,	best estimator xgboost's best error=2.9901
[flaml.automl: 09-18 22:39:56] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 22:40:08] {3072} INFO -  at 59.3s,	estimator xgboost's best error=2.9901,	best estimator xgboost's best error=2.9901
[flaml.automl: 09-18 22:40:13] {3335} INFO - retrain xgboost for 5.4s
[flaml.automl: 09-18 22:40:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.2573870899565567,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=128.0, missing=nan, monotone_constraints='()',
             n_estimators=13, n_jobs=-1, num_parallel_tree=1, random_state=0,
             reg_alpha=0.0009765625, reg_lambda=0.12342936328189684,
             scale_pos_weight=1, subsample=0.9453052099956202,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:40:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:40:13] {2637} INFO - Time taken to find the best model: 41.80369806289673
NO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 7, 'min_child_weight': 128.0, 'learning_rate': 0.2573870899565567, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.12342936328189684}
NO2(0)最佳损失：-1.9900991036156177
NO2(0)最好结果：{'pred_time': 3.373160531106727e-05, 'wall_clock_time': 41.80369806289673, 'metric_for_logging': {'pred_time': 3.373160531106727e-05}, 'val_loss': 2.9900991036156177, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 7, 'min_child_weight': 128.0, 'learning_rate': 0.2573870899565567, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.12342936328189684}, 'config/n_estimators': 13, 'config/max_leaves': 7, 'config/min_child_weight': 128.0, 'config/learning_rate': 0.2573870899565567, 'config/subsample': 0.9453052099956202, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.92006762344246, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.12342936328189684, 'experiment_tag': 'exp', 'time_total_s': 5.439411640167236}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.2573870899565567,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=128.0, missing=nan, monotone_constraints='()',
             n_estimators=13, n_jobs=-1, num_parallel_tree=1, random_state=0,
             reg_alpha=0.0009765625, reg_lambda=0.12342936328189684,
             scale_pos_weight=1, subsample=0.9453052099956202,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.3332178447257592
NO2(0)的mse=23.349049439512754
NO2(0)的mae=3.104088673196965
NO2(0)的mar=0.3060925183855048
总共花费的时间为：64.94
阿里地区
2630A
[flaml.automl: 09-18 22:43:31] {2390} INFO - task = regression
[flaml.automl: 09-18 22:43:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:43:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:43:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:43:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:43:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:43:33] {3025} INFO - Estimated sufficient time budget=22188s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 22:43:33] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.0240,	best estimator xgboost's best error=5.0240
[flaml.automl: 09-18 22:43:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:43:37] {3072} INFO -  at 5.7s,	estimator xgboost's best error=3.2154,	best estimator xgboost's best error=3.2154
[flaml.automl: 09-18 22:43:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:43:39] {3072} INFO -  at 7.9s,	estimator xgboost's best error=3.2154,	best estimator xgboost's best error=3.2154
[flaml.automl: 09-18 22:43:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:43:52] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.2154,	best estimator xgboost's best error=3.2154
[flaml.automl: 09-18 22:43:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:43:54] {3072} INFO -  at 23.1s,	estimator xgboost's best error=2.7603,	best estimator xgboost's best error=2.7603
[flaml.automl: 09-18 22:43:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:43:57] {3072} INFO -  at 26.0s,	estimator xgboost's best error=2.7603,	best estimator xgboost's best error=2.7603
[flaml.automl: 09-18 22:43:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:44:00] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.6969,	best estimator xgboost's best error=2.6969
[flaml.automl: 09-18 22:44:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:44:05] {3072} INFO -  at 33.5s,	estimator xgboost's best error=2.6969,	best estimator xgboost's best error=2.6969
[flaml.automl: 09-18 22:44:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:44:08] {3072} INFO -  at 36.5s,	estimator xgboost's best error=2.6969,	best estimator xgboost's best error=2.6969
[flaml.automl: 09-18 22:44:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:44:13] {3072} INFO -  at 41.3s,	estimator xgboost's best error=2.6274,	best estimator xgboost's best error=2.6274
[flaml.automl: 09-18 22:44:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:44:16] {3072} INFO -  at 44.5s,	estimator xgboost's best error=2.6274,	best estimator xgboost's best error=2.6274
[flaml.automl: 09-18 22:44:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:44:18] {3072} INFO -  at 46.6s,	estimator xgboost's best error=2.6274,	best estimator xgboost's best error=2.6274
[flaml.automl: 09-18 22:44:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:44:27] {3072} INFO -  at 56.3s,	estimator xgboost's best error=2.6274,	best estimator xgboost's best error=2.6274
[flaml.automl: 09-18 22:44:30] {3335} INFO - retrain xgboost for 2.6s
[flaml.automl: 09-18 22:44:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:44:30] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:44:30] {2637} INFO - Time taken to find the best model: 41.336363077163696
NO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}
NO2(0)最佳损失：-1.627397083845295
NO2(0)最好结果：{'pred_time': 6.962501354727692e-05, 'wall_clock_time': 41.336363077163696, 'metric_for_logging': {'pred_time': 6.962501354727692e-05}, 'val_loss': 2.627397083845295, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}, 'config/n_estimators': 6, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'experiment_tag': 'exp', 'time_total_s': 4.850265979766846}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=-0.13331794731715907
NO2(0)的mse=19.07227593653375
NO2(0)的mae=2.7156165954023717
NO2(0)的mar=0.332679656500507
总共花费的时间为：59.07
林芝市
2632A
2633A
[flaml.automl: 09-18 22:51:46] {2390} INFO - task = regression
[flaml.automl: 09-18 22:51:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:51:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:51:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:51:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:51:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:51:47] {3025} INFO - Estimated sufficient time budget=15584s. Estimated necessary time budget=16s.
[flaml.automl: 09-18 22:51:47] {3072} INFO -  at 1.7s,	estimator xgboost's best error=3.0568,	best estimator xgboost's best error=3.0568
[flaml.automl: 09-18 22:51:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:51:51] {3072} INFO -  at 5.5s,	estimator xgboost's best error=1.4309,	best estimator xgboost's best error=1.4309
[flaml.automl: 09-18 22:51:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:51:53] {3072} INFO -  at 7.7s,	estimator xgboost's best error=1.4309,	best estimator xgboost's best error=1.4309
[flaml.automl: 09-18 22:51:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:52:11] {3072} INFO -  at 25.3s,	estimator xgboost's best error=1.4309,	best estimator xgboost's best error=1.4309
[flaml.automl: 09-18 22:52:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:52:13] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.8017,	best estimator xgboost's best error=0.8017
[flaml.automl: 09-18 22:52:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:52:16] {3072} INFO -  at 30.4s,	estimator xgboost's best error=0.7197,	best estimator xgboost's best error=0.7197
[flaml.automl: 09-18 22:52:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:52:19] {3072} INFO -  at 33.5s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-18 22:52:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:52:23] {3072} INFO -  at 37.9s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-18 22:52:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:52:26] {3072} INFO -  at 40.9s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-18 22:52:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:52:32] {3072} INFO -  at 46.5s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-18 22:52:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:52:35] {3072} INFO -  at 49.7s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-18 22:52:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:52:37] {3072} INFO -  at 51.8s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-18 22:52:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:52:45] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-18 22:52:48] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-18 22:52:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:52:48] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:52:48] {2637} INFO - Time taken to find the best model: 33.4817750453949
NO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
NO2(0)最佳损失：0.2957845659906676
NO2(0)最好结果：{'pred_time': 3.572191510881696e-05, 'wall_clock_time': 33.4817750453949, 'metric_for_logging': {'pred_time': 3.572191510881696e-05}, 'val_loss': 0.7042154340093324, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 3.097564935684204}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.4700789753508666
NO2(0)的mse=1.2194499095896743
NO2(0)的mae=0.6939287851703962
NO2(0)的mar=0.14209666613131608
总共花费的时间为：62.80
汉中市
2634A
2635A
2636A
2637A
[flaml.automl: 09-18 23:06:28] {2390} INFO - task = regression
[flaml.automl: 09-18 23:06:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:06:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:06:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:06:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:06:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:06:29] {3025} INFO - Estimated sufficient time budget=50139s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 23:06:29] {3072} INFO -  at 1.4s,	estimator xgboost's best error=12.0063,	best estimator xgboost's best error=12.0063
[flaml.automl: 09-18 23:06:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:06:32] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.8017,	best estimator xgboost's best error=5.8017
[flaml.automl: 09-18 23:06:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:06:33] {3072} INFO -  at 4.8s,	estimator xgboost's best error=5.8017,	best estimator xgboost's best error=5.8017
[flaml.automl: 09-18 23:06:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:06:39] {3072} INFO -  at 11.1s,	estimator xgboost's best error=5.8017,	best estimator xgboost's best error=5.8017
[flaml.automl: 09-18 23:06:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:06:40] {3072} INFO -  at 12.3s,	estimator xgboost's best error=3.9546,	best estimator xgboost's best error=3.9546
[flaml.automl: 09-18 23:06:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:06:42] {3072} INFO -  at 13.9s,	estimator xgboost's best error=3.4813,	best estimator xgboost's best error=3.4813
[flaml.automl: 09-18 23:06:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:06:43] {3072} INFO -  at 15.5s,	estimator xgboost's best error=3.4813,	best estimator xgboost's best error=3.4813
[flaml.automl: 09-18 23:06:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:06:46] {3072} INFO -  at 18.0s,	estimator xgboost's best error=3.4813,	best estimator xgboost's best error=3.4813
[flaml.automl: 09-18 23:06:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:06:47] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.4813,	best estimator xgboost's best error=3.4813
[flaml.automl: 09-18 23:06:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:06:50] {3072} INFO -  at 21.8s,	estimator xgboost's best error=3.4813,	best estimator xgboost's best error=3.4813
[flaml.automl: 09-18 23:06:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:06:51] {3072} INFO -  at 23.4s,	estimator xgboost's best error=3.4580,	best estimator xgboost's best error=3.4580
[flaml.automl: 09-18 23:06:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:06:52] {3072} INFO -  at 24.5s,	estimator xgboost's best error=3.4580,	best estimator xgboost's best error=3.4580
[flaml.automl: 09-18 23:06:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:06:59] {3072} INFO -  at 31.1s,	estimator xgboost's best error=3.3329,	best estimator xgboost's best error=3.3329
[flaml.automl: 09-18 23:06:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 23:07:11] {3072} INFO -  at 43.2s,	estimator xgboost's best error=3.2284,	best estimator xgboost's best error=3.2284
[flaml.automl: 09-18 23:07:11] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 23:07:18] {3072} INFO -  at 49.7s,	estimator xgboost's best error=3.2284,	best estimator xgboost's best error=3.2284
[flaml.automl: 09-18 23:07:30] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 23:07:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:07:30] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:07:30] {2637} INFO - Time taken to find the best model: 43.165852546691895
[flaml.automl: 09-18 23:07:30] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41241}
NO2(0)最佳损失：-2.2284156884425026
NO2(0)最好结果：{'pred_time': 8.813475979849315e-06, 'wall_clock_time': 43.165852546691895, 'metric_for_logging': {'pred_time': 8.813475979849315e-06}, 'val_loss': 3.2284156884425026, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41241}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41241, 'experiment_tag': 'exp', 'time_total_s': 12.09098219871521}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.819278253475431
NO2(0)的mse=24.805720306926958
NO2(0)的mae=3.3434248061416096
NO2(0)的mar=0.20011924327179303
总共花费的时间为：62.49
榆林市
2638A
2639A
2640A
2641A
[flaml.automl: 09-18 23:20:13] {2390} INFO - task = regression
[flaml.automl: 09-18 23:20:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:20:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:20:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:20:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:20:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:20:14] {3025} INFO - Estimated sufficient time budget=50478s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 23:20:14] {3072} INFO -  at 1.4s,	estimator xgboost's best error=21.2325,	best estimator xgboost's best error=21.2325
[flaml.automl: 09-18 23:20:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:20:16] {3072} INFO -  at 3.5s,	estimator xgboost's best error=10.5881,	best estimator xgboost's best error=10.5881
[flaml.automl: 09-18 23:20:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:20:17] {3072} INFO -  at 4.7s,	estimator xgboost's best error=10.5881,	best estimator xgboost's best error=10.5881
[flaml.automl: 09-18 23:20:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:20:23] {3072} INFO -  at 11.1s,	estimator xgboost's best error=10.5881,	best estimator xgboost's best error=10.5881
[flaml.automl: 09-18 23:20:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:20:25] {3072} INFO -  at 12.3s,	estimator xgboost's best error=7.3054,	best estimator xgboost's best error=7.3054
[flaml.automl: 09-18 23:20:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:20:26] {3072} INFO -  at 13.8s,	estimator xgboost's best error=6.3545,	best estimator xgboost's best error=6.3545
[flaml.automl: 09-18 23:20:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:20:28] {3072} INFO -  at 15.5s,	estimator xgboost's best error=6.3545,	best estimator xgboost's best error=6.3545
[flaml.automl: 09-18 23:20:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:20:30] {3072} INFO -  at 17.9s,	estimator xgboost's best error=6.3545,	best estimator xgboost's best error=6.3545
[flaml.automl: 09-18 23:20:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:20:31] {3072} INFO -  at 19.1s,	estimator xgboost's best error=6.3545,	best estimator xgboost's best error=6.3545
[flaml.automl: 09-18 23:20:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:20:34] {3072} INFO -  at 21.7s,	estimator xgboost's best error=6.3545,	best estimator xgboost's best error=6.3545
[flaml.automl: 09-18 23:20:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:20:36] {3072} INFO -  at 23.3s,	estimator xgboost's best error=6.3545,	best estimator xgboost's best error=6.3545
[flaml.automl: 09-18 23:20:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:20:37] {3072} INFO -  at 24.5s,	estimator xgboost's best error=6.3545,	best estimator xgboost's best error=6.3545
[flaml.automl: 09-18 23:20:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:20:43] {3072} INFO -  at 31.0s,	estimator xgboost's best error=6.1129,	best estimator xgboost's best error=6.1129
[flaml.automl: 09-18 23:20:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 23:20:55] {3072} INFO -  at 43.1s,	estimator xgboost's best error=5.8875,	best estimator xgboost's best error=5.8875
[flaml.automl: 09-18 23:20:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 23:21:04] {3072} INFO -  at 51.5s,	estimator xgboost's best error=5.8875,	best estimator xgboost's best error=5.8875
[flaml.automl: 09-18 23:21:26] {3335} INFO - retrain xgboost for 22.4s
[flaml.automl: 09-18 23:21:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:21:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:21:26] {2637} INFO - Time taken to find the best model: 43.08667349815369
[flaml.automl: 09-18 23:21:26] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42272}
NO2(0)最佳损失：-4.887482433337365
NO2(0)最好结果：{'pred_time': 8.604235666367203e-06, 'wall_clock_time': 43.08667349815369, 'metric_for_logging': {'pred_time': 8.604235666367203e-06}, 'val_loss': 5.887482433337365, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42272}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42272, 'experiment_tag': 'exp', 'time_total_s': 12.079865217208862}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8293728150573103
NO2(0)的mse=77.3723570100001
NO2(0)的mae=5.787941517238759
NO2(0)的mar=0.2215209986792455
总共花费的时间为：74.67
安康市
2642A
2643A
2644A
[flaml.automl: 09-18 23:31:22] {2390} INFO - task = regression
[flaml.automl: 09-18 23:31:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:31:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:31:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:31:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:31:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:31:24] {3025} INFO - Estimated sufficient time budget=12075s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 23:31:24] {3072} INFO -  at 1.3s,	estimator xgboost's best error=9.2469,	best estimator xgboost's best error=9.2469
[flaml.automl: 09-18 23:31:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:31:26] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.6248,	best estimator xgboost's best error=4.6248
[flaml.automl: 09-18 23:31:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:31:27] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.6248,	best estimator xgboost's best error=4.6248
[flaml.automl: 09-18 23:31:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:31:37] {3072} INFO -  at 14.7s,	estimator xgboost's best error=4.6248,	best estimator xgboost's best error=4.6248
[flaml.automl: 09-18 23:31:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:31:38] {3072} INFO -  at 15.9s,	estimator xgboost's best error=3.2004,	best estimator xgboost's best error=3.2004
[flaml.automl: 09-18 23:31:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:31:40] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.8649,	best estimator xgboost's best error=2.8649
[flaml.automl: 09-18 23:31:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:31:41] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.8649,	best estimator xgboost's best error=2.8649
[flaml.automl: 09-18 23:31:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:31:44] {3072} INFO -  at 21.6s,	estimator xgboost's best error=2.8649,	best estimator xgboost's best error=2.8649
[flaml.automl: 09-18 23:31:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:31:45] {3072} INFO -  at 22.8s,	estimator xgboost's best error=2.8649,	best estimator xgboost's best error=2.8649
[flaml.automl: 09-18 23:31:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:31:48] {3072} INFO -  at 25.5s,	estimator xgboost's best error=2.8649,	best estimator xgboost's best error=2.8649
[flaml.automl: 09-18 23:31:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:31:49] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.8649,	best estimator xgboost's best error=2.8649
[flaml.automl: 09-18 23:31:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:31:50] {3072} INFO -  at 27.8s,	estimator xgboost's best error=2.8649,	best estimator xgboost's best error=2.8649
[flaml.automl: 09-18 23:31:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:31:57] {3072} INFO -  at 34.3s,	estimator xgboost's best error=2.7614,	best estimator xgboost's best error=2.7614
[flaml.automl: 09-18 23:31:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 23:32:09] {3072} INFO -  at 46.4s,	estimator xgboost's best error=2.6560,	best estimator xgboost's best error=2.6560
[flaml.automl: 09-18 23:32:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 23:32:15] {3072} INFO -  at 52.9s,	estimator xgboost's best error=2.6560,	best estimator xgboost's best error=2.6560
[flaml.automl: 09-18 23:32:27] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 23:32:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:32:27] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:32:27] {2637} INFO - Time taken to find the best model: 46.36351418495178
[flaml.automl: 09-18 23:32:27] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-1.6560354429537871
NO2(0)最好结果：{'pred_time': 1.3468295748592864e-05, 'wall_clock_time': 46.36351418495178, 'metric_for_logging': {'pred_time': 1.3468295748592864e-05}, 'val_loss': 2.656035442953787, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.084776163101196}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7990760971977359
NO2(0)的mse=16.043775725539362
NO2(0)的mae=2.6987189303026273
NO2(0)的mar=0.22365820980570306
总共花费的时间为：65.50
商洛市
2645A
[flaml.automl: 09-18 23:36:19] {2390} INFO - task = regression
[flaml.automl: 09-18 23:36:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:36:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:36:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:36:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:36:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:36:20] {3025} INFO - Estimated sufficient time budget=12127s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 23:36:20] {3072} INFO -  at 1.3s,	estimator xgboost's best error=11.2155,	best estimator xgboost's best error=11.2155
[flaml.automl: 09-18 23:36:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:36:22] {3072} INFO -  at 3.1s,	estimator xgboost's best error=6.2214,	best estimator xgboost's best error=6.2214
[flaml.automl: 09-18 23:36:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:36:23] {3072} INFO -  at 4.3s,	estimator xgboost's best error=6.2214,	best estimator xgboost's best error=6.2214
[flaml.automl: 09-18 23:36:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:36:31] {3072} INFO -  at 11.4s,	estimator xgboost's best error=6.2214,	best estimator xgboost's best error=6.2214
[flaml.automl: 09-18 23:36:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:36:32] {3072} INFO -  at 12.6s,	estimator xgboost's best error=3.7854,	best estimator xgboost's best error=3.7854
[flaml.automl: 09-18 23:36:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:36:33] {3072} INFO -  at 14.1s,	estimator xgboost's best error=3.5110,	best estimator xgboost's best error=3.5110
[flaml.automl: 09-18 23:36:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:36:35] {3072} INFO -  at 15.8s,	estimator xgboost's best error=3.5110,	best estimator xgboost's best error=3.5110
[flaml.automl: 09-18 23:36:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:36:37] {3072} INFO -  at 18.0s,	estimator xgboost's best error=3.5110,	best estimator xgboost's best error=3.5110
[flaml.automl: 09-18 23:36:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:36:38] {3072} INFO -  at 19.2s,	estimator xgboost's best error=3.5110,	best estimator xgboost's best error=3.5110
[flaml.automl: 09-18 23:36:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:36:41] {3072} INFO -  at 21.6s,	estimator xgboost's best error=3.5110,	best estimator xgboost's best error=3.5110
[flaml.automl: 09-18 23:36:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:36:42] {3072} INFO -  at 22.8s,	estimator xgboost's best error=3.5110,	best estimator xgboost's best error=3.5110
[flaml.automl: 09-18 23:36:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:36:43] {3072} INFO -  at 23.9s,	estimator xgboost's best error=3.5110,	best estimator xgboost's best error=3.5110
[flaml.automl: 09-18 23:36:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:36:49] {3072} INFO -  at 29.6s,	estimator xgboost's best error=3.5110,	best estimator xgboost's best error=3.5110
[flaml.automl: 09-18 23:36:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 23:36:52] {3072} INFO -  at 32.4s,	estimator xgboost's best error=3.4198,	best estimator xgboost's best error=3.4198
[flaml.automl: 09-18 23:36:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 23:36:53] {3072} INFO -  at 34.0s,	estimator xgboost's best error=3.4198,	best estimator xgboost's best error=3.4198
[flaml.automl: 09-18 23:36:53] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 23:36:57] {3072} INFO -  at 38.3s,	estimator xgboost's best error=3.4198,	best estimator xgboost's best error=3.4198
[flaml.automl: 09-18 23:36:57] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 23:37:00] {3072} INFO -  at 40.7s,	estimator xgboost's best error=3.4198,	best estimator xgboost's best error=3.4198
[flaml.automl: 09-18 23:37:00] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 23:37:02] {3072} INFO -  at 42.8s,	estimator xgboost's best error=3.4198,	best estimator xgboost's best error=3.4198
[flaml.automl: 09-18 23:37:02] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 23:37:08] {3072} INFO -  at 48.7s,	estimator xgboost's best error=3.4198,	best estimator xgboost's best error=3.4198
[flaml.automl: 09-18 23:37:08] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 23:37:09] {3072} INFO -  at 50.3s,	estimator xgboost's best error=3.4198,	best estimator xgboost's best error=3.4198
[flaml.automl: 09-18 23:37:09] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 23:37:17] {3072} INFO -  at 57.7s,	estimator xgboost's best error=3.4198,	best estimator xgboost's best error=3.4198
[flaml.automl: 09-18 23:37:20] {3335} INFO - retrain xgboost for 2.8s
[flaml.automl: 09-18 23:37:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:37:20] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:37:20] {2637} INFO - Time taken to find the best model: 32.39547896385193
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}
NO2(0)最佳损失：-2.4197708318097644
NO2(0)最好结果：{'pred_time': 3.299350880277147e-05, 'wall_clock_time': 32.39547896385193, 'metric_for_logging': {'pred_time': 3.299350880277147e-05}, 'val_loss': 3.4197708318097644, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}, 'config/n_estimators': 9, 'config/max_leaves': 5, 'config/min_child_weight': 0.758814211387459, 'config/learning_rate': 0.29313806474778853, 'config/subsample': 0.7520389032316431, 'config/colsample_bylevel': 0.8985205690300798, 'config/colsample_bytree': 0.7992659131995336, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.2266745236797676, 'experiment_tag': 'exp', 'time_total_s': 2.836587429046631}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6242545788655709
NO2(0)的mse=28.014797499274724
NO2(0)的mae=3.548186336548509
NO2(0)的mar=0.2620740504131936
总共花费的时间为：60.73
白银市
2647A
2648A
[flaml.automl: 09-18 23:44:15] {2390} INFO - task = regression
[flaml.automl: 09-18 23:44:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:44:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:44:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:44:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:44:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:44:17] {3025} INFO - Estimated sufficient time budget=22385s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 23:44:17] {3072} INFO -  at 2.4s,	estimator xgboost's best error=13.1278,	best estimator xgboost's best error=13.1278
[flaml.automl: 09-18 23:44:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:44:21] {3072} INFO -  at 6.3s,	estimator xgboost's best error=6.9258,	best estimator xgboost's best error=6.9258
[flaml.automl: 09-18 23:44:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:44:23] {3072} INFO -  at 8.5s,	estimator xgboost's best error=6.9258,	best estimator xgboost's best error=6.9258
[flaml.automl: 09-18 23:44:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:44:41] {3072} INFO -  at 25.8s,	estimator xgboost's best error=6.9258,	best estimator xgboost's best error=6.9258
[flaml.automl: 09-18 23:44:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:44:43] {3072} INFO -  at 28.0s,	estimator xgboost's best error=5.2046,	best estimator xgboost's best error=5.2046
[flaml.automl: 09-18 23:44:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:44:46] {3072} INFO -  at 30.9s,	estimator xgboost's best error=4.7399,	best estimator xgboost's best error=4.7399
[flaml.automl: 09-18 23:44:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:44:49] {3072} INFO -  at 33.9s,	estimator xgboost's best error=4.7399,	best estimator xgboost's best error=4.7399
[flaml.automl: 09-18 23:44:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:44:53] {3072} INFO -  at 38.5s,	estimator xgboost's best error=4.7399,	best estimator xgboost's best error=4.7399
[flaml.automl: 09-18 23:44:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:44:55] {3072} INFO -  at 40.6s,	estimator xgboost's best error=4.7399,	best estimator xgboost's best error=4.7399
[flaml.automl: 09-18 23:44:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:45:00] {3072} INFO -  at 45.2s,	estimator xgboost's best error=4.7399,	best estimator xgboost's best error=4.7399
[flaml.automl: 09-18 23:45:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:45:02] {3072} INFO -  at 47.4s,	estimator xgboost's best error=4.7399,	best estimator xgboost's best error=4.7399
[flaml.automl: 09-18 23:45:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:45:04] {3072} INFO -  at 49.5s,	estimator xgboost's best error=4.7399,	best estimator xgboost's best error=4.7399
[flaml.automl: 09-18 23:45:04] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:45:14] {3072} INFO -  at 59.4s,	estimator xgboost's best error=4.6225,	best estimator xgboost's best error=4.6225
[flaml.automl: 09-18 23:45:25] {3335} INFO - retrain xgboost for 11.3s
[flaml.automl: 09-18 23:45:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:45:25] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:45:25] {2637} INFO - Time taken to find the best model: 59.44207811355591
[flaml.automl: 09-18 23:45:25] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-3.6225325706157276
NO2(0)最好结果：{'pred_time': 3.595008022545475e-05, 'wall_clock_time': 59.44207811355591, 'metric_for_logging': {'pred_time': 3.595008022545475e-05}, 'val_loss': 4.622532570615728, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.910784244537354}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7083944043657708
NO2(0)的mse=43.15112433492457
NO2(0)的mae=4.55380490064263
NO2(0)的mar=0.29240002220153066
总共花费的时间为：71.24
天水市
2649A
2650A
2651A
[flaml.automl: 09-18 23:55:48] {2390} INFO - task = regression
[flaml.automl: 09-18 23:55:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:55:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:55:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:55:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:55:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:55:50] {3025} INFO - Estimated sufficient time budget=19507s. Estimated necessary time budget=20s.
[flaml.automl: 09-18 23:55:50] {3072} INFO -  at 2.1s,	estimator xgboost's best error=11.9858,	best estimator xgboost's best error=11.9858
[flaml.automl: 09-18 23:55:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:55:54] {3072} INFO -  at 5.6s,	estimator xgboost's best error=6.1285,	best estimator xgboost's best error=6.1285
[flaml.automl: 09-18 23:55:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:55:56] {3072} INFO -  at 7.5s,	estimator xgboost's best error=6.1285,	best estimator xgboost's best error=6.1285
[flaml.automl: 09-18 23:55:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:56:19] {3072} INFO -  at 30.6s,	estimator xgboost's best error=6.1285,	best estimator xgboost's best error=6.1285
[flaml.automl: 09-18 23:56:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:56:22] {3072} INFO -  at 33.7s,	estimator xgboost's best error=4.3025,	best estimator xgboost's best error=4.3025
[flaml.automl: 09-18 23:56:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:56:26] {3072} INFO -  at 37.7s,	estimator xgboost's best error=3.7429,	best estimator xgboost's best error=3.7429
[flaml.automl: 09-18 23:56:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:56:30] {3072} INFO -  at 41.9s,	estimator xgboost's best error=3.7429,	best estimator xgboost's best error=3.7429
[flaml.automl: 09-18 23:56:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:56:37] {3072} INFO -  at 48.7s,	estimator xgboost's best error=3.7429,	best estimator xgboost's best error=3.7429
[flaml.automl: 09-18 23:56:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:56:41] {3072} INFO -  at 52.7s,	estimator xgboost's best error=3.7429,	best estimator xgboost's best error=3.7429
[flaml.automl: 09-18 23:56:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:56:45] {3072} INFO -  at 56.7s,	estimator xgboost's best error=3.7429,	best estimator xgboost's best error=3.7429
[flaml.automl: 09-18 23:56:54] {3335} INFO - retrain xgboost for 9.4s
[flaml.automl: 09-18 23:56:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:56:54] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:56:54] {2637} INFO - Time taken to find the best model: 37.7012619972229
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-2.7428626377454517
NO2(0)最好结果：{'pred_time': 3.327077635832899e-05, 'wall_clock_time': 37.7012619972229, 'metric_for_logging': {'pred_time': 3.327077635832899e-05}, 'val_loss': 3.7428626377454517, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 4.007297515869141}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.8400921910390842
NO2(0)的mse=32.85665469883571
NO2(0)的mae=3.592271456648688
NO2(0)的mar=0.28063042695588897
总共花费的时间为：67.04
武威市
2652A
[flaml.automl: 09-19 00:00:33] {2390} INFO - task = regression
[flaml.automl: 09-19 00:00:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:00:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:00:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:00:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:00:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:00:34] {3025} INFO - Estimated sufficient time budget=11943s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:00:34] {3072} INFO -  at 1.3s,	estimator xgboost's best error=15.0864,	best estimator xgboost's best error=15.0864
[flaml.automl: 09-19 00:00:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:00:36] {3072} INFO -  at 3.1s,	estimator xgboost's best error=8.8829,	best estimator xgboost's best error=8.8829
[flaml.automl: 09-19 00:00:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:00:37] {3072} INFO -  at 4.3s,	estimator xgboost's best error=8.8829,	best estimator xgboost's best error=8.8829
[flaml.automl: 09-19 00:00:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:00:44] {3072} INFO -  at 11.4s,	estimator xgboost's best error=8.8829,	best estimator xgboost's best error=8.8829
[flaml.automl: 09-19 00:00:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:00:46] {3072} INFO -  at 12.5s,	estimator xgboost's best error=6.4082,	best estimator xgboost's best error=6.4082
[flaml.automl: 09-19 00:00:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:00:47] {3072} INFO -  at 14.1s,	estimator xgboost's best error=6.0878,	best estimator xgboost's best error=6.0878
[flaml.automl: 09-19 00:00:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:00:49] {3072} INFO -  at 15.7s,	estimator xgboost's best error=6.0878,	best estimator xgboost's best error=6.0878
[flaml.automl: 09-19 00:00:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:00:51] {3072} INFO -  at 17.9s,	estimator xgboost's best error=6.0878,	best estimator xgboost's best error=6.0878
[flaml.automl: 09-19 00:00:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:00:52] {3072} INFO -  at 19.1s,	estimator xgboost's best error=6.0878,	best estimator xgboost's best error=6.0878
[flaml.automl: 09-19 00:00:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:00:55] {3072} INFO -  at 21.5s,	estimator xgboost's best error=6.0878,	best estimator xgboost's best error=6.0878
[flaml.automl: 09-19 00:00:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:00:56] {3072} INFO -  at 22.6s,	estimator xgboost's best error=6.0878,	best estimator xgboost's best error=6.0878
[flaml.automl: 09-19 00:00:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:00:57] {3072} INFO -  at 23.7s,	estimator xgboost's best error=6.0878,	best estimator xgboost's best error=6.0878
[flaml.automl: 09-19 00:00:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:01:02] {3072} INFO -  at 28.8s,	estimator xgboost's best error=6.0878,	best estimator xgboost's best error=6.0878
[flaml.automl: 09-19 00:01:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:01:03] {3072} INFO -  at 29.9s,	estimator xgboost's best error=5.9981,	best estimator xgboost's best error=5.9981
[flaml.automl: 09-19 00:01:03] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 00:01:04] {3072} INFO -  at 31.5s,	estimator xgboost's best error=5.9981,	best estimator xgboost's best error=5.9981
[flaml.automl: 09-19 00:01:04] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 00:01:09] {3072} INFO -  at 35.6s,	estimator xgboost's best error=5.9627,	best estimator xgboost's best error=5.9627
[flaml.automl: 09-19 00:01:09] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 00:01:11] {3072} INFO -  at 38.2s,	estimator xgboost's best error=5.9627,	best estimator xgboost's best error=5.9627
[flaml.automl: 09-19 00:01:11] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-19 00:01:13] {3072} INFO -  at 40.2s,	estimator xgboost's best error=5.9627,	best estimator xgboost's best error=5.9627
[flaml.automl: 09-19 00:01:13] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-19 00:01:22] {3072} INFO -  at 49.0s,	estimator xgboost's best error=5.9627,	best estimator xgboost's best error=5.9627
[flaml.automl: 09-19 00:01:22] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-19 00:01:24] {3072} INFO -  at 50.5s,	estimator xgboost's best error=5.9627,	best estimator xgboost's best error=5.9627
[flaml.automl: 09-19 00:01:24] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-19 00:01:33] {3072} INFO -  at 59.6s,	estimator xgboost's best error=5.9627,	best estimator xgboost's best error=5.9627
[flaml.automl: 09-19 00:01:37] {3335} INFO - retrain xgboost for 4.2s
[flaml.automl: 09-19 00:01:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=8,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:01:37] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:01:37] {2637} INFO - Time taken to find the best model: 35.56573009490967
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 8, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}
NO2(0)最佳损失：-4.9626742388792096
NO2(0)最好结果：{'pred_time': 3.333627038700565e-05, 'wall_clock_time': 35.56573009490967, 'metric_for_logging': {'pred_time': 3.333627038700565e-05}, 'val_loss': 5.9626742388792096, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 8, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}, 'config/n_estimators': 9, 'config/max_leaves': 8, 'config/min_child_weight': 3.029166996333365, 'config/learning_rate': 0.28782051713764356, 'config/subsample': 0.8051874633193139, 'config/colsample_bylevel': 0.656637827405616, 'config/colsample_bytree': 0.7193335366419935, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 2.3848121277079324, 'experiment_tag': 'exp', 'time_total_s': 4.113028049468994}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=8,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.44617999199301817
NO2(0)的mse=90.27161907304603
NO2(0)的mae=6.494782303329184
NO2(0)的mar=0.3860254632480883
总共花费的时间为：64.18
张掖市
2654A
2655A
[flaml.automl: 09-19 00:09:05] {2390} INFO - task = regression
[flaml.automl: 09-19 00:09:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:09:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:09:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:09:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:09:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:09:07] {3025} INFO - Estimated sufficient time budget=22972s. Estimated necessary time budget=23s.
[flaml.automl: 09-19 00:09:07] {3072} INFO -  at 2.4s,	estimator xgboost's best error=12.6488,	best estimator xgboost's best error=12.6488
[flaml.automl: 09-19 00:09:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:09:11] {3072} INFO -  at 6.3s,	estimator xgboost's best error=6.1973,	best estimator xgboost's best error=6.1973
[flaml.automl: 09-19 00:09:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:09:13] {3072} INFO -  at 8.5s,	estimator xgboost's best error=6.1973,	best estimator xgboost's best error=6.1973
[flaml.automl: 09-19 00:09:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:09:26] {3072} INFO -  at 21.2s,	estimator xgboost's best error=6.1973,	best estimator xgboost's best error=6.1973
[flaml.automl: 09-19 00:09:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:09:27] {3072} INFO -  at 22.3s,	estimator xgboost's best error=4.4669,	best estimator xgboost's best error=4.4669
[flaml.automl: 09-19 00:09:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:09:29] {3072} INFO -  at 23.9s,	estimator xgboost's best error=4.1260,	best estimator xgboost's best error=4.1260
[flaml.automl: 09-19 00:09:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:09:30] {3072} INFO -  at 25.5s,	estimator xgboost's best error=4.1260,	best estimator xgboost's best error=4.1260
[flaml.automl: 09-19 00:09:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:09:33] {3072} INFO -  at 28.0s,	estimator xgboost's best error=4.1260,	best estimator xgboost's best error=4.1260
[flaml.automl: 09-19 00:09:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:09:34] {3072} INFO -  at 29.1s,	estimator xgboost's best error=4.1260,	best estimator xgboost's best error=4.1260
[flaml.automl: 09-19 00:09:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:09:36] {3072} INFO -  at 31.6s,	estimator xgboost's best error=4.1260,	best estimator xgboost's best error=4.1260
[flaml.automl: 09-19 00:09:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:09:38] {3072} INFO -  at 32.7s,	estimator xgboost's best error=4.1260,	best estimator xgboost's best error=4.1260
[flaml.automl: 09-19 00:09:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:09:39] {3072} INFO -  at 33.8s,	estimator xgboost's best error=4.1260,	best estimator xgboost's best error=4.1260
[flaml.automl: 09-19 00:09:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:09:45] {3072} INFO -  at 39.9s,	estimator xgboost's best error=4.1260,	best estimator xgboost's best error=4.1260
[flaml.automl: 09-19 00:09:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:09:48] {3072} INFO -  at 42.7s,	estimator xgboost's best error=4.0479,	best estimator xgboost's best error=4.0479
[flaml.automl: 09-19 00:09:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 00:09:49] {3072} INFO -  at 44.3s,	estimator xgboost's best error=4.0479,	best estimator xgboost's best error=4.0479
[flaml.automl: 09-19 00:09:49] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 00:09:54] {3072} INFO -  at 49.1s,	estimator xgboost's best error=3.9941,	best estimator xgboost's best error=3.9941
[flaml.automl: 09-19 00:09:54] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 00:09:57] {3072} INFO -  at 52.0s,	estimator xgboost's best error=3.9941,	best estimator xgboost's best error=3.9941
[flaml.automl: 09-19 00:09:57] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-19 00:09:59] {3072} INFO -  at 54.1s,	estimator xgboost's best error=3.9941,	best estimator xgboost's best error=3.9941
[flaml.automl: 09-19 00:09:59] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-19 00:10:04] {3072} INFO -  at 59.6s,	estimator xgboost's best error=3.9941,	best estimator xgboost's best error=3.9941
[flaml.automl: 09-19 00:10:09] {3335} INFO - retrain xgboost for 4.8s
[flaml.automl: 09-19 00:10:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:10:09] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:10:09] {2637} INFO - Time taken to find the best model: 49.13951921463013
[flaml.automl: 09-19 00:10:09] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}
NO2(0)最佳损失：-2.994083841339948
NO2(0)最好结果：{'pred_time': 1.6586891206507945e-05, 'wall_clock_time': 49.13951921463013, 'metric_for_logging': {'pred_time': 1.6586891206507945e-05}, 'val_loss': 3.994083841339948, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}, 'config/n_estimators': 9, 'config/max_leaves': 9, 'config/min_child_weight': 3.029166996333365, 'config/learning_rate': 0.28782051713764356, 'config/subsample': 0.8051874633193139, 'config/colsample_bylevel': 0.656637827405616, 'config/colsample_bytree': 0.7193335366419935, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 2.3848121277079324, 'experiment_tag': 'exp', 'time_total_s': 4.829793453216553}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.5304751970122313
NO2(0)的mse=37.14678174929022
NO2(0)的mae=3.956730209825793
NO2(0)的mar=0.2095079756347289
总共花费的时间为：64.81
平凉市
2656A
2657A
[flaml.automl: 09-19 00:17:15] {2390} INFO - task = regression
[flaml.automl: 09-19 00:17:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:17:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:17:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:17:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:17:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:17:16] {3025} INFO - Estimated sufficient time budget=11965s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:17:16] {3072} INFO -  at 1.4s,	estimator xgboost's best error=20.3009,	best estimator xgboost's best error=20.3009
[flaml.automl: 09-19 00:17:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:17:18] {3072} INFO -  at 3.4s,	estimator xgboost's best error=10.2140,	best estimator xgboost's best error=10.2140
[flaml.automl: 09-19 00:17:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:17:19] {3072} INFO -  at 4.6s,	estimator xgboost's best error=10.2140,	best estimator xgboost's best error=10.2140
[flaml.automl: 09-19 00:17:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:17:29] {3072} INFO -  at 14.0s,	estimator xgboost's best error=10.2140,	best estimator xgboost's best error=10.2140
[flaml.automl: 09-19 00:17:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:17:30] {3072} INFO -  at 15.1s,	estimator xgboost's best error=7.1084,	best estimator xgboost's best error=7.1084
[flaml.automl: 09-19 00:17:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:17:31] {3072} INFO -  at 16.7s,	estimator xgboost's best error=6.4568,	best estimator xgboost's best error=6.4568
[flaml.automl: 09-19 00:17:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:17:33] {3072} INFO -  at 18.3s,	estimator xgboost's best error=6.4568,	best estimator xgboost's best error=6.4568
[flaml.automl: 09-19 00:17:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:17:35] {3072} INFO -  at 20.7s,	estimator xgboost's best error=6.4568,	best estimator xgboost's best error=6.4568
[flaml.automl: 09-19 00:17:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:17:36] {3072} INFO -  at 21.9s,	estimator xgboost's best error=6.4568,	best estimator xgboost's best error=6.4568
[flaml.automl: 09-19 00:17:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:17:39] {3072} INFO -  at 24.3s,	estimator xgboost's best error=6.4568,	best estimator xgboost's best error=6.4568
[flaml.automl: 09-19 00:17:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:17:40] {3072} INFO -  at 25.5s,	estimator xgboost's best error=6.4568,	best estimator xgboost's best error=6.4568
[flaml.automl: 09-19 00:17:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:17:41] {3072} INFO -  at 26.6s,	estimator xgboost's best error=6.4568,	best estimator xgboost's best error=6.4568
[flaml.automl: 09-19 00:17:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:17:47] {3072} INFO -  at 32.7s,	estimator xgboost's best error=6.2845,	best estimator xgboost's best error=6.2845
[flaml.automl: 09-19 00:17:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:17:58] {3072} INFO -  at 43.1s,	estimator xgboost's best error=6.1848,	best estimator xgboost's best error=6.1848
[flaml.automl: 09-19 00:17:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 00:18:04] {3072} INFO -  at 49.2s,	estimator xgboost's best error=6.1848,	best estimator xgboost's best error=6.1848
[flaml.automl: 09-19 00:18:04] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 00:18:14] {3072} INFO -  at 59.8s,	estimator xgboost's best error=6.1373,	best estimator xgboost's best error=6.1373
[flaml.automl: 09-19 00:18:32] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-19 00:18:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:18:32] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:18:32] {2637} INFO - Time taken to find the best model: 59.77592468261719
[flaml.automl: 09-19 00:18:32] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-5.137332605935624
NO2(0)最好结果：{'pred_time': 1.6827557980405295e-05, 'wall_clock_time': 59.77592468261719, 'metric_for_logging': {'pred_time': 1.6827557980405295e-05}, 'val_loss': 6.137332605935624, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 10.604512691497803}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7341542689880416
NO2(0)的mse=77.51096932557027
NO2(0)的mae=6.004388888516203
NO2(0)的mar=0.24842069713714773
总共花费的时间为：77.55
酒泉市
2658A
2659A
[flaml.automl: 09-19 00:25:45] {2390} INFO - task = regression
[flaml.automl: 09-19 00:25:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:25:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:25:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:25:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:25:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:25:48] {3025} INFO - Estimated sufficient time budget=31567s. Estimated necessary time budget=32s.
[flaml.automl: 09-19 00:25:48] {3072} INFO -  at 3.3s,	estimator xgboost's best error=12.8631,	best estimator xgboost's best error=12.8631
[flaml.automl: 09-19 00:25:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:25:53] {3072} INFO -  at 8.2s,	estimator xgboost's best error=6.8392,	best estimator xgboost's best error=6.8392
[flaml.automl: 09-19 00:25:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:25:55] {3072} INFO -  at 10.4s,	estimator xgboost's best error=6.8392,	best estimator xgboost's best error=6.8392
[flaml.automl: 09-19 00:25:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:26:11] {3072} INFO -  at 26.0s,	estimator xgboost's best error=6.8392,	best estimator xgboost's best error=6.8392
[flaml.automl: 09-19 00:26:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:26:12] {3072} INFO -  at 27.9s,	estimator xgboost's best error=5.5220,	best estimator xgboost's best error=5.5220
[flaml.automl: 09-19 00:26:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:26:15] {3072} INFO -  at 30.0s,	estimator xgboost's best error=5.3005,	best estimator xgboost's best error=5.3005
[flaml.automl: 09-19 00:26:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:26:18] {3072} INFO -  at 33.0s,	estimator xgboost's best error=5.3005,	best estimator xgboost's best error=5.3005
[flaml.automl: 09-19 00:26:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:26:20] {3072} INFO -  at 35.4s,	estimator xgboost's best error=5.3005,	best estimator xgboost's best error=5.3005
[flaml.automl: 09-19 00:26:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:26:21] {3072} INFO -  at 36.5s,	estimator xgboost's best error=5.3005,	best estimator xgboost's best error=5.3005
[flaml.automl: 09-19 00:26:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:26:23] {3072} INFO -  at 38.9s,	estimator xgboost's best error=5.3005,	best estimator xgboost's best error=5.3005
[flaml.automl: 09-19 00:26:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:26:25] {3072} INFO -  at 40.1s,	estimator xgboost's best error=5.3005,	best estimator xgboost's best error=5.3005
[flaml.automl: 09-19 00:26:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:26:26] {3072} INFO -  at 41.2s,	estimator xgboost's best error=5.3005,	best estimator xgboost's best error=5.3005
[flaml.automl: 09-19 00:26:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:26:32] {3072} INFO -  at 47.2s,	estimator xgboost's best error=5.2372,	best estimator xgboost's best error=5.2372
[flaml.automl: 09-19 00:26:32] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:26:42] {3072} INFO -  at 57.6s,	estimator xgboost's best error=5.1399,	best estimator xgboost's best error=5.1399
[flaml.automl: 09-19 00:26:53] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-19 00:26:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:26:53] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:26:53] {2637} INFO - Time taken to find the best model: 57.61662554740906
[flaml.automl: 09-19 00:26:53] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-4.139940709331123
NO2(0)最好结果：{'pred_time': 1.6307791418794857e-05, 'wall_clock_time': 57.61662554740906, 'metric_for_logging': {'pred_time': 1.6307791418794857e-05}, 'val_loss': 5.139940709331123, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.414514303207397}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.555974453669519
NO2(0)的mse=50.29437901348486
NO2(0)的mae=4.904943775687519
NO2(0)的mar=0.2779272116282723
总共花费的时间为：68.76
庆阳市
2662A
[flaml.automl: 09-19 00:30:27] {2390} INFO - task = regression
[flaml.automl: 09-19 00:30:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:30:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:30:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:30:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:30:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:30:30] {3025} INFO - Estimated sufficient time budget=33751s. Estimated necessary time budget=34s.
[flaml.automl: 09-19 00:30:30] {3072} INFO -  at 3.4s,	estimator xgboost's best error=7.4051,	best estimator xgboost's best error=7.4051
[flaml.automl: 09-19 00:30:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:30:35] {3072} INFO -  at 8.7s,	estimator xgboost's best error=4.0798,	best estimator xgboost's best error=4.0798
[flaml.automl: 09-19 00:30:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:30:39] {3072} INFO -  at 11.8s,	estimator xgboost's best error=4.0798,	best estimator xgboost's best error=4.0798
[flaml.automl: 09-19 00:30:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:30:49] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.0798,	best estimator xgboost's best error=4.0798
[flaml.automl: 09-19 00:30:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:30:50] {3072} INFO -  at 23.8s,	estimator xgboost's best error=2.7250,	best estimator xgboost's best error=2.7250
[flaml.automl: 09-19 00:30:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:30:52] {3072} INFO -  at 25.3s,	estimator xgboost's best error=2.6520,	best estimator xgboost's best error=2.6520
[flaml.automl: 09-19 00:30:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:30:54] {3072} INFO -  at 26.8s,	estimator xgboost's best error=2.6352,	best estimator xgboost's best error=2.6352
[flaml.automl: 09-19 00:30:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:30:56] {3072} INFO -  at 29.1s,	estimator xgboost's best error=2.6352,	best estimator xgboost's best error=2.6352
[flaml.automl: 09-19 00:30:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:30:57] {3072} INFO -  at 30.7s,	estimator xgboost's best error=2.6352,	best estimator xgboost's best error=2.6352
[flaml.automl: 09-19 00:30:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:31:00] {3072} INFO -  at 33.3s,	estimator xgboost's best error=2.5700,	best estimator xgboost's best error=2.5700
[flaml.automl: 09-19 00:31:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:31:02] {3072} INFO -  at 34.9s,	estimator xgboost's best error=2.5700,	best estimator xgboost's best error=2.5700
[flaml.automl: 09-19 00:31:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:31:03] {3072} INFO -  at 36.0s,	estimator xgboost's best error=2.5700,	best estimator xgboost's best error=2.5700
[flaml.automl: 09-19 00:31:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:31:12] {3072} INFO -  at 45.6s,	estimator xgboost's best error=2.5700,	best estimator xgboost's best error=2.5700
[flaml.automl: 09-19 00:31:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:31:17] {3072} INFO -  at 50.3s,	estimator xgboost's best error=2.5249,	best estimator xgboost's best error=2.5249
[flaml.automl: 09-19 00:31:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 00:31:20] {3072} INFO -  at 52.9s,	estimator xgboost's best error=2.5249,	best estimator xgboost's best error=2.5249
[flaml.automl: 09-19 00:31:20] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 00:31:27] {3072} INFO -  at 59.9s,	estimator xgboost's best error=2.5249,	best estimator xgboost's best error=2.5249
[flaml.automl: 09-19 00:31:31] {3335} INFO - retrain xgboost for 4.7s
[flaml.automl: 09-19 00:31:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:31:31] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:31:31] {2637} INFO - Time taken to find the best model: 50.32020115852356
[flaml.automl: 09-19 00:31:31] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 6, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}
NO2(0)最佳损失：-1.5249003725680144
NO2(0)最好结果：{'pred_time': 3.292733753049696e-05, 'wall_clock_time': 50.32020115852356, 'metric_for_logging': {'pred_time': 3.292733753049696e-05}, 'val_loss': 2.5249003725680144, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 6, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}, 'config/n_estimators': 13, 'config/max_leaves': 6, 'config/min_child_weight': 0.07186500551974488, 'config/learning_rate': 0.18147476059072173, 'config/subsample': 0.7641710216408443, 'config/colsample_bylevel': 0.8883913141026579, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.14043216453256416, 'experiment_tag': 'exp', 'time_total_s': 4.706571340560913}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.3209535405001104
NO2(0)的mse=14.725748501362903
NO2(0)的mae=2.5423040558501944
NO2(0)的mar=0.22208496602747535
总共花费的时间为：64.80
定西市
2663A
2664A
[flaml.automl: 09-19 00:38:35] {2390} INFO - task = regression
[flaml.automl: 09-19 00:38:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:38:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:38:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:38:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:38:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:38:38] {3025} INFO - Estimated sufficient time budget=33505s. Estimated necessary time budget=34s.
[flaml.automl: 09-19 00:38:38] {3072} INFO -  at 3.5s,	estimator xgboost's best error=14.2598,	best estimator xgboost's best error=14.2598
[flaml.automl: 09-19 00:38:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:38:44] {3072} INFO -  at 9.6s,	estimator xgboost's best error=7.4577,	best estimator xgboost's best error=7.4577
[flaml.automl: 09-19 00:38:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:38:49] {3072} INFO -  at 14.4s,	estimator xgboost's best error=7.4577,	best estimator xgboost's best error=7.4577
[flaml.automl: 09-19 00:38:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:39:18] {3072} INFO -  at 42.9s,	estimator xgboost's best error=7.4577,	best estimator xgboost's best error=7.4577
[flaml.automl: 09-19 00:39:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:39:21] {3072} INFO -  at 45.9s,	estimator xgboost's best error=5.4397,	best estimator xgboost's best error=5.4397
[flaml.automl: 09-19 00:39:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:39:25] {3072} INFO -  at 50.2s,	estimator xgboost's best error=4.8966,	best estimator xgboost's best error=4.8966
[flaml.automl: 09-19 00:39:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:39:29] {3072} INFO -  at 54.7s,	estimator xgboost's best error=4.8966,	best estimator xgboost's best error=4.8966
[flaml.automl: 09-19 00:39:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:39:33] {3072} INFO -  at 58.2s,	estimator xgboost's best error=4.8966,	best estimator xgboost's best error=4.8966
[flaml.automl: 09-19 00:39:37] {3335} INFO - retrain xgboost for 3.5s
[flaml.automl: 09-19 00:39:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:39:37] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:39:37] {2637} INFO - Time taken to find the best model: 50.151100397109985
[flaml.automl: 09-19 00:39:37] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-3.896639201892513
NO2(0)最好结果：{'pred_time': 3.71280438497901e-05, 'wall_clock_time': 50.151100397109985, 'metric_for_logging': {'pred_time': 3.71280438497901e-05}, 'val_loss': 4.896639201892513, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 4.243673086166382}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.6929288382342756
NO2(0)的mse=54.75566118681461
NO2(0)的mae=5.0447495408037035
NO2(0)的mar=0.34813347282341667
总共花费的时间为：62.16
陇南市
2665A
3247A
[flaml.automl: 09-19 00:46:46] {2390} INFO - task = regression
[flaml.automl: 09-19 00:46:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:46:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:46:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:46:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:46:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:46:49] {3025} INFO - Estimated sufficient time budget=22656s. Estimated necessary time budget=23s.
[flaml.automl: 09-19 00:46:49] {3072} INFO -  at 2.4s,	estimator xgboost's best error=10.2155,	best estimator xgboost's best error=10.2155
[flaml.automl: 09-19 00:46:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:46:52] {3072} INFO -  at 5.8s,	estimator xgboost's best error=5.8539,	best estimator xgboost's best error=5.8539
[flaml.automl: 09-19 00:46:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:46:54] {3072} INFO -  at 7.9s,	estimator xgboost's best error=5.8539,	best estimator xgboost's best error=5.8539
[flaml.automl: 09-19 00:46:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:47:10] {3072} INFO -  at 23.7s,	estimator xgboost's best error=5.8539,	best estimator xgboost's best error=5.8539
[flaml.automl: 09-19 00:47:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:47:12] {3072} INFO -  at 25.9s,	estimator xgboost's best error=3.8619,	best estimator xgboost's best error=3.8619
[flaml.automl: 09-19 00:47:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:47:15] {3072} INFO -  at 28.9s,	estimator xgboost's best error=3.5067,	best estimator xgboost's best error=3.5067
[flaml.automl: 09-19 00:47:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:47:18] {3072} INFO -  at 32.0s,	estimator xgboost's best error=3.5067,	best estimator xgboost's best error=3.5067
[flaml.automl: 09-19 00:47:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:47:23] {3072} INFO -  at 36.6s,	estimator xgboost's best error=3.5067,	best estimator xgboost's best error=3.5067
[flaml.automl: 09-19 00:47:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:47:24] {3072} INFO -  at 38.2s,	estimator xgboost's best error=3.5067,	best estimator xgboost's best error=3.5067
[flaml.automl: 09-19 00:47:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:47:27] {3072} INFO -  at 40.6s,	estimator xgboost's best error=3.5067,	best estimator xgboost's best error=3.5067
[flaml.automl: 09-19 00:47:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:47:28] {3072} INFO -  at 41.8s,	estimator xgboost's best error=3.5067,	best estimator xgboost's best error=3.5067
[flaml.automl: 09-19 00:47:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:47:29] {3072} INFO -  at 42.9s,	estimator xgboost's best error=3.5067,	best estimator xgboost's best error=3.5067
[flaml.automl: 09-19 00:47:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:47:35] {3072} INFO -  at 48.9s,	estimator xgboost's best error=3.4240,	best estimator xgboost's best error=3.4240
[flaml.automl: 09-19 00:47:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:47:45] {3072} INFO -  at 59.2s,	estimator xgboost's best error=3.3112,	best estimator xgboost's best error=3.3112
[flaml.automl: 09-19 00:47:57] {3335} INFO - retrain xgboost for 11.7s
[flaml.automl: 09-19 00:47:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:47:57] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:47:57] {2637} INFO - Time taken to find the best model: 59.16967487335205
[flaml.automl: 09-19 00:47:57] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.3111928498467735
NO2(0)最好结果：{'pred_time': 2.014991848967796e-05, 'wall_clock_time': 59.16967487335205, 'metric_for_logging': {'pred_time': 2.014991848967796e-05}, 'val_loss': 3.3111928498467735, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.3090238571167}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7821320014986998
NO2(0)的mse=22.98149605048858
NO2(0)的mae=3.2202675965302356
NO2(0)的mar=0.24305456527853733
总共花费的时间为：71.34
临夏回族自治州
2667A
2668A
[flaml.automl: 09-19 00:54:58] {2390} INFO - task = regression
[flaml.automl: 09-19 00:54:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:54:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:54:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:54:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:54:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:54:59] {3025} INFO - Estimated sufficient time budget=12242s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:54:59] {3072} INFO -  at 1.3s,	estimator xgboost's best error=15.2521,	best estimator xgboost's best error=15.2521
[flaml.automl: 09-19 00:54:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:55:01] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.5074,	best estimator xgboost's best error=7.5074
[flaml.automl: 09-19 00:55:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:55:02] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.5074,	best estimator xgboost's best error=7.5074
[flaml.automl: 09-19 00:55:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:55:12] {3072} INFO -  at 14.1s,	estimator xgboost's best error=7.5074,	best estimator xgboost's best error=7.5074
[flaml.automl: 09-19 00:55:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:55:13] {3072} INFO -  at 15.3s,	estimator xgboost's best error=5.2471,	best estimator xgboost's best error=5.2471
[flaml.automl: 09-19 00:55:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:55:14] {3072} INFO -  at 16.9s,	estimator xgboost's best error=4.7779,	best estimator xgboost's best error=4.7779
[flaml.automl: 09-19 00:55:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:55:16] {3072} INFO -  at 18.5s,	estimator xgboost's best error=4.7779,	best estimator xgboost's best error=4.7779
[flaml.automl: 09-19 00:55:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:55:18] {3072} INFO -  at 20.9s,	estimator xgboost's best error=4.7779,	best estimator xgboost's best error=4.7779
[flaml.automl: 09-19 00:55:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:55:20] {3072} INFO -  at 22.1s,	estimator xgboost's best error=4.7779,	best estimator xgboost's best error=4.7779
[flaml.automl: 09-19 00:55:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:55:22] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.7779,	best estimator xgboost's best error=4.7779
[flaml.automl: 09-19 00:55:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:55:23] {3072} INFO -  at 25.7s,	estimator xgboost's best error=4.7779,	best estimator xgboost's best error=4.7779
[flaml.automl: 09-19 00:55:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:55:24] {3072} INFO -  at 26.8s,	estimator xgboost's best error=4.7779,	best estimator xgboost's best error=4.7779
[flaml.automl: 09-19 00:55:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:55:30] {3072} INFO -  at 32.9s,	estimator xgboost's best error=4.6458,	best estimator xgboost's best error=4.6458
[flaml.automl: 09-19 00:55:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:55:41] {3072} INFO -  at 43.4s,	estimator xgboost's best error=4.5392,	best estimator xgboost's best error=4.5392
[flaml.automl: 09-19 00:55:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 00:55:47] {3072} INFO -  at 49.4s,	estimator xgboost's best error=4.5392,	best estimator xgboost's best error=4.5392
[flaml.automl: 09-19 00:55:47] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 00:55:57] {3072} INFO -  at 59.0s,	estimator xgboost's best error=4.4548,	best estimator xgboost's best error=4.4548
[flaml.automl: 09-19 00:56:14] {3335} INFO - retrain xgboost for 17.1s
[flaml.automl: 09-19 00:56:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:56:14] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:56:14] {2637} INFO - Time taken to find the best model: 59.04149103164673
[flaml.automl: 09-19 00:56:14] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-3.454845712996021
NO2(0)最好结果：{'pred_time': 1.6186507706789625e-05, 'wall_clock_time': 59.04149103164673, 'metric_for_logging': {'pred_time': 1.6186507706789625e-05}, 'val_loss': 4.454845712996021, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.587609052658081}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7626333472659685
NO2(0)的mse=43.19128756525103
NO2(0)的mae=4.488399475447985
NO2(0)的mar=0.24051586744336892
总共花费的时间为：76.53
甘南州
2669A
[flaml.automl: 09-19 00:59:51] {2390} INFO - task = regression
[flaml.automl: 09-19 00:59:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:59:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:59:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:59:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:59:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:59:55] {3025} INFO - Estimated sufficient time budget=34897s. Estimated necessary time budget=35s.
[flaml.automl: 09-19 00:59:55] {3072} INFO -  at 3.6s,	estimator xgboost's best error=11.9276,	best estimator xgboost's best error=11.9276
[flaml.automl: 09-19 00:59:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:00:00] {3072} INFO -  at 8.9s,	estimator xgboost's best error=6.9849,	best estimator xgboost's best error=6.9849
[flaml.automl: 09-19 01:00:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:00:04] {3072} INFO -  at 12.4s,	estimator xgboost's best error=6.9849,	best estimator xgboost's best error=6.9849
[flaml.automl: 09-19 01:00:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:00:22] {3072} INFO -  at 30.2s,	estimator xgboost's best error=6.9849,	best estimator xgboost's best error=6.9849
[flaml.automl: 09-19 01:00:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:00:23] {3072} INFO -  at 31.3s,	estimator xgboost's best error=4.5579,	best estimator xgboost's best error=4.5579
[flaml.automl: 09-19 01:00:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:00:24] {3072} INFO -  at 32.9s,	estimator xgboost's best error=4.0652,	best estimator xgboost's best error=4.0652
[flaml.automl: 09-19 01:00:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:00:26] {3072} INFO -  at 34.5s,	estimator xgboost's best error=4.0652,	best estimator xgboost's best error=4.0652
[flaml.automl: 09-19 01:00:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:00:28] {3072} INFO -  at 36.7s,	estimator xgboost's best error=4.0652,	best estimator xgboost's best error=4.0652
[flaml.automl: 09-19 01:00:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:00:29] {3072} INFO -  at 37.8s,	estimator xgboost's best error=4.0652,	best estimator xgboost's best error=4.0652
[flaml.automl: 09-19 01:00:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:00:32] {3072} INFO -  at 40.3s,	estimator xgboost's best error=4.0652,	best estimator xgboost's best error=4.0652
[flaml.automl: 09-19 01:00:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:00:33] {3072} INFO -  at 41.4s,	estimator xgboost's best error=4.0652,	best estimator xgboost's best error=4.0652
[flaml.automl: 09-19 01:00:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:00:34] {3072} INFO -  at 42.5s,	estimator xgboost's best error=4.0652,	best estimator xgboost's best error=4.0652
[flaml.automl: 09-19 01:00:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:00:39] {3072} INFO -  at 48.1s,	estimator xgboost's best error=4.0652,	best estimator xgboost's best error=4.0652
[flaml.automl: 09-19 01:00:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:00:42] {3072} INFO -  at 50.6s,	estimator xgboost's best error=3.9894,	best estimator xgboost's best error=3.9894
[flaml.automl: 09-19 01:00:42] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:00:43] {3072} INFO -  at 52.2s,	estimator xgboost's best error=3.9894,	best estimator xgboost's best error=3.9894
[flaml.automl: 09-19 01:00:43] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 01:00:47] {3072} INFO -  at 56.0s,	estimator xgboost's best error=3.9894,	best estimator xgboost's best error=3.9894
[flaml.automl: 09-19 01:00:47] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 01:00:49] {3072} INFO -  at 58.0s,	estimator xgboost's best error=3.9894,	best estimator xgboost's best error=3.9894
[flaml.automl: 09-19 01:00:52] {3335} INFO - retrain xgboost for 2.5s
[flaml.automl: 09-19 01:00:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:00:52] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:00:52] {2637} INFO - Time taken to find the best model: 50.62527823448181
[flaml.automl: 09-19 01:00:52] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}
NO2(0)最佳损失：-2.989352160692215
NO2(0)最好结果：{'pred_time': 3.760993480682373e-05, 'wall_clock_time': 50.62527823448181, 'metric_for_logging': {'pred_time': 3.760993480682373e-05}, 'val_loss': 3.989352160692215, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}, 'config/n_estimators': 8, 'config/max_leaves': 5, 'config/min_child_weight': 0.758814211387459, 'config/learning_rate': 0.29313806474778853, 'config/subsample': 0.7520389032316431, 'config/colsample_bylevel': 0.8985205690300798, 'config/colsample_bytree': 0.7992659131995336, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.2266745236797676, 'experiment_tag': 'exp', 'time_total_s': 2.5260462760925293}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6699182443861955
NO2(0)的mse=41.64741599883044
NO2(0)的mae=4.146246883266418
NO2(0)的mar=0.32098178654633214
总共花费的时间为：60.78
海东地区
3867A
[flaml.automl: 09-19 01:03:41] {2390} INFO - task = regression
[flaml.automl: 09-19 01:03:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:03:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:03:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:03:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:03:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:03:44] {3025} INFO - Estimated sufficient time budget=22051s. Estimated necessary time budget=22s.
[flaml.automl: 09-19 01:03:44] {3072} INFO -  at 2.2s,	estimator xgboost's best error=11.5764,	best estimator xgboost's best error=11.5764
[flaml.automl: 09-19 01:03:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:03:47] {3072} INFO -  at 5.7s,	estimator xgboost's best error=6.4306,	best estimator xgboost's best error=6.4306
[flaml.automl: 09-19 01:03:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:03:49] {3072} INFO -  at 8.0s,	estimator xgboost's best error=6.4306,	best estimator xgboost's best error=6.4306
[flaml.automl: 09-19 01:03:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:04:00] {3072} INFO -  at 19.2s,	estimator xgboost's best error=6.4306,	best estimator xgboost's best error=6.4306
[flaml.automl: 09-19 01:04:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:04:03] {3072} INFO -  at 21.3s,	estimator xgboost's best error=4.2889,	best estimator xgboost's best error=4.2889
[flaml.automl: 09-19 01:04:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:04:06] {3072} INFO -  at 24.3s,	estimator xgboost's best error=4.1415,	best estimator xgboost's best error=4.1415
[flaml.automl: 09-19 01:04:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:04:08] {3072} INFO -  at 26.9s,	estimator xgboost's best error=4.1385,	best estimator xgboost's best error=4.1385
[flaml.automl: 09-19 01:04:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:04:12] {3072} INFO -  at 30.5s,	estimator xgboost's best error=4.1385,	best estimator xgboost's best error=4.1385
[flaml.automl: 09-19 01:04:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:04:14] {3072} INFO -  at 33.1s,	estimator xgboost's best error=4.0888,	best estimator xgboost's best error=4.0888
[flaml.automl: 09-19 01:04:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:04:18] {3072} INFO -  at 36.6s,	estimator xgboost's best error=4.0888,	best estimator xgboost's best error=4.0888
[flaml.automl: 09-19 01:04:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:04:21] {3072} INFO -  at 39.2s,	estimator xgboost's best error=4.0888,	best estimator xgboost's best error=4.0888
[flaml.automl: 09-19 01:04:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:04:23] {3072} INFO -  at 41.4s,	estimator xgboost's best error=4.0888,	best estimator xgboost's best error=4.0888
[flaml.automl: 09-19 01:04:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:04:31] {3072} INFO -  at 49.4s,	estimator xgboost's best error=4.0888,	best estimator xgboost's best error=4.0888
[flaml.automl: 09-19 01:04:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:04:35] {3072} INFO -  at 54.2s,	estimator xgboost's best error=3.9205,	best estimator xgboost's best error=3.9205
[flaml.automl: 09-19 01:04:35] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:04:39] {3072} INFO -  at 57.2s,	estimator xgboost's best error=3.9205,	best estimator xgboost's best error=3.9205
[flaml.automl: 09-19 01:04:43] {3335} INFO - retrain xgboost for 4.8s
[flaml.automl: 09-19 01:04:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8817302986121539, colsample_bynode=1,
             colsample_bytree=0.8863631583667041, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.45094003731722565,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=0.6775984205695179, missing=nan,
             monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001218053451685995, reg_lambda=2.9771801857606444,
             scale_pos_weight=1, subsample=0.8035619074937755,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 01:04:43] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:04:43] {2637} INFO - Time taken to find the best model: 54.15537452697754
[flaml.automl: 09-19 01:04:43] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 10, 'max_leaves': 4, 'min_child_weight': 0.6775984205695179, 'learning_rate': 0.45094003731722565, 'subsample': 0.8035619074937755, 'colsample_bylevel': 0.8817302986121539, 'colsample_bytree': 0.8863631583667041, 'reg_alpha': 0.001218053451685995, 'reg_lambda': 2.9771801857606444}
NO2(0)最佳损失：-2.9204879984681904
NO2(0)最好结果：{'pred_time': 0.00015435672482015634, 'wall_clock_time': 54.15537452697754, 'metric_for_logging': {'pred_time': 0.00015435672482015634}, 'val_loss': 3.9204879984681904, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 4, 'min_child_weight': 0.6775984205695179, 'learning_rate': 0.45094003731722565, 'subsample': 0.8035619074937755, 'colsample_bylevel': 0.8817302986121539, 'colsample_bytree': 0.8863631583667041, 'reg_alpha': 0.001218053451685995, 'reg_lambda': 2.9771801857606444}, 'config/n_estimators': 10, 'config/max_leaves': 4, 'config/min_child_weight': 0.6775984205695179, 'config/learning_rate': 0.45094003731722565, 'config/subsample': 0.8035619074937755, 'config/colsample_bylevel': 0.8817302986121539, 'config/colsample_bytree': 0.8863631583667041, 'config/reg_alpha': 0.001218053451685995, 'config/reg_lambda': 2.9771801857606444, 'experiment_tag': 'exp', 'time_total_s': 4.781641960144043}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8817302986121539, colsample_bynode=1,
             colsample_bytree=0.8863631583667041, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.45094003731722565,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=0.6775984205695179, missing=nan,
             monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001218053451685995, reg_lambda=2.9771801857606444,
             scale_pos_weight=1, subsample=0.8035619074937755,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.23546716784505706
NO2(0)的mse=31.922959690958063
NO2(0)的mae=3.9187967894507234
NO2(0)的mar=0.24765459437213777
总共花费的时间为：62.23
海北藏族自治州
2671A
[flaml.automl: 09-19 01:08:24] {2390} INFO - task = regression
[flaml.automl: 09-19 01:08:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:08:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:08:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:08:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:08:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:08:25] {3025} INFO - Estimated sufficient time budget=12032s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:08:25] {3072} INFO -  at 1.3s,	estimator xgboost's best error=7.2439,	best estimator xgboost's best error=7.2439
[flaml.automl: 09-19 01:08:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:08:27] {3072} INFO -  at 3.1s,	estimator xgboost's best error=3.9829,	best estimator xgboost's best error=3.9829
[flaml.automl: 09-19 01:08:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:08:28] {3072} INFO -  at 4.3s,	estimator xgboost's best error=3.9829,	best estimator xgboost's best error=3.9829
[flaml.automl: 09-19 01:08:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:08:35] {3072} INFO -  at 11.4s,	estimator xgboost's best error=3.9829,	best estimator xgboost's best error=3.9829
[flaml.automl: 09-19 01:08:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:08:36] {3072} INFO -  at 12.5s,	estimator xgboost's best error=2.4919,	best estimator xgboost's best error=2.4919
[flaml.automl: 09-19 01:08:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:08:38] {3072} INFO -  at 14.1s,	estimator xgboost's best error=2.3309,	best estimator xgboost's best error=2.3309
[flaml.automl: 09-19 01:08:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:08:40] {3072} INFO -  at 15.7s,	estimator xgboost's best error=2.2859,	best estimator xgboost's best error=2.2859
[flaml.automl: 09-19 01:08:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:08:42] {3072} INFO -  at 18.1s,	estimator xgboost's best error=2.2859,	best estimator xgboost's best error=2.2859
[flaml.automl: 09-19 01:08:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:08:44] {3072} INFO -  at 19.7s,	estimator xgboost's best error=2.2859,	best estimator xgboost's best error=2.2859
[flaml.automl: 09-19 01:08:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:08:46] {3072} INFO -  at 22.3s,	estimator xgboost's best error=2.2763,	best estimator xgboost's best error=2.2763
[flaml.automl: 09-19 01:08:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:08:48] {3072} INFO -  at 23.9s,	estimator xgboost's best error=2.2763,	best estimator xgboost's best error=2.2763
[flaml.automl: 09-19 01:08:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:08:49] {3072} INFO -  at 25.1s,	estimator xgboost's best error=2.2763,	best estimator xgboost's best error=2.2763
[flaml.automl: 09-19 01:08:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:08:59] {3072} INFO -  at 34.7s,	estimator xgboost's best error=2.2638,	best estimator xgboost's best error=2.2638
[flaml.automl: 09-19 01:08:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:09:15] {3072} INFO -  at 51.1s,	estimator xgboost's best error=2.1456,	best estimator xgboost's best error=2.1456
[flaml.automl: 09-19 01:09:31] {3335} INFO - retrain xgboost for 16.3s
[flaml.automl: 09-19 01:09:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:09:31] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:09:31] {2637} INFO - Time taken to find the best model: 51.077202796936035
[flaml.automl: 09-19 01:09:31] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 11, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
NO2(0)最佳损失：-1.1456419726622982
NO2(0)最好结果：{'pred_time': 3.8053455872539645e-05, 'wall_clock_time': 51.077202796936035, 'metric_for_logging': {'pred_time': 3.8053455872539645e-05}, 'val_loss': 2.145641972662298, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 11, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 26, 'config/max_leaves': 11, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.373481035232544}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.42767081485213154
NO2(0)的mse=13.458419467388417
NO2(0)的mae=2.296183947736889
NO2(0)的mar=0.20319025636019028
总共花费的时间为：67.67
黄南藏族自治州
2672A
[flaml.automl: 09-19 01:12:31] {2390} INFO - task = regression
[flaml.automl: 09-19 01:12:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:12:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:12:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:12:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:12:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:12:33] {3025} INFO - Estimated sufficient time budget=22317s. Estimated necessary time budget=22s.
[flaml.automl: 09-19 01:12:33] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.6293,	best estimator xgboost's best error=5.6293
[flaml.automl: 09-19 01:12:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:12:37] {3072} INFO -  at 5.7s,	estimator xgboost's best error=3.2279,	best estimator xgboost's best error=3.2279
[flaml.automl: 09-19 01:12:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:12:39] {3072} INFO -  at 7.8s,	estimator xgboost's best error=3.2279,	best estimator xgboost's best error=3.2279
[flaml.automl: 09-19 01:12:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:12:52] {3072} INFO -  at 20.7s,	estimator xgboost's best error=3.2279,	best estimator xgboost's best error=3.2279
[flaml.automl: 09-19 01:12:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:12:54] {3072} INFO -  at 22.8s,	estimator xgboost's best error=2.2760,	best estimator xgboost's best error=2.2760
[flaml.automl: 09-19 01:12:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:12:57] {3072} INFO -  at 25.8s,	estimator xgboost's best error=2.2447,	best estimator xgboost's best error=2.2447
[flaml.automl: 09-19 01:12:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:13:00] {3072} INFO -  at 28.8s,	estimator xgboost's best error=2.2427,	best estimator xgboost's best error=2.2427
[flaml.automl: 09-19 01:13:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:13:03] {3072} INFO -  at 32.0s,	estimator xgboost's best error=2.2427,	best estimator xgboost's best error=2.2427
[flaml.automl: 09-19 01:13:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:13:05] {3072} INFO -  at 33.6s,	estimator xgboost's best error=2.2427,	best estimator xgboost's best error=2.2427
[flaml.automl: 09-19 01:13:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:13:07] {3072} INFO -  at 36.2s,	estimator xgboost's best error=2.1948,	best estimator xgboost's best error=2.1948
[flaml.automl: 09-19 01:13:07] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:13:09] {3072} INFO -  at 37.8s,	estimator xgboost's best error=2.1948,	best estimator xgboost's best error=2.1948
[flaml.automl: 09-19 01:13:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:13:10] {3072} INFO -  at 39.0s,	estimator xgboost's best error=2.1948,	best estimator xgboost's best error=2.1948
[flaml.automl: 09-19 01:13:10] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:13:19] {3072} INFO -  at 47.8s,	estimator xgboost's best error=2.1948,	best estimator xgboost's best error=2.1948
[flaml.automl: 09-19 01:13:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:13:24] {3072} INFO -  at 52.5s,	estimator xgboost's best error=2.1649,	best estimator xgboost's best error=2.1649
[flaml.automl: 09-19 01:13:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:13:26] {3072} INFO -  at 55.1s,	estimator xgboost's best error=2.1649,	best estimator xgboost's best error=2.1649
[flaml.automl: 09-19 01:13:26] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 01:13:31] {3072} INFO -  at 59.8s,	estimator xgboost's best error=2.1649,	best estimator xgboost's best error=2.1649
[flaml.automl: 09-19 01:13:36] {3335} INFO - retrain xgboost for 4.7s
[flaml.automl: 09-19 01:13:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:13:36] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:13:36] {2637} INFO - Time taken to find the best model: 52.461334466934204
[flaml.automl: 09-19 01:13:36] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 6, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}
NO2(0)最佳损失：-1.1649197289030373
NO2(0)最好结果：{'pred_time': 3.5814418914211784e-05, 'wall_clock_time': 52.461334466934204, 'metric_for_logging': {'pred_time': 3.5814418914211784e-05}, 'val_loss': 2.1649197289030373, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 6, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}, 'config/n_estimators': 13, 'config/max_leaves': 6, 'config/min_child_weight': 0.07186500551974488, 'config/learning_rate': 0.18147476059072173, 'config/subsample': 0.7641710216408443, 'config/colsample_bylevel': 0.8883913141026579, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.14043216453256416, 'experiment_tag': 'exp', 'time_total_s': 4.700429916381836}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.004241571751449347
NO2(0)的mse=12.381132509770291
NO2(0)的mae=2.2553486208950857
NO2(0)的mar=0.2536782814099441
总共花费的时间为：64.69
海南藏族自治州
2673A
[flaml.automl: 09-19 01:17:10] {2390} INFO - task = regression
[flaml.automl: 09-19 01:17:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:17:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:17:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:17:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:17:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:17:13] {3025} INFO - Estimated sufficient time budget=22146s. Estimated necessary time budget=22s.
[flaml.automl: 09-19 01:17:13] {3072} INFO -  at 2.3s,	estimator xgboost's best error=8.1110,	best estimator xgboost's best error=8.1110
[flaml.automl: 09-19 01:17:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:17:16] {3072} INFO -  at 5.7s,	estimator xgboost's best error=5.1462,	best estimator xgboost's best error=5.1462
[flaml.automl: 09-19 01:17:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:17:18] {3072} INFO -  at 7.9s,	estimator xgboost's best error=5.1462,	best estimator xgboost's best error=5.1462
[flaml.automl: 09-19 01:17:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:17:31] {3072} INFO -  at 21.0s,	estimator xgboost's best error=5.1462,	best estimator xgboost's best error=5.1462
[flaml.automl: 09-19 01:17:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:17:34] {3072} INFO -  at 23.1s,	estimator xgboost's best error=4.2875,	best estimator xgboost's best error=4.2875
[flaml.automl: 09-19 01:17:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:17:36] {3072} INFO -  at 26.1s,	estimator xgboost's best error=4.2344,	best estimator xgboost's best error=4.2344
[flaml.automl: 09-19 01:17:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:17:39] {3072} INFO -  at 29.1s,	estimator xgboost's best error=4.2138,	best estimator xgboost's best error=4.2138
[flaml.automl: 09-19 01:17:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:17:44] {3072} INFO -  at 33.6s,	estimator xgboost's best error=4.2138,	best estimator xgboost's best error=4.2138
[flaml.automl: 09-19 01:17:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:17:47] {3072} INFO -  at 36.5s,	estimator xgboost's best error=4.2138,	best estimator xgboost's best error=4.2138
[flaml.automl: 09-19 01:17:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:17:51] {3072} INFO -  at 41.1s,	estimator xgboost's best error=4.1113,	best estimator xgboost's best error=4.1113
[flaml.automl: 09-19 01:17:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:17:53] {3072} INFO -  at 42.7s,	estimator xgboost's best error=4.1113,	best estimator xgboost's best error=4.1113
[flaml.automl: 09-19 01:17:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:17:54] {3072} INFO -  at 43.8s,	estimator xgboost's best error=4.1113,	best estimator xgboost's best error=4.1113
[flaml.automl: 09-19 01:17:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:18:04] {3072} INFO -  at 53.4s,	estimator xgboost's best error=4.1113,	best estimator xgboost's best error=4.1113
[flaml.automl: 09-19 01:18:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:18:09] {3072} INFO -  at 58.1s,	estimator xgboost's best error=4.0799,	best estimator xgboost's best error=4.0799
[flaml.automl: 09-19 01:18:13] {3335} INFO - retrain xgboost for 4.7s
[flaml.automl: 09-19 01:18:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:18:13] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:18:13] {2637} INFO - Time taken to find the best model: 58.14531993865967
[flaml.automl: 09-19 01:18:13] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 6, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}
NO2(0)最佳损失：-3.079854241760459
NO2(0)最好结果：{'pred_time': 3.682597784664285e-05, 'wall_clock_time': 58.14531993865967, 'metric_for_logging': {'pred_time': 3.682597784664285e-05}, 'val_loss': 4.079854241760459, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 6, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}, 'config/n_estimators': 13, 'config/max_leaves': 6, 'config/min_child_weight': 0.07186500551974488, 'config/learning_rate': 0.18147476059072173, 'config/subsample': 0.7641710216408443, 'config/colsample_bylevel': 0.8883913141026579, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.14043216453256416, 'experiment_tag': 'exp', 'time_total_s': 4.720144033432007}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.008862930139704961
NO2(0)的mse=42.02956846899885
NO2(0)的mae=4.323271638582337
NO2(0)的mar=0.39538944817020766
总共花费的时间为：63.07
果洛藏族自治州
2674A
[flaml.automl: 09-19 01:21:14] {2390} INFO - task = regression
[flaml.automl: 09-19 01:21:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:21:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:21:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:21:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:21:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:21:18] {3025} INFO - Estimated sufficient time budget=33742s. Estimated necessary time budget=34s.
[flaml.automl: 09-19 01:21:18] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.5535,	best estimator xgboost's best error=8.5535
[flaml.automl: 09-19 01:21:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:21:23] {3072} INFO -  at 8.7s,	estimator xgboost's best error=4.5346,	best estimator xgboost's best error=4.5346
[flaml.automl: 09-19 01:21:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:21:26] {3072} INFO -  at 11.2s,	estimator xgboost's best error=4.5346,	best estimator xgboost's best error=4.5346
[flaml.automl: 09-19 01:21:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:21:38] {3072} INFO -  at 23.9s,	estimator xgboost's best error=4.5346,	best estimator xgboost's best error=4.5346
[flaml.automl: 09-19 01:21:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:21:40] {3072} INFO -  at 26.0s,	estimator xgboost's best error=2.4879,	best estimator xgboost's best error=2.4879
[flaml.automl: 09-19 01:21:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:21:43] {3072} INFO -  at 28.9s,	estimator xgboost's best error=2.4617,	best estimator xgboost's best error=2.4617
[flaml.automl: 09-19 01:21:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:21:46] {3072} INFO -  at 32.0s,	estimator xgboost's best error=2.3939,	best estimator xgboost's best error=2.3939
[flaml.automl: 09-19 01:21:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:21:51] {3072} INFO -  at 36.4s,	estimator xgboost's best error=2.3939,	best estimator xgboost's best error=2.3939
[flaml.automl: 09-19 01:21:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:21:54] {3072} INFO -  at 39.4s,	estimator xgboost's best error=2.3939,	best estimator xgboost's best error=2.3939
[flaml.automl: 09-19 01:21:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:21:59] {3072} INFO -  at 44.3s,	estimator xgboost's best error=2.3939,	best estimator xgboost's best error=2.3939
[flaml.automl: 09-19 01:21:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:22:01] {3072} INFO -  at 46.2s,	estimator xgboost's best error=2.3939,	best estimator xgboost's best error=2.3939
[flaml.automl: 09-19 01:22:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:22:02] {3072} INFO -  at 47.3s,	estimator xgboost's best error=2.3939,	best estimator xgboost's best error=2.3939
[flaml.automl: 09-19 01:22:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:22:07] {3072} INFO -  at 53.0s,	estimator xgboost's best error=2.3939,	best estimator xgboost's best error=2.3939
[flaml.automl: 09-19 01:22:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:22:11] {3072} INFO -  at 56.3s,	estimator xgboost's best error=2.3610,	best estimator xgboost's best error=2.3610
[flaml.automl: 09-19 01:22:11] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:22:13] {3072} INFO -  at 58.2s,	estimator xgboost's best error=2.3610,	best estimator xgboost's best error=2.3610
[flaml.automl: 09-19 01:22:16] {3335} INFO - retrain xgboost for 3.3s
[flaml.automl: 09-19 01:22:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9564417338648162, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2406995205234967, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=1.4858835173589837, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.22197115035378334, scale_pos_weight=1,
             subsample=0.8536409652116742, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:22:16] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:22:16] {2637} INFO - Time taken to find the best model: 56.260350704193115
[flaml.automl: 09-19 01:22:16] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 4, 'min_child_weight': 1.4858835173589837, 'learning_rate': 0.2406995205234967, 'subsample': 0.8536409652116742, 'colsample_bylevel': 0.9564417338648162, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.22197115035378334}
NO2(0)最佳损失：-1.36103936348352
NO2(0)最好结果：{'pred_time': 3.449033958513175e-05, 'wall_clock_time': 56.260350704193115, 'metric_for_logging': {'pred_time': 3.449033958513175e-05}, 'val_loss': 2.36103936348352, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 4, 'min_child_weight': 1.4858835173589837, 'learning_rate': 0.2406995205234967, 'subsample': 0.8536409652116742, 'colsample_bylevel': 0.9564417338648162, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.22197115035378334}, 'config/n_estimators': 13, 'config/max_leaves': 4, 'config/min_child_weight': 1.4858835173589837, 'config/learning_rate': 0.2406995205234967, 'config/subsample': 0.8536409652116742, 'config/colsample_bylevel': 0.9564417338648162, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.22197115035378334, 'experiment_tag': 'exp', 'time_total_s': 3.3050501346588135}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9564417338648162, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2406995205234967, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=1.4858835173589837, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.22197115035378334, scale_pos_weight=1,
             subsample=0.8536409652116742, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.15707975731561974
NO2(0)的mse=14.371929672842253
NO2(0)的mae=2.4635505493152343
NO2(0)的mar=0.16954049420312578
总共花费的时间为：61.66
玉树藏族自治州
2675A
[flaml.automl: 09-19 01:25:30] {2390} INFO - task = regression
[flaml.automl: 09-19 01:25:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:25:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:25:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:25:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:25:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:25:32] {3025} INFO - Estimated sufficient time budget=22260s. Estimated necessary time budget=22s.
[flaml.automl: 09-19 01:25:32] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.7983,	best estimator xgboost's best error=5.7983
[flaml.automl: 09-19 01:25:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:25:35] {3072} INFO -  at 5.6s,	estimator xgboost's best error=3.8566,	best estimator xgboost's best error=3.8566
[flaml.automl: 09-19 01:25:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:25:38] {3072} INFO -  at 7.8s,	estimator xgboost's best error=3.8566,	best estimator xgboost's best error=3.8566
[flaml.automl: 09-19 01:25:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:25:51] {3072} INFO -  at 20.9s,	estimator xgboost's best error=3.8566,	best estimator xgboost's best error=3.8566
[flaml.automl: 09-19 01:25:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:25:53] {3072} INFO -  at 23.1s,	estimator xgboost's best error=3.0449,	best estimator xgboost's best error=3.0449
[flaml.automl: 09-19 01:25:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:25:56] {3072} INFO -  at 26.1s,	estimator xgboost's best error=2.9401,	best estimator xgboost's best error=2.9401
[flaml.automl: 09-19 01:25:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:25:59] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.9068,	best estimator xgboost's best error=2.9068
[flaml.automl: 09-19 01:25:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:26:03] {3072} INFO -  at 33.6s,	estimator xgboost's best error=2.9068,	best estimator xgboost's best error=2.9068
[flaml.automl: 09-19 01:26:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:26:06] {3072} INFO -  at 36.7s,	estimator xgboost's best error=2.8653,	best estimator xgboost's best error=2.8653
[flaml.automl: 09-19 01:26:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:26:11] {3072} INFO -  at 41.5s,	estimator xgboost's best error=2.8300,	best estimator xgboost's best error=2.8300
[flaml.automl: 09-19 01:26:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:26:13] {3072} INFO -  at 43.6s,	estimator xgboost's best error=2.8300,	best estimator xgboost's best error=2.8300
[flaml.automl: 09-19 01:26:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:26:25] {3072} INFO -  at 55.7s,	estimator xgboost's best error=2.8300,	best estimator xgboost's best error=2.8300
[flaml.automl: 09-19 01:26:28] {3335} INFO - retrain xgboost for 2.6s
[flaml.automl: 09-19 01:26:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 01:26:28] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:26:28] {2637} INFO - Time taken to find the best model: 41.544060945510864
NO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}
NO2(0)最佳损失：-1.830030467252584
NO2(0)最好结果：{'pred_time': 6.611882924624618e-05, 'wall_clock_time': 41.544060945510864, 'metric_for_logging': {'pred_time': 6.611882924624618e-05}, 'val_loss': 2.830030467252584, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}, 'config/n_estimators': 6, 'config/max_leaves': 7, 'config/min_child_weight': 0.056902942069697794, 'config/learning_rate': 0.6286550459839113, 'config/subsample': 0.8219353140149964, 'config/colsample_bylevel': 0.6552426578592088, 'config/colsample_bytree': 0.8391963407651736, 'config/reg_alpha': 0.007347191482940116, 'config/reg_lambda': 9.546883854467293, 'experiment_tag': 'exp', 'time_total_s': 4.8748939037323}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.6377991348726426
NO2(0)的mse=16.77705392777964
NO2(0)的mae=2.8615294424472033
NO2(0)的mar=0.5589295161421352
总共花费的时间为：58.48
海西蒙古族藏族自治州
2676A
[flaml.automl: 09-19 01:29:40] {2390} INFO - task = regression
[flaml.automl: 09-19 01:29:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:29:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:29:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:29:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:29:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:29:42] {3025} INFO - Estimated sufficient time budget=21453s. Estimated necessary time budget=21s.
[flaml.automl: 09-19 01:29:42] {3072} INFO -  at 2.2s,	estimator xgboost's best error=7.1037,	best estimator xgboost's best error=7.1037
[flaml.automl: 09-19 01:29:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:29:46] {3072} INFO -  at 5.7s,	estimator xgboost's best error=3.9740,	best estimator xgboost's best error=3.9740
[flaml.automl: 09-19 01:29:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:29:48] {3072} INFO -  at 8.1s,	estimator xgboost's best error=3.9740,	best estimator xgboost's best error=3.9740
[flaml.automl: 09-19 01:29:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:29:59] {3072} INFO -  at 19.2s,	estimator xgboost's best error=3.9740,	best estimator xgboost's best error=3.9740
[flaml.automl: 09-19 01:29:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:30:00] {3072} INFO -  at 20.4s,	estimator xgboost's best error=2.6816,	best estimator xgboost's best error=2.6816
[flaml.automl: 09-19 01:30:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:30:02] {3072} INFO -  at 21.9s,	estimator xgboost's best error=2.5728,	best estimator xgboost's best error=2.5728
[flaml.automl: 09-19 01:30:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:30:04] {3072} INFO -  at 23.6s,	estimator xgboost's best error=2.5728,	best estimator xgboost's best error=2.5728
[flaml.automl: 09-19 01:30:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:30:06] {3072} INFO -  at 25.8s,	estimator xgboost's best error=2.5728,	best estimator xgboost's best error=2.5728
[flaml.automl: 09-19 01:30:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:30:07] {3072} INFO -  at 27.5s,	estimator xgboost's best error=2.5728,	best estimator xgboost's best error=2.5728
[flaml.automl: 09-19 01:30:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:30:12] {3072} INFO -  at 32.0s,	estimator xgboost's best error=2.5401,	best estimator xgboost's best error=2.5401
[flaml.automl: 09-19 01:30:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:30:15] {3072} INFO -  at 34.8s,	estimator xgboost's best error=2.5401,	best estimator xgboost's best error=2.5401
[flaml.automl: 09-19 01:30:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:30:17] {3072} INFO -  at 37.3s,	estimator xgboost's best error=2.5401,	best estimator xgboost's best error=2.5401
[flaml.automl: 09-19 01:30:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:30:40] {3072} INFO -  at 60.3s,	estimator xgboost's best error=2.5401,	best estimator xgboost's best error=2.5401
[flaml.automl: 09-19 01:30:48] {3335} INFO - retrain xgboost for 7.9s
[flaml.automl: 09-19 01:30:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:30:48] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:30:48] {2637} INFO - Time taken to find the best model: 32.00979208946228
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}
NO2(0)最佳损失：-1.5401424949024847
NO2(0)最好结果：{'pred_time': 6.0910357437068744e-05, 'wall_clock_time': 32.00979208946228, 'metric_for_logging': {'pred_time': 6.0910357437068744e-05}, 'val_loss': 2.5401424949024847, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}, 'config/n_estimators': 4, 'config/max_leaves': 10, 'config/min_child_weight': 0.06372323163910634, 'config/learning_rate': 0.4086634770112844, 'config/subsample': 0.770412309752864, 'config/colsample_bylevel': 0.6720329282771347, 'config/colsample_bytree': 0.752099095598003, 'config/reg_alpha': 0.00271725862127768, 'config/reg_lambda': 3.9335607770117846, 'experiment_tag': 'exp', 'time_total_s': 4.5292136669158936}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.31431907688368266
NO2(0)的mse=22.0852058501065
NO2(0)的mae=2.8120022317764857
NO2(0)的mar=0.23641646013550088
总共花费的时间为：68.49
吴忠市
2677A
3648A
[flaml.automl: 09-19 01:37:18] {2390} INFO - task = regression
[flaml.automl: 09-19 01:37:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:37:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:37:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:37:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:37:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:37:20] {3025} INFO - Estimated sufficient time budget=11974s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:37:20] {3072} INFO -  at 1.3s,	estimator xgboost's best error=12.2904,	best estimator xgboost's best error=12.2904
[flaml.automl: 09-19 01:37:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:37:22] {3072} INFO -  at 3.4s,	estimator xgboost's best error=6.5140,	best estimator xgboost's best error=6.5140
[flaml.automl: 09-19 01:37:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:37:23] {3072} INFO -  at 4.6s,	estimator xgboost's best error=6.5140,	best estimator xgboost's best error=6.5140
[flaml.automl: 09-19 01:37:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:37:32] {3072} INFO -  at 14.1s,	estimator xgboost's best error=6.5140,	best estimator xgboost's best error=6.5140
[flaml.automl: 09-19 01:37:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:37:33] {3072} INFO -  at 15.2s,	estimator xgboost's best error=4.9024,	best estimator xgboost's best error=4.9024
[flaml.automl: 09-19 01:37:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:37:35] {3072} INFO -  at 16.8s,	estimator xgboost's best error=4.4968,	best estimator xgboost's best error=4.4968
[flaml.automl: 09-19 01:37:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:37:37] {3072} INFO -  at 18.5s,	estimator xgboost's best error=4.4968,	best estimator xgboost's best error=4.4968
[flaml.automl: 09-19 01:37:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:37:39] {3072} INFO -  at 20.9s,	estimator xgboost's best error=4.4968,	best estimator xgboost's best error=4.4968
[flaml.automl: 09-19 01:37:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:37:40] {3072} INFO -  at 22.1s,	estimator xgboost's best error=4.4968,	best estimator xgboost's best error=4.4968
[flaml.automl: 09-19 01:37:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:37:43] {3072} INFO -  at 24.6s,	estimator xgboost's best error=4.4968,	best estimator xgboost's best error=4.4968
[flaml.automl: 09-19 01:37:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:37:44] {3072} INFO -  at 25.7s,	estimator xgboost's best error=4.4968,	best estimator xgboost's best error=4.4968
[flaml.automl: 09-19 01:37:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:37:45] {3072} INFO -  at 26.9s,	estimator xgboost's best error=4.4968,	best estimator xgboost's best error=4.4968
[flaml.automl: 09-19 01:37:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:37:51] {3072} INFO -  at 32.9s,	estimator xgboost's best error=4.4139,	best estimator xgboost's best error=4.4139
[flaml.automl: 09-19 01:37:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:38:02] {3072} INFO -  at 43.4s,	estimator xgboost's best error=4.3356,	best estimator xgboost's best error=4.3356
[flaml.automl: 09-19 01:38:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:38:08] {3072} INFO -  at 49.5s,	estimator xgboost's best error=4.3356,	best estimator xgboost's best error=4.3356
[flaml.automl: 09-19 01:38:08] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 01:38:17] {3072} INFO -  at 59.2s,	estimator xgboost's best error=4.3356,	best estimator xgboost's best error=4.3356
[flaml.automl: 09-19 01:38:29] {3335} INFO - retrain xgboost for 11.3s
[flaml.automl: 09-19 01:38:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:38:29] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:38:29] {2637} INFO - Time taken to find the best model: 43.403339862823486
[flaml.automl: 09-19 01:38:29] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.3356262380081825
NO2(0)最好结果：{'pred_time': 1.6514752987988976e-05, 'wall_clock_time': 43.403339862823486, 'metric_for_logging': {'pred_time': 1.6514752987988976e-05}, 'val_loss': 4.3356262380081825, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.449529647827148}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7283226863566539
NO2(0)的mse=46.00143758782518
NO2(0)的mae=4.519184879099595
NO2(0)的mar=0.32022362691344214
总共花费的时间为：71.01
中卫市
2680A
3650A
[flaml.automl: 09-19 01:45:22] {2390} INFO - task = regression
[flaml.automl: 09-19 01:45:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:45:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:45:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:45:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:45:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:45:23] {3025} INFO - Estimated sufficient time budget=12052s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:45:23] {3072} INFO -  at 1.3s,	estimator xgboost's best error=14.8479,	best estimator xgboost's best error=14.8479
[flaml.automl: 09-19 01:45:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:45:25] {3072} INFO -  at 3.2s,	estimator xgboost's best error=8.5021,	best estimator xgboost's best error=8.5021
[flaml.automl: 09-19 01:45:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:45:26] {3072} INFO -  at 4.4s,	estimator xgboost's best error=8.5021,	best estimator xgboost's best error=8.5021
[flaml.automl: 09-19 01:45:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:45:35] {3072} INFO -  at 12.8s,	estimator xgboost's best error=8.5021,	best estimator xgboost's best error=8.5021
[flaml.automl: 09-19 01:45:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:45:36] {3072} INFO -  at 13.9s,	estimator xgboost's best error=5.8485,	best estimator xgboost's best error=5.8485
[flaml.automl: 09-19 01:45:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:45:37] {3072} INFO -  at 15.5s,	estimator xgboost's best error=5.3816,	best estimator xgboost's best error=5.3816
[flaml.automl: 09-19 01:45:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:45:39] {3072} INFO -  at 17.2s,	estimator xgboost's best error=5.3816,	best estimator xgboost's best error=5.3816
[flaml.automl: 09-19 01:45:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:45:41] {3072} INFO -  at 19.6s,	estimator xgboost's best error=5.3816,	best estimator xgboost's best error=5.3816
[flaml.automl: 09-19 01:45:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:45:43] {3072} INFO -  at 20.8s,	estimator xgboost's best error=5.3816,	best estimator xgboost's best error=5.3816
[flaml.automl: 09-19 01:45:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:45:45] {3072} INFO -  at 23.2s,	estimator xgboost's best error=5.3816,	best estimator xgboost's best error=5.3816
[flaml.automl: 09-19 01:45:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:45:46] {3072} INFO -  at 24.4s,	estimator xgboost's best error=5.3816,	best estimator xgboost's best error=5.3816
[flaml.automl: 09-19 01:45:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:45:47] {3072} INFO -  at 25.5s,	estimator xgboost's best error=5.3816,	best estimator xgboost's best error=5.3816
[flaml.automl: 09-19 01:45:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:45:53] {3072} INFO -  at 31.6s,	estimator xgboost's best error=5.3309,	best estimator xgboost's best error=5.3309
[flaml.automl: 09-19 01:45:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:46:04] {3072} INFO -  at 42.1s,	estimator xgboost's best error=5.1718,	best estimator xgboost's best error=5.1718
[flaml.automl: 09-19 01:46:04] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:46:10] {3072} INFO -  at 48.1s,	estimator xgboost's best error=5.1718,	best estimator xgboost's best error=5.1718
[flaml.automl: 09-19 01:46:10] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 01:46:22] {3072} INFO -  at 60.3s,	estimator xgboost's best error=5.1107,	best estimator xgboost's best error=5.1107
[flaml.automl: 09-19 01:47:15] {3335} INFO - retrain xgboost for 52.7s
[flaml.automl: 09-19 01:47:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:47:15] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:47:15] {2637} INFO - Time taken to find the best model: 60.257147550582886
[flaml.automl: 09-19 01:47:15] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
NO2(0)最佳损失：-4.110652108842676
NO2(0)最好结果：{'pred_time': 4.762020978060635e-05, 'wall_clock_time': 60.257147550582886, 'metric_for_logging': {'pred_time': 4.762020978060635e-05}, 'val_loss': 5.110652108842676, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 12.137160778045654}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7079870213528037
NO2(0)的mse=63.67674326552207
NO2(0)的mae=5.367947838594089
NO2(0)的mar=0.2847201939000166
总共花费的时间为：113.41
固原市
2683A
2684A
2685A
3522A
[flaml.automl: 09-19 02:00:42] {2390} INFO - task = regression
[flaml.automl: 09-19 02:00:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:00:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:00:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:00:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:00:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:00:44] {3025} INFO - Estimated sufficient time budget=49956s. Estimated necessary time budget=50s.
[flaml.automl: 09-19 02:00:44] {3072} INFO -  at 1.4s,	estimator xgboost's best error=12.9682,	best estimator xgboost's best error=12.9682
[flaml.automl: 09-19 02:00:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:00:46] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.9661,	best estimator xgboost's best error=6.9661
[flaml.automl: 09-19 02:00:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:00:47] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.9661,	best estimator xgboost's best error=6.9661
[flaml.automl: 09-19 02:00:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:00:53] {3072} INFO -  at 11.1s,	estimator xgboost's best error=6.9661,	best estimator xgboost's best error=6.9661
[flaml.automl: 09-19 02:00:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:00:54] {3072} INFO -  at 12.2s,	estimator xgboost's best error=5.4068,	best estimator xgboost's best error=5.4068
[flaml.automl: 09-19 02:00:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:00:56] {3072} INFO -  at 13.8s,	estimator xgboost's best error=5.0548,	best estimator xgboost's best error=5.0548
[flaml.automl: 09-19 02:00:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:00:58] {3072} INFO -  at 15.5s,	estimator xgboost's best error=5.0265,	best estimator xgboost's best error=5.0265
[flaml.automl: 09-19 02:00:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:01:00] {3072} INFO -  at 18.1s,	estimator xgboost's best error=5.0265,	best estimator xgboost's best error=5.0265
[flaml.automl: 09-19 02:01:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:01:02] {3072} INFO -  at 19.8s,	estimator xgboost's best error=5.0265,	best estimator xgboost's best error=5.0265
[flaml.automl: 09-19 02:01:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:01:05] {3072} INFO -  at 22.8s,	estimator xgboost's best error=4.9431,	best estimator xgboost's best error=4.9431
[flaml.automl: 09-19 02:01:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:01:07] {3072} INFO -  at 24.4s,	estimator xgboost's best error=4.9431,	best estimator xgboost's best error=4.9431
[flaml.automl: 09-19 02:01:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:01:08] {3072} INFO -  at 25.6s,	estimator xgboost's best error=4.9431,	best estimator xgboost's best error=4.9431
[flaml.automl: 09-19 02:01:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:01:12] {3072} INFO -  at 29.3s,	estimator xgboost's best error=4.9431,	best estimator xgboost's best error=4.9431
[flaml.automl: 09-19 02:01:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 02:01:15] {3072} INFO -  at 32.7s,	estimator xgboost's best error=4.9431,	best estimator xgboost's best error=4.9431
[flaml.automl: 09-19 02:01:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 02:01:18] {3072} INFO -  at 35.8s,	estimator xgboost's best error=4.9431,	best estimator xgboost's best error=4.9431
[flaml.automl: 09-19 02:01:18] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 02:01:25] {3072} INFO -  at 42.5s,	estimator xgboost's best error=4.9431,	best estimator xgboost's best error=4.9431
[flaml.automl: 09-19 02:01:25] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 02:01:28] {3072} INFO -  at 45.4s,	estimator xgboost's best error=4.9431,	best estimator xgboost's best error=4.9431
[flaml.automl: 09-19 02:01:28] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-19 02:01:30] {3072} INFO -  at 47.9s,	estimator xgboost's best error=4.9431,	best estimator xgboost's best error=4.9431
[flaml.automl: 09-19 02:01:30] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-19 02:01:42] {3072} INFO -  at 59.5s,	estimator xgboost's best error=4.9431,	best estimator xgboost's best error=4.9431
[flaml.automl: 09-19 02:01:47] {3335} INFO - retrain xgboost for 5.0s
[flaml.automl: 09-19 02:01:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:01:47] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:01:47] {2637} INFO - Time taken to find the best model: 22.798722743988037
NO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978, 'FLAML_sample_size': 10000}
NO2(0)最佳损失：-3.9431450058468727
NO2(0)最好结果：{'pred_time': 9.013216132140398e-06, 'wall_clock_time': 22.798722743988037, 'metric_for_logging': {'pred_time': 9.013216132140398e-06}, 'val_loss': 4.943145005846873, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978, 'FLAML_sample_size': 10000}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.0212299823760986}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.6180978204560891
NO2(0)的mse=58.93436414649059
NO2(0)的mae=5.014881642175029
NO2(0)的mar=0.30499012894062366
总共花费的时间为：65.28
吐鲁番地区
2686A
2687A
[flaml.automl: 09-19 02:08:15] {2390} INFO - task = regression
[flaml.automl: 09-19 02:08:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:08:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:08:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:08:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:08:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:08:16] {3025} INFO - Estimated sufficient time budget=12153s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 02:08:16] {3072} INFO -  at 1.3s,	estimator xgboost's best error=18.6895,	best estimator xgboost's best error=18.6895
[flaml.automl: 09-19 02:08:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:08:18] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.1154,	best estimator xgboost's best error=9.1154
[flaml.automl: 09-19 02:08:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:08:19] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.1154,	best estimator xgboost's best error=9.1154
[flaml.automl: 09-19 02:08:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:08:29] {3072} INFO -  at 14.2s,	estimator xgboost's best error=9.1154,	best estimator xgboost's best error=9.1154
[flaml.automl: 09-19 02:08:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:08:30] {3072} INFO -  at 15.3s,	estimator xgboost's best error=6.4596,	best estimator xgboost's best error=6.4596
[flaml.automl: 09-19 02:08:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:08:31] {3072} INFO -  at 16.9s,	estimator xgboost's best error=5.7663,	best estimator xgboost's best error=5.7663
[flaml.automl: 09-19 02:08:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:08:33] {3072} INFO -  at 18.5s,	estimator xgboost's best error=5.7663,	best estimator xgboost's best error=5.7663
[flaml.automl: 09-19 02:08:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:08:36] {3072} INFO -  at 21.0s,	estimator xgboost's best error=5.7663,	best estimator xgboost's best error=5.7663
[flaml.automl: 09-19 02:08:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:08:37] {3072} INFO -  at 22.2s,	estimator xgboost's best error=5.7663,	best estimator xgboost's best error=5.7663
[flaml.automl: 09-19 02:08:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:08:39] {3072} INFO -  at 24.6s,	estimator xgboost's best error=5.7663,	best estimator xgboost's best error=5.7663
[flaml.automl: 09-19 02:08:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:08:40] {3072} INFO -  at 25.8s,	estimator xgboost's best error=5.7663,	best estimator xgboost's best error=5.7663
[flaml.automl: 09-19 02:08:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:08:41] {3072} INFO -  at 26.9s,	estimator xgboost's best error=5.7663,	best estimator xgboost's best error=5.7663
[flaml.automl: 09-19 02:08:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:08:48] {3072} INFO -  at 33.0s,	estimator xgboost's best error=5.7663,	best estimator xgboost's best error=5.7663
[flaml.automl: 09-19 02:08:48] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 02:08:50] {3072} INFO -  at 35.9s,	estimator xgboost's best error=5.7007,	best estimator xgboost's best error=5.7007
[flaml.automl: 09-19 02:08:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 02:08:53] {3072} INFO -  at 38.1s,	estimator xgboost's best error=5.7007,	best estimator xgboost's best error=5.7007
[flaml.automl: 09-19 02:08:53] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 02:09:01] {3072} INFO -  at 46.7s,	estimator xgboost's best error=5.5552,	best estimator xgboost's best error=5.5552
[flaml.automl: 09-19 02:09:01] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 02:09:06] {3072} INFO -  at 51.6s,	estimator xgboost's best error=5.5552,	best estimator xgboost's best error=5.5552
[flaml.automl: 09-19 02:09:13] {3335} INFO - retrain xgboost for 7.4s
[flaml.automl: 09-19 02:09:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:09:13] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:09:13] {2637} INFO - Time taken to find the best model: 46.73650360107422
[flaml.automl: 09-19 02:09:13] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}
NO2(0)最佳损失：-4.555153843995772
NO2(0)最好结果：{'pred_time': 3.4178166013014944e-05, 'wall_clock_time': 46.73650360107422, 'metric_for_logging': {'pred_time': 3.4178166013014944e-05}, 'val_loss': 5.555153843995772, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}, 'config/n_estimators': 9, 'config/max_leaves': 9, 'config/min_child_weight': 3.029166996333365, 'config/learning_rate': 0.28782051713764356, 'config/subsample': 0.8051874633193139, 'config/colsample_bylevel': 0.656637827405616, 'config/colsample_bytree': 0.7193335366419935, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 2.3848121277079324, 'experiment_tag': 'exp', 'time_total_s': 8.5918710231781}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7500903461958943
NO2(0)的mse=68.82248776744396
NO2(0)的mae=5.364265203652428
NO2(0)的mar=0.20043812917340995
总共花费的时间为：59.46
哈密地区
2688A
2689A
[flaml.automl: 09-19 02:15:46] {2390} INFO - task = regression
[flaml.automl: 09-19 02:15:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:15:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:15:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:15:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:15:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:15:47] {3025} INFO - Estimated sufficient time budget=12108s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 02:15:47] {3072} INFO -  at 1.3s,	estimator xgboost's best error=14.7757,	best estimator xgboost's best error=14.7757
[flaml.automl: 09-19 02:15:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:15:49] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.0588,	best estimator xgboost's best error=8.0588
[flaml.automl: 09-19 02:15:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:15:50] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.0588,	best estimator xgboost's best error=8.0588
[flaml.automl: 09-19 02:15:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:16:00] {3072} INFO -  at 14.2s,	estimator xgboost's best error=8.0588,	best estimator xgboost's best error=8.0588
[flaml.automl: 09-19 02:16:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:16:01] {3072} INFO -  at 15.3s,	estimator xgboost's best error=6.5419,	best estimator xgboost's best error=6.5419
[flaml.automl: 09-19 02:16:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:16:03] {3072} INFO -  at 16.9s,	estimator xgboost's best error=5.9511,	best estimator xgboost's best error=5.9511
[flaml.automl: 09-19 02:16:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:16:04] {3072} INFO -  at 18.5s,	estimator xgboost's best error=5.9511,	best estimator xgboost's best error=5.9511
[flaml.automl: 09-19 02:16:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:16:07] {3072} INFO -  at 21.0s,	estimator xgboost's best error=5.9511,	best estimator xgboost's best error=5.9511
[flaml.automl: 09-19 02:16:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:16:08] {3072} INFO -  at 22.1s,	estimator xgboost's best error=5.9511,	best estimator xgboost's best error=5.9511
[flaml.automl: 09-19 02:16:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:16:10] {3072} INFO -  at 24.6s,	estimator xgboost's best error=5.9511,	best estimator xgboost's best error=5.9511
[flaml.automl: 09-19 02:16:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:16:11] {3072} INFO -  at 25.7s,	estimator xgboost's best error=5.9511,	best estimator xgboost's best error=5.9511
[flaml.automl: 09-19 02:16:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:16:13] {3072} INFO -  at 26.9s,	estimator xgboost's best error=5.9511,	best estimator xgboost's best error=5.9511
[flaml.automl: 09-19 02:16:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:16:19] {3072} INFO -  at 32.9s,	estimator xgboost's best error=5.9511,	best estimator xgboost's best error=5.9511
[flaml.automl: 09-19 02:16:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 02:16:21] {3072} INFO -  at 35.7s,	estimator xgboost's best error=5.9045,	best estimator xgboost's best error=5.9045
[flaml.automl: 09-19 02:16:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 02:16:23] {3072} INFO -  at 37.3s,	estimator xgboost's best error=5.9045,	best estimator xgboost's best error=5.9045
[flaml.automl: 09-19 02:16:23] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 02:16:28] {3072} INFO -  at 42.4s,	estimator xgboost's best error=5.8169,	best estimator xgboost's best error=5.8169
[flaml.automl: 09-19 02:16:28] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 02:16:33] {3072} INFO -  at 47.5s,	estimator xgboost's best error=5.8169,	best estimator xgboost's best error=5.8169
[flaml.automl: 09-19 02:16:33] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-19 02:16:37] {3072} INFO -  at 51.3s,	estimator xgboost's best error=5.8169,	best estimator xgboost's best error=5.8169
[flaml.automl: 09-19 02:16:37] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-19 02:16:45] {3072} INFO -  at 59.3s,	estimator xgboost's best error=5.8169,	best estimator xgboost's best error=5.8169
[flaml.automl: 09-19 02:16:52] {3335} INFO - retrain xgboost for 7.4s
[flaml.automl: 09-19 02:16:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:16:52] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:16:52] {2637} INFO - Time taken to find the best model: 42.448816537857056
[flaml.automl: 09-19 02:16:52] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}
NO2(0)最佳损失：-4.816925248325213
NO2(0)最好结果：{'pred_time': 3.727818239507761e-05, 'wall_clock_time': 42.448816537857056, 'metric_for_logging': {'pred_time': 3.727818239507761e-05}, 'val_loss': 5.816925248325213, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}, 'config/n_estimators': 9, 'config/max_leaves': 9, 'config/min_child_weight': 3.029166996333365, 'config/learning_rate': 0.28782051713764356, 'config/subsample': 0.8051874633193139, 'config/colsample_bylevel': 0.656637827405616, 'config/colsample_bytree': 0.7193335366419935, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 2.3848121277079324, 'experiment_tag': 'exp', 'time_total_s': 5.145554065704346}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6880985833151212
NO2(0)的mse=82.09228338553908
NO2(0)的mae=6.12226161696278
NO2(0)的mar=0.3502638399957593
总共花费的时间为：67.10
昌吉州
2690A
3613A
[flaml.automl: 09-19 02:23:34] {2390} INFO - task = regression
[flaml.automl: 09-19 02:23:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:23:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:23:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:23:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:23:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:23:36] {3025} INFO - Estimated sufficient time budget=20882s. Estimated necessary time budget=21s.
[flaml.automl: 09-19 02:23:36] {3072} INFO -  at 2.2s,	estimator xgboost's best error=13.5547,	best estimator xgboost's best error=13.5547
[flaml.automl: 09-19 02:23:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:23:39] {3072} INFO -  at 5.1s,	estimator xgboost's best error=7.7313,	best estimator xgboost's best error=7.7313
[flaml.automl: 09-19 02:23:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:23:40] {3072} INFO -  at 6.7s,	estimator xgboost's best error=7.7313,	best estimator xgboost's best error=7.7313
[flaml.automl: 09-19 02:23:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:23:49] {3072} INFO -  at 15.1s,	estimator xgboost's best error=7.7313,	best estimator xgboost's best error=7.7313
[flaml.automl: 09-19 02:23:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:23:50] {3072} INFO -  at 16.2s,	estimator xgboost's best error=5.0594,	best estimator xgboost's best error=5.0594
[flaml.automl: 09-19 02:23:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:23:51] {3072} INFO -  at 17.8s,	estimator xgboost's best error=4.2724,	best estimator xgboost's best error=4.2724
[flaml.automl: 09-19 02:23:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:23:53] {3072} INFO -  at 19.4s,	estimator xgboost's best error=4.2724,	best estimator xgboost's best error=4.2724
[flaml.automl: 09-19 02:23:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:23:56] {3072} INFO -  at 21.9s,	estimator xgboost's best error=4.2724,	best estimator xgboost's best error=4.2724
[flaml.automl: 09-19 02:23:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:23:57] {3072} INFO -  at 23.0s,	estimator xgboost's best error=4.2724,	best estimator xgboost's best error=4.2724
[flaml.automl: 09-19 02:23:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:23:59] {3072} INFO -  at 25.5s,	estimator xgboost's best error=4.2724,	best estimator xgboost's best error=4.2724
[flaml.automl: 09-19 02:23:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:24:00] {3072} INFO -  at 26.6s,	estimator xgboost's best error=4.2724,	best estimator xgboost's best error=4.2724
[flaml.automl: 09-19 02:24:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:24:01] {3072} INFO -  at 27.8s,	estimator xgboost's best error=4.2724,	best estimator xgboost's best error=4.2724
[flaml.automl: 09-19 02:24:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:24:07] {3072} INFO -  at 33.7s,	estimator xgboost's best error=3.9854,	best estimator xgboost's best error=3.9854
[flaml.automl: 09-19 02:24:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 02:24:18] {3072} INFO -  at 44.1s,	estimator xgboost's best error=3.8294,	best estimator xgboost's best error=3.8294
[flaml.automl: 09-19 02:24:18] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 02:24:24] {3072} INFO -  at 50.1s,	estimator xgboost's best error=3.8294,	best estimator xgboost's best error=3.8294
[flaml.automl: 09-19 02:24:40] {3335} INFO - retrain xgboost for 16.2s
[flaml.automl: 09-19 02:24:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:24:40] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:24:40] {2637} INFO - Time taken to find the best model: 44.053220987319946
[flaml.automl: 09-19 02:24:40] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.8293942152943417
NO2(0)最好结果：{'pred_time': 1.9007473191570714e-05, 'wall_clock_time': 44.053220987319946, 'metric_for_logging': {'pred_time': 1.9007473191570714e-05}, 'val_loss': 3.8293942152943417, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.313823223114014}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.9009889043037793
NO2(0)的mse=49.757255154987845
NO2(0)的mae=3.763603780917999
NO2(0)的mar=0.2011319328176553
总共花费的时间为：66.69
博尔塔拉蒙古自治州
2693A
2694A
[flaml.automl: 09-19 02:31:26] {2390} INFO - task = regression
[flaml.automl: 09-19 02:31:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:31:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:31:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:31:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:31:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:31:30] {3025} INFO - Estimated sufficient time budget=35833s. Estimated necessary time budget=36s.
[flaml.automl: 09-19 02:31:30] {3072} INFO -  at 3.7s,	estimator xgboost's best error=9.7164,	best estimator xgboost's best error=9.7164
[flaml.automl: 09-19 02:31:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:31:35] {3072} INFO -  at 9.2s,	estimator xgboost's best error=5.3236,	best estimator xgboost's best error=5.3236
[flaml.automl: 09-19 02:31:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:31:38] {3072} INFO -  at 11.4s,	estimator xgboost's best error=5.3236,	best estimator xgboost's best error=5.3236
[flaml.automl: 09-19 02:31:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:31:51] {3072} INFO -  at 24.6s,	estimator xgboost's best error=5.3236,	best estimator xgboost's best error=5.3236
[flaml.automl: 09-19 02:31:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:31:52] {3072} INFO -  at 25.7s,	estimator xgboost's best error=4.1956,	best estimator xgboost's best error=4.1956
[flaml.automl: 09-19 02:31:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:31:53] {3072} INFO -  at 27.3s,	estimator xgboost's best error=3.8195,	best estimator xgboost's best error=3.8195
[flaml.automl: 09-19 02:31:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:31:55] {3072} INFO -  at 28.9s,	estimator xgboost's best error=3.8195,	best estimator xgboost's best error=3.8195
[flaml.automl: 09-19 02:31:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:31:58] {3072} INFO -  at 31.4s,	estimator xgboost's best error=3.8195,	best estimator xgboost's best error=3.8195
[flaml.automl: 09-19 02:31:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:31:59] {3072} INFO -  at 32.5s,	estimator xgboost's best error=3.8195,	best estimator xgboost's best error=3.8195
[flaml.automl: 09-19 02:31:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:32:01] {3072} INFO -  at 35.0s,	estimator xgboost's best error=3.8195,	best estimator xgboost's best error=3.8195
[flaml.automl: 09-19 02:32:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:32:02] {3072} INFO -  at 36.1s,	estimator xgboost's best error=3.8195,	best estimator xgboost's best error=3.8195
[flaml.automl: 09-19 02:32:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:32:03] {3072} INFO -  at 37.3s,	estimator xgboost's best error=3.8195,	best estimator xgboost's best error=3.8195
[flaml.automl: 09-19 02:32:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:32:09] {3072} INFO -  at 43.3s,	estimator xgboost's best error=3.7835,	best estimator xgboost's best error=3.7835
[flaml.automl: 09-19 02:32:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 02:32:20] {3072} INFO -  at 53.8s,	estimator xgboost's best error=3.7087,	best estimator xgboost's best error=3.7087
[flaml.automl: 09-19 02:32:30] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-19 02:32:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:32:30] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:32:30] {2637} INFO - Time taken to find the best model: 53.78251671791077
[flaml.automl: 09-19 02:32:30] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-2.7087226515353966
NO2(0)最好结果：{'pred_time': 1.6755398778199845e-05, 'wall_clock_time': 53.78251671791077, 'metric_for_logging': {'pred_time': 1.6755398778199845e-05}, 'val_loss': 3.7087226515353966, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.476655721664429}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.7411892296715319
NO2(0)的mse=34.111329419614066
NO2(0)的mae=3.8885218665219736
NO2(0)的mar=0.3527234055283721
总共花费的时间为：64.70
阿克苏地区
2695A
2696A
[flaml.automl: 09-19 02:39:03] {2390} INFO - task = regression
[flaml.automl: 09-19 02:39:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:39:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:39:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:39:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:39:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:39:06] {3025} INFO - Estimated sufficient time budget=33432s. Estimated necessary time budget=33s.
[flaml.automl: 09-19 02:39:06] {3072} INFO -  at 3.5s,	estimator xgboost's best error=16.7691,	best estimator xgboost's best error=16.7691
[flaml.automl: 09-19 02:39:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:39:12] {3072} INFO -  at 9.6s,	estimator xgboost's best error=8.8979,	best estimator xgboost's best error=8.8979
[flaml.automl: 09-19 02:39:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:39:16] {3072} INFO -  at 13.1s,	estimator xgboost's best error=8.8979,	best estimator xgboost's best error=8.8979
[flaml.automl: 09-19 02:39:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:39:38] {3072} INFO -  at 35.3s,	estimator xgboost's best error=8.8979,	best estimator xgboost's best error=8.8979
[flaml.automl: 09-19 02:39:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:39:40] {3072} INFO -  at 37.4s,	estimator xgboost's best error=7.0237,	best estimator xgboost's best error=7.0237
[flaml.automl: 09-19 02:39:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:39:43] {3072} INFO -  at 40.1s,	estimator xgboost's best error=6.4257,	best estimator xgboost's best error=6.4257
[flaml.automl: 09-19 02:39:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:39:46] {3072} INFO -  at 43.0s,	estimator xgboost's best error=6.4257,	best estimator xgboost's best error=6.4257
[flaml.automl: 09-19 02:39:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:39:49] {3072} INFO -  at 46.4s,	estimator xgboost's best error=6.4257,	best estimator xgboost's best error=6.4257
[flaml.automl: 09-19 02:39:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:39:50] {3072} INFO -  at 47.5s,	estimator xgboost's best error=6.4257,	best estimator xgboost's best error=6.4257
[flaml.automl: 09-19 02:39:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:39:53] {3072} INFO -  at 49.9s,	estimator xgboost's best error=6.4257,	best estimator xgboost's best error=6.4257
[flaml.automl: 09-19 02:39:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:39:54] {3072} INFO -  at 51.0s,	estimator xgboost's best error=6.4257,	best estimator xgboost's best error=6.4257
[flaml.automl: 09-19 02:39:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:39:55] {3072} INFO -  at 52.2s,	estimator xgboost's best error=6.4257,	best estimator xgboost's best error=6.4257
[flaml.automl: 09-19 02:39:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:40:01] {3072} INFO -  at 58.1s,	estimator xgboost's best error=6.2884,	best estimator xgboost's best error=6.2884
[flaml.automl: 09-19 02:40:07] {3335} INFO - retrain xgboost for 6.0s
[flaml.automl: 09-19 02:40:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:40:07] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:40:07] {2637} INFO - Time taken to find the best model: 58.138638973236084
[flaml.automl: 09-19 02:40:07] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
NO2(0)最佳损失：-5.288400304157328
NO2(0)最好结果：{'pred_time': 1.6271317335460288e-05, 'wall_clock_time': 58.138638973236084, 'metric_for_logging': {'pred_time': 1.6271317335460288e-05}, 'val_loss': 6.288400304157328, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 5.983034133911133}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.7955413668139721
NO2(0)的mse=81.84196938961898
NO2(0)的mae=6.194822833837092
NO2(0)的mar=0.34816923802675137
总共花费的时间为：64.55
克孜勒苏柯尔克孜自治州
2697A
3665A
[flaml.automl: 09-19 02:46:29] {2390} INFO - task = regression
[flaml.automl: 09-19 02:46:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:46:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:46:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:46:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:46:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:46:31] {3025} INFO - Estimated sufficient time budget=26991s. Estimated necessary time budget=27s.
[flaml.automl: 09-19 02:46:31] {3072} INFO -  at 2.8s,	estimator xgboost's best error=7.1972,	best estimator xgboost's best error=7.1972
[flaml.automl: 09-19 02:46:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:46:36] {3072} INFO -  at 7.8s,	estimator xgboost's best error=4.1314,	best estimator xgboost's best error=4.1314
[flaml.automl: 09-19 02:46:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:46:39] {3072} INFO -  at 10.5s,	estimator xgboost's best error=4.1314,	best estimator xgboost's best error=4.1314
[flaml.automl: 09-19 02:46:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:47:07] {3072} INFO -  at 38.4s,	estimator xgboost's best error=4.1314,	best estimator xgboost's best error=4.1314
[flaml.automl: 09-19 02:47:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:47:10] {3072} INFO -  at 41.7s,	estimator xgboost's best error=3.5349,	best estimator xgboost's best error=3.5349
[flaml.automl: 09-19 02:47:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:47:14] {3072} INFO -  at 45.1s,	estimator xgboost's best error=3.3758,	best estimator xgboost's best error=3.3758
[flaml.automl: 09-19 02:47:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:47:16] {3072} INFO -  at 47.8s,	estimator xgboost's best error=3.3758,	best estimator xgboost's best error=3.3758
[flaml.automl: 09-19 02:47:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:47:21] {3072} INFO -  at 52.2s,	estimator xgboost's best error=3.3758,	best estimator xgboost's best error=3.3758
[flaml.automl: 09-19 02:47:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:47:23] {3072} INFO -  at 54.3s,	estimator xgboost's best error=3.3758,	best estimator xgboost's best error=3.3758
[flaml.automl: 09-19 02:47:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:47:27] {3072} INFO -  at 58.6s,	estimator xgboost's best error=3.3366,	best estimator xgboost's best error=3.3366
[flaml.automl: 09-19 02:47:32] {3335} INFO - retrain xgboost for 4.4s
[flaml.automl: 09-19 02:47:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:47:32] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:47:32] {2637} INFO - Time taken to find the best model: 58.55412721633911
[flaml.automl: 09-19 02:47:32] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}
NO2(0)最佳损失：-2.336588393608561
NO2(0)最好结果：{'pred_time': 2.7761600486319866e-05, 'wall_clock_time': 58.55412721633911, 'metric_for_logging': {'pred_time': 2.7761600486319866e-05}, 'val_loss': 3.336588393608561, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}, 'config/n_estimators': 4, 'config/max_leaves': 10, 'config/min_child_weight': 0.06372323163910634, 'config/learning_rate': 0.4086634770112844, 'config/subsample': 0.770412309752864, 'config/colsample_bylevel': 0.6720329282771347, 'config/colsample_bytree': 0.752099095598003, 'config/reg_alpha': 0.00271725862127768, 'config/reg_lambda': 3.9335607770117846, 'experiment_tag': 'exp', 'time_total_s': 4.272881746292114}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.42691226842861474
NO2(0)的mse=30.28283084767584
NO2(0)的mae=3.293960490818256
NO2(0)的mar=0.3497288665030203
总共花费的时间为：63.42
喀什地区
2698A
2699A
2700A
[flaml.automl: 09-19 02:57:16] {2390} INFO - task = regression
[flaml.automl: 09-19 02:57:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:57:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:57:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:57:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:57:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:57:17] {3025} INFO - Estimated sufficient time budget=12217s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 02:57:17] {3072} INFO -  at 1.4s,	estimator xgboost's best error=22.4059,	best estimator xgboost's best error=22.4059
[flaml.automl: 09-19 02:57:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:57:19] {3072} INFO -  at 3.5s,	estimator xgboost's best error=11.7653,	best estimator xgboost's best error=11.7653
[flaml.automl: 09-19 02:57:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:57:20] {3072} INFO -  at 4.7s,	estimator xgboost's best error=11.7653,	best estimator xgboost's best error=11.7653
[flaml.automl: 09-19 02:57:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:57:30] {3072} INFO -  at 14.7s,	estimator xgboost's best error=11.7653,	best estimator xgboost's best error=11.7653
[flaml.automl: 09-19 02:57:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:57:31] {3072} INFO -  at 15.9s,	estimator xgboost's best error=8.3840,	best estimator xgboost's best error=8.3840
[flaml.automl: 09-19 02:57:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:57:33] {3072} INFO -  at 17.6s,	estimator xgboost's best error=7.5773,	best estimator xgboost's best error=7.5773
[flaml.automl: 09-19 02:57:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:57:35] {3072} INFO -  at 19.2s,	estimator xgboost's best error=7.5773,	best estimator xgboost's best error=7.5773
[flaml.automl: 09-19 02:57:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:57:37] {3072} INFO -  at 21.7s,	estimator xgboost's best error=7.5773,	best estimator xgboost's best error=7.5773
[flaml.automl: 09-19 02:57:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:57:38] {3072} INFO -  at 22.8s,	estimator xgboost's best error=7.5773,	best estimator xgboost's best error=7.5773
[flaml.automl: 09-19 02:57:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:57:41] {3072} INFO -  at 25.5s,	estimator xgboost's best error=7.5773,	best estimator xgboost's best error=7.5773
[flaml.automl: 09-19 02:57:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:57:42] {3072} INFO -  at 26.7s,	estimator xgboost's best error=7.5773,	best estimator xgboost's best error=7.5773
[flaml.automl: 09-19 02:57:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:57:43] {3072} INFO -  at 27.8s,	estimator xgboost's best error=7.5773,	best estimator xgboost's best error=7.5773
[flaml.automl: 09-19 02:57:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:57:50] {3072} INFO -  at 34.3s,	estimator xgboost's best error=7.1212,	best estimator xgboost's best error=7.1212
[flaml.automl: 09-19 02:57:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 02:58:02] {3072} INFO -  at 46.4s,	estimator xgboost's best error=6.9078,	best estimator xgboost's best error=6.9078
[flaml.automl: 09-19 02:58:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 02:58:08] {3072} INFO -  at 53.0s,	estimator xgboost's best error=6.9078,	best estimator xgboost's best error=6.9078
[flaml.automl: 09-19 02:58:20] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-19 02:58:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:58:20] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:58:20] {2637} INFO - Time taken to find the best model: 46.44257044792175
[flaml.automl: 09-19 02:58:20] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-5.907846845070191
NO2(0)最好结果：{'pred_time': 1.259733862237152e-05, 'wall_clock_time': 46.44257044792175, 'metric_for_logging': {'pred_time': 1.259733862237152e-05}, 'val_loss': 6.907846845070191, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.14760446548462}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8155976634474722
NO2(0)的mse=103.9963398306467
NO2(0)的mae=6.856281762222845
NO2(0)的mar=0.2936121749822247
总共花费的时间为：65.58
和田地区
3614A
3615A
[flaml.automl: 09-19 03:05:20] {2390} INFO - task = regression
[flaml.automl: 09-19 03:05:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:05:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:05:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:05:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:05:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:05:21] {3025} INFO - Estimated sufficient time budget=12413s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 03:05:21] {3072} INFO -  at 1.3s,	estimator xgboost's best error=13.3070,	best estimator xgboost's best error=13.3070
[flaml.automl: 09-19 03:05:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:05:23] {3072} INFO -  at 3.5s,	estimator xgboost's best error=7.5423,	best estimator xgboost's best error=7.5423
[flaml.automl: 09-19 03:05:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:05:24] {3072} INFO -  at 4.7s,	estimator xgboost's best error=7.5423,	best estimator xgboost's best error=7.5423
[flaml.automl: 09-19 03:05:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:05:34] {3072} INFO -  at 14.2s,	estimator xgboost's best error=7.5423,	best estimator xgboost's best error=7.5423
[flaml.automl: 09-19 03:05:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:05:35] {3072} INFO -  at 15.3s,	estimator xgboost's best error=6.1738,	best estimator xgboost's best error=6.1738
[flaml.automl: 09-19 03:05:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:05:37] {3072} INFO -  at 16.9s,	estimator xgboost's best error=5.8986,	best estimator xgboost's best error=5.8986
[flaml.automl: 09-19 03:05:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:05:38] {3072} INFO -  at 18.5s,	estimator xgboost's best error=5.8986,	best estimator xgboost's best error=5.8986
[flaml.automl: 09-19 03:05:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:05:41] {3072} INFO -  at 21.0s,	estimator xgboost's best error=5.8986,	best estimator xgboost's best error=5.8986
[flaml.automl: 09-19 03:05:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:05:42] {3072} INFO -  at 22.2s,	estimator xgboost's best error=5.8986,	best estimator xgboost's best error=5.8986
[flaml.automl: 09-19 03:05:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:05:44] {3072} INFO -  at 24.6s,	estimator xgboost's best error=5.8986,	best estimator xgboost's best error=5.8986
[flaml.automl: 09-19 03:05:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:05:46] {3072} INFO -  at 25.8s,	estimator xgboost's best error=5.8986,	best estimator xgboost's best error=5.8986
[flaml.automl: 09-19 03:05:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:05:47] {3072} INFO -  at 26.9s,	estimator xgboost's best error=5.8986,	best estimator xgboost's best error=5.8986
[flaml.automl: 09-19 03:05:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:05:53] {3072} INFO -  at 33.0s,	estimator xgboost's best error=5.8986,	best estimator xgboost's best error=5.8986
[flaml.automl: 09-19 03:05:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:05:56] {3072} INFO -  at 35.8s,	estimator xgboost's best error=5.8441,	best estimator xgboost's best error=5.8441
[flaml.automl: 09-19 03:05:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 03:05:57] {3072} INFO -  at 37.4s,	estimator xgboost's best error=5.8441,	best estimator xgboost's best error=5.8441
[flaml.automl: 09-19 03:05:57] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 03:06:02] {3072} INFO -  at 42.2s,	estimator xgboost's best error=5.8270,	best estimator xgboost's best error=5.8270
[flaml.automl: 09-19 03:06:02] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 03:06:05] {3072} INFO -  at 45.1s,	estimator xgboost's best error=5.8270,	best estimator xgboost's best error=5.8270
[flaml.automl: 09-19 03:06:05] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-19 03:06:07] {3072} INFO -  at 47.2s,	estimator xgboost's best error=5.8270,	best estimator xgboost's best error=5.8270
[flaml.automl: 09-19 03:06:07] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-19 03:06:19] {3072} INFO -  at 59.1s,	estimator xgboost's best error=5.8270,	best estimator xgboost's best error=5.8270
[flaml.automl: 09-19 03:06:24] {3335} INFO - retrain xgboost for 4.8s
[flaml.automl: 09-19 03:06:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:06:24] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:06:24] {2637} INFO - Time taken to find the best model: 42.219412326812744
[flaml.automl: 09-19 03:06:24] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}
NO2(0)最佳损失：-4.826972614281784
NO2(0)最好结果：{'pred_time': 1.7413659343285808e-05, 'wall_clock_time': 42.219412326812744, 'metric_for_logging': {'pred_time': 1.7413659343285808e-05}, 'val_loss': 5.826972614281784, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}, 'config/n_estimators': 9, 'config/max_leaves': 9, 'config/min_child_weight': 3.029166996333365, 'config/learning_rate': 0.28782051713764356, 'config/subsample': 0.8051874633193139, 'config/colsample_bylevel': 0.656637827405616, 'config/colsample_bytree': 0.7193335366419935, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 2.3848121277079324, 'experiment_tag': 'exp', 'time_total_s': 4.826449871063232}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.575986594746179
NO2(0)的mse=77.83512940414384
NO2(0)的mae=5.98704405917164
NO2(0)的mar=0.4236324519611896
总共花费的时间为：64.26
伊犁哈萨克州
2703A
2704A
2705A
[flaml.automl: 09-19 03:16:33] {2390} INFO - task = regression
[flaml.automl: 09-19 03:16:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:16:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:16:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:16:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:16:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:16:35] {3025} INFO - Estimated sufficient time budget=11911s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 03:16:35] {3072} INFO -  at 1.3s,	estimator xgboost's best error=17.3584,	best estimator xgboost's best error=17.3584
[flaml.automl: 09-19 03:16:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:16:37] {3072} INFO -  at 3.5s,	estimator xgboost's best error=8.6084,	best estimator xgboost's best error=8.6084
[flaml.automl: 09-19 03:16:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:16:38] {3072} INFO -  at 4.7s,	estimator xgboost's best error=8.6084,	best estimator xgboost's best error=8.6084
[flaml.automl: 09-19 03:16:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:16:48] {3072} INFO -  at 14.7s,	estimator xgboost's best error=8.6084,	best estimator xgboost's best error=8.6084
[flaml.automl: 09-19 03:16:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:16:49] {3072} INFO -  at 15.8s,	estimator xgboost's best error=6.2278,	best estimator xgboost's best error=6.2278
[flaml.automl: 09-19 03:16:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:16:51] {3072} INFO -  at 17.4s,	estimator xgboost's best error=5.5840,	best estimator xgboost's best error=5.5840
[flaml.automl: 09-19 03:16:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:16:52] {3072} INFO -  at 19.1s,	estimator xgboost's best error=5.5840,	best estimator xgboost's best error=5.5840
[flaml.automl: 09-19 03:16:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:16:55] {3072} INFO -  at 21.6s,	estimator xgboost's best error=5.5840,	best estimator xgboost's best error=5.5840
[flaml.automl: 09-19 03:16:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:16:56] {3072} INFO -  at 22.7s,	estimator xgboost's best error=5.5840,	best estimator xgboost's best error=5.5840
[flaml.automl: 09-19 03:16:56] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:16:59] {3072} INFO -  at 25.4s,	estimator xgboost's best error=5.5840,	best estimator xgboost's best error=5.5840
[flaml.automl: 09-19 03:16:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:17:00] {3072} INFO -  at 26.6s,	estimator xgboost's best error=5.5840,	best estimator xgboost's best error=5.5840
[flaml.automl: 09-19 03:17:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:17:01] {3072} INFO -  at 27.7s,	estimator xgboost's best error=5.5840,	best estimator xgboost's best error=5.5840
[flaml.automl: 09-19 03:17:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:17:07] {3072} INFO -  at 34.3s,	estimator xgboost's best error=5.5248,	best estimator xgboost's best error=5.5248
[flaml.automl: 09-19 03:17:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:17:20] {3072} INFO -  at 46.3s,	estimator xgboost's best error=5.1897,	best estimator xgboost's best error=5.1897
[flaml.automl: 09-19 03:17:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 03:17:26] {3072} INFO -  at 52.8s,	estimator xgboost's best error=5.1897,	best estimator xgboost's best error=5.1897
[flaml.automl: 09-19 03:17:38] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-19 03:17:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:17:38] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:17:38] {2637} INFO - Time taken to find the best model: 46.33404588699341
[flaml.automl: 09-19 03:17:38] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-4.18973767506002
NO2(0)最好结果：{'pred_time': 1.100042819062575e-05, 'wall_clock_time': 46.33404588699341, 'metric_for_logging': {'pred_time': 1.100042819062575e-05}, 'val_loss': 5.18973767506002, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.081974744796753}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8037334537991808
NO2(0)的mse=60.787508124588655
NO2(0)的mae=5.093826166949367
NO2(0)的mar=0.21369683239652645
总共花费的时间为：65.41
塔城地区
2706A
[flaml.automl: 09-19 03:20:59] {2390} INFO - task = regression
[flaml.automl: 09-19 03:20:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:20:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:20:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:20:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:20:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:21:02] {3025} INFO - Estimated sufficient time budget=22972s. Estimated necessary time budget=23s.
[flaml.automl: 09-19 03:21:02] {3072} INFO -  at 2.4s,	estimator xgboost's best error=5.9994,	best estimator xgboost's best error=5.9994
[flaml.automl: 09-19 03:21:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:21:05] {3072} INFO -  at 5.9s,	estimator xgboost's best error=3.4304,	best estimator xgboost's best error=3.4304
[flaml.automl: 09-19 03:21:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:21:08] {3072} INFO -  at 8.3s,	estimator xgboost's best error=3.4304,	best estimator xgboost's best error=3.4304
[flaml.automl: 09-19 03:21:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:21:21] {3072} INFO -  at 21.4s,	estimator xgboost's best error=3.4304,	best estimator xgboost's best error=3.4304
[flaml.automl: 09-19 03:21:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:21:23] {3072} INFO -  at 23.6s,	estimator xgboost's best error=2.4548,	best estimator xgboost's best error=2.4548
[flaml.automl: 09-19 03:21:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:21:26] {3072} INFO -  at 26.7s,	estimator xgboost's best error=2.3876,	best estimator xgboost's best error=2.3876
[flaml.automl: 09-19 03:21:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:21:29] {3072} INFO -  at 29.9s,	estimator xgboost's best error=2.3517,	best estimator xgboost's best error=2.3517
[flaml.automl: 09-19 03:21:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:21:34] {3072} INFO -  at 34.6s,	estimator xgboost's best error=2.3517,	best estimator xgboost's best error=2.3517
[flaml.automl: 09-19 03:21:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:21:37] {3072} INFO -  at 37.6s,	estimator xgboost's best error=2.3517,	best estimator xgboost's best error=2.3517
[flaml.automl: 09-19 03:21:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:21:42] {3072} INFO -  at 42.7s,	estimator xgboost's best error=2.3275,	best estimator xgboost's best error=2.3275
[flaml.automl: 09-19 03:21:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:21:45] {3072} INFO -  at 45.9s,	estimator xgboost's best error=2.3275,	best estimator xgboost's best error=2.3275
[flaml.automl: 09-19 03:21:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:21:47] {3072} INFO -  at 48.2s,	estimator xgboost's best error=2.3275,	best estimator xgboost's best error=2.3275
[flaml.automl: 09-19 03:21:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:21:59] {3072} INFO -  at 59.4s,	estimator xgboost's best error=2.3275,	best estimator xgboost's best error=2.3275
[flaml.automl: 09-19 03:22:03] {3335} INFO - retrain xgboost for 4.5s
[flaml.automl: 09-19 03:22:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 03:22:03] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:22:03] {2637} INFO - Time taken to find the best model: 42.69376611709595
[flaml.automl: 09-19 03:22:03] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
NO2(0)最佳损失：-1.3274868690916395
NO2(0)最好结果：{'pred_time': 8.3536753838666e-05, 'wall_clock_time': 42.69376611709595, 'metric_for_logging': {'pred_time': 8.3536753838666e-05}, 'val_loss': 2.3274868690916395, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 6, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 5.057438850402832}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=-0.20723411247602552
NO2(0)的mse=12.39236083181793
NO2(0)的mae=2.3167217767220194
NO2(0)的mar=0.2636230940495014
总共花费的时间为：64.08
阿勒泰地区
阿勒泰地区没有数据
石河子市
2709A
2710A
3442A
[flaml.automl: 09-19 03:32:30] {2390} INFO - task = regression
[flaml.automl: 09-19 03:32:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:32:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:32:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:32:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:32:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:32:33] {3025} INFO - Estimated sufficient time budget=29574s. Estimated necessary time budget=30s.
[flaml.automl: 09-19 03:32:33] {3072} INFO -  at 3.1s,	estimator xgboost's best error=17.4616,	best estimator xgboost's best error=17.4616
[flaml.automl: 09-19 03:32:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:32:36] {3072} INFO -  at 6.8s,	estimator xgboost's best error=8.9782,	best estimator xgboost's best error=8.9782
[flaml.automl: 09-19 03:32:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:32:38] {3072} INFO -  at 8.9s,	estimator xgboost's best error=8.9782,	best estimator xgboost's best error=8.9782
[flaml.automl: 09-19 03:32:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:32:52] {3072} INFO -  at 22.5s,	estimator xgboost's best error=8.9782,	best estimator xgboost's best error=8.9782
[flaml.automl: 09-19 03:32:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:32:53] {3072} INFO -  at 23.7s,	estimator xgboost's best error=6.3075,	best estimator xgboost's best error=6.3075
[flaml.automl: 09-19 03:32:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:32:55] {3072} INFO -  at 25.2s,	estimator xgboost's best error=5.5671,	best estimator xgboost's best error=5.5671
[flaml.automl: 09-19 03:32:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:32:56] {3072} INFO -  at 26.8s,	estimator xgboost's best error=5.5671,	best estimator xgboost's best error=5.5671
[flaml.automl: 09-19 03:32:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:32:59] {3072} INFO -  at 29.3s,	estimator xgboost's best error=5.5671,	best estimator xgboost's best error=5.5671
[flaml.automl: 09-19 03:32:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:33:00] {3072} INFO -  at 30.4s,	estimator xgboost's best error=5.5671,	best estimator xgboost's best error=5.5671
[flaml.automl: 09-19 03:33:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:33:02] {3072} INFO -  at 33.1s,	estimator xgboost's best error=5.5671,	best estimator xgboost's best error=5.5671
[flaml.automl: 09-19 03:33:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:33:04] {3072} INFO -  at 34.2s,	estimator xgboost's best error=5.5671,	best estimator xgboost's best error=5.5671
[flaml.automl: 09-19 03:33:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:33:05] {3072} INFO -  at 35.3s,	estimator xgboost's best error=5.5671,	best estimator xgboost's best error=5.5671
[flaml.automl: 09-19 03:33:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:33:11] {3072} INFO -  at 41.8s,	estimator xgboost's best error=5.2179,	best estimator xgboost's best error=5.2179
[flaml.automl: 09-19 03:33:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:33:23] {3072} INFO -  at 53.8s,	estimator xgboost's best error=5.0810,	best estimator xgboost's best error=5.0810
[flaml.automl: 09-19 03:33:35] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-19 03:33:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:33:35] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:33:35] {2637} INFO - Time taken to find the best model: 53.81193780899048
[flaml.automl: 09-19 03:33:35] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-4.080987159758097
NO2(0)最好结果：{'pred_time': 1.110237851851533e-05, 'wall_clock_time': 53.81193780899048, 'metric_for_logging': {'pred_time': 1.110237851851533e-05}, 'val_loss': 5.080987159758097, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.024549007415771}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.850028468149886
NO2(0)的mse=58.125135487997696
NO2(0)的mae=5.016063973634076
NO2(0)的mar=0.2562307395037775
总共花费的时间为：66.88
五家渠市
2711A
3441A
[flaml.automl: 09-19 03:40:07] {2390} INFO - task = regression
[flaml.automl: 09-19 03:40:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:40:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:40:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:40:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:40:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:40:08] {3025} INFO - Estimated sufficient time budget=12139s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 03:40:08] {3072} INFO -  at 1.3s,	estimator xgboost's best error=17.8330,	best estimator xgboost's best error=17.8330
[flaml.automl: 09-19 03:40:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:40:10] {3072} INFO -  at 3.5s,	estimator xgboost's best error=9.0172,	best estimator xgboost's best error=9.0172
[flaml.automl: 09-19 03:40:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:40:11] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.0172,	best estimator xgboost's best error=9.0172
[flaml.automl: 09-19 03:40:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:40:21] {3072} INFO -  at 14.2s,	estimator xgboost's best error=9.0172,	best estimator xgboost's best error=9.0172
[flaml.automl: 09-19 03:40:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:40:22] {3072} INFO -  at 15.4s,	estimator xgboost's best error=6.1851,	best estimator xgboost's best error=6.1851
[flaml.automl: 09-19 03:40:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:40:23] {3072} INFO -  at 17.0s,	estimator xgboost's best error=5.2621,	best estimator xgboost's best error=5.2621
[flaml.automl: 09-19 03:40:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:40:25] {3072} INFO -  at 18.6s,	estimator xgboost's best error=5.2621,	best estimator xgboost's best error=5.2621
[flaml.automl: 09-19 03:40:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:40:28] {3072} INFO -  at 21.1s,	estimator xgboost's best error=5.2621,	best estimator xgboost's best error=5.2621
[flaml.automl: 09-19 03:40:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:40:29] {3072} INFO -  at 22.3s,	estimator xgboost's best error=5.2621,	best estimator xgboost's best error=5.2621
[flaml.automl: 09-19 03:40:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:40:31] {3072} INFO -  at 24.8s,	estimator xgboost's best error=5.2621,	best estimator xgboost's best error=5.2621
[flaml.automl: 09-19 03:40:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:40:32] {3072} INFO -  at 26.0s,	estimator xgboost's best error=5.2621,	best estimator xgboost's best error=5.2621
[flaml.automl: 09-19 03:40:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:40:34] {3072} INFO -  at 27.1s,	estimator xgboost's best error=5.2621,	best estimator xgboost's best error=5.2621
[flaml.automl: 09-19 03:40:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:40:40] {3072} INFO -  at 33.3s,	estimator xgboost's best error=4.9540,	best estimator xgboost's best error=4.9540
[flaml.automl: 09-19 03:40:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:40:50] {3072} INFO -  at 43.9s,	estimator xgboost's best error=4.7746,	best estimator xgboost's best error=4.7746
[flaml.automl: 09-19 03:40:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 03:40:56] {3072} INFO -  at 50.0s,	estimator xgboost's best error=4.7746,	best estimator xgboost's best error=4.7746
[flaml.automl: 09-19 03:41:07] {3335} INFO - retrain xgboost for 10.5s
[flaml.automl: 09-19 03:41:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:41:07] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:41:07] {2637} INFO - Time taken to find the best model: 43.89756774902344
[flaml.automl: 09-19 03:41:07] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-3.77461528627242
NO2(0)最好结果：{'pred_time': 1.956289300230775e-05, 'wall_clock_time': 43.89756774902344, 'metric_for_logging': {'pred_time': 1.956289300230775e-05}, 'val_loss': 4.77461528627242, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.57837700843811}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8871408880851204
NO2(0)的mse=45.033968727517134
NO2(0)的mae=4.558689074091034
NO2(0)的mar=0.2312344042389936
总共花费的时间为：60.97
三沙市
三沙市没有数据
兰州新区
3245A
3246A
[flaml.automl: 09-19 03:47:34] {2390} INFO - task = regression
[flaml.automl: 09-19 03:47:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:47:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:47:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:47:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:47:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:47:35] {3025} INFO - Estimated sufficient time budget=12288s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 03:47:35] {3072} INFO -  at 1.4s,	estimator xgboost's best error=13.0095,	best estimator xgboost's best error=13.0095
[flaml.automl: 09-19 03:47:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:47:37] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.8108,	best estimator xgboost's best error=6.8108
[flaml.automl: 09-19 03:47:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:47:38] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.8108,	best estimator xgboost's best error=6.8108
[flaml.automl: 09-19 03:47:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:47:49] {3072} INFO -  at 15.2s,	estimator xgboost's best error=6.8108,	best estimator xgboost's best error=6.8108
[flaml.automl: 09-19 03:47:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:47:51] {3072} INFO -  at 17.4s,	estimator xgboost's best error=5.3440,	best estimator xgboost's best error=5.3440
[flaml.automl: 09-19 03:47:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:47:54] {3072} INFO -  at 20.4s,	estimator xgboost's best error=4.9022,	best estimator xgboost's best error=4.9022
[flaml.automl: 09-19 03:47:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:47:57] {3072} INFO -  at 23.5s,	estimator xgboost's best error=4.9022,	best estimator xgboost's best error=4.9022
[flaml.automl: 09-19 03:47:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:48:02] {3072} INFO -  at 28.4s,	estimator xgboost's best error=4.9022,	best estimator xgboost's best error=4.9022
[flaml.automl: 09-19 03:48:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:48:04] {3072} INFO -  at 30.5s,	estimator xgboost's best error=4.9022,	best estimator xgboost's best error=4.9022
[flaml.automl: 09-19 03:48:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:48:08] {3072} INFO -  at 34.8s,	estimator xgboost's best error=4.9022,	best estimator xgboost's best error=4.9022
[flaml.automl: 09-19 03:48:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:48:10] {3072} INFO -  at 36.9s,	estimator xgboost's best error=4.9022,	best estimator xgboost's best error=4.9022
[flaml.automl: 09-19 03:48:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:48:12] {3072} INFO -  at 39.1s,	estimator xgboost's best error=4.9022,	best estimator xgboost's best error=4.9022
[flaml.automl: 09-19 03:48:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:48:23] {3072} INFO -  at 49.7s,	estimator xgboost's best error=4.9022,	best estimator xgboost's best error=4.9022
[flaml.automl: 09-19 03:48:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:48:28] {3072} INFO -  at 54.8s,	estimator xgboost's best error=4.9022,	best estimator xgboost's best error=4.9022
[flaml.automl: 09-19 03:48:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 03:48:32] {3072} INFO -  at 58.4s,	estimator xgboost's best error=4.9022,	best estimator xgboost's best error=4.9022
[flaml.automl: 09-19 03:48:34] {3335} INFO - retrain xgboost for 2.6s
[flaml.automl: 09-19 03:48:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 03:48:34] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:48:34] {2637} INFO - Time taken to find the best model: 20.397634506225586
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
NO2(0)最佳损失：-3.902244972907277
NO2(0)最好结果：{'pred_time': 3.824483836838121e-05, 'wall_clock_time': 20.397634506225586, 'metric_for_logging': {'pred_time': 3.824483836838121e-05}, 'val_loss': 4.902244972907277, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 3.0248939990997314}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
NO2(0)的R2=0.593612518773462
NO2(0)的mse=64.34800529555386
NO2(0)的mae=5.159564083586649
NO2(0)的mar=0.2783686262415486
总共花费的时间为：61.45
赣江新区
3414A
[flaml.automl: 09-19 03:51:38] {2390} INFO - task = regression
[flaml.automl: 09-19 03:51:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:51:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:51:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:51:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:51:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:51:39] {3025} INFO - Estimated sufficient time budget=11928s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 03:51:39] {3072} INFO -  at 1.3s,	estimator xgboost's best error=15.5146,	best estimator xgboost's best error=15.5146
[flaml.automl: 09-19 03:51:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:51:41] {3072} INFO -  at 3.1s,	estimator xgboost's best error=8.7798,	best estimator xgboost's best error=8.7798
[flaml.automl: 09-19 03:51:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:51:42] {3072} INFO -  at 4.3s,	estimator xgboost's best error=8.7798,	best estimator xgboost's best error=8.7798
[flaml.automl: 09-19 03:51:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:51:49] {3072} INFO -  at 11.4s,	estimator xgboost's best error=8.7798,	best estimator xgboost's best error=8.7798
[flaml.automl: 09-19 03:51:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:51:51] {3072} INFO -  at 12.5s,	estimator xgboost's best error=5.7788,	best estimator xgboost's best error=5.7788
[flaml.automl: 09-19 03:51:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:51:52] {3072} INFO -  at 14.1s,	estimator xgboost's best error=5.4469,	best estimator xgboost's best error=5.4469
[flaml.automl: 09-19 03:51:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:51:54] {3072} INFO -  at 15.7s,	estimator xgboost's best error=5.4469,	best estimator xgboost's best error=5.4469
[flaml.automl: 09-19 03:51:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:51:56] {3072} INFO -  at 18.0s,	estimator xgboost's best error=5.4469,	best estimator xgboost's best error=5.4469
[flaml.automl: 09-19 03:51:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:51:57] {3072} INFO -  at 19.4s,	estimator xgboost's best error=5.4469,	best estimator xgboost's best error=5.4469
[flaml.automl: 09-19 03:51:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:52:02] {3072} INFO -  at 23.8s,	estimator xgboost's best error=5.3382,	best estimator xgboost's best error=5.3382
[flaml.automl: 09-19 03:52:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:52:05] {3072} INFO -  at 26.5s,	estimator xgboost's best error=5.3382,	best estimator xgboost's best error=5.3382
[flaml.automl: 09-19 03:52:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:52:07] {3072} INFO -  at 28.7s,	estimator xgboost's best error=5.3382,	best estimator xgboost's best error=5.3382
[flaml.automl: 09-19 03:52:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:52:22] {3072} INFO -  at 43.8s,	estimator xgboost's best error=5.3382,	best estimator xgboost's best error=5.3382
[flaml.automl: 09-19 03:52:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:52:29] {3072} INFO -  at 50.5s,	estimator xgboost's best error=5.3382,	best estimator xgboost's best error=5.3382
[flaml.automl: 09-19 03:52:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 03:52:34] {3072} INFO -  at 55.7s,	estimator xgboost's best error=5.3382,	best estimator xgboost's best error=5.3382
[flaml.automl: 09-19 03:52:38] {3335} INFO - retrain xgboost for 4.3s
[flaml.automl: 09-19 03:52:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:52:38] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:52:38] {2637} INFO - Time taken to find the best model: 23.790552139282227
NO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}
NO2(0)最佳损失：-4.338244899328402
NO2(0)最好结果：{'pred_time': 5.703631782210094e-05, 'wall_clock_time': 23.790552139282227, 'metric_for_logging': {'pred_time': 5.703631782210094e-05}, 'val_loss': 5.338244899328402, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}, 'config/n_estimators': 4, 'config/max_leaves': 10, 'config/min_child_weight': 0.06372323163910634, 'config/learning_rate': 0.4086634770112844, 'config/subsample': 0.770412309752864, 'config/colsample_bylevel': 0.6720329282771347, 'config/colsample_bytree': 0.752099095598003, 'config/reg_alpha': 0.00271725862127768, 'config/reg_lambda': 3.9335607770117846, 'experiment_tag': 'exp', 'time_total_s': 4.400944948196411}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.4927998220458001
NO2(0)的mse=76.85339293806115
NO2(0)的mae=5.711637678204249
NO2(0)的mar=0.2543637796138466
总共花费的时间为：60.31
儋州市
3541A
3542A
[flaml.automl: 09-19 03:59:31] {2390} INFO - task = regression
[flaml.automl: 09-19 03:59:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:59:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:59:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:59:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:59:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:59:33] {3025} INFO - Estimated sufficient time budget=22525s. Estimated necessary time budget=23s.
[flaml.automl: 09-19 03:59:33] {3072} INFO -  at 2.4s,	estimator xgboost's best error=5.8037,	best estimator xgboost's best error=5.8037
[flaml.automl: 09-19 03:59:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:59:37] {3072} INFO -  at 6.4s,	estimator xgboost's best error=2.7688,	best estimator xgboost's best error=2.7688
[flaml.automl: 09-19 03:59:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:59:40] {3072} INFO -  at 8.5s,	estimator xgboost's best error=2.7688,	best estimator xgboost's best error=2.7688
[flaml.automl: 09-19 03:59:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:59:57] {3072} INFO -  at 26.0s,	estimator xgboost's best error=2.7688,	best estimator xgboost's best error=2.7688
[flaml.automl: 09-19 03:59:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:59:59] {3072} INFO -  at 28.1s,	estimator xgboost's best error=1.7387,	best estimator xgboost's best error=1.7387
[flaml.automl: 09-19 03:59:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:00:02] {3072} INFO -  at 31.0s,	estimator xgboost's best error=1.5888,	best estimator xgboost's best error=1.5888
[flaml.automl: 09-19 04:00:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:00:05] {3072} INFO -  at 33.9s,	estimator xgboost's best error=1.5888,	best estimator xgboost's best error=1.5888
[flaml.automl: 09-19 04:00:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:00:09] {3072} INFO -  at 38.3s,	estimator xgboost's best error=1.5888,	best estimator xgboost's best error=1.5888
[flaml.automl: 09-19 04:00:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:00:10] {3072} INFO -  at 39.5s,	estimator xgboost's best error=1.5888,	best estimator xgboost's best error=1.5888
[flaml.automl: 09-19 04:00:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:00:13] {3072} INFO -  at 41.9s,	estimator xgboost's best error=1.5888,	best estimator xgboost's best error=1.5888
[flaml.automl: 09-19 04:00:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 04:00:14] {3072} INFO -  at 43.0s,	estimator xgboost's best error=1.5888,	best estimator xgboost's best error=1.5888
[flaml.automl: 09-19 04:00:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 04:00:15] {3072} INFO -  at 44.2s,	estimator xgboost's best error=1.5888,	best estimator xgboost's best error=1.5888
[flaml.automl: 09-19 04:00:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 04:00:21] {3072} INFO -  at 50.2s,	estimator xgboost's best error=1.5535,	best estimator xgboost's best error=1.5535
[flaml.automl: 09-19 04:00:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 04:00:31] {3072} INFO -  at 59.5s,	estimator xgboost's best error=1.5192,	best estimator xgboost's best error=1.5192
[flaml.automl: 09-19 04:00:41] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-19 04:00:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 04:00:41] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:00:41] {2637} INFO - Time taken to find the best model: 59.47030305862427
[flaml.automl: 09-19 04:00:41] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-0.5192337014084856
NO2(0)最好结果：{'pred_time': 1.6515221902431184e-05, 'wall_clock_time': 59.47030305862427, 'metric_for_logging': {'pred_time': 1.6515221902431184e-05}, 'val_loss': 1.5192337014084856, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 9.239462852478027}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.6233550908618173
NO2(0)的mse=4.482764079498417
NO2(0)的mae=1.5150513340025926
NO2(0)的mar=0.1830603766660654
总共花费的时间为：70.30
雄安新区
3584A
3585A
3586A
3587A
3588A
3589A
[flaml.automl: 09-19 04:18:39] {2390} INFO - task = regression
[flaml.automl: 09-19 04:18:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 04:18:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 04:18:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 04:18:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 04:18:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 04:18:40] {3025} INFO - Estimated sufficient time budget=75096s. Estimated necessary time budget=75s.
[flaml.automl: 09-19 04:18:40] {3072} INFO -  at 1.5s,	estimator xgboost's best error=20.4045,	best estimator xgboost's best error=20.4045
[flaml.automl: 09-19 04:18:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 04:18:42] {3072} INFO -  at 3.6s,	estimator xgboost's best error=9.9883,	best estimator xgboost's best error=9.9883
[flaml.automl: 09-19 04:18:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 04:18:43] {3072} INFO -  at 4.7s,	estimator xgboost's best error=9.9883,	best estimator xgboost's best error=9.9883
[flaml.automl: 09-19 04:18:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 04:18:47] {3072} INFO -  at 8.5s,	estimator xgboost's best error=9.9883,	best estimator xgboost's best error=9.9883
[flaml.automl: 09-19 04:18:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 04:18:48] {3072} INFO -  at 9.6s,	estimator xgboost's best error=6.3093,	best estimator xgboost's best error=6.3093
[flaml.automl: 09-19 04:18:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:18:49] {3072} INFO -  at 11.1s,	estimator xgboost's best error=5.3139,	best estimator xgboost's best error=5.3139
[flaml.automl: 09-19 04:18:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:18:51] {3072} INFO -  at 12.7s,	estimator xgboost's best error=5.3139,	best estimator xgboost's best error=5.3139
[flaml.automl: 09-19 04:18:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:18:53] {3072} INFO -  at 15.1s,	estimator xgboost's best error=5.3139,	best estimator xgboost's best error=5.3139
[flaml.automl: 09-19 04:18:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:18:54] {3072} INFO -  at 16.2s,	estimator xgboost's best error=5.3139,	best estimator xgboost's best error=5.3139
[flaml.automl: 09-19 04:18:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:18:57] {3072} INFO -  at 18.9s,	estimator xgboost's best error=5.3139,	best estimator xgboost's best error=5.3139
[flaml.automl: 09-19 04:18:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 04:18:59] {3072} INFO -  at 20.4s,	estimator xgboost's best error=5.2881,	best estimator xgboost's best error=5.2881
[flaml.automl: 09-19 04:18:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 04:19:00] {3072} INFO -  at 21.6s,	estimator xgboost's best error=5.2881,	best estimator xgboost's best error=5.2881
[flaml.automl: 09-19 04:19:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 04:19:06] {3072} INFO -  at 28.0s,	estimator xgboost's best error=4.9280,	best estimator xgboost's best error=4.9280
[flaml.automl: 09-19 04:19:06] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 04:19:20] {3072} INFO -  at 41.7s,	estimator xgboost's best error=4.7818,	best estimator xgboost's best error=4.7818
[flaml.automl: 09-19 04:19:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 04:19:30] {3072} INFO -  at 52.1s,	estimator xgboost's best error=4.7818,	best estimator xgboost's best error=4.7818
[flaml.automl: 09-19 04:19:49] {3335} INFO - retrain xgboost for 18.8s
[flaml.automl: 09-19 04:19:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 04:19:49] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:19:49] {2637} INFO - Time taken to find the best model: 41.66818380355835
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64395}
NO2(0)最佳损失：-3.7818147284263173
NO2(0)最好结果：{'pred_time': 9.374498571239927e-06, 'wall_clock_time': 41.66818380355835, 'metric_for_logging': {'pred_time': 9.374498571239927e-06}, 'val_loss': 4.781814728426317, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64395}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 64395, 'experiment_tag': 'exp', 'time_total_s': 13.619081020355225}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.8698617248814711
NO2(0)的mse=56.08556695562255
NO2(0)的mae=4.922017555926581
NO2(0)的mar=0.20884299321545957
总共花费的时间为：72.06
西咸新区
3606A
3607A
3608A
[flaml.automl: 09-19 04:28:24] {2390} INFO - task = regression
[flaml.automl: 09-19 04:28:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 04:28:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 04:28:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 04:28:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 04:28:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 04:28:25] {3025} INFO - Estimated sufficient time budget=11829s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 04:28:25] {3072} INFO -  at 1.3s,	estimator xgboost's best error=23.3541,	best estimator xgboost's best error=23.3541
[flaml.automl: 09-19 04:28:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 04:28:27] {3072} INFO -  at 3.4s,	estimator xgboost's best error=10.9713,	best estimator xgboost's best error=10.9713
[flaml.automl: 09-19 04:28:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 04:28:29] {3072} INFO -  at 4.6s,	estimator xgboost's best error=10.9713,	best estimator xgboost's best error=10.9713
[flaml.automl: 09-19 04:28:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 04:28:38] {3072} INFO -  at 14.4s,	estimator xgboost's best error=10.9713,	best estimator xgboost's best error=10.9713
[flaml.automl: 09-19 04:28:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 04:28:40] {3072} INFO -  at 15.5s,	estimator xgboost's best error=6.6495,	best estimator xgboost's best error=6.6495
[flaml.automl: 09-19 04:28:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:28:41] {3072} INFO -  at 17.1s,	estimator xgboost's best error=5.7282,	best estimator xgboost's best error=5.7282
[flaml.automl: 09-19 04:28:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:28:43] {3072} INFO -  at 18.7s,	estimator xgboost's best error=5.7282,	best estimator xgboost's best error=5.7282
[flaml.automl: 09-19 04:28:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:28:45] {3072} INFO -  at 21.1s,	estimator xgboost's best error=5.7282,	best estimator xgboost's best error=5.7282
[flaml.automl: 09-19 04:28:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:28:46] {3072} INFO -  at 22.2s,	estimator xgboost's best error=5.7282,	best estimator xgboost's best error=5.7282
[flaml.automl: 09-19 04:28:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:28:49] {3072} INFO -  at 24.8s,	estimator xgboost's best error=5.7282,	best estimator xgboost's best error=5.7282
[flaml.automl: 09-19 04:28:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 04:28:50] {3072} INFO -  at 26.0s,	estimator xgboost's best error=5.7282,	best estimator xgboost's best error=5.7282
[flaml.automl: 09-19 04:28:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 04:28:51] {3072} INFO -  at 27.1s,	estimator xgboost's best error=5.7282,	best estimator xgboost's best error=5.7282
[flaml.automl: 09-19 04:28:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 04:28:58] {3072} INFO -  at 33.5s,	estimator xgboost's best error=5.5199,	best estimator xgboost's best error=5.5199
[flaml.automl: 09-19 04:28:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 04:29:09] {3072} INFO -  at 45.3s,	estimator xgboost's best error=5.4051,	best estimator xgboost's best error=5.4051
[flaml.automl: 09-19 04:29:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 04:29:16] {3072} INFO -  at 51.7s,	estimator xgboost's best error=5.4051,	best estimator xgboost's best error=5.4051
[flaml.automl: 09-19 04:29:28] {3335} INFO - retrain xgboost for 11.8s
[flaml.automl: 09-19 04:29:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 04:29:28] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:29:28] {2637} INFO - Time taken to find the best model: 45.339345932006836
[flaml.automl: 09-19 04:29:28] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
NO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
NO2(0)最佳损失：-4.405076673077653
NO2(0)最好结果：{'pred_time': 1.0821850505233981e-05, 'wall_clock_time': 45.339345932006836, 'metric_for_logging': {'pred_time': 1.0821850505233981e-05}, 'val_loss': 5.405076673077653, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.813215017318726}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
NO2(0)的R2=0.856173020801378
NO2(0)的mse=65.17157895296116
NO2(0)的mae=5.406533266898313
NO2(0)的mar=0.1787491790077185
总共花费的时间为：64.09
