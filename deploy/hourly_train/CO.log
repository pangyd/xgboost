nohup: ignoring input
北京市
1005A
1008A
1009A
1010A
1012A
3417A
3418A
3671A
3672A
3673A
3674A
3675A
3695A
3696A
3697A
[flaml.automl: 09-16 04:29:21] {2390} INFO - task = regression
[flaml.automl: 09-16 04:29:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 04:29:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 04:29:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 04:29:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 04:29:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 04:29:23] {3025} INFO - Estimated sufficient time budget=229655s. Estimated necessary time budget=230s.
[flaml.automl: 09-16 04:29:23] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1908,	best estimator xgboost's best error=0.1908
[flaml.automl: 09-16 04:29:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 04:29:24] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1790,	best estimator xgboost's best error=0.1790
[flaml.automl: 09-16 04:29:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 04:29:25] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.1790,	best estimator xgboost's best error=0.1790
[flaml.automl: 09-16 04:29:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 04:29:27] {3072} INFO -  at 6.2s,	estimator xgboost's best error=0.1790,	best estimator xgboost's best error=0.1790
[flaml.automl: 09-16 04:29:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 04:29:28] {3072} INFO -  at 7.4s,	estimator xgboost's best error=0.1090,	best estimator xgboost's best error=0.1090
[flaml.automl: 09-16 04:29:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 04:29:29] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.1090,	best estimator xgboost's best error=0.1090
[flaml.automl: 09-16 04:29:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 04:29:30] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.1090,	best estimator xgboost's best error=0.1090
[flaml.automl: 09-16 04:29:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 04:29:31] {3072} INFO -  at 10.9s,	estimator xgboost's best error=0.1090,	best estimator xgboost's best error=0.1090
[flaml.automl: 09-16 04:29:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 04:29:33] {3072} INFO -  at 12.1s,	estimator xgboost's best error=0.0907,	best estimator xgboost's best error=0.0907
[flaml.automl: 09-16 04:29:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 04:29:34] {3072} INFO -  at 13.2s,	estimator xgboost's best error=0.0907,	best estimator xgboost's best error=0.0907
[flaml.automl: 09-16 04:29:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 04:29:35] {3072} INFO -  at 14.4s,	estimator xgboost's best error=0.0794,	best estimator xgboost's best error=0.0794
[flaml.automl: 09-16 04:29:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 04:29:36] {3072} INFO -  at 15.6s,	estimator xgboost's best error=0.0794,	best estimator xgboost's best error=0.0794
[flaml.automl: 09-16 04:29:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 04:29:37] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0612,	best estimator xgboost's best error=0.0612
[flaml.automl: 09-16 04:29:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 04:29:38] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.0612,	best estimator xgboost's best error=0.0612
[flaml.automl: 09-16 04:29:38] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 04:29:40] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0587,	best estimator xgboost's best error=0.0587
[flaml.automl: 09-16 04:29:40] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 04:29:42] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0587,	best estimator xgboost's best error=0.0587
[flaml.automl: 09-16 04:29:42] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 04:29:43] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.0587,	best estimator xgboost's best error=0.0587
[flaml.automl: 09-16 04:29:43] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 04:29:44] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0587,	best estimator xgboost's best error=0.0587
[flaml.automl: 09-16 04:29:44] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 04:29:47] {3072} INFO -  at 26.1s,	estimator xgboost's best error=0.0587,	best estimator xgboost's best error=0.0587
[flaml.automl: 09-16 04:29:47] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 04:29:51] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-16 04:29:51] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 04:29:53] {3072} INFO -  at 32.5s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-16 04:29:53] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 04:29:58] {3072} INFO -  at 37.9s,	estimator xgboost's best error=0.0514,	best estimator xgboost's best error=0.0514
[flaml.automl: 09-16 04:29:58] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 04:30:06] {3072} INFO -  at 45.7s,	estimator xgboost's best error=0.0514,	best estimator xgboost's best error=0.0514
[flaml.automl: 09-16 04:30:06] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-16 04:30:09] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.0514,	best estimator xgboost's best error=0.0514
[flaml.automl: 09-16 04:30:09] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-16 04:30:18] {3072} INFO -  at 57.5s,	estimator xgboost's best error=0.0504,	best estimator xgboost's best error=0.0504
[flaml.automl: 09-16 04:31:14] {3335} INFO - retrain xgboost for 56.4s
[flaml.automl: 09-16 04:31:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8646296770996755, colsample_bynode=1,
             colsample_bytree=0.4739988593369067, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.8565280908013105,
             missing=nan, monotone_constraints='()', n_estimators=5, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0011940428715149028, reg_lambda=4.976648650586149,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 04:31:14] {2636} INFO - fit succeeded
[flaml.automl: 09-16 04:31:14] {2637} INFO - Time taken to find the best model: 57.48124170303345
[flaml.automl: 09-16 04:31:14] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 9, 'max_leaves': 57, 'min_child_weight': 0.8565280908013105, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.8646296770996755, 'colsample_bytree': 0.4739988593369067, 'reg_alpha': 0.0011940428715149028, 'reg_lambda': 4.976648650586149, 'FLAML_sample_size': 40000}
CO(0)最佳损失：0.9496060676865602
CO(0)最好结果：{'pred_time': 8.506826698348427e-06, 'wall_clock_time': 57.48124170303345, 'metric_for_logging': {'pred_time': 8.506826698348427e-06}, 'val_loss': 0.05039393231343981, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 57, 'min_child_weight': 0.8565280908013105, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.8646296770996755, 'colsample_bytree': 0.4739988593369067, 'reg_alpha': 0.0011940428715149028, 'reg_lambda': 4.976648650586149, 'FLAML_sample_size': 40000}, 'config/n_estimators': 9, 'config/max_leaves': 57, 'config/min_child_weight': 0.8565280908013105, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8646296770996755, 'config/colsample_bytree': 0.4739988593369067, 'config/reg_alpha': 0.0011940428715149028, 'config/reg_lambda': 4.976648650586149, 'config/FLAML_sample_size': 40000, 'experiment_tag': 'exp', 'time_total_s': 9.321175813674927}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8646296770996755, colsample_bynode=1,
             colsample_bytree=0.4739988593369067, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=57, min_child_weight=0.8565280908013105,
             missing=nan, monotone_constraints='()', n_estimators=5, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0011940428715149028, reg_lambda=4.976648650586149,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9134437820528257
CO(0)的mse=0.0069624799723864645
CO(0)的mae=0.04933791642254961
CO(0)的mar=0.11295685455324714
总共花费的时间为：117.10
天津市
1015A
1019A
2859A
2860A
2922A
3325A
3326A
3327A
3460A
3461A
3462A
[flaml.automl: 09-16 05:02:53] {2390} INFO - task = regression
[flaml.automl: 09-16 05:02:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:02:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:02:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:02:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:02:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:02:54] {3025} INFO - Estimated sufficient time budget=144780s. Estimated necessary time budget=145s.
[flaml.automl: 09-16 05:02:54] {3072} INFO -  at 1.8s,	estimator xgboost's best error=0.2209,	best estimator xgboost's best error=0.2209
[flaml.automl: 09-16 05:02:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:02:56] {3072} INFO -  at 3.9s,	estimator xgboost's best error=0.1288,	best estimator xgboost's best error=0.1288
[flaml.automl: 09-16 05:02:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:02:57] {3072} INFO -  at 5.2s,	estimator xgboost's best error=0.1288,	best estimator xgboost's best error=0.1288
[flaml.automl: 09-16 05:02:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:02:59] {3072} INFO -  at 7.0s,	estimator xgboost's best error=0.1288,	best estimator xgboost's best error=0.1288
[flaml.automl: 09-16 05:02:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:03:00] {3072} INFO -  at 8.1s,	estimator xgboost's best error=0.1028,	best estimator xgboost's best error=0.1028
[flaml.automl: 09-16 05:03:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:03:02] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0864,	best estimator xgboost's best error=0.0864
[flaml.automl: 09-16 05:03:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:03:03] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.0864,	best estimator xgboost's best error=0.0864
[flaml.automl: 09-16 05:03:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:03:05] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.0864,	best estimator xgboost's best error=0.0864
[flaml.automl: 09-16 05:03:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:03:07] {3072} INFO -  at 14.5s,	estimator xgboost's best error=0.0864,	best estimator xgboost's best error=0.0864
[flaml.automl: 09-16 05:03:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:03:08] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0864,	best estimator xgboost's best error=0.0864
[flaml.automl: 09-16 05:03:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:03:11] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0861,	best estimator xgboost's best error=0.0861
[flaml.automl: 09-16 05:03:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:03:13] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0861,	best estimator xgboost's best error=0.0861
[flaml.automl: 09-16 05:03:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:03:25] {3072} INFO -  at 33.3s,	estimator xgboost's best error=0.0777,	best estimator xgboost's best error=0.0777
[flaml.automl: 09-16 05:03:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:03:46] {3072} INFO -  at 53.9s,	estimator xgboost's best error=0.0754,	best estimator xgboost's best error=0.0754
[flaml.automl: 09-16 05:04:07] {3335} INFO - retrain xgboost for 20.8s
[flaml.automl: 09-16 05:04:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 05:04:07] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:04:07] {2637} INFO - Time taken to find the best model: 53.8539776802063
[flaml.automl: 09-16 05:04:07] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 120131}
CO(0)最佳损失：0.9246027965585017
CO(0)最好结果：{'pred_time': 6.41793950360658e-06, 'wall_clock_time': 53.8539776802063, 'metric_for_logging': {'pred_time': 6.41793950360658e-06}, 'val_loss': 0.07539720344149829, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 120131}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 120131, 'experiment_tag': 'exp', 'time_total_s': 20.522765636444092}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8424716035520758
CO(0)的mse=0.018594401022207732
CO(0)的mae=0.07985388336535065
CO(0)的mar=0.12101876966385737
总共花费的时间为：76.45
石家庄市
1030A
1032A
1033A
1035A
2862A
3456A
3572A
3573A
3574A
[flaml.automl: 09-16 05:31:03] {2390} INFO - task = regression
[flaml.automl: 09-16 05:31:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:31:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:31:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:31:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:31:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:31:05] {3025} INFO - Estimated sufficient time budget=202129s. Estimated necessary time budget=202s.
[flaml.automl: 09-16 05:31:05] {3072} INFO -  at 2.7s,	estimator xgboost's best error=0.2041,	best estimator xgboost's best error=0.2041
[flaml.automl: 09-16 05:31:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:31:08] {3072} INFO -  at 5.4s,	estimator xgboost's best error=0.1457,	best estimator xgboost's best error=0.1457
[flaml.automl: 09-16 05:31:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:31:10] {3072} INFO -  at 7.4s,	estimator xgboost's best error=0.1457,	best estimator xgboost's best error=0.1457
[flaml.automl: 09-16 05:31:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:31:12] {3072} INFO -  at 10.1s,	estimator xgboost's best error=0.1457,	best estimator xgboost's best error=0.1457
[flaml.automl: 09-16 05:31:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:31:14] {3072} INFO -  at 12.1s,	estimator xgboost's best error=0.0969,	best estimator xgboost's best error=0.0969
[flaml.automl: 09-16 05:31:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:31:17] {3072} INFO -  at 14.3s,	estimator xgboost's best error=0.0851,	best estimator xgboost's best error=0.0851
[flaml.automl: 09-16 05:31:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:31:19] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0851,	best estimator xgboost's best error=0.0851
[flaml.automl: 09-16 05:31:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:31:20] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0851,	best estimator xgboost's best error=0.0851
[flaml.automl: 09-16 05:31:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:31:22] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0851,	best estimator xgboost's best error=0.0851
[flaml.automl: 09-16 05:31:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:31:23] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.0851,	best estimator xgboost's best error=0.0851
[flaml.automl: 09-16 05:31:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:31:26] {3072} INFO -  at 23.9s,	estimator xgboost's best error=0.0781,	best estimator xgboost's best error=0.0781
[flaml.automl: 09-16 05:31:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:31:28] {3072} INFO -  at 26.2s,	estimator xgboost's best error=0.0781,	best estimator xgboost's best error=0.0781
[flaml.automl: 09-16 05:31:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:31:37] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.0699,	best estimator xgboost's best error=0.0699
[flaml.automl: 09-16 05:31:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:31:55] {3072} INFO -  at 53.2s,	estimator xgboost's best error=0.0676,	best estimator xgboost's best error=0.0676
[flaml.automl: 09-16 05:32:26] {3335} INFO - retrain xgboost for 30.5s
[flaml.automl: 09-16 05:32:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 05:32:26] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:32:26] {2637} INFO - Time taken to find the best model: 53.24749183654785
[flaml.automl: 09-16 05:32:26] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 94437}
CO(0)最佳损失：0.9324437502876917
CO(0)最好结果：{'pred_time': 7.3685723765271525e-06, 'wall_clock_time': 53.24749183654785, 'metric_for_logging': {'pred_time': 7.3685723765271525e-06}, 'val_loss': 0.06755624971230832, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 94437}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 94437, 'experiment_tag': 'exp', 'time_total_s': 18.621132612228394}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8936004665717325
CO(0)的mse=0.011965141613861283
CO(0)的mae=0.06800254001719372
CO(0)的mar=0.11391789311285423
总共花费的时间为：85.48
唐山市
1041A
3575A
3576A
3577A
3578A
3692A
[flaml.automl: 09-16 05:51:10] {2390} INFO - task = regression
[flaml.automl: 09-16 05:51:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:51:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:51:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:51:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:51:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:51:11] {3025} INFO - Estimated sufficient time budget=70062s. Estimated necessary time budget=70s.
[flaml.automl: 09-16 05:51:11] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.3162,	best estimator xgboost's best error=0.3162
[flaml.automl: 09-16 05:51:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:51:13] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.2142,	best estimator xgboost's best error=0.2142
[flaml.automl: 09-16 05:51:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:51:14] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.2142,	best estimator xgboost's best error=0.2142
[flaml.automl: 09-16 05:51:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:51:19] {3072} INFO -  at 9.1s,	estimator xgboost's best error=0.2142,	best estimator xgboost's best error=0.2142
[flaml.automl: 09-16 05:51:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:51:20] {3072} INFO -  at 10.2s,	estimator xgboost's best error=0.1918,	best estimator xgboost's best error=0.1918
[flaml.automl: 09-16 05:51:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:51:21] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.1749,	best estimator xgboost's best error=0.1749
[flaml.automl: 09-16 05:51:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:51:23] {3072} INFO -  at 13.4s,	estimator xgboost's best error=0.1749,	best estimator xgboost's best error=0.1749
[flaml.automl: 09-16 05:51:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:51:26] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.1749,	best estimator xgboost's best error=0.1749
[flaml.automl: 09-16 05:51:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:51:27] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.1749,	best estimator xgboost's best error=0.1749
[flaml.automl: 09-16 05:51:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:51:29] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.1749,	best estimator xgboost's best error=0.1749
[flaml.automl: 09-16 05:51:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:51:31] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.1734,	best estimator xgboost's best error=0.1734
[flaml.automl: 09-16 05:51:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:51:32] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.1734,	best estimator xgboost's best error=0.1734
[flaml.automl: 09-16 05:51:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:51:39] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.1683,	best estimator xgboost's best error=0.1683
[flaml.automl: 09-16 05:51:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:51:51] {3072} INFO -  at 41.2s,	estimator xgboost's best error=0.1637,	best estimator xgboost's best error=0.1637
[flaml.automl: 09-16 05:51:51] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 05:52:05] {3072} INFO -  at 55.0s,	estimator xgboost's best error=0.1637,	best estimator xgboost's best error=0.1637
[flaml.automl: 09-16 05:52:39] {3335} INFO - retrain xgboost for 34.1s
[flaml.automl: 09-16 05:52:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 05:52:39] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:52:39] {2637} INFO - Time taken to find the best model: 41.20512008666992
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 58251}
CO(0)最佳损失：0.8363327454395204
CO(0)最好结果：{'pred_time': 6.347724316975773e-06, 'wall_clock_time': 41.20512008666992, 'metric_for_logging': {'pred_time': 6.347724316975773e-06}, 'val_loss': 0.1636672545604796, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 58251}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 58251, 'experiment_tag': 'exp', 'time_total_s': 12.097819566726685}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.735339807594809
CO(0)的mse=0.06784037176266593
CO(0)的mae=0.16527573504838453
CO(0)的mar=0.23109642116262985
总共花费的时间为：90.38
秦皇岛市
1043A
1044A
3132A
[flaml.automl: 09-16 06:01:51] {2390} INFO - task = regression
[flaml.automl: 09-16 06:01:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:01:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:01:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:01:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:01:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:01:52] {3025} INFO - Estimated sufficient time budget=12113s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 06:01:52] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2217,	best estimator xgboost's best error=0.2217
[flaml.automl: 09-16 06:01:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:01:54] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1396,	best estimator xgboost's best error=0.1396
[flaml.automl: 09-16 06:01:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:01:55] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1396,	best estimator xgboost's best error=0.1396
[flaml.automl: 09-16 06:01:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:02:08] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.1396,	best estimator xgboost's best error=0.1396
[flaml.automl: 09-16 06:02:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:02:10] {3072} INFO -  at 19.3s,	estimator xgboost's best error=0.1157,	best estimator xgboost's best error=0.1157
[flaml.automl: 09-16 06:02:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:02:12] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-16 06:02:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:02:15] {3072} INFO -  at 24.8s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-16 06:02:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:02:20] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-16 06:02:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:02:22] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-16 06:02:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:02:26] {3072} INFO -  at 35.7s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-16 06:02:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:02:28] {3072} INFO -  at 37.8s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-16 06:02:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:02:30] {3072} INFO -  at 39.9s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-16 06:02:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:02:41] {3072} INFO -  at 50.7s,	estimator xgboost's best error=0.0900,	best estimator xgboost's best error=0.0900
[flaml.automl: 09-16 06:02:53] {3335} INFO - retrain xgboost for 12.2s
[flaml.automl: 09-16 06:02:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 06:02:53] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:02:53] {2637} INFO - Time taken to find the best model: 50.661577463150024
[flaml.automl: 09-16 06:02:53] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9100462660939095
CO(0)最好结果：{'pred_time': 1.8638623316811486e-05, 'wall_clock_time': 50.661577463150024, 'metric_for_logging': {'pred_time': 1.8638623316811486e-05}, 'val_loss': 0.08995373390609053, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 10.762526512145996}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8633424695881686
CO(0)的mse=0.02161759321163037
CO(0)的mae=0.08886839420664393
CO(0)的mar=0.2149456270161317
总共花费的时间为：63.44
邯郸市
1050A
3579A
3580A
[flaml.automl: 09-16 06:11:48] {2390} INFO - task = regression
[flaml.automl: 09-16 06:11:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:11:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:11:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:11:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:11:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:11:52] {3025} INFO - Estimated sufficient time budget=33833s. Estimated necessary time budget=34s.
[flaml.automl: 09-16 06:11:52] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.3290,	best estimator xgboost's best error=0.3290
[flaml.automl: 09-16 06:11:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:11:58] {3072} INFO -  at 9.5s,	estimator xgboost's best error=0.2131,	best estimator xgboost's best error=0.2131
[flaml.automl: 09-16 06:11:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:12:01] {3072} INFO -  at 12.8s,	estimator xgboost's best error=0.2131,	best estimator xgboost's best error=0.2131
[flaml.automl: 09-16 06:12:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:12:21] {3072} INFO -  at 32.3s,	estimator xgboost's best error=0.2131,	best estimator xgboost's best error=0.2131
[flaml.automl: 09-16 06:12:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:12:23] {3072} INFO -  at 34.8s,	estimator xgboost's best error=0.1881,	best estimator xgboost's best error=0.1881
[flaml.automl: 09-16 06:12:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:12:26] {3072} INFO -  at 37.5s,	estimator xgboost's best error=0.1643,	best estimator xgboost's best error=0.1643
[flaml.automl: 09-16 06:12:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:12:29] {3072} INFO -  at 40.5s,	estimator xgboost's best error=0.1643,	best estimator xgboost's best error=0.1643
[flaml.automl: 09-16 06:12:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:12:33] {3072} INFO -  at 45.0s,	estimator xgboost's best error=0.1643,	best estimator xgboost's best error=0.1643
[flaml.automl: 09-16 06:12:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:12:35] {3072} INFO -  at 46.8s,	estimator xgboost's best error=0.1643,	best estimator xgboost's best error=0.1643
[flaml.automl: 09-16 06:12:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:12:40] {3072} INFO -  at 51.8s,	estimator xgboost's best error=0.1643,	best estimator xgboost's best error=0.1643
[flaml.automl: 09-16 06:12:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:12:42] {3072} INFO -  at 54.0s,	estimator xgboost's best error=0.1643,	best estimator xgboost's best error=0.1643
[flaml.automl: 09-16 06:12:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:12:44] {3072} INFO -  at 55.9s,	estimator xgboost's best error=0.1643,	best estimator xgboost's best error=0.1643
[flaml.automl: 09-16 06:12:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:12:47] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.1604,	best estimator xgboost's best error=0.1604
[flaml.automl: 09-16 06:12:58] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-16 06:12:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 06:12:58] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:12:58] {2637} INFO - Time taken to find the best model: 58.95977163314819
[flaml.automl: 09-16 06:12:58] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.8395534893094583
CO(0)最好结果：{'pred_time': 2.04218674475812e-05, 'wall_clock_time': 58.95977163314819, 'metric_for_logging': {'pred_time': 2.04218674475812e-05}, 'val_loss': 0.16044651069054172, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 3.01098370552063}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7307376570155787
CO(0)的mse=0.0696930686937819
CO(0)的mae=0.16152793804176063
CO(0)的mar=0.217115652198181
总共花费的时间为：70.56
保定市
1051A
1053A
1054A
1055A
1056A
3581A
3582A
3583A
[flaml.automl: 09-16 06:37:28] {2390} INFO - task = regression
[flaml.automl: 09-16 06:37:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:37:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:37:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:37:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:37:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:37:31] {3025} INFO - Estimated sufficient time budget=176947s. Estimated necessary time budget=177s.
[flaml.automl: 09-16 06:37:31] {3072} INFO -  at 2.7s,	estimator xgboost's best error=0.1723,	best estimator xgboost's best error=0.1723
[flaml.automl: 09-16 06:37:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:37:34] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.1229,	best estimator xgboost's best error=0.1229
[flaml.automl: 09-16 06:37:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:37:36] {3072} INFO -  at 8.1s,	estimator xgboost's best error=0.1229,	best estimator xgboost's best error=0.1229
[flaml.automl: 09-16 06:37:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:37:39] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.1229,	best estimator xgboost's best error=0.1229
[flaml.automl: 09-16 06:37:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:37:41] {3072} INFO -  at 13.1s,	estimator xgboost's best error=0.0824,	best estimator xgboost's best error=0.0824
[flaml.automl: 09-16 06:37:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:37:43] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 06:37:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:37:45] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 06:37:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:37:48] {3072} INFO -  at 19.9s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 06:37:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:37:50] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 06:37:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:37:51] {3072} INFO -  at 23.6s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 06:37:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:37:54] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0663,	best estimator xgboost's best error=0.0663
[flaml.automl: 09-16 06:37:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:37:57] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.0663,	best estimator xgboost's best error=0.0663
[flaml.automl: 09-16 06:37:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:38:04] {3072} INFO -  at 36.6s,	estimator xgboost's best error=0.0569,	best estimator xgboost's best error=0.0569
[flaml.automl: 09-16 06:38:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 06:38:17] {3072} INFO -  at 48.7s,	estimator xgboost's best error=0.0560,	best estimator xgboost's best error=0.0560
[flaml.automl: 09-16 06:38:29] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 06:38:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:38:29] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:38:29] {2637} INFO - Time taken to find the best model: 48.718502044677734
[flaml.automl: 09-16 06:38:29] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 83748}
CO(0)最佳损失：0.9439922451287309
CO(0)最好结果：{'pred_time': 5.140858979576663e-06, 'wall_clock_time': 48.718502044677734, 'metric_for_logging': {'pred_time': 5.140858979576663e-06}, 'val_loss': 0.05600775487126905, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 83748}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 83748, 'experiment_tag': 'exp', 'time_total_s': 12.149524450302124}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9127442283754755
CO(0)的mse=0.008562931919027076
CO(0)的mae=0.054814250251996675
CO(0)的mar=0.24207795876253332
总共花费的时间为：62.76
张家口市
1060A
1061A
3131A
3323A
[flaml.automl: 09-16 06:51:35] {2390} INFO - task = regression
[flaml.automl: 09-16 06:51:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:51:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:51:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:51:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:51:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:51:37] {3025} INFO - Estimated sufficient time budget=48656s. Estimated necessary time budget=49s.
[flaml.automl: 09-16 06:51:37] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1501,	best estimator xgboost's best error=0.1501
[flaml.automl: 09-16 06:51:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:51:39] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0864,	best estimator xgboost's best error=0.0864
[flaml.automl: 09-16 06:51:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:51:40] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0864,	best estimator xgboost's best error=0.0864
[flaml.automl: 09-16 06:51:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:51:46] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0864,	best estimator xgboost's best error=0.0864
[flaml.automl: 09-16 06:51:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:51:47] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0632,	best estimator xgboost's best error=0.0632
[flaml.automl: 09-16 06:51:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:51:49] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0525,	best estimator xgboost's best error=0.0525
[flaml.automl: 09-16 06:51:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:51:51] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0525,	best estimator xgboost's best error=0.0525
[flaml.automl: 09-16 06:51:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:51:53] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0525,	best estimator xgboost's best error=0.0525
[flaml.automl: 09-16 06:51:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:51:54] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0525,	best estimator xgboost's best error=0.0525
[flaml.automl: 09-16 06:51:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:51:57] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0525,	best estimator xgboost's best error=0.0525
[flaml.automl: 09-16 06:51:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:51:58] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0525,	best estimator xgboost's best error=0.0525
[flaml.automl: 09-16 06:51:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:52:00] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0525,	best estimator xgboost's best error=0.0525
[flaml.automl: 09-16 06:52:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:52:06] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0448,	best estimator xgboost's best error=0.0448
[flaml.automl: 09-16 06:52:06] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 06:52:18] {3072} INFO -  at 43.1s,	estimator xgboost's best error=0.0432,	best estimator xgboost's best error=0.0432
[flaml.automl: 09-16 06:52:18] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 06:52:25] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0432,	best estimator xgboost's best error=0.0432
[flaml.automl: 09-16 06:52:37] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 06:52:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:52:37] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:52:37] {2637} INFO - Time taken to find the best model: 43.09678888320923
[flaml.automl: 09-16 06:52:37] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40818}
CO(0)最佳损失：0.9568248177890845
CO(0)最好结果：{'pred_time': 8.717573509014472e-06, 'wall_clock_time': 43.09678888320923, 'metric_for_logging': {'pred_time': 8.717573509014472e-06}, 'val_loss': 0.043175182210915514, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40818}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40818, 'experiment_tag': 'exp', 'time_total_s': 12.107897996902466}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9027922942175243
CO(0)的mse=0.00543681562377495
CO(0)的mae=0.043089346354023096
CO(0)的mar=0.15126990888023406
总共花费的时间为：62.33
承德市
1063A
1064A
1065A
[flaml.automl: 09-16 07:02:23] {2390} INFO - task = regression
[flaml.automl: 09-16 07:02:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:02:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:02:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:02:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:02:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:02:25] {3025} INFO - Estimated sufficient time budget=20239s. Estimated necessary time budget=20s.
[flaml.automl: 09-16 07:02:25] {3072} INFO -  at 2.2s,	estimator xgboost's best error=0.2257,	best estimator xgboost's best error=0.2257
[flaml.automl: 09-16 07:02:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:02:27] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-16 07:02:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:02:28] {3072} INFO -  at 5.5s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-16 07:02:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:02:38] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-16 07:02:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:02:39] {3072} INFO -  at 16.7s,	estimator xgboost's best error=0.1068,	best estimator xgboost's best error=0.1068
[flaml.automl: 09-16 07:02:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:02:41] {3072} INFO -  at 18.3s,	estimator xgboost's best error=0.0883,	best estimator xgboost's best error=0.0883
[flaml.automl: 09-16 07:02:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:02:43] {3072} INFO -  at 19.9s,	estimator xgboost's best error=0.0883,	best estimator xgboost's best error=0.0883
[flaml.automl: 09-16 07:02:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:02:45] {3072} INFO -  at 22.4s,	estimator xgboost's best error=0.0883,	best estimator xgboost's best error=0.0883
[flaml.automl: 09-16 07:02:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:02:46] {3072} INFO -  at 23.6s,	estimator xgboost's best error=0.0883,	best estimator xgboost's best error=0.0883
[flaml.automl: 09-16 07:02:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:02:49] {3072} INFO -  at 26.3s,	estimator xgboost's best error=0.0883,	best estimator xgboost's best error=0.0883
[flaml.automl: 09-16 07:02:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:02:50] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.0883,	best estimator xgboost's best error=0.0883
[flaml.automl: 09-16 07:02:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:02:51] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.0883,	best estimator xgboost's best error=0.0883
[flaml.automl: 09-16 07:02:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:03:01] {3072} INFO -  at 38.8s,	estimator xgboost's best error=0.0808,	best estimator xgboost's best error=0.0808
[flaml.automl: 09-16 07:03:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:03:22] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.0780,	best estimator xgboost's best error=0.0780
[flaml.automl: 09-16 07:03:48] {3335} INFO - retrain xgboost for 25.5s
[flaml.automl: 09-16 07:03:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:03:48] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:03:48] {2637} INFO - Time taken to find the best model: 59.69316077232361
[flaml.automl: 09-16 07:03:48] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9220350148793449
CO(0)最好结果：{'pred_time': 2.0947589075293876e-05, 'wall_clock_time': 59.69316077232361, 'metric_for_logging': {'pred_time': 2.0947589075293876e-05}, 'val_loss': 0.07796498512065508, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 20.888118982315063}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8918250070314473
CO(0)的mse=0.014139891716597363
CO(0)的mae=0.06851600988127804
CO(0)的mar=0.10507477380744251
总共花费的时间为：86.01
廊坊市
1070A
2919A
[flaml.automl: 09-16 07:10:20] {2390} INFO - task = regression
[flaml.automl: 09-16 07:10:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:10:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:10:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:10:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:10:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:10:21] {3025} INFO - Estimated sufficient time budget=12134s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 07:10:21] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1748,	best estimator xgboost's best error=0.1748
[flaml.automl: 09-16 07:10:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:10:23] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1069,	best estimator xgboost's best error=0.1069
[flaml.automl: 09-16 07:10:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:10:24] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.1069,	best estimator xgboost's best error=0.1069
[flaml.automl: 09-16 07:10:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:10:34] {3072} INFO -  at 14.2s,	estimator xgboost's best error=0.1069,	best estimator xgboost's best error=0.1069
[flaml.automl: 09-16 07:10:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:10:35] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0844,	best estimator xgboost's best error=0.0844
[flaml.automl: 09-16 07:10:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:10:37] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0682,	best estimator xgboost's best error=0.0682
[flaml.automl: 09-16 07:10:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:10:38] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.0682,	best estimator xgboost's best error=0.0682
[flaml.automl: 09-16 07:10:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:10:41] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0682,	best estimator xgboost's best error=0.0682
[flaml.automl: 09-16 07:10:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:10:42] {3072} INFO -  at 22.2s,	estimator xgboost's best error=0.0682,	best estimator xgboost's best error=0.0682
[flaml.automl: 09-16 07:10:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:10:44] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.0682,	best estimator xgboost's best error=0.0682
[flaml.automl: 09-16 07:10:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:10:45] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0682,	best estimator xgboost's best error=0.0682
[flaml.automl: 09-16 07:10:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:10:47] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.0682,	best estimator xgboost's best error=0.0682
[flaml.automl: 09-16 07:10:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:10:53] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0604,	best estimator xgboost's best error=0.0604
[flaml.automl: 09-16 07:10:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:11:03] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0586,	best estimator xgboost's best error=0.0586
[flaml.automl: 09-16 07:11:03] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 07:11:09] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.0586,	best estimator xgboost's best error=0.0586
[flaml.automl: 09-16 07:11:09] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 07:11:19] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0586,	best estimator xgboost's best error=0.0586
[flaml.automl: 09-16 07:11:29] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-16 07:11:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:11:29] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:11:29] {2637} INFO - Time taken to find the best model: 43.42844533920288
[flaml.automl: 09-16 07:11:29] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9413920181740705
CO(0)最好结果：{'pred_time': 1.630200978696524e-05, 'wall_clock_time': 43.42844533920288, 'metric_for_logging': {'pred_time': 1.630200978696524e-05}, 'val_loss': 0.05860798182592956, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.39966082572937}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.912016458051544
CO(0)的mse=0.008562693471335849
CO(0)的mae=0.05690132587972406
CO(0)的mar=0.12248080580003383
总共花费的时间为：70.07
沧州市
1071A
1073A
3324A
[flaml.automl: 09-16 07:21:06] {2390} INFO - task = regression
[flaml.automl: 09-16 07:21:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:21:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:21:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:21:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:21:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:21:07] {3025} INFO - Estimated sufficient time budget=12010s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 07:21:07] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1637,	best estimator xgboost's best error=0.1637
[flaml.automl: 09-16 07:21:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:21:09] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1026,	best estimator xgboost's best error=0.1026
[flaml.automl: 09-16 07:21:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:21:10] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1026,	best estimator xgboost's best error=0.1026
[flaml.automl: 09-16 07:21:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:21:20] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.1026,	best estimator xgboost's best error=0.1026
[flaml.automl: 09-16 07:21:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:21:21] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.0847,	best estimator xgboost's best error=0.0847
[flaml.automl: 09-16 07:21:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:21:23] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 07:21:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:21:25] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 07:21:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:21:27] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 07:21:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:21:28] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 07:21:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:21:31] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 07:21:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:21:32] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 07:21:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:21:33] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-16 07:21:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:21:40] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-16 07:21:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:21:52] {3072} INFO -  at 46.2s,	estimator xgboost's best error=0.0628,	best estimator xgboost's best error=0.0628
[flaml.automl: 09-16 07:21:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 07:21:58] {3072} INFO -  at 52.6s,	estimator xgboost's best error=0.0628,	best estimator xgboost's best error=0.0628
[flaml.automl: 09-16 07:22:10] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 07:22:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:22:10] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:22:10] {2637} INFO - Time taken to find the best model: 46.19780969619751
[flaml.automl: 09-16 07:22:10] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9372284954403566
CO(0)最好结果：{'pred_time': 1.16356083604156e-05, 'wall_clock_time': 46.19780969619751, 'metric_for_logging': {'pred_time': 1.16356083604156e-05}, 'val_loss': 0.06277150455964343, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.971524238586426}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8790439258967044
CO(0)的mse=0.008897205285388798
CO(0)的mae=0.06157750114880543
CO(0)的mar=0.2416440628493997
总共花费的时间为：65.17
衡水市
1074A
1075A
3188A
3459A
[flaml.automl: 09-16 07:34:28] {2390} INFO - task = regression
[flaml.automl: 09-16 07:34:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:34:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:34:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:34:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:34:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:34:31] {3025} INFO - Estimated sufficient time budget=121813s. Estimated necessary time budget=122s.
[flaml.automl: 09-16 07:34:31] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.1373,	best estimator xgboost's best error=0.1373
[flaml.automl: 09-16 07:34:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:34:36] {3072} INFO -  at 7.5s,	estimator xgboost's best error=0.0831,	best estimator xgboost's best error=0.0831
[flaml.automl: 09-16 07:34:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:34:38] {3072} INFO -  at 10.1s,	estimator xgboost's best error=0.0831,	best estimator xgboost's best error=0.0831
[flaml.automl: 09-16 07:34:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:34:44] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.0831,	best estimator xgboost's best error=0.0831
[flaml.automl: 09-16 07:34:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:34:46] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.0691,	best estimator xgboost's best error=0.0691
[flaml.automl: 09-16 07:34:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:34:50] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 07:34:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:34:54] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 07:34:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:34:57] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 07:34:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:34:59] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 07:34:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:35:01] {3072} INFO -  at 33.4s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 07:35:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:35:04] {3072} INFO -  at 36.1s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 07:35:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:35:06] {3072} INFO -  at 38.3s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 07:35:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:35:13] {3072} INFO -  at 45.0s,	estimator xgboost's best error=0.0520,	best estimator xgboost's best error=0.0520
[flaml.automl: 09-16 07:35:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:35:25] {3072} INFO -  at 56.9s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-16 07:35:37] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 07:35:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:35:37] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:35:37] {2637} INFO - Time taken to find the best model: 56.93699860572815
[flaml.automl: 09-16 07:35:37] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42787}
CO(0)最佳损失：0.9498860196233297
CO(0)最好结果：{'pred_time': 8.384110925074757e-06, 'wall_clock_time': 56.93699860572815, 'metric_for_logging': {'pred_time': 8.384110925074757e-06}, 'val_loss': 0.05011398037667029, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42787}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42787, 'experiment_tag': 'exp', 'time_total_s': 11.97219204902649}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8973647080746578
CO(0)的mse=0.006022312942230937
CO(0)的mae=0.049393509842783984
CO(0)的mar=0.13237479105467104
总共花费的时间为：69.68
邢台市
1078A
1079A
1080A
[flaml.automl: 09-16 07:45:00] {2390} INFO - task = regression
[flaml.automl: 09-16 07:45:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:45:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:45:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:45:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:45:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:45:04] {3025} INFO - Estimated sufficient time budget=35469s. Estimated necessary time budget=35s.
[flaml.automl: 09-16 07:45:04] {3072} INFO -  at 3.8s,	estimator xgboost's best error=0.2581,	best estimator xgboost's best error=0.2581
[flaml.automl: 09-16 07:45:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:45:10] {3072} INFO -  at 9.9s,	estimator xgboost's best error=0.1666,	best estimator xgboost's best error=0.1666
[flaml.automl: 09-16 07:45:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:45:13] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.1666,	best estimator xgboost's best error=0.1666
[flaml.automl: 09-16 07:45:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:45:34] {3072} INFO -  at 33.5s,	estimator xgboost's best error=0.1666,	best estimator xgboost's best error=0.1666
[flaml.automl: 09-16 07:45:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:45:35] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.1449,	best estimator xgboost's best error=0.1449
[flaml.automl: 09-16 07:45:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:45:36] {3072} INFO -  at 36.1s,	estimator xgboost's best error=0.1267,	best estimator xgboost's best error=0.1267
[flaml.automl: 09-16 07:45:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:45:38] {3072} INFO -  at 37.7s,	estimator xgboost's best error=0.1267,	best estimator xgboost's best error=0.1267
[flaml.automl: 09-16 07:45:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:45:40] {3072} INFO -  at 40.2s,	estimator xgboost's best error=0.1267,	best estimator xgboost's best error=0.1267
[flaml.automl: 09-16 07:45:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:45:42] {3072} INFO -  at 41.3s,	estimator xgboost's best error=0.1267,	best estimator xgboost's best error=0.1267
[flaml.automl: 09-16 07:45:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:45:44] {3072} INFO -  at 44.0s,	estimator xgboost's best error=0.1267,	best estimator xgboost's best error=0.1267
[flaml.automl: 09-16 07:45:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:45:45] {3072} INFO -  at 45.1s,	estimator xgboost's best error=0.1267,	best estimator xgboost's best error=0.1267
[flaml.automl: 09-16 07:45:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:45:46] {3072} INFO -  at 46.2s,	estimator xgboost's best error=0.1267,	best estimator xgboost's best error=0.1267
[flaml.automl: 09-16 07:45:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:45:53] {3072} INFO -  at 52.6s,	estimator xgboost's best error=0.1223,	best estimator xgboost's best error=0.1223
[flaml.automl: 09-16 07:45:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 07:46:00] {3072} INFO -  at 59.6s,	estimator xgboost's best error=0.1182,	best estimator xgboost's best error=0.1182
[flaml.automl: 09-16 07:46:12] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-16 07:46:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:46:12] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:46:12] {2637} INFO - Time taken to find the best model: 59.5903799533844
[flaml.automl: 09-16 07:46:12] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.8817621729512446
CO(0)最好结果：{'pred_time': 1.167024446837005e-05, 'wall_clock_time': 59.5903799533844, 'metric_for_logging': {'pred_time': 1.167024446837005e-05}, 'val_loss': 0.11823782704875539, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 6.96076774597168}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7655704601939546
CO(0)的mse=0.03974158522033625
CO(0)的mae=0.11718207024976117
CO(0)的mar=0.18147791574977784
总共花费的时间为：72.21
太原市
1081A
1084A
1085A
1086A
1087A
3185A
[flaml.automl: 09-16 08:06:33] {2390} INFO - task = regression
[flaml.automl: 09-16 08:06:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:06:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:06:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:06:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:06:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:06:34] {3025} INFO - Estimated sufficient time budget=77662s. Estimated necessary time budget=78s.
[flaml.automl: 09-16 08:06:34] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.2591,	best estimator xgboost's best error=0.2591
[flaml.automl: 09-16 08:06:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:06:36] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.1632,	best estimator xgboost's best error=0.1632
[flaml.automl: 09-16 08:06:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:06:38] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1632,	best estimator xgboost's best error=0.1632
[flaml.automl: 09-16 08:06:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:06:41] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.1632,	best estimator xgboost's best error=0.1632
[flaml.automl: 09-16 08:06:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:06:42] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.1347,	best estimator xgboost's best error=0.1347
[flaml.automl: 09-16 08:06:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:06:44] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.1200,	best estimator xgboost's best error=0.1200
[flaml.automl: 09-16 08:06:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:06:46] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.1200,	best estimator xgboost's best error=0.1200
[flaml.automl: 09-16 08:06:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:06:48] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.1200,	best estimator xgboost's best error=0.1200
[flaml.automl: 09-16 08:06:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:06:49] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.1200,	best estimator xgboost's best error=0.1200
[flaml.automl: 09-16 08:06:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:06:52] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.1200,	best estimator xgboost's best error=0.1200
[flaml.automl: 09-16 08:06:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:06:54] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.1200,	best estimator xgboost's best error=0.1200
[flaml.automl: 09-16 08:06:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:06:55] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.1200,	best estimator xgboost's best error=0.1200
[flaml.automl: 09-16 08:06:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:07:01] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.1121,	best estimator xgboost's best error=0.1121
[flaml.automl: 09-16 08:07:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:07:13] {3072} INFO -  at 40.5s,	estimator xgboost's best error=0.1086,	best estimator xgboost's best error=0.1086
[flaml.automl: 09-16 08:07:13] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:07:20] {3072} INFO -  at 46.9s,	estimator xgboost's best error=0.1086,	best estimator xgboost's best error=0.1086
[flaml.automl: 09-16 08:07:20] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 08:07:32] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.1086,	best estimator xgboost's best error=0.1086
[flaml.automl: 09-16 08:07:44] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 08:07:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:07:44] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:07:44] {2637} INFO - Time taken to find the best model: 40.50092911720276
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65155}
CO(0)最佳损失：0.8913775015215188
CO(0)最好结果：{'pred_time': 5.543956440456664e-06, 'wall_clock_time': 40.50092911720276, 'metric_for_logging': {'pred_time': 5.543956440456664e-06}, 'val_loss': 0.10862249847848111, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65155}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 65155, 'experiment_tag': 'exp', 'time_total_s': 11.921183824539185}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.783388875693072
CO(0)的mse=0.028296079637888247
CO(0)的mae=0.10656094883867664
CO(0)的mar=0.14267168038275194
总共花费的时间为：72.61
呼和浩特市
1095A
1097A
3698A
3699A
[flaml.automl: 09-16 08:20:17] {2390} INFO - task = regression
[flaml.automl: 09-16 08:20:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:20:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:20:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:20:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:20:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:20:19] {3025} INFO - Estimated sufficient time budget=98548s. Estimated necessary time budget=99s.
[flaml.automl: 09-16 08:20:19] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.1659,	best estimator xgboost's best error=0.1659
[flaml.automl: 09-16 08:20:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:20:23] {3072} INFO -  at 6.5s,	estimator xgboost's best error=0.1049,	best estimator xgboost's best error=0.1049
[flaml.automl: 09-16 08:20:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:20:25] {3072} INFO -  at 8.7s,	estimator xgboost's best error=0.1049,	best estimator xgboost's best error=0.1049
[flaml.automl: 09-16 08:20:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:20:31] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.1049,	best estimator xgboost's best error=0.1049
[flaml.automl: 09-16 08:20:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:20:33] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.0874,	best estimator xgboost's best error=0.0874
[flaml.automl: 09-16 08:20:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:20:36] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0767,	best estimator xgboost's best error=0.0767
[flaml.automl: 09-16 08:20:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:20:39] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0767,	best estimator xgboost's best error=0.0767
[flaml.automl: 09-16 08:20:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:20:42] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.0767,	best estimator xgboost's best error=0.0767
[flaml.automl: 09-16 08:20:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:20:44] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.0767,	best estimator xgboost's best error=0.0767
[flaml.automl: 09-16 08:20:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:20:47] {3072} INFO -  at 30.1s,	estimator xgboost's best error=0.0767,	best estimator xgboost's best error=0.0767
[flaml.automl: 09-16 08:20:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:20:50] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0767,	best estimator xgboost's best error=0.0767
[flaml.automl: 09-16 08:20:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:20:52] {3072} INFO -  at 35.2s,	estimator xgboost's best error=0.0767,	best estimator xgboost's best error=0.0767
[flaml.automl: 09-16 08:20:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:21:04] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-16 08:21:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:21:16] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.0693,	best estimator xgboost's best error=0.0693
[flaml.automl: 09-16 08:21:35] {3335} INFO - retrain xgboost for 18.8s
[flaml.automl: 09-16 08:21:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:21:35] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:21:35] {2637} INFO - Time taken to find the best model: 58.97096514701843
[flaml.automl: 09-16 08:21:35] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43075}
CO(0)最佳损失：0.9306618613302272
CO(0)最好结果：{'pred_time': 2.0329728613622955e-05, 'wall_clock_time': 58.97096514701843, 'metric_for_logging': {'pred_time': 2.0329728613622955e-05}, 'val_loss': 0.0693381386697728, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43075}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43075, 'experiment_tag': 'exp', 'time_total_s': 11.750290870666504}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.852291903984242
CO(0)的mse=0.013884526655483908
CO(0)的mae=0.06859930027979032
CO(0)的mar=0.11319441219708845
总共花费的时间为：78.56
沈阳市
1098A
1099A
1100A
1104A
1105A
1106A
2900A
[flaml.automl: 09-16 08:43:17] {2390} INFO - task = regression
[flaml.automl: 09-16 08:43:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:43:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:43:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:43:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:43:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:43:20] {3025} INFO - Estimated sufficient time budget=161993s. Estimated necessary time budget=162s.
[flaml.automl: 09-16 08:43:20] {3072} INFO -  at 2.6s,	estimator xgboost's best error=0.2472,	best estimator xgboost's best error=0.2472
[flaml.automl: 09-16 08:43:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:43:23] {3072} INFO -  at 5.7s,	estimator xgboost's best error=0.1520,	best estimator xgboost's best error=0.1520
[flaml.automl: 09-16 08:43:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:43:25] {3072} INFO -  at 7.9s,	estimator xgboost's best error=0.1520,	best estimator xgboost's best error=0.1520
[flaml.automl: 09-16 08:43:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:43:28] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.1520,	best estimator xgboost's best error=0.1520
[flaml.automl: 09-16 08:43:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:43:29] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.1277,	best estimator xgboost's best error=0.1277
[flaml.automl: 09-16 08:43:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:43:32] {3072} INFO -  at 14.4s,	estimator xgboost's best error=0.1142,	best estimator xgboost's best error=0.1142
[flaml.automl: 09-16 08:43:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:43:34] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.1142,	best estimator xgboost's best error=0.1142
[flaml.automl: 09-16 08:43:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:43:37] {3072} INFO -  at 19.4s,	estimator xgboost's best error=0.1142,	best estimator xgboost's best error=0.1142
[flaml.automl: 09-16 08:43:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:43:38] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.1142,	best estimator xgboost's best error=0.1142
[flaml.automl: 09-16 08:43:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:43:40] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.1142,	best estimator xgboost's best error=0.1142
[flaml.automl: 09-16 08:43:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:43:43] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.1125,	best estimator xgboost's best error=0.1125
[flaml.automl: 09-16 08:43:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:43:44] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.1125,	best estimator xgboost's best error=0.1125
[flaml.automl: 09-16 08:43:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:43:55] {3072} INFO -  at 38.1s,	estimator xgboost's best error=0.1078,	best estimator xgboost's best error=0.1078
[flaml.automl: 09-16 08:43:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:44:09] {3072} INFO -  at 52.3s,	estimator xgboost's best error=0.1036,	best estimator xgboost's best error=0.1036
[flaml.automl: 09-16 08:44:22] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 08:44:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:44:22] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:44:22] {2637} INFO - Time taken to find the best model: 52.34914183616638
[flaml.automl: 09-16 08:44:22] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 75042}
CO(0)最佳损失：0.8963684754485292
CO(0)最好结果：{'pred_time': 4.892941644286378e-06, 'wall_clock_time': 52.34914183616638, 'metric_for_logging': {'pred_time': 4.892941644286378e-06}, 'val_loss': 0.10363152455147086, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 75042}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 75042, 'experiment_tag': 'exp', 'time_total_s': 14.254472017288208}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8132825428488001
CO(0)的mse=0.026996185365183667
CO(0)的mae=0.1057706044258971
CO(0)的mar=0.13818865180853396
总共花费的时间为：65.82
大连市
1110A
1117A
[flaml.automl: 09-16 08:51:00] {2390} INFO - task = regression
[flaml.automl: 09-16 08:51:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:51:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:51:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:51:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:51:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:51:02] {3025} INFO - Estimated sufficient time budget=26061s. Estimated necessary time budget=26s.
[flaml.automl: 09-16 08:51:02] {3072} INFO -  at 2.8s,	estimator xgboost's best error=0.1661,	best estimator xgboost's best error=0.1661
[flaml.automl: 09-16 08:51:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:51:07] {3072} INFO -  at 7.7s,	estimator xgboost's best error=0.1021,	best estimator xgboost's best error=0.1021
[flaml.automl: 09-16 08:51:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:51:10] {3072} INFO -  at 10.3s,	estimator xgboost's best error=0.1021,	best estimator xgboost's best error=0.1021
[flaml.automl: 09-16 08:51:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:51:29] {3072} INFO -  at 29.8s,	estimator xgboost's best error=0.1021,	best estimator xgboost's best error=0.1021
[flaml.automl: 09-16 08:51:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:51:31] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.0832,	best estimator xgboost's best error=0.0832
[flaml.automl: 09-16 08:51:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:51:34] {3072} INFO -  at 34.4s,	estimator xgboost's best error=0.0730,	best estimator xgboost's best error=0.0730
[flaml.automl: 09-16 08:51:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:51:37] {3072} INFO -  at 37.4s,	estimator xgboost's best error=0.0730,	best estimator xgboost's best error=0.0730
[flaml.automl: 09-16 08:51:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:51:41] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0730,	best estimator xgboost's best error=0.0730
[flaml.automl: 09-16 08:51:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:51:43] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0730,	best estimator xgboost's best error=0.0730
[flaml.automl: 09-16 08:51:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:51:47] {3072} INFO -  at 47.1s,	estimator xgboost's best error=0.0730,	best estimator xgboost's best error=0.0730
[flaml.automl: 09-16 08:51:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:51:49] {3072} INFO -  at 49.1s,	estimator xgboost's best error=0.0730,	best estimator xgboost's best error=0.0730
[flaml.automl: 09-16 08:51:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:51:51] {3072} INFO -  at 51.1s,	estimator xgboost's best error=0.0730,	best estimator xgboost's best error=0.0730
[flaml.automl: 09-16 08:51:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:51:58] {3072} INFO -  at 58.5s,	estimator xgboost's best error=0.0679,	best estimator xgboost's best error=0.0679
[flaml.automl: 09-16 08:52:04] {3335} INFO - retrain xgboost for 6.0s
[flaml.automl: 09-16 08:52:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 08:52:04] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:52:04] {2637} INFO - Time taken to find the best model: 58.45786952972412
[flaml.automl: 09-16 08:52:04] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9320661096795493
CO(0)最好结果：{'pred_time': 1.644263321183631e-05, 'wall_clock_time': 58.45786952972412, 'metric_for_logging': {'pred_time': 1.644263321183631e-05}, 'val_loss': 0.06793389032045073, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 7.364293813705444}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8143385018800395
CO(0)的mse=0.009918665851457021
CO(0)的mae=0.06625686545623624
CO(0)的mar=0.14383223342436696
总共花费的时间为：64.83
长春市
1119A
1120A
1121A
1122A
1124A
1125A
1126A
1128A
[flaml.automl: 09-16 09:16:58] {2390} INFO - task = regression
[flaml.automl: 09-16 09:16:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 09:16:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 09:16:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 09:16:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 09:16:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 09:16:59] {3025} INFO - Estimated sufficient time budget=102343s. Estimated necessary time budget=102s.
[flaml.automl: 09-16 09:16:59] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.1381,	best estimator xgboost's best error=0.1381
[flaml.automl: 09-16 09:16:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 09:17:01] {3072} INFO -  at 3.8s,	estimator xgboost's best error=0.0846,	best estimator xgboost's best error=0.0846
[flaml.automl: 09-16 09:17:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 09:17:03] {3072} INFO -  at 5.0s,	estimator xgboost's best error=0.0846,	best estimator xgboost's best error=0.0846
[flaml.automl: 09-16 09:17:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 09:17:05] {3072} INFO -  at 7.8s,	estimator xgboost's best error=0.0846,	best estimator xgboost's best error=0.0846
[flaml.automl: 09-16 09:17:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 09:17:07] {3072} INFO -  at 8.9s,	estimator xgboost's best error=0.0709,	best estimator xgboost's best error=0.0709
[flaml.automl: 09-16 09:17:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 09:17:08] {3072} INFO -  at 10.5s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 09:17:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 09:17:10] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 09:17:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 09:17:12] {3072} INFO -  at 14.6s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 09:17:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 09:17:13] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 09:17:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 09:17:15] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 09:17:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 09:17:17] {3072} INFO -  at 19.4s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 09:17:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 09:17:18] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 09:17:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 09:17:25] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-16 09:17:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 09:17:37] {3072} INFO -  at 39.3s,	estimator xgboost's best error=0.0514,	best estimator xgboost's best error=0.0514
[flaml.automl: 09-16 09:17:37] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 09:17:44] {3072} INFO -  at 45.8s,	estimator xgboost's best error=0.0514,	best estimator xgboost's best error=0.0514
[flaml.automl: 09-16 09:17:44] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 09:17:57] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.0514,	best estimator xgboost's best error=0.0514
[flaml.automl: 09-16 09:18:09] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 09:18:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 09:18:09] {2636} INFO - fit succeeded
[flaml.automl: 09-16 09:18:09] {2637} INFO - Time taken to find the best model: 39.26817035675049
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 85104}
CO(0)最佳损失：0.9485868977673263
CO(0)最好结果：{'pred_time': 4.318484773945453e-06, 'wall_clock_time': 39.26817035675049, 'metric_for_logging': {'pred_time': 4.318484773945453e-06}, 'val_loss': 0.05141310223267373, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 85104}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 85104, 'experiment_tag': 'exp', 'time_total_s': 12.10716462135315}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8836517102043789
CO(0)的mse=0.006402648822890915
CO(0)的mae=0.05118192527324111
CO(0)的mar=0.08735276643690502
总共花费的时间为：72.97
哈尔滨市
1129A
1130A
1139A
1140A
[flaml.automl: 09-16 09:30:18] {2390} INFO - task = regression
[flaml.automl: 09-16 09:30:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 09:30:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 09:30:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 09:30:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 09:30:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 09:30:22] {3025} INFO - Estimated sufficient time budget=137144s. Estimated necessary time budget=137s.
[flaml.automl: 09-16 09:30:22] {3072} INFO -  at 3.8s,	estimator xgboost's best error=0.1795,	best estimator xgboost's best error=0.1795
[flaml.automl: 09-16 09:30:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 09:30:28] {3072} INFO -  at 9.9s,	estimator xgboost's best error=0.1161,	best estimator xgboost's best error=0.1161
[flaml.automl: 09-16 09:30:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 09:30:31] {3072} INFO -  at 13.0s,	estimator xgboost's best error=0.1161,	best estimator xgboost's best error=0.1161
[flaml.automl: 09-16 09:30:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 09:30:36] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.1161,	best estimator xgboost's best error=0.1161
[flaml.automl: 09-16 09:30:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 09:30:39] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0871,	best estimator xgboost's best error=0.0871
[flaml.automl: 09-16 09:30:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 09:30:44] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.0749,	best estimator xgboost's best error=0.0749
[flaml.automl: 09-16 09:30:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 09:30:47] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.0749,	best estimator xgboost's best error=0.0749
[flaml.automl: 09-16 09:30:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 09:30:50] {3072} INFO -  at 32.5s,	estimator xgboost's best error=0.0749,	best estimator xgboost's best error=0.0749
[flaml.automl: 09-16 09:30:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 09:30:53] {3072} INFO -  at 35.4s,	estimator xgboost's best error=0.0749,	best estimator xgboost's best error=0.0749
[flaml.automl: 09-16 09:30:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 09:30:55] {3072} INFO -  at 37.3s,	estimator xgboost's best error=0.0749,	best estimator xgboost's best error=0.0749
[flaml.automl: 09-16 09:30:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 09:31:00] {3072} INFO -  at 42.5s,	estimator xgboost's best error=0.0743,	best estimator xgboost's best error=0.0743
[flaml.automl: 09-16 09:31:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 09:31:03] {3072} INFO -  at 45.8s,	estimator xgboost's best error=0.0743,	best estimator xgboost's best error=0.0743
[flaml.automl: 09-16 09:31:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 09:31:15] {3072} INFO -  at 57.5s,	estimator xgboost's best error=0.0692,	best estimator xgboost's best error=0.0692
[flaml.automl: 09-16 09:31:21] {3335} INFO - retrain xgboost for 6.2s
[flaml.automl: 09-16 09:31:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 09:31:21] {2636} INFO - fit succeeded
[flaml.automl: 09-16 09:31:21] {2637} INFO - Time taken to find the best model: 57.538947105407715
[flaml.automl: 09-16 09:31:21] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 41061}
CO(0)最佳损失：0.9308272772201864
CO(0)最好结果：{'pred_time': 6.765643154715279e-06, 'wall_clock_time': 57.538947105407715, 'metric_for_logging': {'pred_time': 6.765643154715279e-06}, 'val_loss': 0.06917272277981362, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 41061}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 41061, 'experiment_tag': 'exp', 'time_total_s': 11.780757904052734}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8738530052127338
CO(0)的mse=0.012045368395245564
CO(0)的mae=0.06967343394005086
CO(0)的mar=0.18472768738942077
总共花费的时间为：65.37
上海市
1143A
1144A
1145A
1148A
1150A
3265A
3266A
3269A
3270A
3271A
3272A
3273A
3274A
3544A
[flaml.automl: 09-16 10:14:26] {2390} INFO - task = regression
[flaml.automl: 09-16 10:14:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 10:14:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 10:14:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 10:14:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 10:14:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 10:14:27] {3025} INFO - Estimated sufficient time budget=184142s. Estimated necessary time budget=184s.
[flaml.automl: 09-16 10:14:27] {3072} INFO -  at 2.1s,	estimator xgboost's best error=0.1361,	best estimator xgboost's best error=0.1361
[flaml.automl: 09-16 10:14:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 10:14:28] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.0936,	best estimator xgboost's best error=0.0936
[flaml.automl: 09-16 10:14:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 10:14:30] {3072} INFO -  at 5.1s,	estimator xgboost's best error=0.0936,	best estimator xgboost's best error=0.0936
[flaml.automl: 09-16 10:14:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 10:14:31] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.0936,	best estimator xgboost's best error=0.0936
[flaml.automl: 09-16 10:14:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 10:14:32] {3072} INFO -  at 7.4s,	estimator xgboost's best error=0.0573,	best estimator xgboost's best error=0.0573
[flaml.automl: 09-16 10:14:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 10:14:34] {3072} INFO -  at 9.0s,	estimator xgboost's best error=0.0460,	best estimator xgboost's best error=0.0460
[flaml.automl: 09-16 10:14:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 10:14:35] {3072} INFO -  at 10.4s,	estimator xgboost's best error=0.0460,	best estimator xgboost's best error=0.0460
[flaml.automl: 09-16 10:14:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 10:14:36] {3072} INFO -  at 11.7s,	estimator xgboost's best error=0.0460,	best estimator xgboost's best error=0.0460
[flaml.automl: 09-16 10:14:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 10:14:38] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.0460,	best estimator xgboost's best error=0.0460
[flaml.automl: 09-16 10:14:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 10:14:39] {3072} INFO -  at 14.2s,	estimator xgboost's best error=0.0460,	best estimator xgboost's best error=0.0460
[flaml.automl: 09-16 10:14:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 10:14:42] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-16 10:14:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 10:14:44] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-16 10:14:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 10:14:57] {3072} INFO -  at 31.8s,	estimator xgboost's best error=0.0385,	best estimator xgboost's best error=0.0385
[flaml.automl: 09-16 10:14:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 10:15:17] {3072} INFO -  at 52.0s,	estimator xgboost's best error=0.0375,	best estimator xgboost's best error=0.0375
[flaml.automl: 09-16 10:15:38] {3335} INFO - retrain xgboost for 21.6s
[flaml.automl: 09-16 10:15:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 10:15:38] {2636} INFO - fit succeeded
[flaml.automl: 09-16 10:15:38] {2637} INFO - Time taken to find the best model: 51.97816467285156
[flaml.automl: 09-16 10:15:38] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 153083}
CO(0)最佳损失：0.9625090111014949
CO(0)最好结果：{'pred_time': 4.978785439143105e-06, 'wall_clock_time': 51.97816467285156, 'metric_for_logging': {'pred_time': 4.978785439143105e-06}, 'val_loss': 0.03749098889850505, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 153083}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 153083, 'experiment_tag': 'exp', 'time_total_s': 20.138425827026367}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9238809390607781
CO(0)的mse=0.0039331042749604075
CO(0)的mae=0.03709283092579987
CO(0)的mar=0.06845368192848846
总共花费的时间为：75.98
南京市
1151A
1152A
1153A
1154A
3423A
3424A
3427A
[flaml.automl: 09-16 10:37:41] {2390} INFO - task = regression
[flaml.automl: 09-16 10:37:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 10:37:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 10:37:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 10:37:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 10:37:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 10:37:42] {3025} INFO - Estimated sufficient time budget=90056s. Estimated necessary time budget=90s.
[flaml.automl: 09-16 10:37:42] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.1551,	best estimator xgboost's best error=0.1551
[flaml.automl: 09-16 10:37:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 10:37:45] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-16 10:37:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 10:37:46] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-16 10:37:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 10:37:49] {3072} INFO -  at 8.1s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-16 10:37:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 10:37:50] {3072} INFO -  at 9.2s,	estimator xgboost's best error=0.0772,	best estimator xgboost's best error=0.0772
[flaml.automl: 09-16 10:37:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 10:37:52] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0662,	best estimator xgboost's best error=0.0662
[flaml.automl: 09-16 10:37:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 10:37:53] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0662,	best estimator xgboost's best error=0.0662
[flaml.automl: 09-16 10:37:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 10:37:56] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0662,	best estimator xgboost's best error=0.0662
[flaml.automl: 09-16 10:37:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 10:37:57] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.0662,	best estimator xgboost's best error=0.0662
[flaml.automl: 09-16 10:37:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 10:38:00] {3072} INFO -  at 18.7s,	estimator xgboost's best error=0.0662,	best estimator xgboost's best error=0.0662
[flaml.automl: 09-16 10:38:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 10:38:01] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0660,	best estimator xgboost's best error=0.0660
[flaml.automl: 09-16 10:38:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 10:38:02] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0660,	best estimator xgboost's best error=0.0660
[flaml.automl: 09-16 10:38:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 10:38:09] {3072} INFO -  at 28.0s,	estimator xgboost's best error=0.0607,	best estimator xgboost's best error=0.0607
[flaml.automl: 09-16 10:38:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 10:38:21] {3072} INFO -  at 40.1s,	estimator xgboost's best error=0.0590,	best estimator xgboost's best error=0.0590
[flaml.automl: 09-16 10:38:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 10:38:28] {3072} INFO -  at 46.6s,	estimator xgboost's best error=0.0590,	best estimator xgboost's best error=0.0590
[flaml.automl: 09-16 10:38:28] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 10:38:40] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0590,	best estimator xgboost's best error=0.0590
[flaml.automl: 09-16 10:38:52] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 10:38:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 10:38:52] {2636} INFO - fit succeeded
[flaml.automl: 09-16 10:38:52] {2637} INFO - Time taken to find the best model: 40.09116721153259
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 76083}
CO(0)最佳损失：0.9410060418601405
CO(0)最好结果：{'pred_time': 4.821616705918611e-06, 'wall_clock_time': 40.09116721153259, 'metric_for_logging': {'pred_time': 4.821616705918611e-06}, 'val_loss': 0.058993958139859505, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 76083}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 76083, 'experiment_tag': 'exp', 'time_total_s': 12.07016110420227}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8667267902021143
CO(0)的mse=0.008160370872306396
CO(0)的mae=0.05792711022899764
CO(0)的mar=0.1091711975955851
总共花费的时间为：72.31
苏州市
1160A
1164A
1165A
1166A
1167A
3289A
3290A
3425A
3431A
[flaml.automl: 09-16 11:05:42] {2390} INFO - task = regression
[flaml.automl: 09-16 11:05:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:05:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:05:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:05:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:05:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:05:43] {3025} INFO - Estimated sufficient time budget=114897s. Estimated necessary time budget=115s.
[flaml.automl: 09-16 11:05:43] {3072} INFO -  at 1.7s,	estimator xgboost's best error=0.1258,	best estimator xgboost's best error=0.1258
[flaml.automl: 09-16 11:05:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:05:45] {3072} INFO -  at 3.8s,	estimator xgboost's best error=0.0721,	best estimator xgboost's best error=0.0721
[flaml.automl: 09-16 11:05:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:05:46] {3072} INFO -  at 5.0s,	estimator xgboost's best error=0.0721,	best estimator xgboost's best error=0.0721
[flaml.automl: 09-16 11:05:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:05:49] {3072} INFO -  at 7.8s,	estimator xgboost's best error=0.0721,	best estimator xgboost's best error=0.0721
[flaml.automl: 09-16 11:05:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:05:50] {3072} INFO -  at 8.9s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-16 11:05:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:05:52] {3072} INFO -  at 10.5s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-16 11:05:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:05:54] {3072} INFO -  at 12.1s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-16 11:05:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:05:55] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-16 11:05:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:05:57] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-16 11:05:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:05:59] {3072} INFO -  at 17.2s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-16 11:05:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:06:00] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0460,	best estimator xgboost's best error=0.0460
[flaml.automl: 09-16 11:06:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:06:01] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.0460,	best estimator xgboost's best error=0.0460
[flaml.automl: 09-16 11:06:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:06:08] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-16 11:06:08] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:06:20] {3072} INFO -  at 38.8s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-16 11:06:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:06:27] {3072} INFO -  at 45.4s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-16 11:06:27] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 11:06:40] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-16 11:06:53] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 11:06:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:06:53] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:06:53] {2637} INFO - Time taken to find the best model: 38.78559422492981
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 96291}
CO(0)最佳损失：0.9606787966007552
CO(0)最好结果：{'pred_time': 3.8089707633045234e-06, 'wall_clock_time': 38.78559422492981, 'metric_for_logging': {'pred_time': 3.8089707633045234e-06}, 'val_loss': 0.039321203399244865, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 96291}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 96291, 'experiment_tag': 'exp', 'time_total_s': 12.147536277770996}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9025421999380647
CO(0)的mse=0.003874961354798212
CO(0)的mae=0.03906069078578703
CO(0)的mar=0.06773750059977314
总共花费的时间为：72.92
南通市
1168A
1169A
1171A
1172A
3291A
3432A
[flaml.automl: 09-16 11:25:55] {2390} INFO - task = regression
[flaml.automl: 09-16 11:25:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:25:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:25:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:25:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:25:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:25:57] {3025} INFO - Estimated sufficient time budget=139060s. Estimated necessary time budget=139s.
[flaml.automl: 09-16 11:25:57] {3072} INFO -  at 2.6s,	estimator xgboost's best error=0.1275,	best estimator xgboost's best error=0.1275
[flaml.automl: 09-16 11:25:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:26:01] {3072} INFO -  at 6.5s,	estimator xgboost's best error=0.0755,	best estimator xgboost's best error=0.0755
[flaml.automl: 09-16 11:26:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:26:03] {3072} INFO -  at 8.8s,	estimator xgboost's best error=0.0755,	best estimator xgboost's best error=0.0755
[flaml.automl: 09-16 11:26:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:26:07] {3072} INFO -  at 11.9s,	estimator xgboost's best error=0.0755,	best estimator xgboost's best error=0.0755
[flaml.automl: 09-16 11:26:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:26:09] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0614,	best estimator xgboost's best error=0.0614
[flaml.automl: 09-16 11:26:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:26:11] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-16 11:26:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:26:13] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-16 11:26:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:26:15] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-16 11:26:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:26:16] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-16 11:26:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:26:19] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-16 11:26:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:26:21] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-16 11:26:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:26:22] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-16 11:26:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:26:28] {3072} INFO -  at 33.7s,	estimator xgboost's best error=0.0455,	best estimator xgboost's best error=0.0455
[flaml.automl: 09-16 11:26:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:26:40] {3072} INFO -  at 45.8s,	estimator xgboost's best error=0.0444,	best estimator xgboost's best error=0.0444
[flaml.automl: 09-16 11:26:40] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:26:47] {3072} INFO -  at 52.3s,	estimator xgboost's best error=0.0444,	best estimator xgboost's best error=0.0444
[flaml.automl: 09-16 11:26:59] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 11:26:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:26:59] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:26:59] {2637} INFO - Time taken to find the best model: 45.775819063186646
[flaml.automl: 09-16 11:26:59] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64408}
CO(0)最佳损失：0.9556018455327931
CO(0)最好结果：{'pred_time': 6.056671387759171e-06, 'wall_clock_time': 45.775819063186646, 'metric_for_logging': {'pred_time': 6.056671387759171e-06}, 'val_loss': 0.044398154467206866, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64408}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 64408, 'experiment_tag': 'exp', 'time_total_s': 12.030765056610107}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8765976073558921
CO(0)的mse=0.0053447387802426395
CO(0)的mae=0.043741115087369685
CO(0)的mar=0.07344332233131447
总共花费的时间为：65.73
连云港市
1173A
3000A
3008A
3009A
3434A
3664A
[flaml.automl: 09-16 11:45:34] {2390} INFO - task = regression
[flaml.automl: 09-16 11:45:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:45:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:45:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:45:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:45:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:45:36] {3025} INFO - Estimated sufficient time budget=144194s. Estimated necessary time budget=144s.
[flaml.automl: 09-16 11:45:36] {3072} INFO -  at 2.6s,	estimator xgboost's best error=0.1546,	best estimator xgboost's best error=0.1546
[flaml.automl: 09-16 11:45:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:45:40] {3072} INFO -  at 6.4s,	estimator xgboost's best error=0.1006,	best estimator xgboost's best error=0.1006
[flaml.automl: 09-16 11:45:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:45:42] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.1006,	best estimator xgboost's best error=0.1006
[flaml.automl: 09-16 11:45:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:45:45] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.1006,	best estimator xgboost's best error=0.1006
[flaml.automl: 09-16 11:45:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:45:47] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.0748,	best estimator xgboost's best error=0.0748
[flaml.automl: 09-16 11:45:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:45:50] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0628,	best estimator xgboost's best error=0.0628
[flaml.automl: 09-16 11:45:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:45:53] {3072} INFO -  at 19.9s,	estimator xgboost's best error=0.0628,	best estimator xgboost's best error=0.0628
[flaml.automl: 09-16 11:45:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:45:55] {3072} INFO -  at 22.2s,	estimator xgboost's best error=0.0628,	best estimator xgboost's best error=0.0628
[flaml.automl: 09-16 11:45:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:45:57] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0628,	best estimator xgboost's best error=0.0628
[flaml.automl: 09-16 11:45:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:45:59] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.0628,	best estimator xgboost's best error=0.0628
[flaml.automl: 09-16 11:45:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:46:01] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.0628,	best estimator xgboost's best error=0.0628
[flaml.automl: 09-16 11:46:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:46:02] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.0628,	best estimator xgboost's best error=0.0628
[flaml.automl: 09-16 11:46:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:46:08] {3072} INFO -  at 35.3s,	estimator xgboost's best error=0.0599,	best estimator xgboost's best error=0.0599
[flaml.automl: 09-16 11:46:08] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:46:20] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.0556,	best estimator xgboost's best error=0.0556
[flaml.automl: 09-16 11:46:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:46:27] {3072} INFO -  at 53.8s,	estimator xgboost's best error=0.0556,	best estimator xgboost's best error=0.0556
[flaml.automl: 09-16 11:46:39] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 11:46:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:46:39] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:46:39] {2637} INFO - Time taken to find the best model: 47.281662940979004
[flaml.automl: 09-16 11:46:39] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65310}
CO(0)最佳损失：0.9443962310946101
CO(0)最好结果：{'pred_time': 5.853788836593386e-06, 'wall_clock_time': 47.281662940979004, 'metric_for_logging': {'pred_time': 5.853788836593386e-06}, 'val_loss': 0.05560376890538989, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65310}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 65310, 'experiment_tag': 'exp', 'time_total_s': 11.998191356658936}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.864080588335959
CO(0)的mse=0.008114215314273053
CO(0)的mae=0.05532438300671203
CO(0)的mar=0.1018370586545322
总共花费的时间为：67.40
徐州市
1177A
3006A
3288A
[flaml.automl: 09-16 11:56:22] {2390} INFO - task = regression
[flaml.automl: 09-16 11:56:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:56:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:56:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:56:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:56:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:56:23] {3025} INFO - Estimated sufficient time budget=12308s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 11:56:23] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1681,	best estimator xgboost's best error=0.1681
[flaml.automl: 09-16 11:56:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:56:25] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0976,	best estimator xgboost's best error=0.0976
[flaml.automl: 09-16 11:56:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:56:26] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0976,	best estimator xgboost's best error=0.0976
[flaml.automl: 09-16 11:56:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:56:36] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0976,	best estimator xgboost's best error=0.0976
[flaml.automl: 09-16 11:56:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:56:38] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0791,	best estimator xgboost's best error=0.0791
[flaml.automl: 09-16 11:56:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:56:39] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-16 11:56:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:56:41] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-16 11:56:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:56:43] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-16 11:56:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:56:44] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-16 11:56:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:56:47] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-16 11:56:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:56:48] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-16 11:56:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:56:49] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-16 11:56:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:56:56] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0629,	best estimator xgboost's best error=0.0629
[flaml.automl: 09-16 11:56:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:57:08] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-16 11:57:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:57:14] {3072} INFO -  at 52.8s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-16 11:57:26] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 11:57:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:57:26] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:57:26] {2637} INFO - Time taken to find the best model: 46.34799003601074
[flaml.automl: 09-16 11:57:26] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9394055636707959
CO(0)最好结果：{'pred_time': 1.1033544070006533e-05, 'wall_clock_time': 46.34799003601074, 'metric_for_logging': {'pred_time': 1.1033544070006533e-05}, 'val_loss': 0.06059443632920408, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.016038656234741}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8514321205888133
CO(0)的mse=0.009815061196983358
CO(0)的mae=0.06102868114802099
CO(0)的mar=0.08841959674112021
总共花费的时间为：65.35
扬州市
1186A
3164A
3195A
3294A
[flaml.automl: 09-16 12:10:16] {2390} INFO - task = regression
[flaml.automl: 09-16 12:10:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:10:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:10:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:10:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:10:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:10:17] {3025} INFO - Estimated sufficient time budget=52544s. Estimated necessary time budget=53s.
[flaml.automl: 09-16 12:10:17] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1455,	best estimator xgboost's best error=0.1455
[flaml.automl: 09-16 12:10:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:10:19] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0859,	best estimator xgboost's best error=0.0859
[flaml.automl: 09-16 12:10:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:10:20] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0859,	best estimator xgboost's best error=0.0859
[flaml.automl: 09-16 12:10:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:10:26] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.0859,	best estimator xgboost's best error=0.0859
[flaml.automl: 09-16 12:10:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:10:27] {3072} INFO -  at 11.7s,	estimator xgboost's best error=0.0692,	best estimator xgboost's best error=0.0692
[flaml.automl: 09-16 12:10:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:10:29] {3072} INFO -  at 13.3s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 12:10:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:10:30] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 12:10:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:10:33] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 12:10:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:10:34] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 12:10:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:10:37] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-16 12:10:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:10:38] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0594,	best estimator xgboost's best error=0.0594
[flaml.automl: 09-16 12:10:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:10:39] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0594,	best estimator xgboost's best error=0.0594
[flaml.automl: 09-16 12:10:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:10:46] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-16 12:10:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:10:58] {3072} INFO -  at 42.6s,	estimator xgboost's best error=0.0527,	best estimator xgboost's best error=0.0527
[flaml.automl: 09-16 12:10:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 12:11:05] {3072} INFO -  at 49.1s,	estimator xgboost's best error=0.0527,	best estimator xgboost's best error=0.0527
[flaml.automl: 09-16 12:11:17] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 12:11:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 12:11:17] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:11:17] {2637} INFO - Time taken to find the best model: 42.58278036117554
[flaml.automl: 09-16 12:11:17] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43748}
CO(0)最佳损失：0.947302838061982
CO(0)最好结果：{'pred_time': 8.363631546092706e-06, 'wall_clock_time': 42.58278036117554, 'metric_for_logging': {'pred_time': 8.363631546092706e-06}, 'val_loss': 0.05269716193801801, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43748}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43748, 'experiment_tag': 'exp', 'time_total_s': 12.078716278076172}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8556941680366021
CO(0)的mse=0.006207858725714428
CO(0)的mae=0.053053499602181343
CO(0)的mar=0.08536241786987878
总共花费的时间为：61.81
无锡市
1189A
1190A
1191A
1192A
1193A
1194A
1195A
3428A
[flaml.automl: 09-16 12:34:56] {2390} INFO - task = regression
[flaml.automl: 09-16 12:34:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:34:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:34:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:34:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:34:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:34:58] {3025} INFO - Estimated sufficient time budget=222626s. Estimated necessary time budget=223s.
[flaml.automl: 09-16 12:34:58] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.1813,	best estimator xgboost's best error=0.1813
[flaml.automl: 09-16 12:34:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:35:01] {3072} INFO -  at 5.9s,	estimator xgboost's best error=0.1468,	best estimator xgboost's best error=0.1468
[flaml.automl: 09-16 12:35:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:35:04] {3072} INFO -  at 8.4s,	estimator xgboost's best error=0.1468,	best estimator xgboost's best error=0.1468
[flaml.automl: 09-16 12:35:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:35:06] {3072} INFO -  at 10.9s,	estimator xgboost's best error=0.1468,	best estimator xgboost's best error=0.1468
[flaml.automl: 09-16 12:35:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:35:09] {3072} INFO -  at 13.4s,	estimator xgboost's best error=0.0804,	best estimator xgboost's best error=0.0804
[flaml.automl: 09-16 12:35:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:35:10] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0804,	best estimator xgboost's best error=0.0804
[flaml.automl: 09-16 12:35:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:35:12] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.0804,	best estimator xgboost's best error=0.0804
[flaml.automl: 09-16 12:35:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:35:15] {3072} INFO -  at 19.5s,	estimator xgboost's best error=0.0804,	best estimator xgboost's best error=0.0804
[flaml.automl: 09-16 12:35:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:35:17] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0804,	best estimator xgboost's best error=0.0804
[flaml.automl: 09-16 12:35:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:35:18] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.0804,	best estimator xgboost's best error=0.0804
[flaml.automl: 09-16 12:35:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:35:20] {3072} INFO -  at 25.0s,	estimator xgboost's best error=0.0804,	best estimator xgboost's best error=0.0804
[flaml.automl: 09-16 12:35:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:35:21] {3072} INFO -  at 26.3s,	estimator xgboost's best error=0.0744,	best estimator xgboost's best error=0.0744
[flaml.automl: 09-16 12:35:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:35:23] {3072} INFO -  at 27.5s,	estimator xgboost's best error=0.0744,	best estimator xgboost's best error=0.0744
[flaml.automl: 09-16 12:35:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:35:27] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.0645,	best estimator xgboost's best error=0.0645
[flaml.automl: 09-16 12:35:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 12:35:35] {3072} INFO -  at 39.9s,	estimator xgboost's best error=0.0616,	best estimator xgboost's best error=0.0616
[flaml.automl: 09-16 12:35:35] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 12:35:39] {3072} INFO -  at 44.3s,	estimator xgboost's best error=0.0616,	best estimator xgboost's best error=0.0616
[flaml.automl: 09-16 12:35:39] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 12:35:53] {3072} INFO -  at 57.3s,	estimator xgboost's best error=0.0604,	best estimator xgboost's best error=0.0604
[flaml.automl: 09-16 12:36:06] {3335} INFO - retrain xgboost for 13.2s
[flaml.automl: 09-16 12:36:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.8035941410052844, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=26.07574904878727, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.19568285831667376, scale_pos_weight=1,
             subsample=0.9453052099956202, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 12:36:06] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:36:06] {2637} INFO - Time taken to find the best model: 57.34208083152771
[flaml.automl: 09-16 12:36:06] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 12, 'min_child_weight': 26.07574904878727, 'learning_rate': 0.5310029457853219, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.8035941410052844, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.19568285831667376, 'FLAML_sample_size': 85617}
CO(0)最佳损失：0.9395526644011646
CO(0)最好结果：{'pred_time': 4.346997507561096e-06, 'wall_clock_time': 57.34208083152771, 'metric_for_logging': {'pred_time': 4.346997507561096e-06}, 'val_loss': 0.060447335598835444, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 12, 'min_child_weight': 26.07574904878727, 'learning_rate': 0.5310029457853219, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.8035941410052844, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.19568285831667376, 'FLAML_sample_size': 85617}, 'config/n_estimators': 19, 'config/max_leaves': 12, 'config/min_child_weight': 26.07574904878727, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.9453052099956202, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.8035941410052844, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.19568285831667376, 'config/FLAML_sample_size': 85617, 'experiment_tag': 'exp', 'time_total_s': 13.068739891052246}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.8035941410052844, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=26.07574904878727, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.19568285831667376, scale_pos_weight=1,
             subsample=0.9453052099956202, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8453259295042238
CO(0)的mse=0.009520842654993988
CO(0)的mae=0.0620784857890633
CO(0)的mar=0.0994038157186041
总共花费的时间为：72.12
常州市
1196A
3003A
3010A
3429A
3430A
[flaml.automl: 09-16 12:52:11] {2390} INFO - task = regression
[flaml.automl: 09-16 12:52:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:52:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:52:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:52:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:52:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:52:13] {3025} INFO - Estimated sufficient time budget=119952s. Estimated necessary time budget=120s.
[flaml.automl: 09-16 12:52:13] {3072} INFO -  at 2.6s,	estimator xgboost's best error=0.1747,	best estimator xgboost's best error=0.1747
[flaml.automl: 09-16 12:52:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:52:17] {3072} INFO -  at 6.2s,	estimator xgboost's best error=0.1113,	best estimator xgboost's best error=0.1113
[flaml.automl: 09-16 12:52:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:52:19] {3072} INFO -  at 8.1s,	estimator xgboost's best error=0.1113,	best estimator xgboost's best error=0.1113
[flaml.automl: 09-16 12:52:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:52:23] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.1113,	best estimator xgboost's best error=0.1113
[flaml.automl: 09-16 12:52:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:52:25] {3072} INFO -  at 14.4s,	estimator xgboost's best error=0.0919,	best estimator xgboost's best error=0.0919
[flaml.automl: 09-16 12:52:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:52:28] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0837,	best estimator xgboost's best error=0.0837
[flaml.automl: 09-16 12:52:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:52:31] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.0837,	best estimator xgboost's best error=0.0837
[flaml.automl: 09-16 12:52:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:52:34] {3072} INFO -  at 23.7s,	estimator xgboost's best error=0.0837,	best estimator xgboost's best error=0.0837
[flaml.automl: 09-16 12:52:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:52:36] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0837,	best estimator xgboost's best error=0.0837
[flaml.automl: 09-16 12:52:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:52:39] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.0837,	best estimator xgboost's best error=0.0837
[flaml.automl: 09-16 12:52:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:52:42] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.0836,	best estimator xgboost's best error=0.0836
[flaml.automl: 09-16 12:52:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:52:44] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0836,	best estimator xgboost's best error=0.0836
[flaml.automl: 09-16 12:52:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:52:55] {3072} INFO -  at 44.2s,	estimator xgboost's best error=0.0789,	best estimator xgboost's best error=0.0789
[flaml.automl: 09-16 12:52:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:53:09] {3072} INFO -  at 58.6s,	estimator xgboost's best error=0.0775,	best estimator xgboost's best error=0.0775
[flaml.automl: 09-16 12:53:21] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 12:53:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 12:53:21] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:53:21] {2637} INFO - Time taken to find the best model: 58.640883922576904
[flaml.automl: 09-16 12:53:21] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53949}
CO(0)最佳损失：0.9225297105801513
CO(0)最好结果：{'pred_time': 6.677788232544047e-06, 'wall_clock_time': 58.640883922576904, 'metric_for_logging': {'pred_time': 6.677788232544047e-06}, 'val_loss': 0.0774702894198487, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53949}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53949, 'experiment_tag': 'exp', 'time_total_s': 14.459935665130615}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.755288665321911
CO(0)的mse=0.014977795868124335
CO(0)的mae=0.07560253324842976
CO(0)的mar=0.11689666358479239
总共花费的时间为：71.92
镇江市
3287A
[flaml.automl: 09-16 12:57:04] {2390} INFO - task = regression
[flaml.automl: 09-16 12:57:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:57:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:57:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:57:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:57:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:57:08] {3025} INFO - Estimated sufficient time budget=34052s. Estimated necessary time budget=34s.
[flaml.automl: 09-16 12:57:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1274,	best estimator xgboost's best error=0.1274
[flaml.automl: 09-16 12:57:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:57:13] {3072} INFO -  at 8.7s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-16 12:57:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:57:16] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-16 12:57:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:57:35] {3072} INFO -  at 31.2s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-16 12:57:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:57:37] {3072} INFO -  at 33.3s,	estimator xgboost's best error=0.0727,	best estimator xgboost's best error=0.0727
[flaml.automl: 09-16 12:57:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:57:40] {3072} INFO -  at 36.3s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 12:57:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:57:43] {3072} INFO -  at 39.1s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 12:57:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:57:47] {3072} INFO -  at 43.1s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 12:57:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:57:49] {3072} INFO -  at 45.2s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 12:57:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:57:54] {3072} INFO -  at 49.8s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 12:57:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:57:56] {3072} INFO -  at 51.9s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 12:57:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:57:58] {3072} INFO -  at 54.0s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 12:57:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:58:04] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 12:58:06] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-16 12:58:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 12:58:06] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:58:06] {2637} INFO - Time taken to find the best model: 36.26869750022888
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
CO(0)最佳损失：0.9343330936579922
CO(0)最好结果：{'pred_time': 5.2583568236407114e-05, 'wall_clock_time': 36.26869750022888, 'metric_for_logging': {'pred_time': 5.2583568236407114e-05}, 'val_loss': 0.06566690634200775, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 2.935701608657837}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7664186907573525
CO(0)的mse=0.007661921114243276
CO(0)的mae=0.06472463220625042
CO(0)的mar=0.12560014643144687
总共花费的时间为：62.63
泰州市
1206A
1207A
3295A
3435A
[flaml.automl: 09-16 13:10:53] {2390} INFO - task = regression
[flaml.automl: 09-16 13:10:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:10:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:10:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:10:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:10:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:10:54] {3025} INFO - Estimated sufficient time budget=52213s. Estimated necessary time budget=52s.
[flaml.automl: 09-16 13:10:54] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1597,	best estimator xgboost's best error=0.1597
[flaml.automl: 09-16 13:10:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:10:56] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0897,	best estimator xgboost's best error=0.0897
[flaml.automl: 09-16 13:10:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:10:57] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0897,	best estimator xgboost's best error=0.0897
[flaml.automl: 09-16 13:10:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:11:03] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.0897,	best estimator xgboost's best error=0.0897
[flaml.automl: 09-16 13:11:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:11:04] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.0690,	best estimator xgboost's best error=0.0690
[flaml.automl: 09-16 13:11:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:11:06] {3072} INFO -  at 13.4s,	estimator xgboost's best error=0.0588,	best estimator xgboost's best error=0.0588
[flaml.automl: 09-16 13:11:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:11:08] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0588,	best estimator xgboost's best error=0.0588
[flaml.automl: 09-16 13:11:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:11:10] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0588,	best estimator xgboost's best error=0.0588
[flaml.automl: 09-16 13:11:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:11:11] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.0588,	best estimator xgboost's best error=0.0588
[flaml.automl: 09-16 13:11:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:11:14] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.0588,	best estimator xgboost's best error=0.0588
[flaml.automl: 09-16 13:11:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:11:16] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0583,	best estimator xgboost's best error=0.0583
[flaml.automl: 09-16 13:11:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:11:17] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.0583,	best estimator xgboost's best error=0.0583
[flaml.automl: 09-16 13:11:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:11:23] {3072} INFO -  at 30.6s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-16 13:11:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:11:35] {3072} INFO -  at 42.8s,	estimator xgboost's best error=0.0510,	best estimator xgboost's best error=0.0510
[flaml.automl: 09-16 13:11:35] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:11:42] {3072} INFO -  at 49.3s,	estimator xgboost's best error=0.0510,	best estimator xgboost's best error=0.0510
[flaml.automl: 09-16 13:11:54] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 13:11:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:11:54] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:11:54] {2637} INFO - Time taken to find the best model: 42.769519329071045
[flaml.automl: 09-16 13:11:54] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43612}
CO(0)最佳损失：0.948961411330543
CO(0)最好结果：{'pred_time': 9.540088699221169e-06, 'wall_clock_time': 42.769519329071045, 'metric_for_logging': {'pred_time': 9.540088699221169e-06}, 'val_loss': 0.05103858866945702, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43612}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43612, 'experiment_tag': 'exp', 'time_total_s': 12.119970560073853}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8752650329538485
CO(0)的mse=0.007097021434961052
CO(0)的mae=0.052733782734147014
CO(0)的mar=0.08363527008484327
总共花费的时间为：62.03
淮安市
1210A
1211A
1213A
1214A
3426A
[flaml.automl: 09-16 13:27:43] {2390} INFO - task = regression
[flaml.automl: 09-16 13:27:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:27:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:27:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:27:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:27:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:27:44] {3025} INFO - Estimated sufficient time budget=64904s. Estimated necessary time budget=65s.
[flaml.automl: 09-16 13:27:44] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1456,	best estimator xgboost's best error=0.1456
[flaml.automl: 09-16 13:27:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:27:46] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0858,	best estimator xgboost's best error=0.0858
[flaml.automl: 09-16 13:27:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:27:47] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0858,	best estimator xgboost's best error=0.0858
[flaml.automl: 09-16 13:27:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:27:52] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0858,	best estimator xgboost's best error=0.0858
[flaml.automl: 09-16 13:27:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:27:53] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-16 13:27:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:27:55] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 13:27:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:27:56] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 13:27:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:27:59] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 13:27:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:28:00] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 13:28:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:28:03] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 13:28:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:28:04] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0574,	best estimator xgboost's best error=0.0574
[flaml.automl: 09-16 13:28:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:28:05] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0574,	best estimator xgboost's best error=0.0574
[flaml.automl: 09-16 13:28:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:28:12] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0515,	best estimator xgboost's best error=0.0515
[flaml.automl: 09-16 13:28:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:28:24] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-16 13:28:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:28:31] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-16 13:28:43] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 13:28:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:28:43] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:28:43] {2637} INFO - Time taken to find the best model: 41.69917297363281
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54333}
CO(0)最佳损失：0.950596813645563
CO(0)最好结果：{'pred_time': 6.532898088210701e-06, 'wall_clock_time': 41.69917297363281, 'metric_for_logging': {'pred_time': 6.532898088210701e-06}, 'val_loss': 0.04940318635443695, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54333}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54333, 'experiment_tag': 'exp', 'time_total_s': 12.139868021011353}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8725598195795425
CO(0)的mse=0.007645786199218483
CO(0)的mae=0.052095974652893
CO(0)的mar=0.11578762128254348
总共花费的时间为：61.35
盐城市
1215A
1216A
3293A
3436A
[flaml.automl: 09-16 13:41:10] {2390} INFO - task = regression
[flaml.automl: 09-16 13:41:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:41:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:41:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:41:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:41:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:41:13] {3025} INFO - Estimated sufficient time budget=114968s. Estimated necessary time budget=115s.
[flaml.automl: 09-16 13:41:13] {3072} INFO -  at 2.9s,	estimator xgboost's best error=0.1252,	best estimator xgboost's best error=0.1252
[flaml.automl: 09-16 13:41:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:41:18] {3072} INFO -  at 7.9s,	estimator xgboost's best error=0.0715,	best estimator xgboost's best error=0.0715
[flaml.automl: 09-16 13:41:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:41:21] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0715,	best estimator xgboost's best error=0.0715
[flaml.automl: 09-16 13:41:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:41:25] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.0715,	best estimator xgboost's best error=0.0715
[flaml.automl: 09-16 13:41:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:41:28] {3072} INFO -  at 18.2s,	estimator xgboost's best error=0.0554,	best estimator xgboost's best error=0.0554
[flaml.automl: 09-16 13:41:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:41:32] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0442,	best estimator xgboost's best error=0.0442
[flaml.automl: 09-16 13:41:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:41:36] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.0442,	best estimator xgboost's best error=0.0442
[flaml.automl: 09-16 13:41:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:41:39] {3072} INFO -  at 29.2s,	estimator xgboost's best error=0.0442,	best estimator xgboost's best error=0.0442
[flaml.automl: 09-16 13:41:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:41:40] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0442,	best estimator xgboost's best error=0.0442
[flaml.automl: 09-16 13:41:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:41:43] {3072} INFO -  at 33.1s,	estimator xgboost's best error=0.0442,	best estimator xgboost's best error=0.0442
[flaml.automl: 09-16 13:41:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:41:44] {3072} INFO -  at 34.7s,	estimator xgboost's best error=0.0440,	best estimator xgboost's best error=0.0440
[flaml.automl: 09-16 13:41:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:41:45] {3072} INFO -  at 35.8s,	estimator xgboost's best error=0.0440,	best estimator xgboost's best error=0.0440
[flaml.automl: 09-16 13:41:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:41:52] {3072} INFO -  at 42.2s,	estimator xgboost's best error=0.0386,	best estimator xgboost's best error=0.0386
[flaml.automl: 09-16 13:41:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:42:04] {3072} INFO -  at 54.0s,	estimator xgboost's best error=0.0363,	best estimator xgboost's best error=0.0363
[flaml.automl: 09-16 13:42:11] {3335} INFO - retrain xgboost for 7.4s
[flaml.automl: 09-16 13:42:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:42:11] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:42:11] {2637} INFO - Time taken to find the best model: 54.02151942253113
[flaml.automl: 09-16 13:42:11] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43135}
CO(0)最佳损失：0.963668184159097
CO(0)最好结果：{'pred_time': 8.141449789599987e-06, 'wall_clock_time': 54.02151942253113, 'metric_for_logging': {'pred_time': 8.141449789599987e-06}, 'val_loss': 0.03633181584090299, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43135}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43135, 'experiment_tag': 'exp', 'time_total_s': 11.791323184967041}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9339509947853173
CO(0)的mse=0.003194639553714733
CO(0)的mae=0.036441183766878237
CO(0)的mar=0.0816463181129654
总共花费的时间为：62.35
宿迁市
3191A
[flaml.automl: 09-16 13:46:10] {2390} INFO - task = regression
[flaml.automl: 09-16 13:46:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:46:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:46:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:46:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:46:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:46:11] {3025} INFO - Estimated sufficient time budget=17003s. Estimated necessary time budget=17s.
[flaml.automl: 09-16 13:46:11] {3072} INFO -  at 1.8s,	estimator xgboost's best error=0.1223,	best estimator xgboost's best error=0.1223
[flaml.automl: 09-16 13:46:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:46:14] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-16 13:46:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:46:16] {3072} INFO -  at 6.6s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-16 13:46:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:46:27] {3072} INFO -  at 17.0s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-16 13:46:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:46:29] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-16 13:46:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:46:31] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.0597,	best estimator xgboost's best error=0.0597
[flaml.automl: 09-16 13:46:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:46:34] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 13:46:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:46:38] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.0576,	best estimator xgboost's best error=0.0576
[flaml.automl: 09-16 13:46:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:46:41] {3072} INFO -  at 31.3s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 13:46:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:46:44] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.0570,	best estimator xgboost's best error=0.0570
[flaml.automl: 09-16 13:46:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:46:45] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.0570,	best estimator xgboost's best error=0.0570
[flaml.automl: 09-16 13:46:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:46:54] {3072} INFO -  at 44.8s,	estimator xgboost's best error=0.0519,	best estimator xgboost's best error=0.0519
[flaml.automl: 09-16 13:46:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:47:09] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.0497,	best estimator xgboost's best error=0.0497
[flaml.automl: 09-16 13:47:25] {3335} INFO - retrain xgboost for 15.8s
[flaml.automl: 09-16 13:47:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 13:47:25] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:47:25] {2637} INFO - Time taken to find the best model: 59.30793118476868
[flaml.automl: 09-16 13:47:25] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 25, 'max_leaves': 11, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}
CO(0)最佳损失：0.950283918801016
CO(0)最好结果：{'pred_time': 3.348928090863983e-05, 'wall_clock_time': 59.30793118476868, 'metric_for_logging': {'pred_time': 3.348928090863983e-05}, 'val_loss': 0.04971608119898395, 'training_iteration': 1, 'config': {'n_estimators': 25, 'max_leaves': 11, 'min_child_weight': 0.006967902231194494, 'learning_rate': 0.5408133424638543, 'subsample': 0.7520708370056695, 'colsample_bylevel': 0.948506134666318, 'colsample_bytree': 0.8223167301060249, 'reg_alpha': 0.0038027739156058313, 'reg_lambda': 0.5403518701157697}, 'config/n_estimators': 25, 'config/max_leaves': 11, 'config/min_child_weight': 0.006967902231194494, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7520708370056695, 'config/colsample_bylevel': 0.948506134666318, 'config/colsample_bytree': 0.8223167301060249, 'config/reg_alpha': 0.0038027739156058313, 'config/reg_lambda': 0.5403518701157697, 'experiment_tag': 'exp', 'time_total_s': 14.546568393707275}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.948506134666318, colsample_bynode=1,
             colsample_bytree=0.8223167301060249, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.006967902231194494, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0038027739156058313, reg_lambda=0.5403518701157697,
             scale_pos_weight=1, subsample=0.7520708370056695,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8943325381496703
CO(0)的mse=0.004543410672459361
CO(0)的mae=0.04528219260639614
CO(0)的mar=0.08204971764680678
总共花费的时间为：75.34
杭州市
1227A
1231A
1232A
3557A
3558A
3656A
[flaml.automl: 09-16 14:05:52] {2390} INFO - task = regression
[flaml.automl: 09-16 14:05:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:05:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:05:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:05:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:05:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:05:55] {3025} INFO - Estimated sufficient time budget=198349s. Estimated necessary time budget=198s.
[flaml.automl: 09-16 14:05:55] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1359,	best estimator xgboost's best error=0.1359
[flaml.automl: 09-16 14:05:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:05:59] {3072} INFO -  at 7.6s,	estimator xgboost's best error=0.0924,	best estimator xgboost's best error=0.0924
[flaml.automl: 09-16 14:05:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:06:02] {3072} INFO -  at 10.3s,	estimator xgboost's best error=0.0924,	best estimator xgboost's best error=0.0924
[flaml.automl: 09-16 14:06:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:06:05] {3072} INFO -  at 13.2s,	estimator xgboost's best error=0.0924,	best estimator xgboost's best error=0.0924
[flaml.automl: 09-16 14:06:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:06:08] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.0538,	best estimator xgboost's best error=0.0538
[flaml.automl: 09-16 14:06:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:06:10] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-16 14:06:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:06:13] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-16 14:06:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:06:15] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-16 14:06:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:06:17] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-16 14:06:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:06:19] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-16 14:06:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:06:23] {3072} INFO -  at 30.8s,	estimator xgboost's best error=0.0435,	best estimator xgboost's best error=0.0435
[flaml.automl: 09-16 14:06:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:06:25] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0435,	best estimator xgboost's best error=0.0435
[flaml.automl: 09-16 14:06:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:06:36] {3072} INFO -  at 44.6s,	estimator xgboost's best error=0.0378,	best estimator xgboost's best error=0.0378
[flaml.automl: 09-16 14:06:36] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:06:51] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0376,	best estimator xgboost's best error=0.0376
[flaml.automl: 09-16 14:07:10] {3335} INFO - retrain xgboost for 18.8s
[flaml.automl: 09-16 14:07:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:07:10] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:07:10] {2637} INFO - Time taken to find the best model: 59.073975801467896
[flaml.automl: 09-16 14:07:10] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65943}
CO(0)最佳损失：0.9623704676471759
CO(0)最好结果：{'pred_time': 1.1681892121165189e-05, 'wall_clock_time': 59.073975801467896, 'metric_for_logging': {'pred_time': 1.1681892121165189e-05}, 'val_loss': 0.03762953235282406, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65943}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 65943, 'experiment_tag': 'exp', 'time_total_s': 14.446771144866943}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9093936068816066
CO(0)的mse=0.0029618284720875176
CO(0)的mae=0.03666117369547373
CO(0)的mar=0.0554735214739364
总共花费的时间为：79.18
宁波市
1235A
1236A
1239A
1240A
2871A
3710A
[flaml.automl: 09-16 14:25:57] {2390} INFO - task = regression
[flaml.automl: 09-16 14:25:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:25:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:25:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:25:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:25:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:26:00] {3025} INFO - Estimated sufficient time budget=133240s. Estimated necessary time budget=133s.
[flaml.automl: 09-16 14:26:00] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1202,	best estimator xgboost's best error=0.1202
[flaml.automl: 09-16 14:26:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:26:03] {3072} INFO -  at 5.7s,	estimator xgboost's best error=0.0708,	best estimator xgboost's best error=0.0708
[flaml.automl: 09-16 14:26:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:26:05] {3072} INFO -  at 7.8s,	estimator xgboost's best error=0.0708,	best estimator xgboost's best error=0.0708
[flaml.automl: 09-16 14:26:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:26:09] {3072} INFO -  at 11.7s,	estimator xgboost's best error=0.0708,	best estimator xgboost's best error=0.0708
[flaml.automl: 09-16 14:26:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:26:11] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.0558,	best estimator xgboost's best error=0.0558
[flaml.automl: 09-16 14:26:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:26:14] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0471,	best estimator xgboost's best error=0.0471
[flaml.automl: 09-16 14:26:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:26:16] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0471,	best estimator xgboost's best error=0.0471
[flaml.automl: 09-16 14:26:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:26:19] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0471,	best estimator xgboost's best error=0.0471
[flaml.automl: 09-16 14:26:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:26:21] {3072} INFO -  at 23.7s,	estimator xgboost's best error=0.0471,	best estimator xgboost's best error=0.0471
[flaml.automl: 09-16 14:26:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:26:23] {3072} INFO -  at 26.3s,	estimator xgboost's best error=0.0471,	best estimator xgboost's best error=0.0471
[flaml.automl: 09-16 14:26:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:26:27] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.0471,	best estimator xgboost's best error=0.0471
[flaml.automl: 09-16 14:26:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:26:29] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.0471,	best estimator xgboost's best error=0.0471
[flaml.automl: 09-16 14:26:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:26:38] {3072} INFO -  at 41.0s,	estimator xgboost's best error=0.0422,	best estimator xgboost's best error=0.0422
[flaml.automl: 09-16 14:26:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:26:50] {3072} INFO -  at 53.1s,	estimator xgboost's best error=0.0407,	best estimator xgboost's best error=0.0407
[flaml.automl: 09-16 14:27:02] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 14:27:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:27:02] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:27:02] {2637} INFO - Time taken to find the best model: 53.14079713821411
[flaml.automl: 09-16 14:27:02] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65469}
CO(0)最佳损失：0.9592974143265203
CO(0)最好结果：{'pred_time': 5.502241993278163e-06, 'wall_clock_time': 53.14079713821411, 'metric_for_logging': {'pred_time': 5.502241993278163e-06}, 'val_loss': 0.04070258567347969, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65469}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 65469, 'experiment_tag': 'exp', 'time_total_s': 12.155121088027954}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8875797384794915
CO(0)的mse=0.004223140075460752
CO(0)的mae=0.04100236759177828
CO(0)的mar=0.07148623523467239
总共花费的时间为：66.31
温州市
1242A
1243A
1244A
[flaml.automl: 09-16 14:37:08] {2390} INFO - task = regression
[flaml.automl: 09-16 14:37:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:37:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:37:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:37:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:37:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:37:10] {3025} INFO - Estimated sufficient time budget=22422s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 14:37:10] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.0921,	best estimator xgboost's best error=0.0921
[flaml.automl: 09-16 14:37:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:37:14] {3072} INFO -  at 6.6s,	estimator xgboost's best error=0.0569,	best estimator xgboost's best error=0.0569
[flaml.automl: 09-16 14:37:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:37:16] {3072} INFO -  at 8.8s,	estimator xgboost's best error=0.0569,	best estimator xgboost's best error=0.0569
[flaml.automl: 09-16 14:37:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:37:35] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.0569,	best estimator xgboost's best error=0.0569
[flaml.automl: 09-16 14:37:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:37:37] {3072} INFO -  at 29.3s,	estimator xgboost's best error=0.0463,	best estimator xgboost's best error=0.0463
[flaml.automl: 09-16 14:37:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:37:40] {3072} INFO -  at 32.3s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-16 14:37:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:37:43] {3072} INFO -  at 35.4s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-16 14:37:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:37:47] {3072} INFO -  at 40.0s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-16 14:37:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:37:49] {3072} INFO -  at 42.1s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-16 14:37:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:37:54] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-16 14:37:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:37:57] {3072} INFO -  at 49.3s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-16 14:37:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:37:59] {3072} INFO -  at 51.4s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-16 14:37:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:38:06] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0357,	best estimator xgboost's best error=0.0357
[flaml.automl: 09-16 14:38:18] {3335} INFO - retrain xgboost for 11.8s
[flaml.automl: 09-16 14:38:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 14:38:18] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:38:18] {2637} INFO - Time taken to find the best model: 59.05715823173523
[flaml.automl: 09-16 14:38:18] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9642879822862086
CO(0)最好结果：{'pred_time': 2.241024174330247e-05, 'wall_clock_time': 59.05715823173523, 'metric_for_logging': {'pred_time': 2.241024174330247e-05}, 'val_loss': 0.03571201771379135, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 7.62252402305603}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8934771623396495
CO(0)的mse=0.002680261650589964
CO(0)的mae=0.035760265446650966
CO(0)的mar=0.07129764835482687
总共花费的时间为：71.87
绍兴市
2921A
3408A
3409A
3410A
3411A
3560A
3658A
[flaml.automl: 09-16 14:59:53] {2390} INFO - task = regression
[flaml.automl: 09-16 14:59:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:59:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:59:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:59:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:59:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:59:54] {3025} INFO - Estimated sufficient time budget=99297s. Estimated necessary time budget=99s.
[flaml.automl: 09-16 14:59:54] {3072} INFO -  at 1.7s,	estimator xgboost's best error=0.1171,	best estimator xgboost's best error=0.1171
[flaml.automl: 09-16 14:59:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:59:56] {3072} INFO -  at 3.8s,	estimator xgboost's best error=0.0647,	best estimator xgboost's best error=0.0647
[flaml.automl: 09-16 14:59:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:59:57] {3072} INFO -  at 5.0s,	estimator xgboost's best error=0.0647,	best estimator xgboost's best error=0.0647
[flaml.automl: 09-16 14:59:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:00:01] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.0647,	best estimator xgboost's best error=0.0647
[flaml.automl: 09-16 15:00:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:00:02] {3072} INFO -  at 9.4s,	estimator xgboost's best error=0.0474,	best estimator xgboost's best error=0.0474
[flaml.automl: 09-16 15:00:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:00:03] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 15:00:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:00:05] {3072} INFO -  at 12.6s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 15:00:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:00:08] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 15:00:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:00:09] {3072} INFO -  at 16.2s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 15:00:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:00:11] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 15:00:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:00:13] {3072} INFO -  at 20.5s,	estimator xgboost's best error=0.0375,	best estimator xgboost's best error=0.0375
[flaml.automl: 09-16 15:00:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:00:14] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0375,	best estimator xgboost's best error=0.0375
[flaml.automl: 09-16 15:00:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:00:21] {3072} INFO -  at 28.2s,	estimator xgboost's best error=0.0320,	best estimator xgboost's best error=0.0320
[flaml.automl: 09-16 15:00:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:00:33] {3072} INFO -  at 40.2s,	estimator xgboost's best error=0.0309,	best estimator xgboost's best error=0.0309
[flaml.automl: 09-16 15:00:33] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:00:39] {3072} INFO -  at 46.7s,	estimator xgboost's best error=0.0309,	best estimator xgboost's best error=0.0309
[flaml.automl: 09-16 15:00:39] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 15:00:52] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.0309,	best estimator xgboost's best error=0.0309
[flaml.automl: 09-16 15:01:13] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-16 15:01:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:01:13] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:01:13] {2637} INFO - Time taken to find the best model: 59.263633728027344
[flaml.automl: 09-16 15:01:13] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 76962}
CO(0)最佳损失：0.9691120589187594
CO(0)最好结果：{'pred_time': 5.013676198204619e-06, 'wall_clock_time': 59.263633728027344, 'metric_for_logging': {'pred_time': 5.013676198204619e-06}, 'val_loss': 0.030887941081240645, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 76962}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 76962, 'experiment_tag': 'exp', 'time_total_s': 12.541930198669434}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9330574226712316
CO(0)的mse=0.0021815341056232555
CO(0)的mae=0.030994023635534957
CO(0)的mar=0.05151311267483023
总共花费的时间为：81.97
湖州市
1250A
3562A
[flaml.automl: 09-16 15:08:01] {2390} INFO - task = regression
[flaml.automl: 09-16 15:08:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:08:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:08:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:08:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:08:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:08:02] {3025} INFO - Estimated sufficient time budget=11896s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:08:02] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1106,	best estimator xgboost's best error=0.1106
[flaml.automl: 09-16 15:08:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:08:04] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-16 15:08:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:08:05] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-16 15:08:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:08:15] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-16 15:08:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:08:16] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.0536,	best estimator xgboost's best error=0.0536
[flaml.automl: 09-16 15:08:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:08:17] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.0440,	best estimator xgboost's best error=0.0440
[flaml.automl: 09-16 15:08:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:08:19] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0440,	best estimator xgboost's best error=0.0440
[flaml.automl: 09-16 15:08:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:08:22] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0440,	best estimator xgboost's best error=0.0440
[flaml.automl: 09-16 15:08:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:08:23] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0440,	best estimator xgboost's best error=0.0440
[flaml.automl: 09-16 15:08:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:08:25] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.0440,	best estimator xgboost's best error=0.0440
[flaml.automl: 09-16 15:08:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:08:26] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0440,	best estimator xgboost's best error=0.0440
[flaml.automl: 09-16 15:08:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:08:27] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.0440,	best estimator xgboost's best error=0.0440
[flaml.automl: 09-16 15:08:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:08:34] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-16 15:08:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:08:44] {3072} INFO -  at 43.3s,	estimator xgboost's best error=0.0389,	best estimator xgboost's best error=0.0389
[flaml.automl: 09-16 15:08:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:08:50] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.0389,	best estimator xgboost's best error=0.0389
[flaml.automl: 09-16 15:08:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 15:09:01] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.0389,	best estimator xgboost's best error=0.0389
[flaml.automl: 09-16 15:09:11] {3335} INFO - retrain xgboost for 10.2s
[flaml.automl: 09-16 15:09:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:09:11] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:09:11] {2637} INFO - Time taken to find the best model: 43.3278272151947
[flaml.automl: 09-16 15:09:11] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9610824811776936
CO(0)最好结果：{'pred_time': 1.8791968252390453e-05, 'wall_clock_time': 43.3278272151947, 'metric_for_logging': {'pred_time': 1.8791968252390453e-05}, 'val_loss': 0.03891751882230638, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.445396900177002}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9076411996658491
CO(0)的mse=0.003211823230744566
CO(0)的mae=0.03872108779058477
CO(0)的mar=0.07217011806582985
总共花费的时间为：70.53
嘉兴市
1253A
3407A
[flaml.automl: 09-16 15:16:00] {2390} INFO - task = regression
[flaml.automl: 09-16 15:16:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:16:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:16:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:16:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:16:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:16:02] {3025} INFO - Estimated sufficient time budget=12080s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:16:02] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1244,	best estimator xgboost's best error=0.1244
[flaml.automl: 09-16 15:16:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:16:04] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-16 15:16:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:16:05] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-16 15:16:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:16:14] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-16 15:16:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:16:16] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0542,	best estimator xgboost's best error=0.0542
[flaml.automl: 09-16 15:16:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:16:17] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-16 15:16:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:16:19] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-16 15:16:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:16:21] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-16 15:16:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:16:22] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-16 15:16:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:16:25] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-16 15:16:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:16:26] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-16 15:16:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:16:27] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-16 15:16:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:16:33] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.0390,	best estimator xgboost's best error=0.0390
[flaml.automl: 09-16 15:16:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:16:44] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0377,	best estimator xgboost's best error=0.0377
[flaml.automl: 09-16 15:16:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:16:50] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.0377,	best estimator xgboost's best error=0.0377
[flaml.automl: 09-16 15:16:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 15:16:59] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0377,	best estimator xgboost's best error=0.0377
[flaml.automl: 09-16 15:17:10] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-16 15:17:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:17:10] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:17:10] {2637} INFO - Time taken to find the best model: 43.375728607177734
[flaml.automl: 09-16 15:17:10] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.96228034858793
CO(0)最好结果：{'pred_time': 1.666374874969955e-05, 'wall_clock_time': 43.375728607177734, 'metric_for_logging': {'pred_time': 1.666374874969955e-05}, 'val_loss': 0.037719651412070045, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.455370426177979}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9155337367394625
CO(0)的mse=0.002945367908058522
CO(0)的mae=0.03734224596184488
CO(0)的mar=0.06460771381417371
总共花费的时间为：69.92
台州市
1256A
1257A
3564A
[flaml.automl: 09-16 15:26:43] {2390} INFO - task = regression
[flaml.automl: 09-16 15:26:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:26:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:26:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:26:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:26:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:26:44] {3025} INFO - Estimated sufficient time budget=12114s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:26:44] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.0995,	best estimator xgboost's best error=0.0995
[flaml.automl: 09-16 15:26:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:26:46] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0596,	best estimator xgboost's best error=0.0596
[flaml.automl: 09-16 15:26:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:26:47] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0596,	best estimator xgboost's best error=0.0596
[flaml.automl: 09-16 15:26:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:26:57] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0596,	best estimator xgboost's best error=0.0596
[flaml.automl: 09-16 15:26:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:26:58] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-16 15:26:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:27:00] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0392,	best estimator xgboost's best error=0.0392
[flaml.automl: 09-16 15:27:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:27:02] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0392,	best estimator xgboost's best error=0.0392
[flaml.automl: 09-16 15:27:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:27:04] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0392,	best estimator xgboost's best error=0.0392
[flaml.automl: 09-16 15:27:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:27:05] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0392,	best estimator xgboost's best error=0.0392
[flaml.automl: 09-16 15:27:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:27:08] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0392,	best estimator xgboost's best error=0.0392
[flaml.automl: 09-16 15:27:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:27:09] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0392,	best estimator xgboost's best error=0.0392
[flaml.automl: 09-16 15:27:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:27:10] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0392,	best estimator xgboost's best error=0.0392
[flaml.automl: 09-16 15:27:10] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:27:17] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0343,	best estimator xgboost's best error=0.0343
[flaml.automl: 09-16 15:27:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:27:29] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0326,	best estimator xgboost's best error=0.0326
[flaml.automl: 09-16 15:27:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:27:39] {3072} INFO -  at 56.9s,	estimator xgboost's best error=0.0326,	best estimator xgboost's best error=0.0326
[flaml.automl: 09-16 15:28:02] {3335} INFO - retrain xgboost for 22.4s
[flaml.automl: 09-16 15:28:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:28:02] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:28:02] {2637} INFO - Time taken to find the best model: 46.334134340286255
[flaml.automl: 09-16 15:28:02] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9673957128297576
CO(0)最好结果：{'pred_time': 1.1521999973529735e-05, 'wall_clock_time': 46.334134340286255, 'metric_for_logging': {'pred_time': 1.1521999973529735e-05}, 'val_loss': 0.032604287170242476, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.04179334640503}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9021329235960714
CO(0)的mse=0.0023505090689796993
CO(0)的mae=0.033170343110645845
CO(0)的mar=0.10640227888959429
总共花费的时间为：79.91
舟山市
1258A
1259A
[flaml.automl: 09-16 15:34:34] {2390} INFO - task = regression
[flaml.automl: 09-16 15:34:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:34:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:34:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:34:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:34:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:34:35] {3025} INFO - Estimated sufficient time budget=12107s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:34:35] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.0887,	best estimator xgboost's best error=0.0887
[flaml.automl: 09-16 15:34:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:34:37] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0493,	best estimator xgboost's best error=0.0493
[flaml.automl: 09-16 15:34:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:34:39] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.0493,	best estimator xgboost's best error=0.0493
[flaml.automl: 09-16 15:34:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:34:48] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0493,	best estimator xgboost's best error=0.0493
[flaml.automl: 09-16 15:34:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:34:49] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.0351,	best estimator xgboost's best error=0.0351
[flaml.automl: 09-16 15:34:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:34:51] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-16 15:34:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:34:52] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-16 15:34:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:34:55] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-16 15:34:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:34:56] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-16 15:34:56] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:34:58] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-16 15:34:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:35:00] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-16 15:35:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:35:01] {3072} INFO -  at 26.8s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-16 15:35:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:35:07] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.0237,	best estimator xgboost's best error=0.0237
[flaml.automl: 09-16 15:35:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:35:17] {3072} INFO -  at 43.2s,	estimator xgboost's best error=0.0227,	best estimator xgboost's best error=0.0227
[flaml.automl: 09-16 15:35:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:35:23] {3072} INFO -  at 49.2s,	estimator xgboost's best error=0.0227,	best estimator xgboost's best error=0.0227
[flaml.automl: 09-16 15:35:23] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 15:35:34] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.0222,	best estimator xgboost's best error=0.0222
[flaml.automl: 09-16 15:36:01] {3335} INFO - retrain xgboost for 27.2s
[flaml.automl: 09-16 15:36:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:36:01] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:36:01] {2637} INFO - Time taken to find the best model: 59.70122003555298
[flaml.automl: 09-16 15:36:01] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.9778016576716094
CO(0)最好结果：{'pred_time': 1.7768298694233352e-05, 'wall_clock_time': 59.70122003555298, 'metric_for_logging': {'pred_time': 1.7768298694233352e-05}, 'val_loss': 0.022198342328390526, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 10.49057149887085}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9327934730053764
CO(0)的mse=0.0012305085662421083
CO(0)的mae=0.02114314691488444
CO(0)的mar=0.045701160540503875
总共花费的时间为：87.32
金华市
金华市没有数据
衢州市
1264A
1265A
[flaml.automl: 09-16 15:43:00] {2390} INFO - task = regression
[flaml.automl: 09-16 15:43:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:43:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:43:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:43:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:43:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:43:02] {3025} INFO - Estimated sufficient time budget=12040s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:43:02] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1197,	best estimator xgboost's best error=0.1197
[flaml.automl: 09-16 15:43:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:43:04] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0695,	best estimator xgboost's best error=0.0695
[flaml.automl: 09-16 15:43:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:43:05] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.0695,	best estimator xgboost's best error=0.0695
[flaml.automl: 09-16 15:43:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:43:14] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0695,	best estimator xgboost's best error=0.0695
[flaml.automl: 09-16 15:43:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:43:16] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0545,	best estimator xgboost's best error=0.0545
[flaml.automl: 09-16 15:43:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:43:17] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-16 15:43:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:43:19] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-16 15:43:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:43:21] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-16 15:43:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:43:22] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-16 15:43:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:43:25] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-16 15:43:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:43:26] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-16 15:43:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:43:27] {3072} INFO -  at 26.8s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-16 15:43:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:43:33] {3072} INFO -  at 32.8s,	estimator xgboost's best error=0.0402,	best estimator xgboost's best error=0.0402
[flaml.automl: 09-16 15:43:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:43:44] {3072} INFO -  at 43.2s,	estimator xgboost's best error=0.0383,	best estimator xgboost's best error=0.0383
[flaml.automl: 09-16 15:43:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 15:43:50] {3072} INFO -  at 49.2s,	estimator xgboost's best error=0.0383,	best estimator xgboost's best error=0.0383
[flaml.automl: 09-16 15:43:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 15:43:59] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.0383,	best estimator xgboost's best error=0.0383
[flaml.automl: 09-16 15:44:10] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-16 15:44:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:44:10] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:44:10] {2637} INFO - Time taken to find the best model: 43.169360399246216
[flaml.automl: 09-16 15:44:10] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9616894151197178
CO(0)最好结果：{'pred_time': 1.8452197880267702e-05, 'wall_clock_time': 43.169360399246216, 'metric_for_logging': {'pred_time': 1.8452197880267702e-05}, 'val_loss': 0.03831058488028221, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.412781476974487}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8842900798694823
CO(0)的mse=0.003942699514261787
CO(0)的mae=0.03826171729735029
CO(0)的mar=0.06283740654289724
总共花费的时间为：69.79
丽水市
1267A
[flaml.automl: 09-16 15:47:33] {2390} INFO - task = regression
[flaml.automl: 09-16 15:47:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:47:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:47:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:47:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:47:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:47:35] {3025} INFO - Estimated sufficient time budget=22432s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 15:47:35] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.0981,	best estimator xgboost's best error=0.0981
[flaml.automl: 09-16 15:47:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:47:39] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.0672,	best estimator xgboost's best error=0.0672
[flaml.automl: 09-16 15:47:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:47:41] {3072} INFO -  at 8.0s,	estimator xgboost's best error=0.0672,	best estimator xgboost's best error=0.0672
[flaml.automl: 09-16 15:47:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:47:54] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.0672,	best estimator xgboost's best error=0.0672
[flaml.automl: 09-16 15:47:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:47:56] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0482,	best estimator xgboost's best error=0.0482
[flaml.automl: 09-16 15:47:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:47:59] {3072} INFO -  at 26.3s,	estimator xgboost's best error=0.0417,	best estimator xgboost's best error=0.0417
[flaml.automl: 09-16 15:47:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:48:02] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-16 15:48:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:48:05] {3072} INFO -  at 32.1s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-16 15:48:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:48:06] {3072} INFO -  at 33.7s,	estimator xgboost's best error=0.0413,	best estimator xgboost's best error=0.0413
[flaml.automl: 09-16 15:48:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:48:09] {3072} INFO -  at 36.3s,	estimator xgboost's best error=0.0413,	best estimator xgboost's best error=0.0413
[flaml.automl: 09-16 15:48:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:48:11] {3072} INFO -  at 38.0s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-16 15:48:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:48:12] {3072} INFO -  at 39.1s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-16 15:48:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:48:18] {3072} INFO -  at 44.9s,	estimator xgboost's best error=0.0329,	best estimator xgboost's best error=0.0329
[flaml.automl: 09-16 15:48:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:48:27] {3072} INFO -  at 54.2s,	estimator xgboost's best error=0.0322,	best estimator xgboost's best error=0.0322
[flaml.automl: 09-16 15:48:36] {3335} INFO - retrain xgboost for 9.2s
[flaml.automl: 09-16 15:48:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 15:48:36] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:48:36] {2637} INFO - Time taken to find the best model: 54.22645974159241
[flaml.automl: 09-16 15:48:36] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}
CO(0)最佳损失：0.9678132157382973
CO(0)最好结果：{'pred_time': 3.234441926364883e-05, 'wall_clock_time': 54.22645974159241, 'metric_for_logging': {'pred_time': 3.234441926364883e-05}, 'val_loss': 0.03218678426170272, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}, 'config/n_estimators': 26, 'config/max_leaves': 6, 'config/min_child_weight': 2.978767810916454, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532448312341252, 'config/reg_alpha': 0.0015779591896894877, 'config/reg_lambda': 1.3500086298278042, 'experiment_tag': 'exp', 'time_total_s': 9.278369188308716}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9160807632232619
CO(0)的mse=0.002002419145347435
CO(0)的mae=0.031119960184368023
CO(0)的mar=0.07916104414513189
总共花费的时间为：63.80
合肥市
1273A
1274A
1275A
1277A
1278A
1279A
3464A
[flaml.automl: 09-16 16:09:50] {2390} INFO - task = regression
[flaml.automl: 09-16 16:09:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:09:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:09:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:09:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:09:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:09:51] {3025} INFO - Estimated sufficient time budget=88686s. Estimated necessary time budget=89s.
[flaml.automl: 09-16 16:09:51] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.1304,	best estimator xgboost's best error=0.1304
[flaml.automl: 09-16 16:09:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:09:53] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.0745,	best estimator xgboost's best error=0.0745
[flaml.automl: 09-16 16:09:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:09:55] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.0745,	best estimator xgboost's best error=0.0745
[flaml.automl: 09-16 16:09:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:09:58] {3072} INFO -  at 8.1s,	estimator xgboost's best error=0.0745,	best estimator xgboost's best error=0.0745
[flaml.automl: 09-16 16:09:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:09:59] {3072} INFO -  at 9.3s,	estimator xgboost's best error=0.0583,	best estimator xgboost's best error=0.0583
[flaml.automl: 09-16 16:09:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:10:00] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0484,	best estimator xgboost's best error=0.0484
[flaml.automl: 09-16 16:10:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:10:02] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0484,	best estimator xgboost's best error=0.0484
[flaml.automl: 09-16 16:10:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:10:04] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0484,	best estimator xgboost's best error=0.0484
[flaml.automl: 09-16 16:10:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:10:06] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.0484,	best estimator xgboost's best error=0.0484
[flaml.automl: 09-16 16:10:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:10:08] {3072} INFO -  at 18.7s,	estimator xgboost's best error=0.0484,	best estimator xgboost's best error=0.0484
[flaml.automl: 09-16 16:10:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:10:10] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0480,	best estimator xgboost's best error=0.0480
[flaml.automl: 09-16 16:10:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:10:11] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0480,	best estimator xgboost's best error=0.0480
[flaml.automl: 09-16 16:10:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:10:18] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.0423,	best estimator xgboost's best error=0.0423
[flaml.automl: 09-16 16:10:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:10:30] {3072} INFO -  at 40.1s,	estimator xgboost's best error=0.0404,	best estimator xgboost's best error=0.0404
[flaml.automl: 09-16 16:10:30] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 16:10:36] {3072} INFO -  at 46.7s,	estimator xgboost's best error=0.0404,	best estimator xgboost's best error=0.0404
[flaml.automl: 09-16 16:10:36] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 16:10:49] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0404,	best estimator xgboost's best error=0.0404
[flaml.automl: 09-16 16:11:10] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-16 16:11:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 16:11:10] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:11:10] {2637} INFO - Time taken to find the best model: 59.20477819442749
[flaml.automl: 09-16 16:11:10] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 74021}
CO(0)最佳损失：0.9596009698151238
CO(0)最好结果：{'pred_time': 5.199119312784954e-06, 'wall_clock_time': 59.20477819442749, 'metric_for_logging': {'pred_time': 5.199119312784954e-06}, 'val_loss': 0.04039903018487623, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 74021}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 74021, 'experiment_tag': 'exp', 'time_total_s': 12.552286386489868}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9237033157194297
CO(0)的mse=0.003433599584218614
CO(0)的mae=0.039735100228998566
CO(0)的mar=0.07118409443184154
总共花费的时间为：81.89
福州市
1280A
1285A
3048A
3526A
[flaml.automl: 09-16 16:23:36] {2390} INFO - task = regression
[flaml.automl: 09-16 16:23:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:23:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:23:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:23:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:23:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:23:37] {3025} INFO - Estimated sufficient time budget=49793s. Estimated necessary time budget=50s.
[flaml.automl: 09-16 16:23:37] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1046,	best estimator xgboost's best error=0.1046
[flaml.automl: 09-16 16:23:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:23:39] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0578,	best estimator xgboost's best error=0.0578
[flaml.automl: 09-16 16:23:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:23:41] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0578,	best estimator xgboost's best error=0.0578
[flaml.automl: 09-16 16:23:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:23:47] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0578,	best estimator xgboost's best error=0.0578
[flaml.automl: 09-16 16:23:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:23:48] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0430,	best estimator xgboost's best error=0.0430
[flaml.automl: 09-16 16:23:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:23:50] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0347,	best estimator xgboost's best error=0.0347
[flaml.automl: 09-16 16:23:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:23:51] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.0347,	best estimator xgboost's best error=0.0347
[flaml.automl: 09-16 16:23:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:23:54] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0347,	best estimator xgboost's best error=0.0347
[flaml.automl: 09-16 16:23:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:23:55] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0347,	best estimator xgboost's best error=0.0347
[flaml.automl: 09-16 16:23:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:23:58] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0347,	best estimator xgboost's best error=0.0347
[flaml.automl: 09-16 16:23:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:23:59] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0347,	best estimator xgboost's best error=0.0347
[flaml.automl: 09-16 16:23:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:24:00] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0347,	best estimator xgboost's best error=0.0347
[flaml.automl: 09-16 16:24:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:24:07] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0294,	best estimator xgboost's best error=0.0294
[flaml.automl: 09-16 16:24:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:24:19] {3072} INFO -  at 43.2s,	estimator xgboost's best error=0.0284,	best estimator xgboost's best error=0.0284
[flaml.automl: 09-16 16:24:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 16:24:26] {3072} INFO -  at 49.7s,	estimator xgboost's best error=0.0284,	best estimator xgboost's best error=0.0284
[flaml.automl: 09-16 16:24:38] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 16:24:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 16:24:38] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:24:38] {2637} INFO - Time taken to find the best model: 43.153719663619995
[flaml.automl: 09-16 16:24:38] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41752}
CO(0)最佳损失：0.9716485984628314
CO(0)最好结果：{'pred_time': 1.0265204413183804e-05, 'wall_clock_time': 43.153719663619995, 'metric_for_logging': {'pred_time': 1.0265204413183804e-05}, 'val_loss': 0.02835140153716864, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41752}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41752, 'experiment_tag': 'exp', 'time_total_s': 12.133768320083618}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9365553244892773
CO(0)的mse=0.0018817865511765992
CO(0)的mae=0.02893404287725369
CO(0)的mar=0.05291899740479062
总共花费的时间为：62.58
厦门市
1286A
3527A
3528A
[flaml.automl: 09-16 16:33:49] {2390} INFO - task = regression
[flaml.automl: 09-16 16:33:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:33:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:33:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:33:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:33:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:33:50] {3025} INFO - Estimated sufficient time budget=12043s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 16:33:50] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.0915,	best estimator xgboost's best error=0.0915
[flaml.automl: 09-16 16:33:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:33:52] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0510,	best estimator xgboost's best error=0.0510
[flaml.automl: 09-16 16:33:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:33:53] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0510,	best estimator xgboost's best error=0.0510
[flaml.automl: 09-16 16:33:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:34:03] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0510,	best estimator xgboost's best error=0.0510
[flaml.automl: 09-16 16:34:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:34:05] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0369,	best estimator xgboost's best error=0.0369
[flaml.automl: 09-16 16:34:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:34:06] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0300,	best estimator xgboost's best error=0.0300
[flaml.automl: 09-16 16:34:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:34:08] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0300,	best estimator xgboost's best error=0.0300
[flaml.automl: 09-16 16:34:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:34:10] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0300,	best estimator xgboost's best error=0.0300
[flaml.automl: 09-16 16:34:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:34:11] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0300,	best estimator xgboost's best error=0.0300
[flaml.automl: 09-16 16:34:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:34:14] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0300,	best estimator xgboost's best error=0.0300
[flaml.automl: 09-16 16:34:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:34:15] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0300,	best estimator xgboost's best error=0.0300
[flaml.automl: 09-16 16:34:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:34:16] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0300,	best estimator xgboost's best error=0.0300
[flaml.automl: 09-16 16:34:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:34:23] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0249,	best estimator xgboost's best error=0.0249
[flaml.automl: 09-16 16:34:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:34:35] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0241,	best estimator xgboost's best error=0.0241
[flaml.automl: 09-16 16:34:35] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 16:34:41] {3072} INFO -  at 52.7s,	estimator xgboost's best error=0.0241,	best estimator xgboost's best error=0.0241
[flaml.automl: 09-16 16:34:53] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 16:34:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 16:34:53] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:34:53] {2637} INFO - Time taken to find the best model: 46.27741193771362
[flaml.automl: 09-16 16:34:53] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9758624904237364
CO(0)最好结果：{'pred_time': 1.2916629367885757e-05, 'wall_clock_time': 46.27741193771362, 'metric_for_logging': {'pred_time': 1.2916629367885757e-05}, 'val_loss': 0.024137509576263597, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.991969347000122}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.939439516500012
CO(0)的mse=0.0012762362469714791
CO(0)的mae=0.023749627711223305
CO(0)的mar=0.058423026808759715
总共花费的时间为：65.29
南昌市
1295A
1296A
1298A
3690A
[flaml.automl: 09-16 16:47:24] {2390} INFO - task = regression
[flaml.automl: 09-16 16:47:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:47:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:47:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:47:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:47:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:47:25] {3025} INFO - Estimated sufficient time budget=51057s. Estimated necessary time budget=51s.
[flaml.automl: 09-16 16:47:25] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1676,	best estimator xgboost's best error=0.1676
[flaml.automl: 09-16 16:47:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:47:27] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0945,	best estimator xgboost's best error=0.0945
[flaml.automl: 09-16 16:47:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:47:28] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0945,	best estimator xgboost's best error=0.0945
[flaml.automl: 09-16 16:47:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:47:34] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0945,	best estimator xgboost's best error=0.0945
[flaml.automl: 09-16 16:47:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:47:36] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0677,	best estimator xgboost's best error=0.0677
[flaml.automl: 09-16 16:47:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:47:37] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 16:47:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:47:39] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 16:47:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:47:41] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 16:47:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:47:42] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 16:47:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:47:45] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 16:47:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:47:47] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 16:47:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:47:48] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 16:47:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:47:54] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-16 16:47:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:48:06] {3072} INFO -  at 43.1s,	estimator xgboost's best error=0.0493,	best estimator xgboost's best error=0.0493
[flaml.automl: 09-16 16:48:06] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 16:48:13] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0493,	best estimator xgboost's best error=0.0493
[flaml.automl: 09-16 16:48:25] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 16:48:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 16:48:25] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:48:25] {2637} INFO - Time taken to find the best model: 43.075891733169556
[flaml.automl: 09-16 16:48:25] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41553}
CO(0)最佳损失：0.9507045731683662
CO(0)最好结果：{'pred_time': 9.170465715163349e-06, 'wall_clock_time': 43.075891733169556, 'metric_for_logging': {'pred_time': 9.170465715163349e-06}, 'val_loss': 0.04929542683163375, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41553}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41553, 'experiment_tag': 'exp', 'time_total_s': 12.109834432601929}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8804401365243697
CO(0)的mse=0.00674564667303715
CO(0)的mae=0.051628631929161485
CO(0)的mar=0.07980157527658738
总共花费的时间为：62.34
济南市
1300A
1301A
1305A
1306A
1961A
3064A
3494A
3495A
3682A
[flaml.automl: 09-16 17:16:11] {2390} INFO - task = regression
[flaml.automl: 09-16 17:16:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 17:16:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 17:16:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 17:16:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 17:16:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 17:16:13] {3025} INFO - Estimated sufficient time budget=117376s. Estimated necessary time budget=117s.
[flaml.automl: 09-16 17:16:13] {3072} INFO -  at 1.7s,	estimator xgboost's best error=0.1895,	best estimator xgboost's best error=0.1895
[flaml.automl: 09-16 17:16:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 17:16:15] {3072} INFO -  at 3.9s,	estimator xgboost's best error=0.1138,	best estimator xgboost's best error=0.1138
[flaml.automl: 09-16 17:16:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 17:16:16] {3072} INFO -  at 5.1s,	estimator xgboost's best error=0.1138,	best estimator xgboost's best error=0.1138
[flaml.automl: 09-16 17:16:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 17:16:18] {3072} INFO -  at 7.4s,	estimator xgboost's best error=0.1138,	best estimator xgboost's best error=0.1138
[flaml.automl: 09-16 17:16:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 17:16:19] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.0937,	best estimator xgboost's best error=0.0937
[flaml.automl: 09-16 17:16:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 17:16:21] {3072} INFO -  at 10.1s,	estimator xgboost's best error=0.0796,	best estimator xgboost's best error=0.0796
[flaml.automl: 09-16 17:16:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 17:16:23] {3072} INFO -  at 11.7s,	estimator xgboost's best error=0.0796,	best estimator xgboost's best error=0.0796
[flaml.automl: 09-16 17:16:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 17:16:24] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.0796,	best estimator xgboost's best error=0.0796
[flaml.automl: 09-16 17:16:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 17:16:26] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0796,	best estimator xgboost's best error=0.0796
[flaml.automl: 09-16 17:16:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 17:16:28] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.0796,	best estimator xgboost's best error=0.0796
[flaml.automl: 09-16 17:16:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 17:16:29] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0796,	best estimator xgboost's best error=0.0796
[flaml.automl: 09-16 17:16:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 17:16:31] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0796,	best estimator xgboost's best error=0.0796
[flaml.automl: 09-16 17:16:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 17:16:37] {3072} INFO -  at 26.1s,	estimator xgboost's best error=0.0729,	best estimator xgboost's best error=0.0729
[flaml.automl: 09-16 17:16:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:16:49] {3072} INFO -  at 38.1s,	estimator xgboost's best error=0.0727,	best estimator xgboost's best error=0.0727
[flaml.automl: 09-16 17:16:49] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 17:16:56] {3072} INFO -  at 44.7s,	estimator xgboost's best error=0.0727,	best estimator xgboost's best error=0.0727
[flaml.automl: 09-16 17:16:56] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 17:17:10] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.0717,	best estimator xgboost's best error=0.0717
[flaml.automl: 09-16 17:17:32] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-16 17:17:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 17:17:32] {2636} INFO - fit succeeded
[flaml.automl: 09-16 17:17:32] {2637} INFO - Time taken to find the best model: 59.52681612968445
[flaml.automl: 09-16 17:17:32] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 97512}
CO(0)最佳损失：0.9283213040021745
CO(0)最好结果：{'pred_time': 3.85093865000712e-06, 'wall_clock_time': 59.52681612968445, 'metric_for_logging': {'pred_time': 3.85093865000712e-06}, 'val_loss': 0.07167869599782546, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 97512}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 97512, 'experiment_tag': 'exp', 'time_total_s': 14.832094669342041}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8478458692913946
CO(0)的mse=0.013502309328363115
CO(0)的mae=0.06979391572528211
CO(0)的mar=0.13172946440143551
总共花费的时间为：82.64
青岛市
1307A
1311A
3362A
3642A
3643A
[flaml.automl: 09-16 17:34:29] {2390} INFO - task = regression
[flaml.automl: 09-16 17:34:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 17:34:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 17:34:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 17:34:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 17:34:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 17:34:30] {3025} INFO - Estimated sufficient time budget=63078s. Estimated necessary time budget=63s.
[flaml.automl: 09-16 17:34:30] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1541,	best estimator xgboost's best error=0.1541
[flaml.automl: 09-16 17:34:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 17:34:32] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0887,	best estimator xgboost's best error=0.0887
[flaml.automl: 09-16 17:34:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 17:34:33] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0887,	best estimator xgboost's best error=0.0887
[flaml.automl: 09-16 17:34:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 17:34:38] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0887,	best estimator xgboost's best error=0.0887
[flaml.automl: 09-16 17:34:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 17:34:39] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-16 17:34:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 17:34:41] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 17:34:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 17:34:42] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 17:34:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 17:34:45] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 17:34:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 17:34:46] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 17:34:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 17:34:49] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 17:34:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 17:34:50] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 17:34:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 17:34:52] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-16 17:34:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 17:34:58] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-16 17:34:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:35:10] {3072} INFO -  at 41.6s,	estimator xgboost's best error=0.0493,	best estimator xgboost's best error=0.0493
[flaml.automl: 09-16 17:35:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 17:35:17] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.0493,	best estimator xgboost's best error=0.0493
[flaml.automl: 09-16 17:35:29] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 17:35:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 17:35:29] {2636} INFO - fit succeeded
[flaml.automl: 09-16 17:35:29] {2637} INFO - Time taken to find the best model: 41.587059020996094
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52447}
CO(0)最佳损失：0.9506575450173943
CO(0)最好结果：{'pred_time': 6.981025269036434e-06, 'wall_clock_time': 41.587059020996094, 'metric_for_logging': {'pred_time': 6.981025269036434e-06}, 'val_loss': 0.04934245498260571, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52447}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52447, 'experiment_tag': 'exp', 'time_total_s': 12.070202827453613}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9057217246006561
CO(0)的mse=0.00697978542388434
CO(0)的mae=0.05086578499219592
CO(0)的mar=0.10227294636594757
总共花费的时间为：61.09
郑州市
1318A
1320A
1321A
1323A
1324A
3471A
3590A
3591A
[flaml.automl: 09-16 17:59:18] {2390} INFO - task = regression
[flaml.automl: 09-16 17:59:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 17:59:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 17:59:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 17:59:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 17:59:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 17:59:19] {3025} INFO - Estimated sufficient time budget=99646s. Estimated necessary time budget=100s.
[flaml.automl: 09-16 17:59:19] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.1754,	best estimator xgboost's best error=0.1754
[flaml.automl: 09-16 17:59:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 17:59:21] {3072} INFO -  at 3.8s,	estimator xgboost's best error=0.0969,	best estimator xgboost's best error=0.0969
[flaml.automl: 09-16 17:59:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 17:59:22] {3072} INFO -  at 5.0s,	estimator xgboost's best error=0.0969,	best estimator xgboost's best error=0.0969
[flaml.automl: 09-16 17:59:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 17:59:25] {3072} INFO -  at 7.7s,	estimator xgboost's best error=0.0969,	best estimator xgboost's best error=0.0969
[flaml.automl: 09-16 17:59:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 17:59:26] {3072} INFO -  at 8.8s,	estimator xgboost's best error=0.0706,	best estimator xgboost's best error=0.0706
[flaml.automl: 09-16 17:59:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 17:59:28] {3072} INFO -  at 10.4s,	estimator xgboost's best error=0.0557,	best estimator xgboost's best error=0.0557
[flaml.automl: 09-16 17:59:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 17:59:29] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.0557,	best estimator xgboost's best error=0.0557
[flaml.automl: 09-16 17:59:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 17:59:32] {3072} INFO -  at 14.5s,	estimator xgboost's best error=0.0557,	best estimator xgboost's best error=0.0557
[flaml.automl: 09-16 17:59:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 17:59:33] {3072} INFO -  at 15.6s,	estimator xgboost's best error=0.0557,	best estimator xgboost's best error=0.0557
[flaml.automl: 09-16 17:59:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 17:59:35] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0557,	best estimator xgboost's best error=0.0557
[flaml.automl: 09-16 17:59:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 17:59:37] {3072} INFO -  at 19.3s,	estimator xgboost's best error=0.0557,	best estimator xgboost's best error=0.0557
[flaml.automl: 09-16 17:59:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 17:59:38] {3072} INFO -  at 20.5s,	estimator xgboost's best error=0.0557,	best estimator xgboost's best error=0.0557
[flaml.automl: 09-16 17:59:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 17:59:44] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.0475,	best estimator xgboost's best error=0.0475
[flaml.automl: 09-16 17:59:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:59:56] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.0463,	best estimator xgboost's best error=0.0463
[flaml.automl: 09-16 17:59:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 18:00:03] {3072} INFO -  at 45.7s,	estimator xgboost's best error=0.0463,	best estimator xgboost's best error=0.0463
[flaml.automl: 09-16 18:00:03] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 18:00:17] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-16 18:00:38] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-16 18:00:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 18:00:38] {2636} INFO - fit succeeded
[flaml.automl: 09-16 18:00:38] {2637} INFO - Time taken to find the best model: 59.37433862686157
[flaml.automl: 09-16 18:00:38] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 83886}
CO(0)最佳损失：0.9538855060746186
CO(0)最好结果：{'pred_time': 4.446950289788976e-06, 'wall_clock_time': 59.37433862686157, 'metric_for_logging': {'pred_time': 4.446950289788976e-06}, 'val_loss': 0.04611449392538139, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 83886}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 83886, 'experiment_tag': 'exp', 'time_total_s': 13.665894746780396}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9300123458879055
CO(0)的mse=0.004501643479820358
CO(0)的mae=0.04398556473770922
CO(0)的mar=0.06797791893120284
总共花费的时间为：82.20
武汉市
1325A
1326A
1327A
1328A
1329A
1331A
3153A
[flaml.automl: 09-16 18:21:44] {2390} INFO - task = regression
[flaml.automl: 09-16 18:21:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 18:21:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 18:21:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 18:21:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 18:21:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 18:21:45] {3025} INFO - Estimated sufficient time budget=91317s. Estimated necessary time budget=91s.
[flaml.automl: 09-16 18:21:45] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.2262,	best estimator xgboost's best error=0.2262
[flaml.automl: 09-16 18:21:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 18:21:47] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.1196,	best estimator xgboost's best error=0.1196
[flaml.automl: 09-16 18:21:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 18:21:48] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.1196,	best estimator xgboost's best error=0.1196
[flaml.automl: 09-16 18:21:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 18:21:52] {3072} INFO -  at 8.1s,	estimator xgboost's best error=0.1196,	best estimator xgboost's best error=0.1196
[flaml.automl: 09-16 18:21:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 18:21:53] {3072} INFO -  at 9.3s,	estimator xgboost's best error=0.0883,	best estimator xgboost's best error=0.0883
[flaml.automl: 09-16 18:21:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 18:21:54] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-16 18:21:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 18:21:56] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-16 18:21:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 18:21:58] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-16 18:21:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 18:22:00] {3072} INFO -  at 16.1s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-16 18:22:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 18:22:02] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-16 18:22:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 18:22:04] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.0752,	best estimator xgboost's best error=0.0752
[flaml.automl: 09-16 18:22:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 18:22:05] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0752,	best estimator xgboost's best error=0.0752
[flaml.automl: 09-16 18:22:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 18:22:12] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.0704,	best estimator xgboost's best error=0.0704
[flaml.automl: 09-16 18:22:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 18:22:24] {3072} INFO -  at 40.3s,	estimator xgboost's best error=0.0684,	best estimator xgboost's best error=0.0684
[flaml.automl: 09-16 18:22:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 18:22:30] {3072} INFO -  at 46.8s,	estimator xgboost's best error=0.0684,	best estimator xgboost's best error=0.0684
[flaml.automl: 09-16 18:22:30] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 18:22:43] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0684,	best estimator xgboost's best error=0.0684
[flaml.automl: 09-16 18:22:55] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 18:22:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 18:22:55] {2636} INFO - fit succeeded
[flaml.automl: 09-16 18:22:55] {2637} INFO - Time taken to find the best model: 40.26086449623108
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 76082}
CO(0)最佳损失：0.9316462461133079
CO(0)最好结果：{'pred_time': 6.304724661486634e-06, 'wall_clock_time': 40.26086449623108, 'metric_for_logging': {'pred_time': 6.304724661486634e-06}, 'val_loss': 0.06835375388669211, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 76082}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 76082, 'experiment_tag': 'exp', 'time_total_s': 12.184035778045654}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7906115312429929
CO(0)的mse=0.013419478784035058
CO(0)的mae=0.06950871198484214
CO(0)的mar=0.08181113204153675
总共花费的时间为：72.67
长沙市
1340A
1342A
1343A
1344A
[flaml.automl: 09-16 18:35:30] {2390} INFO - task = regression
[flaml.automl: 09-16 18:35:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 18:35:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 18:35:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 18:35:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 18:35:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 18:35:32] {3025} INFO - Estimated sufficient time budget=87453s. Estimated necessary time budget=87s.
[flaml.automl: 09-16 18:35:32] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1771,	best estimator xgboost's best error=0.1771
[flaml.automl: 09-16 18:35:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 18:35:36] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.0932,	best estimator xgboost's best error=0.0932
[flaml.automl: 09-16 18:35:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 18:35:38] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.0932,	best estimator xgboost's best error=0.0932
[flaml.automl: 09-16 18:35:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 18:35:44] {3072} INFO -  at 14.2s,	estimator xgboost's best error=0.0932,	best estimator xgboost's best error=0.0932
[flaml.automl: 09-16 18:35:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 18:35:46] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.0615,	best estimator xgboost's best error=0.0615
[flaml.automl: 09-16 18:35:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 18:35:49] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0506,	best estimator xgboost's best error=0.0506
[flaml.automl: 09-16 18:35:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 18:35:51] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0506,	best estimator xgboost's best error=0.0506
[flaml.automl: 09-16 18:35:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 18:35:56] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0506,	best estimator xgboost's best error=0.0506
[flaml.automl: 09-16 18:35:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 18:35:58] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.0506,	best estimator xgboost's best error=0.0506
[flaml.automl: 09-16 18:35:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 18:36:01] {3072} INFO -  at 31.3s,	estimator xgboost's best error=0.0506,	best estimator xgboost's best error=0.0506
[flaml.automl: 09-16 18:36:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 18:36:03] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-16 18:36:03] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 18:36:06] {3072} INFO -  at 36.3s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-16 18:36:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 18:36:18] {3072} INFO -  at 48.4s,	estimator xgboost's best error=0.0425,	best estimator xgboost's best error=0.0425
[flaml.automl: 09-16 18:36:30] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 18:36:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 18:36:30] {2636} INFO - fit succeeded
[flaml.automl: 09-16 18:36:30] {2637} INFO - Time taken to find the best model: 48.36688208580017
[flaml.automl: 09-16 18:36:30] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 41310}
CO(0)最佳损失：0.9574798272378824
CO(0)最好结果：{'pred_time': 1.9931533497662845e-05, 'wall_clock_time': 48.36688208580017, 'metric_for_logging': {'pred_time': 1.9931533497662845e-05}, 'val_loss': 0.042520172762117614, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 41310}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 41310, 'experiment_tag': 'exp', 'time_total_s': 12.111592292785645}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9042306508978313
CO(0)的mse=0.005223446713944028
CO(0)的mae=0.04497948169167241
CO(0)的mar=0.06648533607634714
总共花费的时间为：61.34
广州市
1345A
1346A
1349A
1351A
1352A
1354A
1355A
2846A
3299A
3300A
3301A
3302A
3303A
3304A
3443A
3445A
3446A
[flaml.automl: 09-16 19:27:52] {2390} INFO - task = regression
[flaml.automl: 09-16 19:27:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 19:27:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 19:27:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 19:27:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 19:27:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 19:27:54] {3025} INFO - Estimated sufficient time budget=219227s. Estimated necessary time budget=219s.
[flaml.automl: 09-16 19:27:54] {3072} INFO -  at 2.2s,	estimator xgboost's best error=0.1523,	best estimator xgboost's best error=0.1523
[flaml.automl: 09-16 19:27:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 19:27:55] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1119,	best estimator xgboost's best error=0.1119
[flaml.automl: 09-16 19:27:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 19:27:56] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1119,	best estimator xgboost's best error=0.1119
[flaml.automl: 09-16 19:27:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 19:27:58] {3072} INFO -  at 6.0s,	estimator xgboost's best error=0.1119,	best estimator xgboost's best error=0.1119
[flaml.automl: 09-16 19:27:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 19:27:59] {3072} INFO -  at 7.1s,	estimator xgboost's best error=0.0466,	best estimator xgboost's best error=0.0466
[flaml.automl: 09-16 19:27:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 19:28:00] {3072} INFO -  at 8.3s,	estimator xgboost's best error=0.0425,	best estimator xgboost's best error=0.0425
[flaml.automl: 09-16 19:28:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 19:28:01] {3072} INFO -  at 9.5s,	estimator xgboost's best error=0.0425,	best estimator xgboost's best error=0.0425
[flaml.automl: 09-16 19:28:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 19:28:02] {3072} INFO -  at 10.2s,	estimator xgboost's best error=0.0425,	best estimator xgboost's best error=0.0425
[flaml.automl: 09-16 19:28:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 19:28:03] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.0425,	best estimator xgboost's best error=0.0425
[flaml.automl: 09-16 19:28:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 19:28:04] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0425,	best estimator xgboost's best error=0.0425
[flaml.automl: 09-16 19:28:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 19:28:05] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-16 19:28:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 19:28:06] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-16 19:28:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 19:28:11] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0313,	best estimator xgboost's best error=0.0313
[flaml.automl: 09-16 19:28:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 19:28:15] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0313,	best estimator xgboost's best error=0.0313
[flaml.automl: 09-16 19:28:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 19:28:18] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.0313,	best estimator xgboost's best error=0.0313
[flaml.automl: 09-16 19:28:18] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 19:28:21] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.0313,	best estimator xgboost's best error=0.0313
[flaml.automl: 09-16 19:28:21] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 19:28:24] {3072} INFO -  at 32.4s,	estimator xgboost's best error=0.0313,	best estimator xgboost's best error=0.0313
[flaml.automl: 09-16 19:28:24] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 19:28:31] {3072} INFO -  at 39.1s,	estimator xgboost's best error=0.0308,	best estimator xgboost's best error=0.0308
[flaml.automl: 09-16 19:28:31] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 19:28:33] {3072} INFO -  at 41.9s,	estimator xgboost's best error=0.0308,	best estimator xgboost's best error=0.0308
[flaml.automl: 09-16 19:28:33] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 19:28:49] {3072} INFO -  at 57.7s,	estimator xgboost's best error=0.0295,	best estimator xgboost's best error=0.0295
[flaml.automl: 09-16 19:29:05] {3335} INFO - retrain xgboost for 15.9s
[flaml.automl: 09-16 19:29:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6942532417834797, colsample_bynode=1,
             colsample_bytree=0.6005789256305074, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=31, min_child_weight=0.0970195919911998,
             missing=nan, monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001195124218002065, reg_lambda=1.3571237433590773,
             scale_pos_weight=1, subsample=0.960999744825463,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 19:29:05] {2636} INFO - fit succeeded
[flaml.automl: 09-16 19:29:05] {2637} INFO - Time taken to find the best model: 57.74899482727051
[flaml.automl: 09-16 19:29:05] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 9, 'max_leaves': 31, 'min_child_weight': 0.0970195919911998, 'learning_rate': 1.0, 'subsample': 0.960999744825463, 'colsample_bylevel': 0.6942532417834797, 'colsample_bytree': 0.6005789256305074, 'reg_alpha': 0.001195124218002065, 'reg_lambda': 1.3571237433590773, 'FLAML_sample_size': 182688}
CO(0)最佳损失：0.9704915873371642
CO(0)最好结果：{'pred_time': 2.4789706539817616e-06, 'wall_clock_time': 57.74899482727051, 'metric_for_logging': {'pred_time': 2.4789706539817616e-06}, 'val_loss': 0.029508412662835725, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 31, 'min_child_weight': 0.0970195919911998, 'learning_rate': 1.0, 'subsample': 0.960999744825463, 'colsample_bylevel': 0.6942532417834797, 'colsample_bytree': 0.6005789256305074, 'reg_alpha': 0.001195124218002065, 'reg_lambda': 1.3571237433590773, 'FLAML_sample_size': 182688}, 'config/n_estimators': 9, 'config/max_leaves': 31, 'config/min_child_weight': 0.0970195919911998, 'config/learning_rate': 1.0, 'config/subsample': 0.960999744825463, 'config/colsample_bylevel': 0.6942532417834797, 'config/colsample_bytree': 0.6005789256305074, 'config/reg_alpha': 0.001195124218002065, 'config/reg_lambda': 1.3571237433590773, 'config/FLAML_sample_size': 182688, 'experiment_tag': 'exp', 'time_total_s': 15.847636938095093}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6942532417834797, colsample_bynode=1,
             colsample_bytree=0.6005789256305074, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=31, min_child_weight=0.0970195919911998,
             missing=nan, monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001195124218002065, reg_lambda=1.3571237433590773,
             scale_pos_weight=1, subsample=0.960999744825463,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9387492004732636
CO(0)的mse=0.0020365796849896294
CO(0)的mae=0.02970044620337282
CO(0)的mar=0.04193138954976987
总共花费的时间为：76.80
深圳市
1356A
1363A
1364A
1365A
1366A
3305A
3306A
3307A
3447A
3623A
[flaml.automl: 09-16 20:00:03] {2390} INFO - task = regression
[flaml.automl: 09-16 20:00:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:00:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:00:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:00:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:00:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:00:04] {3025} INFO - Estimated sufficient time budget=131821s. Estimated necessary time budget=132s.
[flaml.automl: 09-16 20:00:04] {3072} INFO -  at 1.8s,	estimator xgboost's best error=0.0948,	best estimator xgboost's best error=0.0948
[flaml.automl: 09-16 20:00:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:00:07] {3072} INFO -  at 3.9s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-16 20:00:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:00:08] {3072} INFO -  at 5.2s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-16 20:00:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:00:10] {3072} INFO -  at 7.4s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-16 20:00:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:00:11] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.0339,	best estimator xgboost's best error=0.0339
[flaml.automl: 09-16 20:00:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:00:13] {3072} INFO -  at 10.1s,	estimator xgboost's best error=0.0269,	best estimator xgboost's best error=0.0269
[flaml.automl: 09-16 20:00:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:00:14] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.0269,	best estimator xgboost's best error=0.0269
[flaml.automl: 09-16 20:00:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:00:16] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.0269,	best estimator xgboost's best error=0.0269
[flaml.automl: 09-16 20:00:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:00:17] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0269,	best estimator xgboost's best error=0.0269
[flaml.automl: 09-16 20:00:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:00:19] {3072} INFO -  at 16.2s,	estimator xgboost's best error=0.0269,	best estimator xgboost's best error=0.0269
[flaml.automl: 09-16 20:00:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:00:21] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0269,	best estimator xgboost's best error=0.0269
[flaml.automl: 09-16 20:00:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:00:22] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0269,	best estimator xgboost's best error=0.0269
[flaml.automl: 09-16 20:00:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:00:28] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0212,	best estimator xgboost's best error=0.0212
[flaml.automl: 09-16 20:00:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:00:41] {3072} INFO -  at 37.9s,	estimator xgboost's best error=0.0204,	best estimator xgboost's best error=0.0204
[flaml.automl: 09-16 20:00:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:00:47] {3072} INFO -  at 44.5s,	estimator xgboost's best error=0.0204,	best estimator xgboost's best error=0.0204
[flaml.automl: 09-16 20:00:47] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 20:01:02] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.0204,	best estimator xgboost's best error=0.0204
[flaml.automl: 09-16 20:01:23] {3335} INFO - retrain xgboost for 21.4s
[flaml.automl: 09-16 20:01:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 20:01:23] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:01:23] {2637} INFO - Time taken to find the best model: 59.34910297393799
[flaml.automl: 09-16 20:01:23] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 109728}
CO(0)最佳损失：0.979629244313876
CO(0)最好结果：{'pred_time': 3.4048566668052373e-06, 'wall_clock_time': 59.34910297393799, 'metric_for_logging': {'pred_time': 3.4048566668052373e-06}, 'val_loss': 0.02037075568612401, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 109728}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 109728, 'experiment_tag': 'exp', 'time_total_s': 14.838176012039185}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9559780281891412
CO(0)的mse=0.001046779197531483
CO(0)的mae=0.020607858291074976
CO(0)的mar=0.03676613151110043
总共花费的时间为：82.52
珠海市
1368A
1369A
1370A
3308A
3448A
[flaml.automl: 09-16 20:18:21] {2390} INFO - task = regression
[flaml.automl: 09-16 20:18:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:18:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:18:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:18:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:18:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:18:23] {3025} INFO - Estimated sufficient time budget=65306s. Estimated necessary time budget=65s.
[flaml.automl: 09-16 20:18:23] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1008,	best estimator xgboost's best error=0.1008
[flaml.automl: 09-16 20:18:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:18:25] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0561,	best estimator xgboost's best error=0.0561
[flaml.automl: 09-16 20:18:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:18:26] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0561,	best estimator xgboost's best error=0.0561
[flaml.automl: 09-16 20:18:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:18:31] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0561,	best estimator xgboost's best error=0.0561
[flaml.automl: 09-16 20:18:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:18:32] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-16 20:18:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:18:33] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-16 20:18:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:18:35] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-16 20:18:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:18:37] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-16 20:18:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:18:39] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-16 20:18:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:18:41] {3072} INFO -  at 20.0s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-16 20:18:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:18:43] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0316,	best estimator xgboost's best error=0.0316
[flaml.automl: 09-16 20:18:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:18:44] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0316,	best estimator xgboost's best error=0.0316
[flaml.automl: 09-16 20:18:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:18:50] {3072} INFO -  at 29.2s,	estimator xgboost's best error=0.0284,	best estimator xgboost's best error=0.0284
[flaml.automl: 09-16 20:18:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:19:02] {3072} INFO -  at 41.2s,	estimator xgboost's best error=0.0255,	best estimator xgboost's best error=0.0255
[flaml.automl: 09-16 20:19:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:19:09] {3072} INFO -  at 47.7s,	estimator xgboost's best error=0.0255,	best estimator xgboost's best error=0.0255
[flaml.automl: 09-16 20:19:09] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 20:19:20] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0255,	best estimator xgboost's best error=0.0255
[flaml.automl: 09-16 20:19:32] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 20:19:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 20:19:32] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:19:32] {2637} INFO - Time taken to find the best model: 41.23604130744934
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54826}
CO(0)最佳损失：0.9745444593189269
CO(0)最好结果：{'pred_time': 6.695170975482863e-06, 'wall_clock_time': 41.23604130744934, 'metric_for_logging': {'pred_time': 6.695170975482863e-06}, 'val_loss': 0.025455540681073172, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54826}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54826, 'experiment_tag': 'exp', 'time_total_s': 12.022980213165283}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9456221565964639
CO(0)的mse=0.0017166320586017659
CO(0)的mae=0.025351392617692693
CO(0)的mar=0.048840861069176615
总共花费的时间为：72.07
佛山市
1371A
1372A
1373A
1377A
1378A
3625A
[flaml.automl: 09-16 20:38:21] {2390} INFO - task = regression
[flaml.automl: 09-16 20:38:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:38:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:38:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:38:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:38:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:38:23] {3025} INFO - Estimated sufficient time budget=77846s. Estimated necessary time budget=78s.
[flaml.automl: 09-16 20:38:23] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1267,	best estimator xgboost's best error=0.1267
[flaml.automl: 09-16 20:38:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:38:25] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0702,	best estimator xgboost's best error=0.0702
[flaml.automl: 09-16 20:38:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:38:26] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0702,	best estimator xgboost's best error=0.0702
[flaml.automl: 09-16 20:38:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:38:30] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.0702,	best estimator xgboost's best error=0.0702
[flaml.automl: 09-16 20:38:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:38:31] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0505,	best estimator xgboost's best error=0.0505
[flaml.automl: 09-16 20:38:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:38:32] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.0417,	best estimator xgboost's best error=0.0417
[flaml.automl: 09-16 20:38:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:38:34] {3072} INFO -  at 13.0s,	estimator xgboost's best error=0.0417,	best estimator xgboost's best error=0.0417
[flaml.automl: 09-16 20:38:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:38:37] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0417,	best estimator xgboost's best error=0.0417
[flaml.automl: 09-16 20:38:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:38:38] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.0417,	best estimator xgboost's best error=0.0417
[flaml.automl: 09-16 20:38:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:38:40] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0417,	best estimator xgboost's best error=0.0417
[flaml.automl: 09-16 20:38:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:38:42] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0416,	best estimator xgboost's best error=0.0416
[flaml.automl: 09-16 20:38:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:38:43] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0416,	best estimator xgboost's best error=0.0416
[flaml.automl: 09-16 20:38:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:38:50] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.0348,	best estimator xgboost's best error=0.0348
[flaml.automl: 09-16 20:38:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:39:02] {3072} INFO -  at 40.6s,	estimator xgboost's best error=0.0338,	best estimator xgboost's best error=0.0338
[flaml.automl: 09-16 20:39:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:39:08] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.0338,	best estimator xgboost's best error=0.0338
[flaml.automl: 09-16 20:39:08] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 20:39:21] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.0336,	best estimator xgboost's best error=0.0336
[flaml.automl: 09-16 20:39:42] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-16 20:39:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 20:39:42] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:39:42] {2637} INFO - Time taken to find the best model: 59.69681787490845
[flaml.automl: 09-16 20:39:42] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 65673}
CO(0)最佳损失：0.9664445711452699
CO(0)最好结果：{'pred_time': 5.810133672763636e-06, 'wall_clock_time': 59.69681787490845, 'metric_for_logging': {'pred_time': 5.810133672763636e-06}, 'val_loss': 0.03355542885473007, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 65673}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 65673, 'experiment_tag': 'exp', 'time_total_s': 12.541792392730713}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9338315789598258
CO(0)的mse=0.0026183052750471227
CO(0)的mae=0.03378524878766658
CO(0)的mar=0.05629779270570166
总共花费的时间为：82.18
中山市
1379A
3454A
[flaml.automl: 09-16 20:46:29] {2390} INFO - task = regression
[flaml.automl: 09-16 20:46:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:46:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:46:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:46:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:46:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:46:30] {3025} INFO - Estimated sufficient time budget=12047s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 20:46:30] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.0983,	best estimator xgboost's best error=0.0983
[flaml.automl: 09-16 20:46:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:46:32] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0580,	best estimator xgboost's best error=0.0580
[flaml.automl: 09-16 20:46:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:46:33] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0580,	best estimator xgboost's best error=0.0580
[flaml.automl: 09-16 20:46:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:46:43] {3072} INFO -  at 14.2s,	estimator xgboost's best error=0.0580,	best estimator xgboost's best error=0.0580
[flaml.automl: 09-16 20:46:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:46:44] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0459,	best estimator xgboost's best error=0.0459
[flaml.automl: 09-16 20:46:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:46:45] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 20:46:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:46:47] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 20:46:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:46:49] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 20:46:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:46:51] {3072} INFO -  at 22.2s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 20:46:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:46:53] {3072} INFO -  at 24.7s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 20:46:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:46:54] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 20:46:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:46:55] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-16 20:46:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:47:01] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0322,	best estimator xgboost's best error=0.0322
[flaml.automl: 09-16 20:47:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:47:12] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0313,	best estimator xgboost's best error=0.0313
[flaml.automl: 09-16 20:47:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:47:18] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0313,	best estimator xgboost's best error=0.0313
[flaml.automl: 09-16 20:47:18] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 20:47:28] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0313,	best estimator xgboost's best error=0.0313
[flaml.automl: 09-16 20:47:38] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-16 20:47:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 20:47:38] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:47:38] {2637} INFO - Time taken to find the best model: 43.43634796142578
[flaml.automl: 09-16 20:47:38] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9686798512115459
CO(0)最好结果：{'pred_time': 1.903072861600513e-05, 'wall_clock_time': 43.43634796142578, 'metric_for_logging': {'pred_time': 1.903072861600513e-05}, 'val_loss': 0.03132014878845412, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.432474851608276}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9044695089617031
CO(0)的mse=0.002777186917170731
CO(0)的mae=0.03265980798562426
CO(0)的mar=0.05332674948412822
总共花费的时间为：70.02
江门市
1386A
3449A
[flaml.automl: 09-16 20:54:10] {2390} INFO - task = regression
[flaml.automl: 09-16 20:54:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:54:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:54:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:54:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:54:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:54:11] {3025} INFO - Estimated sufficient time budget=12010s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 20:54:11] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1149,	best estimator xgboost's best error=0.1149
[flaml.automl: 09-16 20:54:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:54:13] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-16 20:54:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:54:14] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-16 20:54:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:54:24] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-16 20:54:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:54:25] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-16 20:54:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:54:27] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0412,	best estimator xgboost's best error=0.0412
[flaml.automl: 09-16 20:54:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:54:28] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0412,	best estimator xgboost's best error=0.0412
[flaml.automl: 09-16 20:54:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:54:31] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0412,	best estimator xgboost's best error=0.0412
[flaml.automl: 09-16 20:54:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:54:32] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0412,	best estimator xgboost's best error=0.0412
[flaml.automl: 09-16 20:54:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:54:34] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.0412,	best estimator xgboost's best error=0.0412
[flaml.automl: 09-16 20:54:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:54:35] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0412,	best estimator xgboost's best error=0.0412
[flaml.automl: 09-16 20:54:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:54:37] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.0412,	best estimator xgboost's best error=0.0412
[flaml.automl: 09-16 20:54:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:54:43] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0368,	best estimator xgboost's best error=0.0368
[flaml.automl: 09-16 20:54:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:54:53] {3072} INFO -  at 43.5s,	estimator xgboost's best error=0.0349,	best estimator xgboost's best error=0.0349
[flaml.automl: 09-16 20:54:53] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:54:59] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0349,	best estimator xgboost's best error=0.0349
[flaml.automl: 09-16 20:55:10] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-16 20:55:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 20:55:10] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:55:10] {2637} INFO - Time taken to find the best model: 43.48798203468323
[flaml.automl: 09-16 20:55:10] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9650788603338558
CO(0)最好结果：{'pred_time': 1.6807926896148905e-05, 'wall_clock_time': 43.48798203468323, 'metric_for_logging': {'pred_time': 1.6807926896148905e-05}, 'val_loss': 0.034921139666144206, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.494482517242432}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9161548408668041
CO(0)的mse=0.0032179649577178254
CO(0)的mae=0.03624326376860752
CO(0)的mar=0.06116208266607699
总共花费的时间为：60.32
东莞市
1389A
1391A
3319A
3626A
3627A
[flaml.automl: 09-16 21:11:11] {2390} INFO - task = regression
[flaml.automl: 09-16 21:11:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:11:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:11:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:11:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:11:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:11:12] {3025} INFO - Estimated sufficient time budget=65048s. Estimated necessary time budget=65s.
[flaml.automl: 09-16 21:11:12] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1242,	best estimator xgboost's best error=0.1242
[flaml.automl: 09-16 21:11:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:11:14] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.0651,	best estimator xgboost's best error=0.0651
[flaml.automl: 09-16 21:11:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:11:15] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0651,	best estimator xgboost's best error=0.0651
[flaml.automl: 09-16 21:11:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:11:20] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0651,	best estimator xgboost's best error=0.0651
[flaml.automl: 09-16 21:11:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:11:21] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0443,	best estimator xgboost's best error=0.0443
[flaml.automl: 09-16 21:11:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:11:23] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0366,	best estimator xgboost's best error=0.0366
[flaml.automl: 09-16 21:11:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:11:24] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0366,	best estimator xgboost's best error=0.0366
[flaml.automl: 09-16 21:11:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:11:27] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0366,	best estimator xgboost's best error=0.0366
[flaml.automl: 09-16 21:11:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:11:28] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0366,	best estimator xgboost's best error=0.0366
[flaml.automl: 09-16 21:11:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:11:30] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0366,	best estimator xgboost's best error=0.0366
[flaml.automl: 09-16 21:11:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:11:32] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0366,	best estimator xgboost's best error=0.0366
[flaml.automl: 09-16 21:11:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:11:33] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0366,	best estimator xgboost's best error=0.0366
[flaml.automl: 09-16 21:11:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:11:40] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-16 21:11:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:11:52] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0303,	best estimator xgboost's best error=0.0303
[flaml.automl: 09-16 21:11:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:11:58] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.0303,	best estimator xgboost's best error=0.0303
[flaml.automl: 09-16 21:12:10] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 21:12:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:12:10] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:12:10] {2637} INFO - Time taken to find the best model: 41.674017906188965
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54031}
CO(0)最佳损失：0.9697263445988407
CO(0)最好结果：{'pred_time': 6.644031669520125e-06, 'wall_clock_time': 41.674017906188965, 'metric_for_logging': {'pred_time': 6.644031669520125e-06}, 'val_loss': 0.03027365540115933, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54031}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54031, 'experiment_tag': 'exp', 'time_total_s': 12.094982862472534}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9346329060528228
CO(0)的mse=0.002029174015287581
CO(0)的mae=0.030559784537436737
CO(0)的mar=0.04849171197557348
总共花费的时间为：60.99
惠州市
1392A
1393A
1395A
1396A
3314A
3452A
[flaml.automl: 09-16 21:31:14] {2390} INFO - task = regression
[flaml.automl: 09-16 21:31:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:31:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:31:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:31:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:31:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:31:15] {3025} INFO - Estimated sufficient time budget=79440s. Estimated necessary time budget=79s.
[flaml.automl: 09-16 21:31:15] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.0870,	best estimator xgboost's best error=0.0870
[flaml.automl: 09-16 21:31:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:31:19] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-16 21:31:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:31:21] {3072} INFO -  at 7.1s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-16 21:31:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:31:24] {3072} INFO -  at 10.3s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-16 21:31:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:31:26] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0396,	best estimator xgboost's best error=0.0396
[flaml.automl: 09-16 21:31:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:31:29] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0328,	best estimator xgboost's best error=0.0328
[flaml.automl: 09-16 21:31:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:31:32] {3072} INFO -  at 18.3s,	estimator xgboost's best error=0.0328,	best estimator xgboost's best error=0.0328
[flaml.automl: 09-16 21:31:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:31:35] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.0328,	best estimator xgboost's best error=0.0328
[flaml.automl: 09-16 21:31:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:31:37] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0328,	best estimator xgboost's best error=0.0328
[flaml.automl: 09-16 21:31:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:31:39] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0328,	best estimator xgboost's best error=0.0328
[flaml.automl: 09-16 21:31:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:31:42] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.0320,	best estimator xgboost's best error=0.0320
[flaml.automl: 09-16 21:31:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:31:44] {3072} INFO -  at 30.6s,	estimator xgboost's best error=0.0320,	best estimator xgboost's best error=0.0320
[flaml.automl: 09-16 21:31:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:31:57] {3072} INFO -  at 42.8s,	estimator xgboost's best error=0.0279,	best estimator xgboost's best error=0.0279
[flaml.automl: 09-16 21:31:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:32:13] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.0265,	best estimator xgboost's best error=0.0265
[flaml.automl: 09-16 21:32:36] {3335} INFO - retrain xgboost for 22.1s
[flaml.automl: 09-16 21:32:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:32:36] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:32:36] {2637} INFO - Time taken to find the best model: 59.664798974990845
[flaml.automl: 09-16 21:32:36] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65553}
CO(0)最佳损失：0.9734603936824087
CO(0)最好结果：{'pred_time': 1.2923766465582473e-05, 'wall_clock_time': 59.664798974990845, 'metric_for_logging': {'pred_time': 1.2923766465582473e-05}, 'val_loss': 0.026539606317591363, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65553}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 65553, 'experiment_tag': 'exp', 'time_total_s': 16.903515100479126}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9273591935268579
CO(0)的mse=0.001563014233298953
CO(0)的mae=0.026436963938034876
CO(0)的mar=0.05189493087598694
总共花费的时间为：82.90
肇庆市
1397A
1398A
1400A
3451A
[flaml.automl: 09-16 21:45:13] {2390} INFO - task = regression
[flaml.automl: 09-16 21:45:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:45:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:45:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:45:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:45:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:45:14] {3025} INFO - Estimated sufficient time budget=47909s. Estimated necessary time budget=48s.
[flaml.automl: 09-16 21:45:14] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1188,	best estimator xgboost's best error=0.1188
[flaml.automl: 09-16 21:45:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:45:16] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 21:45:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:45:18] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 21:45:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:45:24] {3072} INFO -  at 10.9s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 21:45:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:45:25] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-16 21:45:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:45:27] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.0385,	best estimator xgboost's best error=0.0385
[flaml.automl: 09-16 21:45:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:45:28] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.0385,	best estimator xgboost's best error=0.0385
[flaml.automl: 09-16 21:45:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:45:31] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0385,	best estimator xgboost's best error=0.0385
[flaml.automl: 09-16 21:45:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:45:32] {3072} INFO -  at 18.7s,	estimator xgboost's best error=0.0385,	best estimator xgboost's best error=0.0385
[flaml.automl: 09-16 21:45:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:45:34] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.0385,	best estimator xgboost's best error=0.0385
[flaml.automl: 09-16 21:45:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:45:36] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0385,	best estimator xgboost's best error=0.0385
[flaml.automl: 09-16 21:45:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:45:37] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.0385,	best estimator xgboost's best error=0.0385
[flaml.automl: 09-16 21:45:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:45:44] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.0338,	best estimator xgboost's best error=0.0338
[flaml.automl: 09-16 21:45:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:45:56] {3072} INFO -  at 42.8s,	estimator xgboost's best error=0.0325,	best estimator xgboost's best error=0.0325
[flaml.automl: 09-16 21:45:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:46:02] {3072} INFO -  at 49.3s,	estimator xgboost's best error=0.0325,	best estimator xgboost's best error=0.0325
[flaml.automl: 09-16 21:46:14] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 21:46:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:46:14] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:46:14] {2637} INFO - Time taken to find the best model: 42.77948069572449
[flaml.automl: 09-16 21:46:14] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40708}
CO(0)最佳损失：0.9674652663478801
CO(0)最好结果：{'pred_time': 8.84478119598038e-06, 'wall_clock_time': 42.77948069572449, 'metric_for_logging': {'pred_time': 8.84478119598038e-06}, 'val_loss': 0.03253473365211993, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40708}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40708, 'experiment_tag': 'exp', 'time_total_s': 12.111024141311646}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9308558512901612
CO(0)的mse=0.002339757213106303
CO(0)的mae=0.032631236017722796
CO(0)的mar=0.055917762043260315
总共花费的时间为：62.00
南宁市
1401A
1408A
[flaml.automl: 09-16 21:52:41] {2390} INFO - task = regression
[flaml.automl: 09-16 21:52:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:52:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:52:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:52:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:52:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:52:42] {3025} INFO - Estimated sufficient time budget=12305s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 21:52:42] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1667,	best estimator xgboost's best error=0.1667
[flaml.automl: 09-16 21:52:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:52:44] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0860,	best estimator xgboost's best error=0.0860
[flaml.automl: 09-16 21:52:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:52:45] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0860,	best estimator xgboost's best error=0.0860
[flaml.automl: 09-16 21:52:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:52:55] {3072} INFO -  at 14.2s,	estimator xgboost's best error=0.0860,	best estimator xgboost's best error=0.0860
[flaml.automl: 09-16 21:52:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:52:56] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0556,	best estimator xgboost's best error=0.0556
[flaml.automl: 09-16 21:52:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:52:58] {3072} INFO -  at 17.0s,	estimator xgboost's best error=0.0443,	best estimator xgboost's best error=0.0443
[flaml.automl: 09-16 21:52:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:52:59] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.0443,	best estimator xgboost's best error=0.0443
[flaml.automl: 09-16 21:52:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:53:02] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.0443,	best estimator xgboost's best error=0.0443
[flaml.automl: 09-16 21:53:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:53:03] {3072} INFO -  at 22.2s,	estimator xgboost's best error=0.0443,	best estimator xgboost's best error=0.0443
[flaml.automl: 09-16 21:53:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:53:05] {3072} INFO -  at 24.7s,	estimator xgboost's best error=0.0443,	best estimator xgboost's best error=0.0443
[flaml.automl: 09-16 21:53:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:53:07] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.0443,	best estimator xgboost's best error=0.0443
[flaml.automl: 09-16 21:53:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:53:08] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.0443,	best estimator xgboost's best error=0.0443
[flaml.automl: 09-16 21:53:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:53:14] {3072} INFO -  at 33.1s,	estimator xgboost's best error=0.0376,	best estimator xgboost's best error=0.0376
[flaml.automl: 09-16 21:53:14] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:53:24] {3072} INFO -  at 43.5s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-16 21:53:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:53:30] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-16 21:53:41] {3335} INFO - retrain xgboost for 10.5s
[flaml.automl: 09-16 21:53:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:53:41] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:53:41] {2637} INFO - Time taken to find the best model: 43.52093243598938
[flaml.automl: 09-16 21:53:41] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9634924742364371
CO(0)最好结果：{'pred_time': 1.826766696870081e-05, 'wall_clock_time': 43.52093243598938, 'metric_for_logging': {'pred_time': 1.826766696870081e-05}, 'val_loss': 0.036507525763562844, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.427687168121338}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9281433250590608
CO(0)的mse=0.002767604582074358
CO(0)的mae=0.035204328692389084
CO(0)的mar=0.05066479535138412
总共花费的时间为：60.59
海口市
1409A
1411A
1413A
3539A
[flaml.automl: 09-16 22:06:46] {2390} INFO - task = regression
[flaml.automl: 09-16 22:06:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 22:06:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 22:06:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 22:06:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 22:06:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 22:06:47] {3025} INFO - Estimated sufficient time budget=48322s. Estimated necessary time budget=48s.
[flaml.automl: 09-16 22:06:47] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.0748,	best estimator xgboost's best error=0.0748
[flaml.automl: 09-16 22:06:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 22:06:49] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0406,	best estimator xgboost's best error=0.0406
[flaml.automl: 09-16 22:06:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 22:06:50] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0406,	best estimator xgboost's best error=0.0406
[flaml.automl: 09-16 22:06:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 22:06:57] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0406,	best estimator xgboost's best error=0.0406
[flaml.automl: 09-16 22:06:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 22:06:58] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0294,	best estimator xgboost's best error=0.0294
[flaml.automl: 09-16 22:06:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 22:06:59] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0223,	best estimator xgboost's best error=0.0223
[flaml.automl: 09-16 22:06:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 22:07:01] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0223,	best estimator xgboost's best error=0.0223
[flaml.automl: 09-16 22:07:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 22:07:03] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.0223,	best estimator xgboost's best error=0.0223
[flaml.automl: 09-16 22:07:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 22:07:05] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0223,	best estimator xgboost's best error=0.0223
[flaml.automl: 09-16 22:07:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 22:07:07] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0223,	best estimator xgboost's best error=0.0223
[flaml.automl: 09-16 22:07:07] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 22:07:09] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0223,	best estimator xgboost's best error=0.0223
[flaml.automl: 09-16 22:07:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 22:07:10] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.0223,	best estimator xgboost's best error=0.0223
[flaml.automl: 09-16 22:07:10] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 22:07:16] {3072} INFO -  at 30.6s,	estimator xgboost's best error=0.0175,	best estimator xgboost's best error=0.0175
[flaml.automl: 09-16 22:07:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 22:07:28] {3072} INFO -  at 42.7s,	estimator xgboost's best error=0.0170,	best estimator xgboost's best error=0.0170
[flaml.automl: 09-16 22:07:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 22:07:35] {3072} INFO -  at 49.2s,	estimator xgboost's best error=0.0170,	best estimator xgboost's best error=0.0170
[flaml.automl: 09-16 22:07:47] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 22:07:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 22:07:47] {2636} INFO - fit succeeded
[flaml.automl: 09-16 22:07:47] {2637} INFO - Time taken to find the best model: 42.670226097106934
[flaml.automl: 09-16 22:07:47] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40579}
CO(0)最佳损失：0.9830200804250022
CO(0)最好结果：{'pred_time': 1.0138791055313419e-05, 'wall_clock_time': 42.670226097106934, 'metric_for_logging': {'pred_time': 1.0138791055313419e-05}, 'val_loss': 0.016979919574997806, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40579}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40579, 'experiment_tag': 'exp', 'time_total_s': 12.057494640350342}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.961721506304885
CO(0)的mse=0.0006142849816056942
CO(0)的mae=0.016310924417161178
CO(0)的mar=0.0360812796401347
总共花费的时间为：62.06
重庆市
1414A
1418A
1419A
1420A
1422A
1428A
1429A
3015A
3016A
3346A
3347A
3348A
3349A
3350A
3351A
3352A
3353A
3354A
3355A
3356A
3482A
3483A
3484A
3485A
3599A
3600A
3601A
3610A
[flaml.automl: 09-16 23:33:49] {2390} INFO - task = regression
[flaml.automl: 09-16 23:33:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 23:33:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 23:33:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 23:33:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 23:33:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 23:33:50] {3025} INFO - Estimated sufficient time budget=211158s. Estimated necessary time budget=211s.
[flaml.automl: 09-16 23:33:50] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.1934,	best estimator xgboost's best error=0.1934
[flaml.automl: 09-16 23:33:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 23:33:51] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.1820,	best estimator xgboost's best error=0.1820
[flaml.automl: 09-16 23:33:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 23:33:51] {3072} INFO -  at 3.9s,	estimator xgboost's best error=0.1820,	best estimator xgboost's best error=0.1820
[flaml.automl: 09-16 23:33:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 23:33:52] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.1820,	best estimator xgboost's best error=0.1820
[flaml.automl: 09-16 23:33:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 23:33:53] {3072} INFO -  at 5.2s,	estimator xgboost's best error=0.1026,	best estimator xgboost's best error=0.1026
[flaml.automl: 09-16 23:33:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 23:33:53] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.1026,	best estimator xgboost's best error=0.1026
[flaml.automl: 09-16 23:33:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 23:33:54] {3072} INFO -  at 6.5s,	estimator xgboost's best error=0.1026,	best estimator xgboost's best error=0.1026
[flaml.automl: 09-16 23:33:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 23:33:54] {3072} INFO -  at 7.1s,	estimator xgboost's best error=0.1026,	best estimator xgboost's best error=0.1026
[flaml.automl: 09-16 23:33:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 23:33:55] {3072} INFO -  at 7.7s,	estimator xgboost's best error=0.0828,	best estimator xgboost's best error=0.0828
[flaml.automl: 09-16 23:33:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 23:33:56] {3072} INFO -  at 8.3s,	estimator xgboost's best error=0.0828,	best estimator xgboost's best error=0.0828
[flaml.automl: 09-16 23:33:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 23:33:56] {3072} INFO -  at 9.0s,	estimator xgboost's best error=0.0817,	best estimator xgboost's best error=0.0817
[flaml.automl: 09-16 23:33:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 23:33:57] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0817,	best estimator xgboost's best error=0.0817
[flaml.automl: 09-16 23:33:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 23:33:58] {3072} INFO -  at 10.4s,	estimator xgboost's best error=0.0647,	best estimator xgboost's best error=0.0647
[flaml.automl: 09-16 23:33:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 23:33:58] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.0647,	best estimator xgboost's best error=0.0647
[flaml.automl: 09-16 23:33:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 23:33:59] {3072} INFO -  at 11.9s,	estimator xgboost's best error=0.0647,	best estimator xgboost's best error=0.0647
[flaml.automl: 09-16 23:33:59] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 23:34:00] {3072} INFO -  at 13.0s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-16 23:34:00] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 23:34:01] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-16 23:34:01] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 23:34:02] {3072} INFO -  at 14.3s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-16 23:34:02] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 23:34:04] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-16 23:34:04] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 23:34:05] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-16 23:34:05] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 23:34:07] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0569,	best estimator xgboost's best error=0.0569
[flaml.automl: 09-16 23:34:07] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 23:34:08] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0569,	best estimator xgboost's best error=0.0569
[flaml.automl: 09-16 23:34:08] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 23:34:10] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0569,	best estimator xgboost's best error=0.0569
[flaml.automl: 09-16 23:34:10] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-16 23:34:13] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.0569,	best estimator xgboost's best error=0.0569
[flaml.automl: 09-16 23:34:13] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-16 23:34:15] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.0569,	best estimator xgboost's best error=0.0569
[flaml.automl: 09-16 23:34:15] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-16 23:34:22] {3072} INFO -  at 34.9s,	estimator xgboost's best error=0.0544,	best estimator xgboost's best error=0.0544
[flaml.automl: 09-16 23:34:22] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-16 23:34:24] {3072} INFO -  at 36.6s,	estimator xgboost's best error=0.0544,	best estimator xgboost's best error=0.0544
[flaml.automl: 09-16 23:34:24] {2897} INFO - iteration 27, current learner xgboost
[flaml.automl: 09-16 23:34:45] {3072} INFO -  at 58.0s,	estimator xgboost's best error=0.0544,	best estimator xgboost's best error=0.0544
[flaml.automl: 09-16 23:34:53] {3335} INFO - retrain xgboost for 7.7s
[flaml.automl: 09-16 23:34:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.684287011575384, colsample_bynode=1,
             colsample_bytree=0.5916096876391048, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.981859921144257,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=20.646861804518355, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.005948512972666526, reg_lambda=13.30294613686073,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 23:34:53] {2636} INFO - fit succeeded
[flaml.automl: 09-16 23:34:53] {2637} INFO - Time taken to find the best model: 34.921175479888916
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 20.646861804518355, 'learning_rate': 0.981859921144257, 'subsample': 1.0, 'colsample_bylevel': 0.684287011575384, 'colsample_bytree': 0.5916096876391048, 'reg_alpha': 0.005948512972666526, 'reg_lambda': 13.30294613686073, 'FLAML_sample_size': 301624}
CO(0)最佳损失：0.945577448805909
CO(0)最好结果：{'pred_time': 1.5092349930296738e-06, 'wall_clock_time': 34.921175479888916, 'metric_for_logging': {'pred_time': 1.5092349930296738e-06}, 'val_loss': 0.05442255119409105, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 20.646861804518355, 'learning_rate': 0.981859921144257, 'subsample': 1.0, 'colsample_bylevel': 0.684287011575384, 'colsample_bytree': 0.5916096876391048, 'reg_alpha': 0.005948512972666526, 'reg_lambda': 13.30294613686073, 'FLAML_sample_size': 301624}, 'config/n_estimators': 8, 'config/max_leaves': 16, 'config/min_child_weight': 20.646861804518355, 'config/learning_rate': 0.981859921144257, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.684287011575384, 'config/colsample_bytree': 0.5916096876391048, 'config/reg_alpha': 0.005948512972666526, 'config/reg_lambda': 13.30294613686073, 'config/FLAML_sample_size': 301624, 'experiment_tag': 'exp', 'time_total_s': 7.756634712219238}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.684287011575384, colsample_bynode=1,
             colsample_bytree=0.5916096876391048, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.981859921144257,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=20.646861804518355, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.005948512972666526, reg_lambda=13.30294613686073,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8799854738564488
CO(0)的mse=0.006507831608436609
CO(0)的mae=0.052350464284743346
CO(0)的mar=0.11257501307712509
总共花费的时间为：70.95
贵阳市
1440A
1442A
1443A
1444A
1445A
1446A
[flaml.automl: 09-16 23:53:07] {2390} INFO - task = regression
[flaml.automl: 09-16 23:53:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 23:53:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 23:53:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 23:53:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 23:53:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 23:53:09] {3025} INFO - Estimated sufficient time budget=77795s. Estimated necessary time budget=78s.
[flaml.automl: 09-16 23:53:09] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1086,	best estimator xgboost's best error=0.1086
[flaml.automl: 09-16 23:53:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 23:53:11] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 23:53:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 23:53:12] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 23:53:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 23:53:16] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-16 23:53:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 23:53:17] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0521,	best estimator xgboost's best error=0.0521
[flaml.automl: 09-16 23:53:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 23:53:18] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-16 23:53:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 23:53:20] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-16 23:53:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 23:53:22] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-16 23:53:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 23:53:24] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-16 23:53:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 23:53:26] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-16 23:53:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 23:53:28] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-16 23:53:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 23:53:29] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-16 23:53:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 23:53:36] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.0418,	best estimator xgboost's best error=0.0418
[flaml.automl: 09-16 23:53:36] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 23:53:48] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.0401,	best estimator xgboost's best error=0.0401
[flaml.automl: 09-16 23:53:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 23:53:54] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.0401,	best estimator xgboost's best error=0.0401
[flaml.automl: 09-16 23:53:54] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 23:54:07] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.0401,	best estimator xgboost's best error=0.0401
[flaml.automl: 09-16 23:54:19] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 23:54:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 23:54:19] {2636} INFO - fit succeeded
[flaml.automl: 09-16 23:54:19] {2637} INFO - Time taken to find the best model: 40.68104290962219
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65286}
CO(0)最佳损失：0.9598981175967114
CO(0)最好结果：{'pred_time': 5.637708694967055e-06, 'wall_clock_time': 40.68104290962219, 'metric_for_logging': {'pred_time': 5.637708694967055e-06}, 'val_loss': 0.04010188240328866, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65286}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 65286, 'experiment_tag': 'exp', 'time_total_s': 12.129683494567871}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8827269202046266
CO(0)的mse=0.0038576752997671296
CO(0)的mae=0.04052569391230262
CO(0)的mar=0.07492434437997195
总共花费的时间为：73.01
昆明市
1452A
1453A
1455A
3179A
3375A
3550A
3551A
3552A
[flaml.automl: 09-17 00:18:48] {2390} INFO - task = regression
[flaml.automl: 09-17 00:18:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 00:18:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 00:18:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 00:18:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 00:18:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 00:18:49] {3025} INFO - Estimated sufficient time budget=105149s. Estimated necessary time budget=105s.
[flaml.automl: 09-17 00:18:49] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.1237,	best estimator xgboost's best error=0.1237
[flaml.automl: 09-17 00:18:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 00:18:51] {3072} INFO -  at 3.9s,	estimator xgboost's best error=0.0841,	best estimator xgboost's best error=0.0841
[flaml.automl: 09-17 00:18:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 00:18:53] {3072} INFO -  at 5.1s,	estimator xgboost's best error=0.0841,	best estimator xgboost's best error=0.0841
[flaml.automl: 09-17 00:18:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 00:18:55] {3072} INFO -  at 7.8s,	estimator xgboost's best error=0.0841,	best estimator xgboost's best error=0.0841
[flaml.automl: 09-17 00:18:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 00:18:56] {3072} INFO -  at 9.0s,	estimator xgboost's best error=0.0734,	best estimator xgboost's best error=0.0734
[flaml.automl: 09-17 00:18:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 00:18:58] {3072} INFO -  at 10.5s,	estimator xgboost's best error=0.0658,	best estimator xgboost's best error=0.0658
[flaml.automl: 09-17 00:18:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 00:19:00] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0658,	best estimator xgboost's best error=0.0658
[flaml.automl: 09-17 00:19:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 00:19:02] {3072} INFO -  at 14.6s,	estimator xgboost's best error=0.0658,	best estimator xgboost's best error=0.0658
[flaml.automl: 09-17 00:19:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 00:19:03] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.0658,	best estimator xgboost's best error=0.0658
[flaml.automl: 09-17 00:19:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 00:19:05] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.0658,	best estimator xgboost's best error=0.0658
[flaml.automl: 09-17 00:19:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 00:19:07] {3072} INFO -  at 19.4s,	estimator xgboost's best error=0.0646,	best estimator xgboost's best error=0.0646
[flaml.automl: 09-17 00:19:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 00:19:08] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.0646,	best estimator xgboost's best error=0.0646
[flaml.automl: 09-17 00:19:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 00:19:15] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.0600,	best estimator xgboost's best error=0.0600
[flaml.automl: 09-17 00:19:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 00:19:27] {3072} INFO -  at 39.4s,	estimator xgboost's best error=0.0583,	best estimator xgboost's best error=0.0583
[flaml.automl: 09-17 00:19:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 00:19:33] {3072} INFO -  at 45.9s,	estimator xgboost's best error=0.0583,	best estimator xgboost's best error=0.0583
[flaml.automl: 09-17 00:19:33] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 00:19:47] {3072} INFO -  at 59.6s,	estimator xgboost's best error=0.0571,	best estimator xgboost's best error=0.0571
[flaml.automl: 09-17 00:20:09] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-17 00:20:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 00:20:09] {2636} INFO - fit succeeded
[flaml.automl: 09-17 00:20:09] {2637} INFO - Time taken to find the best model: 59.621517181396484
[flaml.automl: 09-17 00:20:09] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 87766}
CO(0)最佳损失：0.9428776815319716
CO(0)最好结果：{'pred_time': 4.384901994950855e-06, 'wall_clock_time': 59.621517181396484, 'metric_for_logging': {'pred_time': 4.384901994950855e-06}, 'val_loss': 0.05712231846802836, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 87766}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 87766, 'experiment_tag': 'exp', 'time_total_s': 13.682124376296997}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7935425903811292
CO(0)的mse=0.009160396268516575
CO(0)的mae=0.058854782000866475
CO(0)的mar=0.10721583734556733
总共花费的时间为：82.53
拉萨市
1456A
1457A
1458A
1461A
[flaml.automl: 09-17 00:33:17] {2390} INFO - task = regression
[flaml.automl: 09-17 00:33:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 00:33:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 00:33:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 00:33:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 00:33:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 00:33:18] {3025} INFO - Estimated sufficient time budget=50243s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 00:33:18] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1348,	best estimator xgboost's best error=0.1348
[flaml.automl: 09-17 00:33:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 00:33:20] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0803,	best estimator xgboost's best error=0.0803
[flaml.automl: 09-17 00:33:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 00:33:21] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0803,	best estimator xgboost's best error=0.0803
[flaml.automl: 09-17 00:33:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 00:33:27] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0803,	best estimator xgboost's best error=0.0803
[flaml.automl: 09-17 00:33:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 00:33:29] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0582,	best estimator xgboost's best error=0.0582
[flaml.automl: 09-17 00:33:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 00:33:30] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.0521,	best estimator xgboost's best error=0.0521
[flaml.automl: 09-17 00:33:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 00:33:32] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-17 00:33:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 00:33:34] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-17 00:33:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 00:33:36] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0514,	best estimator xgboost's best error=0.0514
[flaml.automl: 09-17 00:33:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 00:33:39] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0514,	best estimator xgboost's best error=0.0514
[flaml.automl: 09-17 00:33:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 00:33:40] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.0514,	best estimator xgboost's best error=0.0514
[flaml.automl: 09-17 00:33:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 00:33:42] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.0514,	best estimator xgboost's best error=0.0514
[flaml.automl: 09-17 00:33:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 00:33:45] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.0458,	best estimator xgboost's best error=0.0458
[flaml.automl: 09-17 00:33:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 00:33:49] {3072} INFO -  at 32.5s,	estimator xgboost's best error=0.0447,	best estimator xgboost's best error=0.0447
[flaml.automl: 09-17 00:33:49] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 00:33:52] {3072} INFO -  at 35.3s,	estimator xgboost's best error=0.0447,	best estimator xgboost's best error=0.0447
[flaml.automl: 09-17 00:33:52] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 00:33:54] {3072} INFO -  at 37.5s,	estimator xgboost's best error=0.0447,	best estimator xgboost's best error=0.0447
[flaml.automl: 09-17 00:33:54] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 00:33:56] {3072} INFO -  at 39.9s,	estimator xgboost's best error=0.0447,	best estimator xgboost's best error=0.0447
[flaml.automl: 09-17 00:33:56] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 00:33:58] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.0447,	best estimator xgboost's best error=0.0447
[flaml.automl: 09-17 00:33:58] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 00:34:11] {3072} INFO -  at 54.8s,	estimator xgboost's best error=0.0407,	best estimator xgboost's best error=0.0407
[flaml.automl: 09-17 00:34:24] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 00:34:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 00:34:24] {2636} INFO - fit succeeded
[flaml.automl: 09-17 00:34:24] {2637} INFO - Time taken to find the best model: 54.756046772003174
[flaml.automl: 09-17 00:34:24] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 42197}
CO(0)最佳损失：0.9593145792946731
CO(0)最好结果：{'pred_time': 8.677388844293785e-06, 'wall_clock_time': 54.756046772003174, 'metric_for_logging': {'pred_time': 8.677388844293785e-06}, 'val_loss': 0.040685420705326894, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 42197}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'config/FLAML_sample_size': 42197, 'experiment_tag': 'exp', 'time_total_s': 12.797358512878418}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8767912269263222
CO(0)的mse=0.004902064983080559
CO(0)的mae=0.040603244942747016
CO(0)的mar=0.1178194883806633
总共花费的时间为：68.30
西安市
1462A
1463A
1464A
1465A
1466A
1468A
1474A
3524A
3605A
[flaml.automl: 09-17 01:01:04] {2390} INFO - task = regression
[flaml.automl: 09-17 01:01:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:01:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:01:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:01:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:01:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:01:06] {3025} INFO - Estimated sufficient time budget=128086s. Estimated necessary time budget=128s.
[flaml.automl: 09-17 01:01:06] {3072} INFO -  at 1.8s,	estimator xgboost's best error=0.1850,	best estimator xgboost's best error=0.1850
[flaml.automl: 09-17 01:01:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:01:08] {3072} INFO -  at 3.9s,	estimator xgboost's best error=0.1065,	best estimator xgboost's best error=0.1065
[flaml.automl: 09-17 01:01:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:01:09] {3072} INFO -  at 5.1s,	estimator xgboost's best error=0.1065,	best estimator xgboost's best error=0.1065
[flaml.automl: 09-17 01:01:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:01:11] {3072} INFO -  at 7.4s,	estimator xgboost's best error=0.1065,	best estimator xgboost's best error=0.1065
[flaml.automl: 09-17 01:01:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:01:12] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.0812,	best estimator xgboost's best error=0.0812
[flaml.automl: 09-17 01:01:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:01:14] {3072} INFO -  at 10.1s,	estimator xgboost's best error=0.0684,	best estimator xgboost's best error=0.0684
[flaml.automl: 09-17 01:01:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:01:15] {3072} INFO -  at 11.7s,	estimator xgboost's best error=0.0684,	best estimator xgboost's best error=0.0684
[flaml.automl: 09-17 01:01:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:01:17] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.0684,	best estimator xgboost's best error=0.0684
[flaml.automl: 09-17 01:01:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:01:19] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0684,	best estimator xgboost's best error=0.0684
[flaml.automl: 09-17 01:01:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:01:21] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.0684,	best estimator xgboost's best error=0.0684
[flaml.automl: 09-17 01:01:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:01:22] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.0684,	best estimator xgboost's best error=0.0684
[flaml.automl: 09-17 01:01:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:01:23] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0684,	best estimator xgboost's best error=0.0684
[flaml.automl: 09-17 01:01:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:01:30] {3072} INFO -  at 26.2s,	estimator xgboost's best error=0.0589,	best estimator xgboost's best error=0.0589
[flaml.automl: 09-17 01:01:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:01:42] {3072} INFO -  at 38.4s,	estimator xgboost's best error=0.0574,	best estimator xgboost's best error=0.0574
[flaml.automl: 09-17 01:01:42] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:01:49] {3072} INFO -  at 45.0s,	estimator xgboost's best error=0.0574,	best estimator xgboost's best error=0.0574
[flaml.automl: 09-17 01:01:49] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 01:02:04] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.0573,	best estimator xgboost's best error=0.0573
[flaml.automl: 09-17 01:02:25] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-17 01:02:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:02:25] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:02:25] {2637} INFO - Time taken to find the best model: 59.783445835113525
[flaml.automl: 09-17 01:02:25] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 97839}
CO(0)最佳损失：0.9427278472684582
CO(0)最好结果：{'pred_time': 3.818578804302426e-06, 'wall_clock_time': 59.783445835113525, 'metric_for_logging': {'pred_time': 3.818578804302426e-06}, 'val_loss': 0.057272152731541855, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 97839}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 97839, 'experiment_tag': 'exp', 'time_total_s': 14.79647183418274}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9009818391792667
CO(0)的mse=0.009069516988200532
CO(0)的mae=0.05893468812213886
CO(0)的mar=0.09244204616751446
总共花费的时间为：82.88
兰州市
1478A
3186A
3241A
3242A
[flaml.automl: 09-17 01:15:59] {2390} INFO - task = regression
[flaml.automl: 09-17 01:15:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:15:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:15:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:15:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:15:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:16:01] {3025} INFO - Estimated sufficient time budget=97283s. Estimated necessary time budget=97s.
[flaml.automl: 09-17 01:16:01] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.2499,	best estimator xgboost's best error=0.2499
[flaml.automl: 09-17 01:16:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:16:05] {3072} INFO -  at 6.4s,	estimator xgboost's best error=0.1615,	best estimator xgboost's best error=0.1615
[flaml.automl: 09-17 01:16:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:16:07] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.1615,	best estimator xgboost's best error=0.1615
[flaml.automl: 09-17 01:16:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:16:13] {3072} INFO -  at 14.4s,	estimator xgboost's best error=0.1615,	best estimator xgboost's best error=0.1615
[flaml.automl: 09-17 01:16:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:16:14] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.1362,	best estimator xgboost's best error=0.1362
[flaml.automl: 09-17 01:16:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:16:15] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.1234,	best estimator xgboost's best error=0.1234
[flaml.automl: 09-17 01:16:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:16:17] {3072} INFO -  at 18.7s,	estimator xgboost's best error=0.1234,	best estimator xgboost's best error=0.1234
[flaml.automl: 09-17 01:16:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:16:19] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.1234,	best estimator xgboost's best error=0.1234
[flaml.automl: 09-17 01:16:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:16:21] {3072} INFO -  at 22.2s,	estimator xgboost's best error=0.1234,	best estimator xgboost's best error=0.1234
[flaml.automl: 09-17 01:16:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:16:23] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.1234,	best estimator xgboost's best error=0.1234
[flaml.automl: 09-17 01:16:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:16:25] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.1234,	best estimator xgboost's best error=0.1234
[flaml.automl: 09-17 01:16:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:16:26] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.1234,	best estimator xgboost's best error=0.1234
[flaml.automl: 09-17 01:16:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:16:33] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.1175,	best estimator xgboost's best error=0.1175
[flaml.automl: 09-17 01:16:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:16:45] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.1116,	best estimator xgboost's best error=0.1116
[flaml.automl: 09-17 01:16:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:16:55] {3072} INFO -  at 56.6s,	estimator xgboost's best error=0.1116,	best estimator xgboost's best error=0.1116
[flaml.automl: 09-17 01:17:15] {3335} INFO - retrain xgboost for 20.1s
[flaml.automl: 09-17 01:17:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:17:15] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:17:15] {2637} INFO - Time taken to find the best model: 46.425753355026245
[flaml.automl: 09-17 01:17:15] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42674}
CO(0)最佳损失：0.8883669323805313
CO(0)最好结果：{'pred_time': 1.6353694460453337e-05, 'wall_clock_time': 46.425753355026245, 'metric_for_logging': {'pred_time': 1.6353694460453337e-05}, 'val_loss': 0.11163306761946871, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42674}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42674, 'experiment_tag': 'exp', 'time_total_s': 12.284561395645142}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8283056144639365
CO(0)的mse=0.030865285831934583
CO(0)的mae=0.1081133595020863
CO(0)的mar=0.19079152076930905
总共花费的时间为：77.53
西宁市
3629A
3630A
[flaml.automl: 09-17 01:23:35] {2390} INFO - task = regression
[flaml.automl: 09-17 01:23:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:23:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:23:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:23:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:23:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:23:36] {3025} INFO - Estimated sufficient time budget=12135s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 01:23:36] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.3858,	best estimator xgboost's best error=0.3858
[flaml.automl: 09-17 01:23:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:23:38] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.2507,	best estimator xgboost's best error=0.2507
[flaml.automl: 09-17 01:23:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:23:39] {3072} INFO -  at 4.4s,	estimator xgboost's best error=0.2507,	best estimator xgboost's best error=0.2507
[flaml.automl: 09-17 01:23:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:23:48] {3072} INFO -  at 12.8s,	estimator xgboost's best error=0.2507,	best estimator xgboost's best error=0.2507
[flaml.automl: 09-17 01:23:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:23:49] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.1986,	best estimator xgboost's best error=0.1986
[flaml.automl: 09-17 01:23:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:23:50] {3072} INFO -  at 15.6s,	estimator xgboost's best error=0.1831,	best estimator xgboost's best error=0.1831
[flaml.automl: 09-17 01:23:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:23:52] {3072} INFO -  at 17.2s,	estimator xgboost's best error=0.1831,	best estimator xgboost's best error=0.1831
[flaml.automl: 09-17 01:23:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:23:54] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.1831,	best estimator xgboost's best error=0.1831
[flaml.automl: 09-17 01:23:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:23:56] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.1831,	best estimator xgboost's best error=0.1831
[flaml.automl: 09-17 01:23:56] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:23:58] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.1831,	best estimator xgboost's best error=0.1831
[flaml.automl: 09-17 01:23:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:23:59] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.1831,	best estimator xgboost's best error=0.1831
[flaml.automl: 09-17 01:23:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:24:00] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.1831,	best estimator xgboost's best error=0.1831
[flaml.automl: 09-17 01:24:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:24:06] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.1789,	best estimator xgboost's best error=0.1789
[flaml.automl: 09-17 01:24:06] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:24:17] {3072} INFO -  at 42.1s,	estimator xgboost's best error=0.1673,	best estimator xgboost's best error=0.1673
[flaml.automl: 09-17 01:24:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:24:23] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.1673,	best estimator xgboost's best error=0.1673
[flaml.automl: 09-17 01:24:23] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 01:24:34] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.1647,	best estimator xgboost's best error=0.1647
[flaml.automl: 09-17 01:24:52] {3335} INFO - retrain xgboost for 17.2s
[flaml.automl: 09-17 01:24:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:24:52] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:24:52] {2637} INFO - Time taken to find the best model: 59.6929132938385
[flaml.automl: 09-17 01:24:52] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.835282331901138
CO(0)最好结果：{'pred_time': 1.6827354264571678e-05, 'wall_clock_time': 59.6929132938385, 'metric_for_logging': {'pred_time': 1.6827354264571678e-05}, 'val_loss': 0.16471766809886199, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 11.587869644165039}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7246280646605081
CO(0)的mse=0.08636323095460963
CO(0)的mae=0.16831945265520293
CO(0)的mar=0.23473337663731342
总共花费的时间为：77.38
银川市
1484A
1488A
2925A
2926A
3523A
[flaml.automl: 09-17 01:40:28] {2390} INFO - task = regression
[flaml.automl: 09-17 01:40:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:40:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:40:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:40:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:40:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:40:30] {3025} INFO - Estimated sufficient time budget=61752s. Estimated necessary time budget=62s.
[flaml.automl: 09-17 01:40:30] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1823,	best estimator xgboost's best error=0.1823
[flaml.automl: 09-17 01:40:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:40:32] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1146,	best estimator xgboost's best error=0.1146
[flaml.automl: 09-17 01:40:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:40:33] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1146,	best estimator xgboost's best error=0.1146
[flaml.automl: 09-17 01:40:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:40:38] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.1146,	best estimator xgboost's best error=0.1146
[flaml.automl: 09-17 01:40:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:40:39] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0935,	best estimator xgboost's best error=0.0935
[flaml.automl: 09-17 01:40:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:40:40] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0824,	best estimator xgboost's best error=0.0824
[flaml.automl: 09-17 01:40:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:40:42] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0824,	best estimator xgboost's best error=0.0824
[flaml.automl: 09-17 01:40:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:40:45] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0824,	best estimator xgboost's best error=0.0824
[flaml.automl: 09-17 01:40:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:40:46] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0824,	best estimator xgboost's best error=0.0824
[flaml.automl: 09-17 01:40:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:40:48] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0824,	best estimator xgboost's best error=0.0824
[flaml.automl: 09-17 01:40:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:40:50] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0816,	best estimator xgboost's best error=0.0816
[flaml.automl: 09-17 01:40:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:40:51] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0816,	best estimator xgboost's best error=0.0816
[flaml.automl: 09-17 01:40:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:40:58] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-17 01:40:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:41:10] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0736,	best estimator xgboost's best error=0.0736
[flaml.automl: 09-17 01:41:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:41:16] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.0736,	best estimator xgboost's best error=0.0736
[flaml.automl: 09-17 01:41:28] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 01:41:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:41:28] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:41:28] {2637} INFO - Time taken to find the best model: 41.67848443984985
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51088}
CO(0)最佳损失：0.9264301852676489
CO(0)最好结果：{'pred_time': 7.759880091452316e-06, 'wall_clock_time': 41.67848443984985, 'metric_for_logging': {'pred_time': 7.759880091452316e-06}, 'val_loss': 0.07356981473235108, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51088}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51088, 'experiment_tag': 'exp', 'time_total_s': 12.105560779571533}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8656756173593205
CO(0)的mse=0.01531605676036618
CO(0)的mae=0.0748749112530305
CO(0)的mar=0.16226924893882488
总共花费的时间为：61.17
乌鲁木齐市
1491A
3033A
3437A
3438A
3439A
3440A
[flaml.automl: 09-17 02:00:28] {2390} INFO - task = regression
[flaml.automl: 09-17 02:00:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:00:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:00:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:00:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:00:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:00:29] {3025} INFO - Estimated sufficient time budget=76917s. Estimated necessary time budget=77s.
[flaml.automl: 09-17 02:00:29] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.2344,	best estimator xgboost's best error=0.2344
[flaml.automl: 09-17 02:00:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:00:31] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1353,	best estimator xgboost's best error=0.1353
[flaml.automl: 09-17 02:00:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:00:32] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1353,	best estimator xgboost's best error=0.1353
[flaml.automl: 09-17 02:00:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:00:36] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.1353,	best estimator xgboost's best error=0.1353
[flaml.automl: 09-17 02:00:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:00:37] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.1012,	best estimator xgboost's best error=0.1012
[flaml.automl: 09-17 02:00:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:00:39] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.0849,	best estimator xgboost's best error=0.0849
[flaml.automl: 09-17 02:00:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:00:40] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.0849,	best estimator xgboost's best error=0.0849
[flaml.automl: 09-17 02:00:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:00:43] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0849,	best estimator xgboost's best error=0.0849
[flaml.automl: 09-17 02:00:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:00:44] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0849,	best estimator xgboost's best error=0.0849
[flaml.automl: 09-17 02:00:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:00:47] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0849,	best estimator xgboost's best error=0.0849
[flaml.automl: 09-17 02:00:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:00:48] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.0838,	best estimator xgboost's best error=0.0838
[flaml.automl: 09-17 02:00:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:00:49] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0838,	best estimator xgboost's best error=0.0838
[flaml.automl: 09-17 02:00:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:00:56] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.0764,	best estimator xgboost's best error=0.0764
[flaml.automl: 09-17 02:00:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:01:08] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.0727,	best estimator xgboost's best error=0.0727
[flaml.automl: 09-17 02:01:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 02:01:15] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.0727,	best estimator xgboost's best error=0.0727
[flaml.automl: 09-17 02:01:15] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 02:01:27] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.0715,	best estimator xgboost's best error=0.0715
[flaml.automl: 09-17 02:01:49] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-17 02:01:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:01:49] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:01:49] {2637} INFO - Time taken to find the best model: 59.81373572349548
[flaml.automl: 09-17 02:01:49] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 65110}
CO(0)最佳损失：0.9284646478848039
CO(0)最好结果：{'pred_time': 5.576885726918166e-06, 'wall_clock_time': 59.81373572349548, 'metric_for_logging': {'pred_time': 5.576885726918166e-06}, 'val_loss': 0.07153535211519611, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 65110}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 65110, 'experiment_tag': 'exp', 'time_total_s': 12.528192281723022}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9251321907974774
CO(0)的mse=0.01760701959578112
CO(0)的mae=0.07077733197976203
CO(0)的mar=0.1291603326839839
总共花费的时间为：82.33
湘潭市
1508A
1511A
1512A
1513A
1514A
1564A
[flaml.automl: 09-17 02:20:27] {2390} INFO - task = regression
[flaml.automl: 09-17 02:20:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:20:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:20:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:20:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:20:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:20:28] {3025} INFO - Estimated sufficient time budget=75016s. Estimated necessary time budget=75s.
[flaml.automl: 09-17 02:20:28] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1897,	best estimator xgboost's best error=0.1897
[flaml.automl: 09-17 02:20:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:20:30] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.1131,	best estimator xgboost's best error=0.1131
[flaml.automl: 09-17 02:20:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:20:31] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.1131,	best estimator xgboost's best error=0.1131
[flaml.automl: 09-17 02:20:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:20:36] {3072} INFO -  at 9.2s,	estimator xgboost's best error=0.1131,	best estimator xgboost's best error=0.1131
[flaml.automl: 09-17 02:20:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:20:37] {3072} INFO -  at 10.3s,	estimator xgboost's best error=0.0887,	best estimator xgboost's best error=0.0887
[flaml.automl: 09-17 02:20:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:20:38] {3072} INFO -  at 11.9s,	estimator xgboost's best error=0.0781,	best estimator xgboost's best error=0.0781
[flaml.automl: 09-17 02:20:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:20:40] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.0781,	best estimator xgboost's best error=0.0781
[flaml.automl: 09-17 02:20:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:20:42] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.0781,	best estimator xgboost's best error=0.0781
[flaml.automl: 09-17 02:20:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:20:44] {3072} INFO -  at 17.2s,	estimator xgboost's best error=0.0781,	best estimator xgboost's best error=0.0781
[flaml.automl: 09-17 02:20:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:20:46] {3072} INFO -  at 19.8s,	estimator xgboost's best error=0.0781,	best estimator xgboost's best error=0.0781
[flaml.automl: 09-17 02:20:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:20:48] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0781,	best estimator xgboost's best error=0.0781
[flaml.automl: 09-17 02:20:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:20:49] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0781,	best estimator xgboost's best error=0.0781
[flaml.automl: 09-17 02:20:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:20:56] {3072} INFO -  at 29.2s,	estimator xgboost's best error=0.0727,	best estimator xgboost's best error=0.0727
[flaml.automl: 09-17 02:20:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:21:12] {3072} INFO -  at 45.4s,	estimator xgboost's best error=0.0716,	best estimator xgboost's best error=0.0716
[flaml.automl: 09-17 02:21:34] {3335} INFO - retrain xgboost for 22.3s
[flaml.automl: 09-17 02:21:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:21:34] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:21:34] {2637} INFO - Time taken to find the best model: 45.430493116378784
[flaml.automl: 09-17 02:21:34] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 62365}
CO(0)最佳损失：0.9283964079351222
CO(0)最好结果：{'pred_time': 1.2011892695791622e-05, 'wall_clock_time': 45.430493116378784, 'metric_for_logging': {'pred_time': 1.2011892695791622e-05}, 'val_loss': 0.07160359206487778, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 62365}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 62365, 'experiment_tag': 'exp', 'time_total_s': 16.19432282447815}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7584663593886628
CO(0)的mse=0.013825897821925512
CO(0)的mae=0.069279551630771
CO(0)的mar=0.10290860442985843
总共花费的时间为：68.79
株洲市
1515A
1518A
1520A
1524A
1559A
2031A
[flaml.automl: 09-17 02:39:47] {2390} INFO - task = regression
[flaml.automl: 09-17 02:39:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:39:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:39:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:39:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:39:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:39:48] {3025} INFO - Estimated sufficient time budget=75582s. Estimated necessary time budget=76s.
[flaml.automl: 09-17 02:39:48] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.1508,	best estimator xgboost's best error=0.1508
[flaml.automl: 09-17 02:39:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:39:51] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 02:39:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:39:52] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 02:39:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:39:56] {3072} INFO -  at 8.7s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 02:39:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:39:57] {3072} INFO -  at 9.8s,	estimator xgboost's best error=0.0703,	best estimator xgboost's best error=0.0703
[flaml.automl: 09-17 02:39:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:39:58] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.0608,	best estimator xgboost's best error=0.0608
[flaml.automl: 09-17 02:39:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:40:00] {3072} INFO -  at 13.0s,	estimator xgboost's best error=0.0608,	best estimator xgboost's best error=0.0608
[flaml.automl: 09-17 02:40:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:40:02] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.0608,	best estimator xgboost's best error=0.0608
[flaml.automl: 09-17 02:40:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:40:03] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.0608,	best estimator xgboost's best error=0.0608
[flaml.automl: 09-17 02:40:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:40:06] {3072} INFO -  at 19.3s,	estimator xgboost's best error=0.0608,	best estimator xgboost's best error=0.0608
[flaml.automl: 09-17 02:40:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:40:08] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-17 02:40:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:40:09] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-17 02:40:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:40:16] {3072} INFO -  at 28.7s,	estimator xgboost's best error=0.0557,	best estimator xgboost's best error=0.0557
[flaml.automl: 09-17 02:40:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:40:28] {3072} INFO -  at 40.8s,	estimator xgboost's best error=0.0532,	best estimator xgboost's best error=0.0532
[flaml.automl: 09-17 02:40:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 02:40:34] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.0532,	best estimator xgboost's best error=0.0532
[flaml.automl: 09-17 02:40:34] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 02:40:47] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.0532,	best estimator xgboost's best error=0.0532
[flaml.automl: 09-17 02:40:59] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 02:40:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:40:59] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:40:59] {2637} INFO - Time taken to find the best model: 40.81754541397095
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 62836}
CO(0)最佳损失：0.9468126829580024
CO(0)最好结果：{'pred_time': 5.664780364614338e-06, 'wall_clock_time': 40.81754541397095, 'metric_for_logging': {'pred_time': 5.664780364614338e-06}, 'val_loss': 0.05318731704199769, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 62836}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 62836, 'experiment_tag': 'exp', 'time_total_s': 12.14050555229187}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8548903518466348
CO(0)的mse=0.007867604282836034
CO(0)的mae=0.0541686419643333
CO(0)的mar=0.10386443732709491
总共花费的时间为：72.92
包头市
1585A
3283A
3419A
3683A
[flaml.automl: 09-17 02:53:20] {2390} INFO - task = regression
[flaml.automl: 09-17 02:53:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:53:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:53:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:53:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:53:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:53:21] {3025} INFO - Estimated sufficient time budget=51456s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 02:53:21] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2875,	best estimator xgboost's best error=0.2875
[flaml.automl: 09-17 02:53:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:53:23] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1931,	best estimator xgboost's best error=0.1931
[flaml.automl: 09-17 02:53:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:53:24] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1931,	best estimator xgboost's best error=0.1931
[flaml.automl: 09-17 02:53:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:53:30] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.1931,	best estimator xgboost's best error=0.1931
[flaml.automl: 09-17 02:53:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:53:31] {3072} INFO -  at 12.1s,	estimator xgboost's best error=0.1680,	best estimator xgboost's best error=0.1680
[flaml.automl: 09-17 02:53:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:53:33] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.1526,	best estimator xgboost's best error=0.1526
[flaml.automl: 09-17 02:53:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:53:35] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.1526,	best estimator xgboost's best error=0.1526
[flaml.automl: 09-17 02:53:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:53:37] {3072} INFO -  at 17.7s,	estimator xgboost's best error=0.1526,	best estimator xgboost's best error=0.1526
[flaml.automl: 09-17 02:53:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:53:38] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.1526,	best estimator xgboost's best error=0.1526
[flaml.automl: 09-17 02:53:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:53:41] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.1526,	best estimator xgboost's best error=0.1526
[flaml.automl: 09-17 02:53:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:53:43] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.1512,	best estimator xgboost's best error=0.1512
[flaml.automl: 09-17 02:53:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:53:44] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.1512,	best estimator xgboost's best error=0.1512
[flaml.automl: 09-17 02:53:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:53:50] {3072} INFO -  at 30.8s,	estimator xgboost's best error=0.1472,	best estimator xgboost's best error=0.1472
[flaml.automl: 09-17 02:53:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:54:02] {3072} INFO -  at 42.9s,	estimator xgboost's best error=0.1428,	best estimator xgboost's best error=0.1428
[flaml.automl: 09-17 02:54:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 02:54:09] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.1428,	best estimator xgboost's best error=0.1428
[flaml.automl: 09-17 02:54:21] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 02:54:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:54:21] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:54:21] {2637} INFO - Time taken to find the best model: 42.92560338973999
[flaml.automl: 09-17 02:54:21] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43227}
CO(0)最佳损失：0.8572496756269563
CO(0)最好结果：{'pred_time': 8.395729327038033e-06, 'wall_clock_time': 42.92560338973999, 'metric_for_logging': {'pred_time': 8.395729327038033e-06}, 'val_loss': 0.1427503243730437, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43227}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43227, 'experiment_tag': 'exp', 'time_total_s': 12.083125114440918}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7003902126136663
CO(0)的mse=0.055779356202466455
CO(0)的mae=0.1390627535105824
CO(0)的mar=0.17192990144252956
总共花费的时间为：62.21
鄂尔多斯市
1591A
1592A
1594A
1595A
[flaml.automl: 09-17 03:07:15] {2390} INFO - task = regression
[flaml.automl: 09-17 03:07:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:07:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:07:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:07:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:07:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:07:16] {3025} INFO - Estimated sufficient time budget=51635s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 03:07:16] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1342,	best estimator xgboost's best error=0.1342
[flaml.automl: 09-17 03:07:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:07:18] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0917,	best estimator xgboost's best error=0.0917
[flaml.automl: 09-17 03:07:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:07:19] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0917,	best estimator xgboost's best error=0.0917
[flaml.automl: 09-17 03:07:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:07:25] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.0917,	best estimator xgboost's best error=0.0917
[flaml.automl: 09-17 03:07:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:07:26] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.0810,	best estimator xgboost's best error=0.0810
[flaml.automl: 09-17 03:07:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:07:28] {3072} INFO -  at 13.3s,	estimator xgboost's best error=0.0722,	best estimator xgboost's best error=0.0722
[flaml.automl: 09-17 03:07:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:07:29] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0722,	best estimator xgboost's best error=0.0722
[flaml.automl: 09-17 03:07:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:07:32] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0722,	best estimator xgboost's best error=0.0722
[flaml.automl: 09-17 03:07:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:07:33] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0722,	best estimator xgboost's best error=0.0722
[flaml.automl: 09-17 03:07:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:07:36] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0722,	best estimator xgboost's best error=0.0722
[flaml.automl: 09-17 03:07:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:07:37] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0721,	best estimator xgboost's best error=0.0721
[flaml.automl: 09-17 03:07:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:07:39] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0721,	best estimator xgboost's best error=0.0721
[flaml.automl: 09-17 03:07:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:07:45] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0676,	best estimator xgboost's best error=0.0676
[flaml.automl: 09-17 03:07:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:07:57] {3072} INFO -  at 42.7s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-17 03:07:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 03:08:04] {3072} INFO -  at 49.2s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-17 03:08:16] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 03:08:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:08:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:08:16] {2637} INFO - Time taken to find the best model: 42.65197682380676
[flaml.automl: 09-17 03:08:16] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42869}
CO(0)最佳损失：0.9346160073462502
CO(0)最好结果：{'pred_time': 8.80872572699482e-06, 'wall_clock_time': 42.65197682380676, 'metric_for_logging': {'pred_time': 8.80872572699482e-06}, 'val_loss': 0.06538399265374986, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42869}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42869, 'experiment_tag': 'exp', 'time_total_s': 12.115178346633911}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7610260660846921
CO(0)的mse=0.010229475216258874
CO(0)的mae=0.06533700564640492
CO(0)的mar=0.10523890559822888
总共花费的时间为：61.96
营口市
1598A
3378A
3379A
3866A
[flaml.automl: 09-17 03:21:47] {2390} INFO - task = regression
[flaml.automl: 09-17 03:21:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:21:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:21:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:21:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:21:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:21:48] {3025} INFO - Estimated sufficient time budget=12280s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:21:48] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.3048,	best estimator xgboost's best error=0.3048
[flaml.automl: 09-17 03:21:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:21:51] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1971,	best estimator xgboost's best error=0.1971
[flaml.automl: 09-17 03:21:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:21:52] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1971,	best estimator xgboost's best error=0.1971
[flaml.automl: 09-17 03:21:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:22:02] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.1971,	best estimator xgboost's best error=0.1971
[flaml.automl: 09-17 03:22:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:22:04] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.1775,	best estimator xgboost's best error=0.1775
[flaml.automl: 09-17 03:22:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:22:06] {3072} INFO -  at 19.3s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-17 03:22:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:22:09] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-17 03:22:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:22:13] {3072} INFO -  at 26.3s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-17 03:22:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:22:16] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-17 03:22:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:22:22] {3072} INFO -  at 35.2s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-17 03:22:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:22:25] {3072} INFO -  at 37.9s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-17 03:22:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:22:28] {3072} INFO -  at 40.6s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-17 03:22:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:22:42] {3072} INFO -  at 55.3s,	estimator xgboost's best error=0.1613,	best estimator xgboost's best error=0.1613
[flaml.automl: 09-17 03:22:57] {3335} INFO - retrain xgboost for 15.0s
[flaml.automl: 09-17 03:22:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 03:22:57] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:22:57] {2637} INFO - Time taken to find the best model: 55.319711446762085
[flaml.automl: 09-17 03:22:57] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.838683130927547
CO(0)最好结果：{'pred_time': 2.9643572077341974e-05, 'wall_clock_time': 55.319711446762085, 'metric_for_logging': {'pred_time': 2.9643572077341974e-05}, 'val_loss': 0.16131686907245305, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 14.747359037399292}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.6014349473571936
CO(0)的mse=0.08093399301754983
CO(0)的mae=0.15850732067272713
CO(0)的mar=0.2470357220987455
总共花费的时间为：71.08
丹东市
1600A
1602A
1603A
[flaml.automl: 09-17 03:32:21] {2390} INFO - task = regression
[flaml.automl: 09-17 03:32:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:32:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:32:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:32:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:32:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:32:23] {3025} INFO - Estimated sufficient time budget=12274s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:32:23] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1930,	best estimator xgboost's best error=0.1930
[flaml.automl: 09-17 03:32:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:32:25] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1232,	best estimator xgboost's best error=0.1232
[flaml.automl: 09-17 03:32:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:32:26] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1232,	best estimator xgboost's best error=0.1232
[flaml.automl: 09-17 03:32:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:32:36] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.1232,	best estimator xgboost's best error=0.1232
[flaml.automl: 09-17 03:32:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:32:37] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.1078,	best estimator xgboost's best error=0.1078
[flaml.automl: 09-17 03:32:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:32:39] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0965,	best estimator xgboost's best error=0.0965
[flaml.automl: 09-17 03:32:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:32:40] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0965,	best estimator xgboost's best error=0.0965
[flaml.automl: 09-17 03:32:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:32:43] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0965,	best estimator xgboost's best error=0.0965
[flaml.automl: 09-17 03:32:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:32:44] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0965,	best estimator xgboost's best error=0.0965
[flaml.automl: 09-17 03:32:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:32:47] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0965,	best estimator xgboost's best error=0.0965
[flaml.automl: 09-17 03:32:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:32:48] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0965,	best estimator xgboost's best error=0.0965
[flaml.automl: 09-17 03:32:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:32:49] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0965,	best estimator xgboost's best error=0.0965
[flaml.automl: 09-17 03:32:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:32:55] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0945,	best estimator xgboost's best error=0.0945
[flaml.automl: 09-17 03:32:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:33:08] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.0880,	best estimator xgboost's best error=0.0880
[flaml.automl: 09-17 03:33:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 03:33:14] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.0880,	best estimator xgboost's best error=0.0880
[flaml.automl: 09-17 03:33:32] {3335} INFO - retrain xgboost for 17.6s
[flaml.automl: 09-17 03:33:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:33:32] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:33:32] {2637} INFO - Time taken to find the best model: 46.37755298614502
[flaml.automl: 09-17 03:33:32] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9120338529786258
CO(0)最好结果：{'pred_time': 1.1042576095865116e-05, 'wall_clock_time': 46.37755298614502, 'metric_for_logging': {'pred_time': 1.1042576095865116e-05}, 'val_loss': 0.08796614702137419, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.069522619247437}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8097844326996765
CO(0)的mse=0.01985808506258413
CO(0)的mae=0.0865334786702824
CO(0)的mar=0.11636362039396159
总共花费的时间为：71.03
盘锦市
1604A
1605A
[flaml.automl: 09-17 03:40:05] {2390} INFO - task = regression
[flaml.automl: 09-17 03:40:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:40:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:40:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:40:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:40:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:40:06] {3025} INFO - Estimated sufficient time budget=12120s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:40:06] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.2245,	best estimator xgboost's best error=0.2245
[flaml.automl: 09-17 03:40:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:40:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1220,	best estimator xgboost's best error=0.1220
[flaml.automl: 09-17 03:40:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:40:10] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1220,	best estimator xgboost's best error=0.1220
[flaml.automl: 09-17 03:40:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:40:19] {3072} INFO -  at 14.2s,	estimator xgboost's best error=0.1220,	best estimator xgboost's best error=0.1220
[flaml.automl: 09-17 03:40:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:40:20] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-17 03:40:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:40:22] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0825,	best estimator xgboost's best error=0.0825
[flaml.automl: 09-17 03:40:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:40:23] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0825,	best estimator xgboost's best error=0.0825
[flaml.automl: 09-17 03:40:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:40:28] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0825,	best estimator xgboost's best error=0.0825
[flaml.automl: 09-17 03:40:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:40:30] {3072} INFO -  at 25.0s,	estimator xgboost's best error=0.0825,	best estimator xgboost's best error=0.0825
[flaml.automl: 09-17 03:40:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:40:35] {3072} INFO -  at 29.7s,	estimator xgboost's best error=0.0825,	best estimator xgboost's best error=0.0825
[flaml.automl: 09-17 03:40:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:40:37] {3072} INFO -  at 31.8s,	estimator xgboost's best error=0.0825,	best estimator xgboost's best error=0.0825
[flaml.automl: 09-17 03:40:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:40:39] {3072} INFO -  at 33.9s,	estimator xgboost's best error=0.0825,	best estimator xgboost's best error=0.0825
[flaml.automl: 09-17 03:40:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:40:50] {3072} INFO -  at 45.3s,	estimator xgboost's best error=0.0793,	best estimator xgboost's best error=0.0793
[flaml.automl: 09-17 03:40:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:41:04] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0763,	best estimator xgboost's best error=0.0763
[flaml.automl: 09-17 03:41:23] {3335} INFO - retrain xgboost for 19.2s
[flaml.automl: 09-17 03:41:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:41:23] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:41:23] {2637} INFO - Time taken to find the best model: 59.35611867904663
[flaml.automl: 09-17 03:41:23] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9236988561766118
CO(0)最好结果：{'pred_time': 2.781400891408654e-05, 'wall_clock_time': 59.35611867904663, 'metric_for_logging': {'pred_time': 2.781400891408654e-05}, 'val_loss': 0.07630114382338812, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 14.095433473587036}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8149357761448205
CO(0)的mse=0.01573182424981364
CO(0)的mae=0.07427111864430053
CO(0)的mar=0.08319029809393678
总共花费的时间为：79.01
葫芦岛市
1607A
[flaml.automl: 09-17 03:45:12] {2390} INFO - task = regression
[flaml.automl: 09-17 03:45:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:45:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:45:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:45:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:45:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:45:15] {3025} INFO - Estimated sufficient time budget=23081s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 03:45:15] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.2130,	best estimator xgboost's best error=0.2130
[flaml.automl: 09-17 03:45:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:45:18] {3072} INFO -  at 5.9s,	estimator xgboost's best error=0.1545,	best estimator xgboost's best error=0.1545
[flaml.automl: 09-17 03:45:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:45:20] {3072} INFO -  at 8.0s,	estimator xgboost's best error=0.1545,	best estimator xgboost's best error=0.1545
[flaml.automl: 09-17 03:45:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:45:34] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.1545,	best estimator xgboost's best error=0.1545
[flaml.automl: 09-17 03:45:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:45:36] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.1233,	best estimator xgboost's best error=0.1233
[flaml.automl: 09-17 03:45:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:45:39] {3072} INFO -  at 26.3s,	estimator xgboost's best error=0.1203,	best estimator xgboost's best error=0.1203
[flaml.automl: 09-17 03:45:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:45:41] {3072} INFO -  at 28.7s,	estimator xgboost's best error=0.1193,	best estimator xgboost's best error=0.1193
[flaml.automl: 09-17 03:45:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:45:43] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.1193,	best estimator xgboost's best error=0.1193
[flaml.automl: 09-17 03:45:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:45:45] {3072} INFO -  at 32.6s,	estimator xgboost's best error=0.1193,	best estimator xgboost's best error=0.1193
[flaml.automl: 09-17 03:45:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:45:48] {3072} INFO -  at 35.2s,	estimator xgboost's best error=0.1141,	best estimator xgboost's best error=0.1141
[flaml.automl: 09-17 03:45:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:45:49] {3072} INFO -  at 36.8s,	estimator xgboost's best error=0.1141,	best estimator xgboost's best error=0.1141
[flaml.automl: 09-17 03:45:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:45:50] {3072} INFO -  at 37.9s,	estimator xgboost's best error=0.1141,	best estimator xgboost's best error=0.1141
[flaml.automl: 09-17 03:45:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:46:00] {3072} INFO -  at 47.4s,	estimator xgboost's best error=0.1123,	best estimator xgboost's best error=0.1123
[flaml.automl: 09-17 03:46:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:46:12] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.1047,	best estimator xgboost's best error=0.1047
[flaml.automl: 09-17 03:46:28] {3335} INFO - retrain xgboost for 15.7s
[flaml.automl: 09-17 03:46:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:46:28] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:46:28] {2637} INFO - Time taken to find the best model: 59.822195291519165
[flaml.automl: 09-17 03:46:28] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 25, 'max_leaves': 11, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
CO(0)最佳损失：0.8952588569229901
CO(0)最好结果：{'pred_time': 3.350643999247439e-05, 'wall_clock_time': 59.822195291519165, 'metric_for_logging': {'pred_time': 3.350643999247439e-05}, 'val_loss': 0.10474114307700987, 'training_iteration': 1, 'config': {'n_estimators': 25, 'max_leaves': 11, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 25, 'config/max_leaves': 11, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 12.460764646530151}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7281250986947625
CO(0)的mse=0.034559957213090485
CO(0)的mae=0.10570069661447495
CO(0)的mar=0.13747574295015738
总共花费的时间为：75.79
泉州市
1614A
3529A
[flaml.automl: 09-17 03:52:37] {2390} INFO - task = regression
[flaml.automl: 09-17 03:52:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:52:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:52:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:52:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:52:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:52:40] {3025} INFO - Estimated sufficient time budget=22673s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 03:52:40] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.0867,	best estimator xgboost's best error=0.0867
[flaml.automl: 09-17 03:52:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:52:43] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-17 03:52:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:52:45] {3072} INFO -  at 8.0s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-17 03:52:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:53:01] {3072} INFO -  at 23.7s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-17 03:53:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:53:03] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0338,	best estimator xgboost's best error=0.0338
[flaml.automl: 09-17 03:53:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:53:06] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.0283,	best estimator xgboost's best error=0.0283
[flaml.automl: 09-17 03:53:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:53:09] {3072} INFO -  at 32.0s,	estimator xgboost's best error=0.0283,	best estimator xgboost's best error=0.0283
[flaml.automl: 09-17 03:53:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:53:14] {3072} INFO -  at 36.6s,	estimator xgboost's best error=0.0283,	best estimator xgboost's best error=0.0283
[flaml.automl: 09-17 03:53:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:53:16] {3072} INFO -  at 38.7s,	estimator xgboost's best error=0.0283,	best estimator xgboost's best error=0.0283
[flaml.automl: 09-17 03:53:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:53:20] {3072} INFO -  at 42.4s,	estimator xgboost's best error=0.0283,	best estimator xgboost's best error=0.0283
[flaml.automl: 09-17 03:53:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:53:21] {3072} INFO -  at 43.6s,	estimator xgboost's best error=0.0283,	best estimator xgboost's best error=0.0283
[flaml.automl: 09-17 03:53:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:53:22] {3072} INFO -  at 44.7s,	estimator xgboost's best error=0.0283,	best estimator xgboost's best error=0.0283
[flaml.automl: 09-17 03:53:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:53:28] {3072} INFO -  at 50.7s,	estimator xgboost's best error=0.0258,	best estimator xgboost's best error=0.0258
[flaml.automl: 09-17 03:53:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:53:37] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0246,	best estimator xgboost's best error=0.0246
[flaml.automl: 09-17 03:53:47] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-17 03:53:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:53:47] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:53:47] {2637} INFO - Time taken to find the best model: 59.40219521522522
[flaml.automl: 09-17 03:53:47] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9753742967831684
CO(0)最好结果：{'pred_time': 1.7220156863036786e-05, 'wall_clock_time': 59.40219521522522, 'metric_for_logging': {'pred_time': 1.7220156863036786e-05}, 'val_loss': 0.02462570321683154, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 8.657233476638794}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9309586486864866
CO(0)的mse=0.0012954211609722975
CO(0)的mae=0.023887982187011454
CO(0)的mar=0.055317673867317105
总共花费的时间为：70.18
临沂市
1618A
1619A
1620A
3496A
3860A
[flaml.automl: 09-17 04:09:33] {2390} INFO - task = regression
[flaml.automl: 09-17 04:09:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:09:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:09:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:09:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:09:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:09:35] {3025} INFO - Estimated sufficient time budget=112378s. Estimated necessary time budget=112s.
[flaml.automl: 09-17 04:09:35] {3072} INFO -  at 2.6s,	estimator xgboost's best error=0.1916,	best estimator xgboost's best error=0.1916
[flaml.automl: 09-17 04:09:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:09:39] {3072} INFO -  at 6.6s,	estimator xgboost's best error=0.1170,	best estimator xgboost's best error=0.1170
[flaml.automl: 09-17 04:09:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:09:41] {3072} INFO -  at 8.8s,	estimator xgboost's best error=0.1170,	best estimator xgboost's best error=0.1170
[flaml.automl: 09-17 04:09:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:09:46] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.1170,	best estimator xgboost's best error=0.1170
[flaml.automl: 09-17 04:09:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:09:48] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0976,	best estimator xgboost's best error=0.0976
[flaml.automl: 09-17 04:09:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:09:51] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0861,	best estimator xgboost's best error=0.0861
[flaml.automl: 09-17 04:09:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:09:54] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0861,	best estimator xgboost's best error=0.0861
[flaml.automl: 09-17 04:09:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:09:58] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0861,	best estimator xgboost's best error=0.0861
[flaml.automl: 09-17 04:09:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:10:00] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.0861,	best estimator xgboost's best error=0.0861
[flaml.automl: 09-17 04:10:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:10:03] {3072} INFO -  at 30.2s,	estimator xgboost's best error=0.0861,	best estimator xgboost's best error=0.0861
[flaml.automl: 09-17 04:10:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:10:06] {3072} INFO -  at 33.2s,	estimator xgboost's best error=0.0848,	best estimator xgboost's best error=0.0848
[flaml.automl: 09-17 04:10:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:10:08] {3072} INFO -  at 35.4s,	estimator xgboost's best error=0.0848,	best estimator xgboost's best error=0.0848
[flaml.automl: 09-17 04:10:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:10:22] {3072} INFO -  at 49.8s,	estimator xgboost's best error=0.0798,	best estimator xgboost's best error=0.0798
[flaml.automl: 09-17 04:10:38] {3335} INFO - retrain xgboost for 15.5s
[flaml.automl: 09-17 04:10:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 04:10:38] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:10:38] {2637} INFO - Time taken to find the best model: 49.7788200378418
[flaml.automl: 09-17 04:10:38] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 47982}
CO(0)最佳损失：0.9202030973107808
CO(0)最好结果：{'pred_time': 2.3262624175407254e-05, 'wall_clock_time': 49.7788200378418, 'metric_for_logging': {'pred_time': 2.3262624175407254e-05}, 'val_loss': 0.07979690268921923, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 47982}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 47982, 'experiment_tag': 'exp', 'time_total_s': 14.361679315567017}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8119600421256763
CO(0)的mse=0.016336341730683098
CO(0)的mae=0.07970846550855823
CO(0)的mar=0.11268779477429093
总共花费的时间为：66.50
德州市
3066A
3372A
3511A
[flaml.automl: 09-17 04:20:46] {2390} INFO - task = regression
[flaml.automl: 09-17 04:20:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:20:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:20:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:20:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:20:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:20:47] {3025} INFO - Estimated sufficient time budget=12185s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 04:20:47] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1721,	best estimator xgboost's best error=0.1721
[flaml.automl: 09-17 04:20:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:20:49] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0997,	best estimator xgboost's best error=0.0997
[flaml.automl: 09-17 04:20:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:20:50] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0997,	best estimator xgboost's best error=0.0997
[flaml.automl: 09-17 04:20:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:21:00] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0997,	best estimator xgboost's best error=0.0997
[flaml.automl: 09-17 04:21:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:21:01] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-17 04:21:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:21:03] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0626,	best estimator xgboost's best error=0.0626
[flaml.automl: 09-17 04:21:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:21:05] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0626,	best estimator xgboost's best error=0.0626
[flaml.automl: 09-17 04:21:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:21:07] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0626,	best estimator xgboost's best error=0.0626
[flaml.automl: 09-17 04:21:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:21:08] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0626,	best estimator xgboost's best error=0.0626
[flaml.automl: 09-17 04:21:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:21:11] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0626,	best estimator xgboost's best error=0.0626
[flaml.automl: 09-17 04:21:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:21:12] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.0626,	best estimator xgboost's best error=0.0626
[flaml.automl: 09-17 04:21:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:21:13] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.0626,	best estimator xgboost's best error=0.0626
[flaml.automl: 09-17 04:21:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:21:20] {3072} INFO -  at 34.4s,	estimator xgboost's best error=0.0570,	best estimator xgboost's best error=0.0570
[flaml.automl: 09-17 04:21:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:21:32] {3072} INFO -  at 46.5s,	estimator xgboost's best error=0.0552,	best estimator xgboost's best error=0.0552
[flaml.automl: 09-17 04:21:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 04:21:42] {3072} INFO -  at 56.1s,	estimator xgboost's best error=0.0552,	best estimator xgboost's best error=0.0552
[flaml.automl: 09-17 04:22:04] {3335} INFO - retrain xgboost for 22.1s
[flaml.automl: 09-17 04:22:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:22:04] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:22:04] {2637} INFO - Time taken to find the best model: 46.463555097579956
[flaml.automl: 09-17 04:22:04] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9448401961813109
CO(0)最好结果：{'pred_time': 1.1574456945866354e-05, 'wall_clock_time': 46.463555097579956, 'metric_for_logging': {'pred_time': 1.1574456945866354e-05}, 'val_loss': 0.05515980381868908, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.101278305053711}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8939298580966747
CO(0)的mse=0.007745802820969173
CO(0)的mae=0.05654676304902437
CO(0)的mar=0.09964008758161047
总共花费的时间为：78.89
聊城市
1625A
3513A
[flaml.automl: 09-17 04:28:11] {2390} INFO - task = regression
[flaml.automl: 09-17 04:28:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:28:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:28:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:28:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:28:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:28:12] {3025} INFO - Estimated sufficient time budget=12098s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 04:28:12] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1661,	best estimator xgboost's best error=0.1661
[flaml.automl: 09-17 04:28:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:28:15] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0932,	best estimator xgboost's best error=0.0932
[flaml.automl: 09-17 04:28:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:28:16] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0932,	best estimator xgboost's best error=0.0932
[flaml.automl: 09-17 04:28:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:28:25] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0932,	best estimator xgboost's best error=0.0932
[flaml.automl: 09-17 04:28:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:28:26] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0704,	best estimator xgboost's best error=0.0704
[flaml.automl: 09-17 04:28:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:28:28] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-17 04:28:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:28:30] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-17 04:28:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:28:32] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-17 04:28:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:28:33] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-17 04:28:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:28:36] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-17 04:28:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:28:37] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-17 04:28:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:28:38] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-17 04:28:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:28:44] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.0521,	best estimator xgboost's best error=0.0521
[flaml.automl: 09-17 04:28:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:28:55] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 04:28:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 04:29:01] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 04:29:01] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 04:29:10] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 04:29:21] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-17 04:29:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:29:21] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:29:21] {2637} INFO - Time taken to find the best model: 43.36077260971069
[flaml.automl: 09-17 04:29:21] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9497760135740515
CO(0)最好结果：{'pred_time': 1.8386689313443856e-05, 'wall_clock_time': 43.36077260971069, 'metric_for_logging': {'pred_time': 1.8386689313443856e-05}, 'val_loss': 0.050223986425948476, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.440245151519775}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8936618837389354
CO(0)的mse=0.006108894933526669
CO(0)的mae=0.05148039709487599
CO(0)的mar=0.0778876577163825
总共花费的时间为：69.87
滨州市
1629A
1630A
3514A
3515A
3516A
[flaml.automl: 09-17 04:45:29] {2390} INFO - task = regression
[flaml.automl: 09-17 04:45:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:45:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:45:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:45:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:45:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:45:30] {3025} INFO - Estimated sufficient time budget=63437s. Estimated necessary time budget=63s.
[flaml.automl: 09-17 04:45:30] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.2146,	best estimator xgboost's best error=0.2146
[flaml.automl: 09-17 04:45:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:45:32] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1242,	best estimator xgboost's best error=0.1242
[flaml.automl: 09-17 04:45:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:45:33] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1242,	best estimator xgboost's best error=0.1242
[flaml.automl: 09-17 04:45:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:45:38] {3072} INFO -  at 9.4s,	estimator xgboost's best error=0.1242,	best estimator xgboost's best error=0.1242
[flaml.automl: 09-17 04:45:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:45:39] {3072} INFO -  at 10.5s,	estimator xgboost's best error=0.0972,	best estimator xgboost's best error=0.0972
[flaml.automl: 09-17 04:45:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:45:41] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.0809,	best estimator xgboost's best error=0.0809
[flaml.automl: 09-17 04:45:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:45:42] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.0809,	best estimator xgboost's best error=0.0809
[flaml.automl: 09-17 04:45:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:45:45] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.0809,	best estimator xgboost's best error=0.0809
[flaml.automl: 09-17 04:45:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:45:46] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.0809,	best estimator xgboost's best error=0.0809
[flaml.automl: 09-17 04:45:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:45:48] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0809,	best estimator xgboost's best error=0.0809
[flaml.automl: 09-17 04:45:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:45:50] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.0804,	best estimator xgboost's best error=0.0804
[flaml.automl: 09-17 04:45:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:45:51] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.0804,	best estimator xgboost's best error=0.0804
[flaml.automl: 09-17 04:45:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:45:58] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.0737,	best estimator xgboost's best error=0.0737
[flaml.automl: 09-17 04:45:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:46:10] {3072} INFO -  at 41.1s,	estimator xgboost's best error=0.0707,	best estimator xgboost's best error=0.0707
[flaml.automl: 09-17 04:46:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 04:46:16] {3072} INFO -  at 47.6s,	estimator xgboost's best error=0.0707,	best estimator xgboost's best error=0.0707
[flaml.automl: 09-17 04:46:16] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 04:46:27] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.0707,	best estimator xgboost's best error=0.0707
[flaml.automl: 09-17 04:46:49] {3335} INFO - retrain xgboost for 21.3s
[flaml.automl: 09-17 04:46:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:46:49] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:46:49] {2637} INFO - Time taken to find the best model: 58.950146436691284
[flaml.automl: 09-17 04:46:49] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 54005}
CO(0)最佳损失：0.9293451908529847
CO(0)最好结果：{'pred_time': 7.77865942071585e-06, 'wall_clock_time': 58.950146436691284, 'metric_for_logging': {'pred_time': 7.77865942071585e-06}, 'val_loss': 0.07065480914701523, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 54005}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 54005, 'experiment_tag': 'exp', 'time_total_s': 11.360812664031982}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8813730704088021
CO(0)的mse=0.012233873922924876
CO(0)的mae=0.07554018488991167
CO(0)的mar=0.12866047667618152
总共花费的时间为：81.04
淄博市
1631A
3363A
3644A
3645A
[flaml.automl: 09-17 04:59:09] {2390} INFO - task = regression
[flaml.automl: 09-17 04:59:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:59:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:59:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:59:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:59:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:59:10] {3025} INFO - Estimated sufficient time budget=50314s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 04:59:10] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2428,	best estimator xgboost's best error=0.2428
[flaml.automl: 09-17 04:59:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:59:12] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1533,	best estimator xgboost's best error=0.1533
[flaml.automl: 09-17 04:59:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:59:14] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.1533,	best estimator xgboost's best error=0.1533
[flaml.automl: 09-17 04:59:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:59:20] {3072} INFO -  at 10.9s,	estimator xgboost's best error=0.1533,	best estimator xgboost's best error=0.1533
[flaml.automl: 09-17 04:59:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:59:21] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.1282,	best estimator xgboost's best error=0.1282
[flaml.automl: 09-17 04:59:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:59:23] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.1146,	best estimator xgboost's best error=0.1146
[flaml.automl: 09-17 04:59:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:59:24] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.1146,	best estimator xgboost's best error=0.1146
[flaml.automl: 09-17 04:59:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:59:27] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.1146,	best estimator xgboost's best error=0.1146
[flaml.automl: 09-17 04:59:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:59:28] {3072} INFO -  at 18.7s,	estimator xgboost's best error=0.1146,	best estimator xgboost's best error=0.1146
[flaml.automl: 09-17 04:59:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:59:30] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.1146,	best estimator xgboost's best error=0.1146
[flaml.automl: 09-17 04:59:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:59:32] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.1132,	best estimator xgboost's best error=0.1132
[flaml.automl: 09-17 04:59:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:59:33] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.1132,	best estimator xgboost's best error=0.1132
[flaml.automl: 09-17 04:59:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:59:40] {3072} INFO -  at 30.6s,	estimator xgboost's best error=0.1062,	best estimator xgboost's best error=0.1062
[flaml.automl: 09-17 04:59:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:59:52] {3072} INFO -  at 42.6s,	estimator xgboost's best error=0.1036,	best estimator xgboost's best error=0.1036
[flaml.automl: 09-17 04:59:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 04:59:58] {3072} INFO -  at 49.0s,	estimator xgboost's best error=0.1036,	best estimator xgboost's best error=0.1036
[flaml.automl: 09-17 05:00:10] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 05:00:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:00:10] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:00:10] {2637} INFO - Time taken to find the best model: 42.57339787483215
[flaml.automl: 09-17 05:00:10] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42877}
CO(0)最佳损失：0.896422968837672
CO(0)最好结果：{'pred_time': 8.476343385069968e-06, 'wall_clock_time': 42.57339787483215, 'metric_for_logging': {'pred_time': 8.476343385069968e-06}, 'val_loss': 0.10357703116232801, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42877}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42877, 'experiment_tag': 'exp', 'time_total_s': 12.014434576034546}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8242667501345385
CO(0)的mse=0.023965381738621092
CO(0)的mae=0.10218256498569808
CO(0)的mar=0.20375414853696422
总共花费的时间为：61.78
枣庄市
1637A
1638A
1639A
1640A
3364A
[flaml.automl: 09-17 05:15:48] {2390} INFO - task = regression
[flaml.automl: 09-17 05:15:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:15:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:15:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:15:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:15:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:15:50] {3025} INFO - Estimated sufficient time budget=117697s. Estimated necessary time budget=118s.
[flaml.automl: 09-17 05:15:50] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.1612,	best estimator xgboost's best error=0.1612
[flaml.automl: 09-17 05:15:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:15:54] {3072} INFO -  at 6.4s,	estimator xgboost's best error=0.0988,	best estimator xgboost's best error=0.0988
[flaml.automl: 09-17 05:15:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:15:56] {3072} INFO -  at 8.7s,	estimator xgboost's best error=0.0988,	best estimator xgboost's best error=0.0988
[flaml.automl: 09-17 05:15:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:16:00] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.0988,	best estimator xgboost's best error=0.0988
[flaml.automl: 09-17 05:16:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:16:03] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0808,	best estimator xgboost's best error=0.0808
[flaml.automl: 09-17 05:16:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:16:06] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0705,	best estimator xgboost's best error=0.0705
[flaml.automl: 09-17 05:16:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:16:09] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0705,	best estimator xgboost's best error=0.0705
[flaml.automl: 09-17 05:16:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:16:11] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.0705,	best estimator xgboost's best error=0.0705
[flaml.automl: 09-17 05:16:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:16:13] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0705,	best estimator xgboost's best error=0.0705
[flaml.automl: 09-17 05:16:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:16:16] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.0705,	best estimator xgboost's best error=0.0705
[flaml.automl: 09-17 05:16:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:16:17] {3072} INFO -  at 29.7s,	estimator xgboost's best error=0.0705,	best estimator xgboost's best error=0.0705
[flaml.automl: 09-17 05:16:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:16:18] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.0705,	best estimator xgboost's best error=0.0705
[flaml.automl: 09-17 05:16:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:16:25] {3072} INFO -  at 37.4s,	estimator xgboost's best error=0.0651,	best estimator xgboost's best error=0.0651
[flaml.automl: 09-17 05:16:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:16:37] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.0627,	best estimator xgboost's best error=0.0627
[flaml.automl: 09-17 05:16:49] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-17 05:16:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:16:49] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:16:49] {2637} INFO - Time taken to find the best model: 49.388628244400024
[flaml.automl: 09-17 05:16:49] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54506}
CO(0)最佳损失：0.9373415650403626
CO(0)最好结果：{'pred_time': 6.916933530467718e-06, 'wall_clock_time': 49.388628244400024, 'metric_for_logging': {'pred_time': 6.916933530467718e-06}, 'val_loss': 0.06265843495963744, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54506}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54506, 'experiment_tag': 'exp', 'time_total_s': 12.002864599227905}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.867011370024932
CO(0)的mse=0.009616494042735553
CO(0)的mae=0.06456624617343672
CO(0)的mar=0.18209016404393213
总共花费的时间为：62.31
烟台市
1642A
1643A
1644A
1646A
3366A
[flaml.automl: 09-17 05:32:33] {2390} INFO - task = regression
[flaml.automl: 09-17 05:32:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:32:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:32:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:32:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:32:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:32:35] {3025} INFO - Estimated sufficient time budget=66794s. Estimated necessary time budget=67s.
[flaml.automl: 09-17 05:32:35] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1605,	best estimator xgboost's best error=0.1605
[flaml.automl: 09-17 05:32:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:32:37] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0914,	best estimator xgboost's best error=0.0914
[flaml.automl: 09-17 05:32:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:32:38] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0914,	best estimator xgboost's best error=0.0914
[flaml.automl: 09-17 05:32:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:32:43] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0914,	best estimator xgboost's best error=0.0914
[flaml.automl: 09-17 05:32:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:32:44] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0700,	best estimator xgboost's best error=0.0700
[flaml.automl: 09-17 05:32:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:32:45] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 05:32:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:32:47] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 05:32:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:32:49] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 05:32:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:32:51] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 05:32:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:32:53] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 05:32:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:32:55] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 05:32:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:32:56] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 05:32:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:33:03] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0499,	best estimator xgboost's best error=0.0499
[flaml.automl: 09-17 05:33:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:33:15] {3072} INFO -  at 41.6s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-17 05:33:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 05:33:21] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-17 05:33:33] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 05:33:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:33:33] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:33:33] {2637} INFO - Time taken to find the best model: 41.58028030395508
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54267}
CO(0)最佳损失：0.9521236607454013
CO(0)最好结果：{'pred_time': 6.6295389709978755e-06, 'wall_clock_time': 41.58028030395508, 'metric_for_logging': {'pred_time': 6.6295389709978755e-06}, 'val_loss': 0.04787633925459871, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54267}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54267, 'experiment_tag': 'exp', 'time_total_s': 12.056523084640503}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9319110092079091
CO(0)的mse=0.005512243842901741
CO(0)的mae=0.04731617660255574
CO(0)的mar=0.280619971505126
总共花费的时间为：61.03
潍坊市
3178A
3368A
3416A
3861A
[flaml.automl: 09-17 05:46:02] {2390} INFO - task = regression
[flaml.automl: 09-17 05:46:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:46:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:46:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:46:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:46:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:46:04] {3025} INFO - Estimated sufficient time budget=22081s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 05:46:04] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.1808,	best estimator xgboost's best error=0.1808
[flaml.automl: 09-17 05:46:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:46:08] {3072} INFO -  at 6.4s,	estimator xgboost's best error=0.1148,	best estimator xgboost's best error=0.1148
[flaml.automl: 09-17 05:46:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:46:10] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.1148,	best estimator xgboost's best error=0.1148
[flaml.automl: 09-17 05:46:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:46:24] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.1148,	best estimator xgboost's best error=0.1148
[flaml.automl: 09-17 05:46:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:46:25] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.0961,	best estimator xgboost's best error=0.0961
[flaml.automl: 09-17 05:46:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:46:27] {3072} INFO -  at 25.0s,	estimator xgboost's best error=0.0847,	best estimator xgboost's best error=0.0847
[flaml.automl: 09-17 05:46:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:46:28] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0847,	best estimator xgboost's best error=0.0847
[flaml.automl: 09-17 05:46:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:46:31] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.0847,	best estimator xgboost's best error=0.0847
[flaml.automl: 09-17 05:46:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:46:32] {3072} INFO -  at 30.2s,	estimator xgboost's best error=0.0847,	best estimator xgboost's best error=0.0847
[flaml.automl: 09-17 05:46:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:46:34] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.0847,	best estimator xgboost's best error=0.0847
[flaml.automl: 09-17 05:46:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:46:36] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.0847,	best estimator xgboost's best error=0.0847
[flaml.automl: 09-17 05:46:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:46:37] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.0847,	best estimator xgboost's best error=0.0847
[flaml.automl: 09-17 05:46:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:46:43] {3072} INFO -  at 41.6s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-17 05:46:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:46:55] {3072} INFO -  at 53.7s,	estimator xgboost's best error=0.0751,	best estimator xgboost's best error=0.0751
[flaml.automl: 09-17 05:47:07] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 05:47:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:47:07] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:47:07] {2637} INFO - Time taken to find the best model: 53.68332123756409
[flaml.automl: 09-17 05:47:07] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9248948514707686
CO(0)最好结果：{'pred_time': 9.53964931787086e-06, 'wall_clock_time': 53.68332123756409, 'metric_for_logging': {'pred_time': 9.53964931787086e-06}, 'val_loss': 0.07510514852923143, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.082464694976807}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.830409297257639
CO(0)的mse=0.014246457744264265
CO(0)的mae=0.07366510851803994
CO(0)的mar=0.11633144441822205
总共花费的时间为：66.44
济宁市
1653A
3501A
3678A
[flaml.automl: 09-17 05:56:24] {2390} INFO - task = regression
[flaml.automl: 09-17 05:56:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:56:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:56:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:56:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:56:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:56:25] {3025} INFO - Estimated sufficient time budget=11981s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 05:56:25] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1992,	best estimator xgboost's best error=0.1992
[flaml.automl: 09-17 05:56:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:56:27] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1058,	best estimator xgboost's best error=0.1058
[flaml.automl: 09-17 05:56:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:56:29] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.1058,	best estimator xgboost's best error=0.1058
[flaml.automl: 09-17 05:56:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:56:38] {3072} INFO -  at 14.6s,	estimator xgboost's best error=0.1058,	best estimator xgboost's best error=0.1058
[flaml.automl: 09-17 05:56:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:56:40] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.0752,	best estimator xgboost's best error=0.0752
[flaml.automl: 09-17 05:56:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:56:41] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.0619,	best estimator xgboost's best error=0.0619
[flaml.automl: 09-17 05:56:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:56:43] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0619,	best estimator xgboost's best error=0.0619
[flaml.automl: 09-17 05:56:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:56:45] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.0619,	best estimator xgboost's best error=0.0619
[flaml.automl: 09-17 05:56:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:56:46] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.0619,	best estimator xgboost's best error=0.0619
[flaml.automl: 09-17 05:56:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:56:49] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.0619,	best estimator xgboost's best error=0.0619
[flaml.automl: 09-17 05:56:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:56:50] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.0619,	best estimator xgboost's best error=0.0619
[flaml.automl: 09-17 05:56:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:56:51] {3072} INFO -  at 27.5s,	estimator xgboost's best error=0.0619,	best estimator xgboost's best error=0.0619
[flaml.automl: 09-17 05:56:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:56:58] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.0561,	best estimator xgboost's best error=0.0561
[flaml.automl: 09-17 05:56:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:57:10] {3072} INFO -  at 46.1s,	estimator xgboost's best error=0.0538,	best estimator xgboost's best error=0.0538
[flaml.automl: 09-17 05:57:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 05:57:17] {3072} INFO -  at 52.7s,	estimator xgboost's best error=0.0538,	best estimator xgboost's best error=0.0538
[flaml.automl: 09-17 05:57:29] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 05:57:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:57:29] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:57:29] {2637} INFO - Time taken to find the best model: 46.12976789474487
[flaml.automl: 09-17 05:57:29] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9461821099101791
CO(0)最好结果：{'pred_time': 1.0787931824150705e-05, 'wall_clock_time': 46.12976789474487, 'metric_for_logging': {'pred_time': 1.0787931824150705e-05}, 'val_loss': 0.05381789008982087, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.096879720687866}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9090784509229993
CO(0)的mse=0.005966456019625224
CO(0)的mae=0.05203779228120636
CO(0)的mar=0.07269461245649293
总共花费的时间为：65.23
泰安市
3502A
3503A
3504A
[flaml.automl: 09-17 06:07:47] {2390} INFO - task = regression
[flaml.automl: 09-17 06:07:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:07:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:07:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:07:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:07:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:07:48] {3025} INFO - Estimated sufficient time budget=11442s. Estimated necessary time budget=11s.
[flaml.automl: 09-17 06:07:48] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1535,	best estimator xgboost's best error=0.1535
[flaml.automl: 09-17 06:07:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:07:50] {3072} INFO -  at 3.3s,	estimator xgboost's best error=0.0937,	best estimator xgboost's best error=0.0937
[flaml.automl: 09-17 06:07:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:07:51] {3072} INFO -  at 4.4s,	estimator xgboost's best error=0.0937,	best estimator xgboost's best error=0.0937
[flaml.automl: 09-17 06:07:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:08:01] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0937,	best estimator xgboost's best error=0.0937
[flaml.automl: 09-17 06:08:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:08:02] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.0768,	best estimator xgboost's best error=0.0768
[flaml.automl: 09-17 06:08:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:08:03] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.0648,	best estimator xgboost's best error=0.0648
[flaml.automl: 09-17 06:08:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:08:05] {3072} INFO -  at 18.2s,	estimator xgboost's best error=0.0648,	best estimator xgboost's best error=0.0648
[flaml.automl: 09-17 06:08:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:08:07] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.0648,	best estimator xgboost's best error=0.0648
[flaml.automl: 09-17 06:08:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:08:09] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0648,	best estimator xgboost's best error=0.0648
[flaml.automl: 09-17 06:08:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:08:11] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0648,	best estimator xgboost's best error=0.0648
[flaml.automl: 09-17 06:08:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:08:12] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0648,	best estimator xgboost's best error=0.0648
[flaml.automl: 09-17 06:08:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:08:13] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0648,	best estimator xgboost's best error=0.0648
[flaml.automl: 09-17 06:08:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:08:20] {3072} INFO -  at 33.1s,	estimator xgboost's best error=0.0610,	best estimator xgboost's best error=0.0610
[flaml.automl: 09-17 06:08:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:08:32] {3072} INFO -  at 45.1s,	estimator xgboost's best error=0.0591,	best estimator xgboost's best error=0.0591
[flaml.automl: 09-17 06:08:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 06:08:38] {3072} INFO -  at 51.6s,	estimator xgboost's best error=0.0591,	best estimator xgboost's best error=0.0591
[flaml.automl: 09-17 06:08:50] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 06:08:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:08:50] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:08:50] {2637} INFO - Time taken to find the best model: 45.092721462249756
[flaml.automl: 09-17 06:08:50] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9409266952515647
CO(0)最好结果：{'pred_time': 1.2732351292445008e-05, 'wall_clock_time': 45.092721462249756, 'metric_for_logging': {'pred_time': 1.2732351292445008e-05}, 'val_loss': 0.059073304748435265, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.017255067825317}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8542503763681046
CO(0)的mse=0.009761888466674945
CO(0)的mae=0.06108684442221908
CO(0)的mar=0.10738720119284435
总共花费的时间为：64.18
日照市
1659A
1661A
3507A
3604A
[flaml.automl: 09-17 06:22:26] {2390} INFO - task = regression
[flaml.automl: 09-17 06:22:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:22:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:22:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:22:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:22:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:22:28] {3025} INFO - Estimated sufficient time budget=91941s. Estimated necessary time budget=92s.
[flaml.automl: 09-17 06:22:28] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1982,	best estimator xgboost's best error=0.1982
[flaml.automl: 09-17 06:22:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:22:32] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.1325,	best estimator xgboost's best error=0.1325
[flaml.automl: 09-17 06:22:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:22:34] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.1325,	best estimator xgboost's best error=0.1325
[flaml.automl: 09-17 06:22:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:22:39] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.1325,	best estimator xgboost's best error=0.1325
[flaml.automl: 09-17 06:22:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:22:41] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.1204,	best estimator xgboost's best error=0.1204
[flaml.automl: 09-17 06:22:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:22:44] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.1065,	best estimator xgboost's best error=0.1065
[flaml.automl: 09-17 06:22:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:22:47] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.1065,	best estimator xgboost's best error=0.1065
[flaml.automl: 09-17 06:22:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:22:51] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.1065,	best estimator xgboost's best error=0.1065
[flaml.automl: 09-17 06:22:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:22:53] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.1065,	best estimator xgboost's best error=0.1065
[flaml.automl: 09-17 06:22:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:22:56] {3072} INFO -  at 30.0s,	estimator xgboost's best error=0.1065,	best estimator xgboost's best error=0.1065
[flaml.automl: 09-17 06:22:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:22:59] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.1065,	best estimator xgboost's best error=0.1065
[flaml.automl: 09-17 06:22:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:23:01] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.1065,	best estimator xgboost's best error=0.1065
[flaml.automl: 09-17 06:23:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:23:13] {3072} INFO -  at 47.1s,	estimator xgboost's best error=0.1060,	best estimator xgboost's best error=0.1060
[flaml.automl: 09-17 06:23:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:23:25] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.0984,	best estimator xgboost's best error=0.0984
[flaml.automl: 09-17 06:23:48] {3335} INFO - retrain xgboost for 22.9s
[flaml.automl: 09-17 06:23:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:23:48] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:23:48] {2637} INFO - Time taken to find the best model: 58.992595911026
[flaml.automl: 09-17 06:23:48] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42457}
CO(0)最佳损失：0.9015876162350557
CO(0)最好结果：{'pred_time': 1.691604580299083e-05, 'wall_clock_time': 58.992595911026, 'metric_for_logging': {'pred_time': 1.691604580299083e-05}, 'val_loss': 0.09841238376494432, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42457}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42457, 'experiment_tag': 'exp', 'time_total_s': 11.877935886383057}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.6833371003363189
CO(0)的mse=0.04100044760561859
CO(0)的mae=0.09763502710500505
CO(0)的mar=0.14186965452963254
总共花费的时间为：82.73
威海市
1662A
1664A
1982A
3505A
[flaml.automl: 09-17 06:36:41] {2390} INFO - task = regression
[flaml.automl: 09-17 06:36:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:36:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:36:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:36:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:36:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:36:45] {3025} INFO - Estimated sufficient time budget=149978s. Estimated necessary time budget=150s.
[flaml.automl: 09-17 06:36:45] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.1183,	best estimator xgboost's best error=0.1183
[flaml.automl: 09-17 06:36:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:36:51] {3072} INFO -  at 9.8s,	estimator xgboost's best error=0.0686,	best estimator xgboost's best error=0.0686
[flaml.automl: 09-17 06:36:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:36:54] {3072} INFO -  at 13.3s,	estimator xgboost's best error=0.0686,	best estimator xgboost's best error=0.0686
[flaml.automl: 09-17 06:36:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:36:59] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.0686,	best estimator xgboost's best error=0.0686
[flaml.automl: 09-17 06:36:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:37:03] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.0532,	best estimator xgboost's best error=0.0532
[flaml.automl: 09-17 06:37:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:37:06] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-17 06:37:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:37:10] {3072} INFO -  at 28.7s,	estimator xgboost's best error=0.0475,	best estimator xgboost's best error=0.0475
[flaml.automl: 09-17 06:37:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:37:13] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.0475,	best estimator xgboost's best error=0.0475
[flaml.automl: 09-17 06:37:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:37:15] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0475,	best estimator xgboost's best error=0.0475
[flaml.automl: 09-17 06:37:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:37:18] {3072} INFO -  at 37.0s,	estimator xgboost's best error=0.0475,	best estimator xgboost's best error=0.0475
[flaml.automl: 09-17 06:37:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:37:20] {3072} INFO -  at 39.0s,	estimator xgboost's best error=0.0475,	best estimator xgboost's best error=0.0475
[flaml.automl: 09-17 06:37:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:37:25] {3072} INFO -  at 43.7s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-17 06:37:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:37:28] {3072} INFO -  at 47.0s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-17 06:37:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:37:40] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.0355,	best estimator xgboost's best error=0.0355
[flaml.automl: 09-17 06:37:49] {3335} INFO - retrain xgboost for 8.9s
[flaml.automl: 09-17 06:37:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 06:37:49] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:37:49] {2637} INFO - Time taken to find the best model: 59.03369188308716
[flaml.automl: 09-17 06:37:49] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 43470}
CO(0)最佳损失：0.964487160248749
CO(0)最好结果：{'pred_time': 1.4514705902794626e-05, 'wall_clock_time': 59.03369188308716, 'metric_for_logging': {'pred_time': 1.4514705902794626e-05}, 'val_loss': 0.035512839751250996, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 43470}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 43470, 'experiment_tag': 'exp', 'time_total_s': 11.995080471038818}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9153750271175312
CO(0)的mse=0.003265971819461143
CO(0)的mae=0.03677928301795848
CO(0)的mar=0.15066043621258754
总共花费的时间为：68.78
东营市
3365A
3498A
3734A
[flaml.automl: 09-17 06:48:04] {2390} INFO - task = regression
[flaml.automl: 09-17 06:48:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:48:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:48:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:48:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:48:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:48:06] {3025} INFO - Estimated sufficient time budget=21492s. Estimated necessary time budget=21s.
[flaml.automl: 09-17 06:48:06] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.1706,	best estimator xgboost's best error=0.1706
[flaml.automl: 09-17 06:48:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:48:10] {3072} INFO -  at 6.2s,	estimator xgboost's best error=0.1007,	best estimator xgboost's best error=0.1007
[flaml.automl: 09-17 06:48:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:48:12] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.1007,	best estimator xgboost's best error=0.1007
[flaml.automl: 09-17 06:48:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:48:28] {3072} INFO -  at 24.8s,	estimator xgboost's best error=0.1007,	best estimator xgboost's best error=0.1007
[flaml.automl: 09-17 06:48:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:48:30] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0814,	best estimator xgboost's best error=0.0814
[flaml.automl: 09-17 06:48:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:48:33] {3072} INFO -  at 29.3s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-17 06:48:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:48:35] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-17 06:48:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:48:40] {3072} INFO -  at 36.1s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-17 06:48:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:48:42] {3072} INFO -  at 38.2s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-17 06:48:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:48:46] {3072} INFO -  at 42.9s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-17 06:48:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:48:49] {3072} INFO -  at 45.1s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-17 06:48:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:48:51] {3072} INFO -  at 47.1s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-17 06:48:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:49:01] {3072} INFO -  at 57.7s,	estimator xgboost's best error=0.0609,	best estimator xgboost's best error=0.0609
[flaml.automl: 09-17 06:49:12] {3335} INFO - retrain xgboost for 10.6s
[flaml.automl: 09-17 06:49:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 06:49:12] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:49:12] {2637} INFO - Time taken to find the best model: 57.70390868186951
[flaml.automl: 09-17 06:49:12] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9390595842967283
CO(0)最好结果：{'pred_time': 2.2502079041432314e-05, 'wall_clock_time': 57.70390868186951, 'metric_for_logging': {'pred_time': 2.2502079041432314e-05}, 'val_loss': 0.060940415703271654, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 10.64587140083313}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8920341698178001
CO(0)的mse=0.008220560858714922
CO(0)的mae=0.0617802407795157
CO(0)的mar=0.10034038980090558
总共花费的时间为：68.90
韶关市
1669A
1673A
3622A
[flaml.automl: 09-17 06:59:47] {2390} INFO - task = regression
[flaml.automl: 09-17 06:59:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:59:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:59:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:59:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:59:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:59:49] {3025} INFO - Estimated sufficient time budget=19223s. Estimated necessary time budget=19s.
[flaml.automl: 09-17 06:59:49] {3072} INFO -  at 2.1s,	estimator xgboost's best error=0.1345,	best estimator xgboost's best error=0.1345
[flaml.automl: 09-17 06:59:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:59:53] {3072} INFO -  at 6.0s,	estimator xgboost's best error=0.0798,	best estimator xgboost's best error=0.0798
[flaml.automl: 09-17 06:59:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:59:55] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.0798,	best estimator xgboost's best error=0.0798
[flaml.automl: 09-17 06:59:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:00:13] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0798,	best estimator xgboost's best error=0.0798
[flaml.automl: 09-17 07:00:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:00:15] {3072} INFO -  at 28.0s,	estimator xgboost's best error=0.0619,	best estimator xgboost's best error=0.0619
[flaml.automl: 09-17 07:00:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:00:18] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-17 07:00:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:00:21] {3072} INFO -  at 33.9s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-17 07:00:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:00:25] {3072} INFO -  at 38.3s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-17 07:00:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:00:27] {3072} INFO -  at 40.3s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-17 07:00:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:00:32] {3072} INFO -  at 45.2s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-17 07:00:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:00:34] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-17 07:00:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:00:36] {3072} INFO -  at 49.1s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-17 07:00:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:00:46] {3072} INFO -  at 58.8s,	estimator xgboost's best error=0.0504,	best estimator xgboost's best error=0.0504
[flaml.automl: 09-17 07:00:57] {3335} INFO - retrain xgboost for 11.3s
[flaml.automl: 09-17 07:00:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 07:00:57] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:00:57] {2637} INFO - Time taken to find the best model: 58.811877489089966
[flaml.automl: 09-17 07:00:57] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9496343859175759
CO(0)最好结果：{'pred_time': 2.2637337160045065e-05, 'wall_clock_time': 58.811877489089966, 'metric_for_logging': {'pred_time': 2.2637337160045065e-05}, 'val_loss': 0.05036561408242408, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.674386978149414}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7902855541919197
CO(0)的mse=0.006911439423607799
CO(0)的mae=0.047561767107056986
CO(0)的mar=0.07286648759254485
总共花费的时间为：70.65
汕头市
1674A
1675A
3624A
[flaml.automl: 09-17 07:10:55] {2390} INFO - task = regression
[flaml.automl: 09-17 07:10:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:10:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:10:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:10:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:10:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:10:57] {3025} INFO - Estimated sufficient time budget=12233s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 07:10:57] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1149,	best estimator xgboost's best error=0.1149
[flaml.automl: 09-17 07:10:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:10:59] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0675,	best estimator xgboost's best error=0.0675
[flaml.automl: 09-17 07:10:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:11:00] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0675,	best estimator xgboost's best error=0.0675
[flaml.automl: 09-17 07:11:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:11:10] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0675,	best estimator xgboost's best error=0.0675
[flaml.automl: 09-17 07:11:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:11:11] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0524,	best estimator xgboost's best error=0.0524
[flaml.automl: 09-17 07:11:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:11:13] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0464,	best estimator xgboost's best error=0.0464
[flaml.automl: 09-17 07:11:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:11:14] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0464,	best estimator xgboost's best error=0.0464
[flaml.automl: 09-17 07:11:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:11:17] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0464,	best estimator xgboost's best error=0.0464
[flaml.automl: 09-17 07:11:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:11:18] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0464,	best estimator xgboost's best error=0.0464
[flaml.automl: 09-17 07:11:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:11:21] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0464,	best estimator xgboost's best error=0.0464
[flaml.automl: 09-17 07:11:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:11:22] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0464,	best estimator xgboost's best error=0.0464
[flaml.automl: 09-17 07:11:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:11:23] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.0464,	best estimator xgboost's best error=0.0464
[flaml.automl: 09-17 07:11:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:11:30] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-17 07:11:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:11:42] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0390,	best estimator xgboost's best error=0.0390
[flaml.automl: 09-17 07:11:42] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 07:11:48] {3072} INFO -  at 52.8s,	estimator xgboost's best error=0.0390,	best estimator xgboost's best error=0.0390
[flaml.automl: 09-17 07:12:00] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 07:12:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:12:00] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:12:00] {2637} INFO - Time taken to find the best model: 46.29040551185608
[flaml.automl: 09-17 07:12:00] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.960982366128907
CO(0)最好结果：{'pred_time': 1.0746558981619642e-05, 'wall_clock_time': 46.29040551185608, 'metric_for_logging': {'pred_time': 1.0746558981619642e-05}, 'val_loss': 0.03901763387109298, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.059831380844116}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8848935088254233
CO(0)的mse=0.0037385849714353507
CO(0)的mae=0.03936696625374467
CO(0)的mar=0.08467314684785032
总共花费的时间为：65.39
湛江市
1680A
1681A
1682A
1684A
1685A
[flaml.automl: 09-17 07:27:19] {2390} INFO - task = regression
[flaml.automl: 09-17 07:27:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:27:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:27:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:27:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:27:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:27:20] {3025} INFO - Estimated sufficient time budget=67767s. Estimated necessary time budget=68s.
[flaml.automl: 09-17 07:27:20] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.0877,	best estimator xgboost's best error=0.0877
[flaml.automl: 09-17 07:27:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:27:22] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0526,	best estimator xgboost's best error=0.0526
[flaml.automl: 09-17 07:27:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:27:23] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0526,	best estimator xgboost's best error=0.0526
[flaml.automl: 09-17 07:27:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:27:28] {3072} INFO -  at 9.5s,	estimator xgboost's best error=0.0526,	best estimator xgboost's best error=0.0526
[flaml.automl: 09-17 07:27:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:27:29] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-17 07:27:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:27:31] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0338,	best estimator xgboost's best error=0.0338
[flaml.automl: 09-17 07:27:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:27:32] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0338,	best estimator xgboost's best error=0.0338
[flaml.automl: 09-17 07:27:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:27:35] {3072} INFO -  at 16.2s,	estimator xgboost's best error=0.0338,	best estimator xgboost's best error=0.0338
[flaml.automl: 09-17 07:27:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:27:36] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0338,	best estimator xgboost's best error=0.0338
[flaml.automl: 09-17 07:27:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:27:39] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.0338,	best estimator xgboost's best error=0.0338
[flaml.automl: 09-17 07:27:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:27:40] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0338,	best estimator xgboost's best error=0.0338
[flaml.automl: 09-17 07:27:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:27:41] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0338,	best estimator xgboost's best error=0.0338
[flaml.automl: 09-17 07:27:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:27:48] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.0296,	best estimator xgboost's best error=0.0296
[flaml.automl: 09-17 07:27:48] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:28:00] {3072} INFO -  at 41.4s,	estimator xgboost's best error=0.0289,	best estimator xgboost's best error=0.0289
[flaml.automl: 09-17 07:28:00] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 07:28:06] {3072} INFO -  at 47.9s,	estimator xgboost's best error=0.0289,	best estimator xgboost's best error=0.0289
[flaml.automl: 09-17 07:28:06] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 07:28:19] {3072} INFO -  at 60.2s,	estimator xgboost's best error=0.0289,	best estimator xgboost's best error=0.0289
[flaml.automl: 09-17 07:28:39] {3335} INFO - retrain xgboost for 20.1s
[flaml.automl: 09-17 07:28:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:28:39] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:28:39] {2637} INFO - Time taken to find the best model: 41.41541504859924
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 56243}
CO(0)最佳损失：0.9711038516542816
CO(0)最好结果：{'pred_time': 6.2284469604492185e-06, 'wall_clock_time': 41.41541504859924, 'metric_for_logging': {'pred_time': 6.2284469604492185e-06}, 'val_loss': 0.028896148345718384, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 56243}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 56243, 'experiment_tag': 'exp', 'time_total_s': 12.031656265258789}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8962413213793147
CO(0)的mse=0.0021489324170495413
CO(0)的mae=0.02806103564290682
CO(0)的mar=0.0501680919418109
总共花费的时间为：81.22
茂名市
1686A
1688A
1689A
3450A
[flaml.automl: 09-17 07:41:29] {2390} INFO - task = regression
[flaml.automl: 09-17 07:41:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:41:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:41:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:41:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:41:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:41:32] {3025} INFO - Estimated sufficient time budget=146488s. Estimated necessary time budget=146s.
[flaml.automl: 09-17 07:41:32] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1085,	best estimator xgboost's best error=0.1085
[flaml.automl: 09-17 07:41:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:41:38] {3072} INFO -  at 9.8s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-17 07:41:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:41:42] {3072} INFO -  at 13.1s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-17 07:41:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:41:46] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-17 07:41:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:41:48] {3072} INFO -  at 19.9s,	estimator xgboost's best error=0.0486,	best estimator xgboost's best error=0.0486
[flaml.automl: 09-17 07:41:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:41:51] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0416,	best estimator xgboost's best error=0.0416
[flaml.automl: 09-17 07:41:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:41:54] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0416,	best estimator xgboost's best error=0.0416
[flaml.automl: 09-17 07:41:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:41:58] {3072} INFO -  at 29.3s,	estimator xgboost's best error=0.0416,	best estimator xgboost's best error=0.0416
[flaml.automl: 09-17 07:41:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:42:00] {3072} INFO -  at 31.3s,	estimator xgboost's best error=0.0416,	best estimator xgboost's best error=0.0416
[flaml.automl: 09-17 07:42:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:42:03] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.0416,	best estimator xgboost's best error=0.0416
[flaml.automl: 09-17 07:42:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:42:06] {3072} INFO -  at 36.9s,	estimator xgboost's best error=0.0409,	best estimator xgboost's best error=0.0409
[flaml.automl: 09-17 07:42:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:42:08] {3072} INFO -  at 38.9s,	estimator xgboost's best error=0.0409,	best estimator xgboost's best error=0.0409
[flaml.automl: 09-17 07:42:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:42:14] {3072} INFO -  at 45.3s,	estimator xgboost's best error=0.0361,	best estimator xgboost's best error=0.0361
[flaml.automl: 09-17 07:42:14] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:42:26] {3072} INFO -  at 57.3s,	estimator xgboost's best error=0.0346,	best estimator xgboost's best error=0.0346
[flaml.automl: 09-17 07:42:38] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 07:42:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:42:38] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:42:38] {2637} INFO - Time taken to find the best model: 57.311933517456055
[flaml.automl: 09-17 07:42:38] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43453}
CO(0)最佳损失：0.9654289696018847
CO(0)最好结果：{'pred_time': 9.242533946239718e-06, 'wall_clock_time': 57.311933517456055, 'metric_for_logging': {'pred_time': 9.242533946239718e-06}, 'val_loss': 0.03457103039811526, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43453}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43453, 'experiment_tag': 'exp', 'time_total_s': 12.022823333740234}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8958896881294254
CO(0)的mse=0.0031518256331823542
CO(0)的mae=0.035794875847548945
CO(0)的mar=0.05856230516962388
总共花费的时间为：70.31
梅州市
1690A
1692A
3315A
[flaml.automl: 09-17 07:52:35] {2390} INFO - task = regression
[flaml.automl: 09-17 07:52:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:52:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:52:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:52:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:52:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:52:36] {3025} INFO - Estimated sufficient time budget=12126s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 07:52:36] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.0973,	best estimator xgboost's best error=0.0973
[flaml.automl: 09-17 07:52:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:52:39] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0553,	best estimator xgboost's best error=0.0553
[flaml.automl: 09-17 07:52:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:52:40] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0553,	best estimator xgboost's best error=0.0553
[flaml.automl: 09-17 07:52:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:52:50] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0553,	best estimator xgboost's best error=0.0553
[flaml.automl: 09-17 07:52:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:52:51] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0423,	best estimator xgboost's best error=0.0423
[flaml.automl: 09-17 07:52:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:52:53] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0373,	best estimator xgboost's best error=0.0373
[flaml.automl: 09-17 07:52:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:52:54] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0373,	best estimator xgboost's best error=0.0373
[flaml.automl: 09-17 07:52:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:52:57] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0373,	best estimator xgboost's best error=0.0373
[flaml.automl: 09-17 07:52:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:52:58] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0373,	best estimator xgboost's best error=0.0373
[flaml.automl: 09-17 07:52:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:53:01] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0373,	best estimator xgboost's best error=0.0373
[flaml.automl: 09-17 07:53:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:53:02] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.0373,	best estimator xgboost's best error=0.0373
[flaml.automl: 09-17 07:53:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:53:03] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0373,	best estimator xgboost's best error=0.0373
[flaml.automl: 09-17 07:53:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:53:09] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0322,	best estimator xgboost's best error=0.0322
[flaml.automl: 09-17 07:53:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:53:21] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.0315,	best estimator xgboost's best error=0.0315
[flaml.automl: 09-17 07:53:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 07:53:28] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.0315,	best estimator xgboost's best error=0.0315
[flaml.automl: 09-17 07:53:40] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 07:53:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:53:40] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:53:40] {2637} INFO - Time taken to find the best model: 46.37829089164734
[flaml.automl: 09-17 07:53:40] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9685109375091835
CO(0)最好结果：{'pred_time': 1.2720850010658628e-05, 'wall_clock_time': 46.37829089164734, 'metric_for_logging': {'pred_time': 1.2720850010658628e-05}, 'val_loss': 0.03148906249081652, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.05094575881958}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8709391677632458
CO(0)的mse=0.0023199948586766234
CO(0)的mae=0.03163520617850535
CO(0)的mar=0.05071160169247338
总共花费的时间为：65.42
汕尾市
1694A
1695A
[flaml.automl: 09-17 08:00:23] {2390} INFO - task = regression
[flaml.automl: 09-17 08:00:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:00:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:00:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:00:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:00:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:00:26] {3025} INFO - Estimated sufficient time budget=22442s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 08:00:26] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.0684,	best estimator xgboost's best error=0.0684
[flaml.automl: 09-17 08:00:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:00:30] {3072} INFO -  at 6.2s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-17 08:00:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:00:32] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-17 08:00:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:00:49] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-17 08:00:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:00:52] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.0303,	best estimator xgboost's best error=0.0303
[flaml.automl: 09-17 08:00:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:00:54] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0251,	best estimator xgboost's best error=0.0251
[flaml.automl: 09-17 08:00:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:00:57] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.0251,	best estimator xgboost's best error=0.0251
[flaml.automl: 09-17 08:00:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:01:02] {3072} INFO -  at 38.5s,	estimator xgboost's best error=0.0251,	best estimator xgboost's best error=0.0251
[flaml.automl: 09-17 08:01:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:01:04] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.0251,	best estimator xgboost's best error=0.0251
[flaml.automl: 09-17 08:01:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:01:09] {3072} INFO -  at 45.2s,	estimator xgboost's best error=0.0251,	best estimator xgboost's best error=0.0251
[flaml.automl: 09-17 08:01:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:01:11] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.0251,	best estimator xgboost's best error=0.0251
[flaml.automl: 09-17 08:01:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:01:13] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0251,	best estimator xgboost's best error=0.0251
[flaml.automl: 09-17 08:01:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:01:23] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.0223,	best estimator xgboost's best error=0.0223
[flaml.automl: 09-17 08:01:33] {3335} INFO - retrain xgboost for 10.3s
[flaml.automl: 09-17 08:01:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 08:01:33] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:01:33] {2637} INFO - Time taken to find the best model: 59.333128213882446
[flaml.automl: 09-17 08:01:33] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9777483077984431
CO(0)最好结果：{'pred_time': 3.7075752890951725e-05, 'wall_clock_time': 59.333128213882446, 'metric_for_logging': {'pred_time': 3.7075752890951725e-05}, 'val_loss': 0.02225169220155683, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.862754583358765}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9336893068180678
CO(0)的mse=0.0009198107282177452
CO(0)的mae=0.02141145798244568
CO(0)的mar=0.039798015489071574
总共花费的时间为：70.09
河源市
1696A
1697A
[flaml.automl: 09-17 08:08:36] {2390} INFO - task = regression
[flaml.automl: 09-17 08:08:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:08:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:08:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:08:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:08:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:08:38] {3025} INFO - Estimated sufficient time budget=12003s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 08:08:38] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1563,	best estimator xgboost's best error=0.1563
[flaml.automl: 09-17 08:08:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:08:40] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.0875,	best estimator xgboost's best error=0.0875
[flaml.automl: 09-17 08:08:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:08:41] {3072} INFO -  at 4.4s,	estimator xgboost's best error=0.0875,	best estimator xgboost's best error=0.0875
[flaml.automl: 09-17 08:08:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:08:49] {3072} INFO -  at 12.7s,	estimator xgboost's best error=0.0875,	best estimator xgboost's best error=0.0875
[flaml.automl: 09-17 08:08:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:08:50] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0492,	best estimator xgboost's best error=0.0492
[flaml.automl: 09-17 08:08:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:08:52] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0410,	best estimator xgboost's best error=0.0410
[flaml.automl: 09-17 08:08:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:08:53] {3072} INFO -  at 17.0s,	estimator xgboost's best error=0.0410,	best estimator xgboost's best error=0.0410
[flaml.automl: 09-17 08:08:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:08:56] {3072} INFO -  at 19.5s,	estimator xgboost's best error=0.0410,	best estimator xgboost's best error=0.0410
[flaml.automl: 09-17 08:08:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:08:57] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.0410,	best estimator xgboost's best error=0.0410
[flaml.automl: 09-17 08:08:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:08:59] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0410,	best estimator xgboost's best error=0.0410
[flaml.automl: 09-17 08:08:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:09:01] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.0410,	best estimator xgboost's best error=0.0410
[flaml.automl: 09-17 08:09:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:09:02] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0410,	best estimator xgboost's best error=0.0410
[flaml.automl: 09-17 08:09:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:09:08] {3072} INFO -  at 31.4s,	estimator xgboost's best error=0.0369,	best estimator xgboost's best error=0.0369
[flaml.automl: 09-17 08:09:08] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:09:18] {3072} INFO -  at 41.9s,	estimator xgboost's best error=0.0350,	best estimator xgboost's best error=0.0350
[flaml.automl: 09-17 08:09:18] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 08:09:24] {3072} INFO -  at 47.9s,	estimator xgboost's best error=0.0350,	best estimator xgboost's best error=0.0350
[flaml.automl: 09-17 08:09:24] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 08:09:36] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.0350,	best estimator xgboost's best error=0.0350
[flaml.automl: 09-17 08:09:46] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-17 08:09:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:09:46] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:09:46] {2637} INFO - Time taken to find the best model: 41.88388967514038
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9650364464751959
CO(0)最好结果：{'pred_time': 1.8123089994016206e-05, 'wall_clock_time': 41.88388967514038, 'metric_for_logging': {'pred_time': 1.8123089994016206e-05}, 'val_loss': 0.03496355352480405, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.45209813117981}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8788393959560046
CO(0)的mse=0.002544540250275103
CO(0)的mae=0.03522661149895231
CO(0)的mar=0.04846216331867854
总共花费的时间为：70.53
阳江市
1699A
3453A
[flaml.automl: 09-17 08:16:16] {2390} INFO - task = regression
[flaml.automl: 09-17 08:16:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:16:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:16:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:16:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:16:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:16:18] {3025} INFO - Estimated sufficient time budget=12279s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 08:16:18] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1078,	best estimator xgboost's best error=0.1078
[flaml.automl: 09-17 08:16:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:16:19] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.0723,	best estimator xgboost's best error=0.0723
[flaml.automl: 09-17 08:16:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:16:21] {3072} INFO -  at 4.4s,	estimator xgboost's best error=0.0723,	best estimator xgboost's best error=0.0723
[flaml.automl: 09-17 08:16:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:16:29] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.0723,	best estimator xgboost's best error=0.0723
[flaml.automl: 09-17 08:16:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:16:30] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0533,	best estimator xgboost's best error=0.0533
[flaml.automl: 09-17 08:16:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:16:32] {3072} INFO -  at 15.6s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-17 08:16:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:16:33] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-17 08:16:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:16:36] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-17 08:16:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:16:37] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-17 08:16:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:16:40] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-17 08:16:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:16:41] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-17 08:16:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:16:42] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-17 08:16:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:16:48] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.0390,	best estimator xgboost's best error=0.0390
[flaml.automl: 09-17 08:16:48] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:16:58] {3072} INFO -  at 42.1s,	estimator xgboost's best error=0.0386,	best estimator xgboost's best error=0.0386
[flaml.automl: 09-17 08:16:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 08:17:04] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.0386,	best estimator xgboost's best error=0.0386
[flaml.automl: 09-17 08:17:04] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 08:17:16] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.0385,	best estimator xgboost's best error=0.0385
[flaml.automl: 09-17 08:17:33] {3335} INFO - retrain xgboost for 17.2s
[flaml.automl: 09-17 08:17:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:17:33] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:17:33] {2637} INFO - Time taken to find the best model: 59.77466702461243
[flaml.automl: 09-17 08:17:33] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.9615157467751482
CO(0)最好结果：{'pred_time': 1.820654890655932e-05, 'wall_clock_time': 59.77466702461243, 'metric_for_logging': {'pred_time': 1.820654890655932e-05}, 'val_loss': 0.038484253224851855, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 11.608662366867065}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8785403876912684
CO(0)的mse=0.00363610917692978
CO(0)的mae=0.03997106385197027
CO(0)的mar=0.07332640499659632
总共花费的时间为：77.44
清远市
1702A
3318A
3455A
[flaml.automl: 09-17 08:27:38] {2390} INFO - task = regression
[flaml.automl: 09-17 08:27:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:27:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:27:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:27:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:27:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:27:39] {3025} INFO - Estimated sufficient time budget=12145s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 08:27:39] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1714,	best estimator xgboost's best error=0.1714
[flaml.automl: 09-17 08:27:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:27:41] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0840,	best estimator xgboost's best error=0.0840
[flaml.automl: 09-17 08:27:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:27:43] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0840,	best estimator xgboost's best error=0.0840
[flaml.automl: 09-17 08:27:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:27:53] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0840,	best estimator xgboost's best error=0.0840
[flaml.automl: 09-17 08:27:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:27:54] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0474,	best estimator xgboost's best error=0.0474
[flaml.automl: 09-17 08:27:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:27:55] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0374,	best estimator xgboost's best error=0.0374
[flaml.automl: 09-17 08:27:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:27:57] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0374,	best estimator xgboost's best error=0.0374
[flaml.automl: 09-17 08:27:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:28:00] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0374,	best estimator xgboost's best error=0.0374
[flaml.automl: 09-17 08:28:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:28:01] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0374,	best estimator xgboost's best error=0.0374
[flaml.automl: 09-17 08:28:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:28:03] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0374,	best estimator xgboost's best error=0.0374
[flaml.automl: 09-17 08:28:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:28:05] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0374,	best estimator xgboost's best error=0.0374
[flaml.automl: 09-17 08:28:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:28:06] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.0374,	best estimator xgboost's best error=0.0374
[flaml.automl: 09-17 08:28:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:28:12] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0309,	best estimator xgboost's best error=0.0309
[flaml.automl: 09-17 08:28:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:28:25] {3072} INFO -  at 47.0s,	estimator xgboost's best error=0.0295,	best estimator xgboost's best error=0.0295
[flaml.automl: 09-17 08:28:25] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 08:28:37] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.0295,	best estimator xgboost's best error=0.0295
[flaml.automl: 09-17 08:28:59] {3335} INFO - retrain xgboost for 22.0s
[flaml.automl: 09-17 08:28:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:28:59] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:28:59] {2637} INFO - Time taken to find the best model: 47.0472993850708
[flaml.automl: 09-17 08:28:59] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9704637424388325
CO(0)最好结果：{'pred_time': 2.524977346982863e-05, 'wall_clock_time': 47.0472993850708, 'metric_for_logging': {'pred_time': 2.524977346982863e-05}, 'val_loss': 0.029536257561167536, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.809898138046265}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9420568755182775
CO(0)的mse=0.0020265744858895987
CO(0)的mae=0.02952494442466089
CO(0)的mar=0.03995888160211665
总共花费的时间为：81.64
潮州市
1705A
1706A
3026A
[flaml.automl: 09-17 08:38:54] {2390} INFO - task = regression
[flaml.automl: 09-17 08:38:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:38:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:38:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:38:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:38:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:38:57] {3025} INFO - Estimated sufficient time budget=22926s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 08:38:57] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.1321,	best estimator xgboost's best error=0.1321
[flaml.automl: 09-17 08:38:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:39:00] {3072} INFO -  at 6.1s,	estimator xgboost's best error=0.0718,	best estimator xgboost's best error=0.0718
[flaml.automl: 09-17 08:39:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:39:02] {3072} INFO -  at 8.1s,	estimator xgboost's best error=0.0718,	best estimator xgboost's best error=0.0718
[flaml.automl: 09-17 08:39:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:39:29] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.0718,	best estimator xgboost's best error=0.0718
[flaml.automl: 09-17 08:39:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:39:32] {3072} INFO -  at 37.8s,	estimator xgboost's best error=0.0550,	best estimator xgboost's best error=0.0550
[flaml.automl: 09-17 08:39:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:39:36] {3072} INFO -  at 42.3s,	estimator xgboost's best error=0.0488,	best estimator xgboost's best error=0.0488
[flaml.automl: 09-17 08:39:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:39:41] {3072} INFO -  at 46.9s,	estimator xgboost's best error=0.0488,	best estimator xgboost's best error=0.0488
[flaml.automl: 09-17 08:39:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:39:46] {3072} INFO -  at 51.5s,	estimator xgboost's best error=0.0488,	best estimator xgboost's best error=0.0488
[flaml.automl: 09-17 08:39:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:39:48] {3072} INFO -  at 53.6s,	estimator xgboost's best error=0.0488,	best estimator xgboost's best error=0.0488
[flaml.automl: 09-17 08:39:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:39:52] {3072} INFO -  at 58.4s,	estimator xgboost's best error=0.0488,	best estimator xgboost's best error=0.0488
[flaml.automl: 09-17 08:39:55] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-17 08:39:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 08:39:55] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:39:55] {2637} INFO - Time taken to find the best model: 42.34581685066223
[flaml.automl: 09-17 08:39:55] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
CO(0)最佳损失：0.9512170208284612
CO(0)最好结果：{'pred_time': 3.751620137350112e-05, 'wall_clock_time': 42.34581685066223, 'metric_for_logging': {'pred_time': 3.751620137350112e-05}, 'val_loss': 0.04878297917153886, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 4.555717706680298}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.6853546742875946
CO(0)的mse=0.005137598505398357
CO(0)的mae=0.05017955201151967
CO(0)的mar=0.07048461899147156
总共花费的时间为：61.98
揭阳市
1708A
1709A
1710A
3320A
[flaml.automl: 09-17 08:54:13] {2390} INFO - task = regression
[flaml.automl: 09-17 08:54:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:54:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:54:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:54:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:54:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:54:15] {3025} INFO - Estimated sufficient time budget=82436s. Estimated necessary time budget=82s.
[flaml.automl: 09-17 08:54:15] {3072} INFO -  at 2.1s,	estimator xgboost's best error=0.1639,	best estimator xgboost's best error=0.1639
[flaml.automl: 09-17 08:54:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:54:18] {3072} INFO -  at 5.5s,	estimator xgboost's best error=0.0921,	best estimator xgboost's best error=0.0921
[flaml.automl: 09-17 08:54:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:54:20] {3072} INFO -  at 7.5s,	estimator xgboost's best error=0.0921,	best estimator xgboost's best error=0.0921
[flaml.automl: 09-17 08:54:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:54:26] {3072} INFO -  at 13.2s,	estimator xgboost's best error=0.0921,	best estimator xgboost's best error=0.0921
[flaml.automl: 09-17 08:54:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:54:28] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.0695,	best estimator xgboost's best error=0.0695
[flaml.automl: 09-17 08:54:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:54:30] {3072} INFO -  at 17.7s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 08:54:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:54:33] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 08:54:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:54:37] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 08:54:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:54:39] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 08:54:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:54:42] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 08:54:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:54:45] {3072} INFO -  at 32.5s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 08:54:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:54:47] {3072} INFO -  at 34.7s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 08:54:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:54:58] {3072} INFO -  at 45.9s,	estimator xgboost's best error=0.0559,	best estimator xgboost's best error=0.0559
[flaml.automl: 09-17 08:54:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:55:11] {3072} INFO -  at 58.7s,	estimator xgboost's best error=0.0551,	best estimator xgboost's best error=0.0551
[flaml.automl: 09-17 08:55:23] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 08:55:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:55:23] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:55:23] {2637} INFO - Time taken to find the best model: 58.676555156707764
[flaml.automl: 09-17 08:55:23] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43875}
CO(0)最佳损失：0.9448773376051278
CO(0)最好结果：{'pred_time': 8.186763969965896e-06, 'wall_clock_time': 58.676555156707764, 'metric_for_logging': {'pred_time': 8.186763969965896e-06}, 'val_loss': 0.05512266239487216, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43875}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43875, 'experiment_tag': 'exp', 'time_total_s': 12.811188220977783}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8203794706923303
CO(0)的mse=0.006711215805773386
CO(0)的mae=0.05712914324436149
CO(0)的mar=0.08034439464661626
总共花费的时间为：71.55
云浮市
1712A
[flaml.automl: 09-17 08:59:04] {2390} INFO - task = regression
[flaml.automl: 09-17 08:59:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:59:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:59:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:59:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:59:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:59:05] {3025} INFO - Estimated sufficient time budget=12022s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 08:59:05] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1181,	best estimator xgboost's best error=0.1181
[flaml.automl: 09-17 08:59:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:59:07] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-17 08:59:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:59:08] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-17 08:59:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:59:15] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.0725,	best estimator xgboost's best error=0.0725
[flaml.automl: 09-17 08:59:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:59:16] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.0478,	best estimator xgboost's best error=0.0478
[flaml.automl: 09-17 08:59:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:59:18] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0422,	best estimator xgboost's best error=0.0422
[flaml.automl: 09-17 08:59:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:59:19] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.0422,	best estimator xgboost's best error=0.0422
[flaml.automl: 09-17 08:59:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:59:22] {3072} INFO -  at 18.0s,	estimator xgboost's best error=0.0422,	best estimator xgboost's best error=0.0422
[flaml.automl: 09-17 08:59:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:59:23] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0422,	best estimator xgboost's best error=0.0422
[flaml.automl: 09-17 08:59:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:59:25] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0422,	best estimator xgboost's best error=0.0422
[flaml.automl: 09-17 08:59:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:59:26] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0422,	best estimator xgboost's best error=0.0422
[flaml.automl: 09-17 08:59:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:59:28] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.0422,	best estimator xgboost's best error=0.0422
[flaml.automl: 09-17 08:59:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:59:33] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.0422,	best estimator xgboost's best error=0.0422
[flaml.automl: 09-17 08:59:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:59:36] {3072} INFO -  at 32.0s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-17 08:59:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 08:59:37] {3072} INFO -  at 33.5s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-17 08:59:37] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 08:59:41] {3072} INFO -  at 37.4s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-17 08:59:41] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 08:59:43] {3072} INFO -  at 39.5s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-17 08:59:43] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 08:59:45] {3072} INFO -  at 41.4s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-17 08:59:45] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 08:59:50] {3072} INFO -  at 46.7s,	estimator xgboost's best error=0.0351,	best estimator xgboost's best error=0.0351
[flaml.automl: 09-17 08:59:50] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 08:59:53] {3072} INFO -  at 48.9s,	estimator xgboost's best error=0.0351,	best estimator xgboost's best error=0.0351
[flaml.automl: 09-17 08:59:53] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 09:00:03] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0347,	best estimator xgboost's best error=0.0347
[flaml.automl: 09-17 09:00:17] {3335} INFO - retrain xgboost for 14.2s
[flaml.automl: 09-17 09:00:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7984762384270153, colsample_bynode=1,
             colsample_bytree=0.7850109323524345, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.05372235539459749,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.15476030896055476, scale_pos_weight=1,
             subsample=0.9068360058362719, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:00:17] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:00:17] {2637} INFO - Time taken to find the best model: 59.18879842758179
[flaml.automl: 09-17 09:00:17] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 14, 'max_leaves': 18, 'min_child_weight': 0.05372235539459749, 'learning_rate': 1.0, 'subsample': 0.9068360058362719, 'colsample_bylevel': 0.7984762384270153, 'colsample_bytree': 0.7850109323524345, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.15476030896055476}
CO(0)最佳损失：0.9653071376372266
CO(0)最好结果：{'pred_time': 4.8329310162077226e-05, 'wall_clock_time': 59.18879842758179, 'metric_for_logging': {'pred_time': 4.8329310162077226e-05}, 'val_loss': 0.03469286236277333, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 18, 'min_child_weight': 0.05372235539459749, 'learning_rate': 1.0, 'subsample': 0.9068360058362719, 'colsample_bylevel': 0.7984762384270153, 'colsample_bytree': 0.7850109323524345, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.15476030896055476}, 'config/n_estimators': 14, 'config/max_leaves': 18, 'config/min_child_weight': 0.05372235539459749, 'config/learning_rate': 1.0, 'config/subsample': 0.9068360058362719, 'config/colsample_bylevel': 0.7984762384270153, 'config/colsample_bytree': 0.7850109323524345, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.15476030896055476, 'experiment_tag': 'exp', 'time_total_s': 10.255246877670288}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7984762384270153, colsample_bynode=1,
             colsample_bytree=0.7850109323524345, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.05372235539459749,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.15476030896055476, scale_pos_weight=1,
             subsample=0.9068360058362719, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8918140199409967
CO(0)的mse=0.002571555189709176
CO(0)的mae=0.03353532067669762
CO(0)的mar=0.05188430281912045
总共花费的时间为：73.77
玉溪市
2882A
2883A
[flaml.automl: 09-17 09:06:38] {2390} INFO - task = regression
[flaml.automl: 09-17 09:06:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:06:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:06:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:06:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:06:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:06:40] {3025} INFO - Estimated sufficient time budget=12123s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 09:06:40] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.2608,	best estimator xgboost's best error=0.2608
[flaml.automl: 09-17 09:06:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:06:42] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1744,	best estimator xgboost's best error=0.1744
[flaml.automl: 09-17 09:06:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:06:43] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.1744,	best estimator xgboost's best error=0.1744
[flaml.automl: 09-17 09:06:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:06:52] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.1744,	best estimator xgboost's best error=0.1744
[flaml.automl: 09-17 09:06:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:06:54] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.1580,	best estimator xgboost's best error=0.1580
[flaml.automl: 09-17 09:06:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:06:55] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.1525,	best estimator xgboost's best error=0.1525
[flaml.automl: 09-17 09:06:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:06:57] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.1525,	best estimator xgboost's best error=0.1525
[flaml.automl: 09-17 09:06:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:07:00] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.1525,	best estimator xgboost's best error=0.1525
[flaml.automl: 09-17 09:07:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:07:02] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.1525,	best estimator xgboost's best error=0.1525
[flaml.automl: 09-17 09:07:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:07:06] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.1458,	best estimator xgboost's best error=0.1458
[flaml.automl: 09-17 09:07:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:07:09] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.1458,	best estimator xgboost's best error=0.1458
[flaml.automl: 09-17 09:07:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:07:12] {3072} INFO -  at 33.2s,	estimator xgboost's best error=0.1458,	best estimator xgboost's best error=0.1458
[flaml.automl: 09-17 09:07:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:07:30] {3072} INFO -  at 51.6s,	estimator xgboost's best error=0.1458,	best estimator xgboost's best error=0.1458
[flaml.automl: 09-17 09:07:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:07:38] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.1458,	best estimator xgboost's best error=0.1458
[flaml.automl: 09-17 09:07:43] {3335} INFO - retrain xgboost for 4.5s
[flaml.automl: 09-17 09:07:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:07:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:07:43] {2637} INFO - Time taken to find the best model: 27.69495129585266
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}
CO(0)最佳损失：0.854215987807962
CO(0)最好结果：{'pred_time': 3.254567647586519e-05, 'wall_clock_time': 27.69495129585266, 'metric_for_logging': {'pred_time': 3.254567647586519e-05}, 'val_loss': 0.145784012192038, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}, 'config/n_estimators': 4, 'config/max_leaves': 10, 'config/min_child_weight': 0.06372323163910634, 'config/learning_rate': 0.4086634770112844, 'config/subsample': 0.770412309752864, 'config/colsample_bylevel': 0.6720329282771347, 'config/colsample_bytree': 0.752099095598003, 'config/reg_alpha': 0.00271725862127768, 'config/reg_lambda': 3.9335607770117846, 'experiment_tag': 'exp', 'time_total_s': 4.393063306808472}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.36703895896481054
CO(0)的mse=0.06810767516212506
CO(0)的mae=0.14114453815000894
CO(0)的mar=0.14891868860083246
总共花费的时间为：64.69
菏泽市
1719A
[flaml.automl: 09-17 09:10:50] {2390} INFO - task = regression
[flaml.automl: 09-17 09:10:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:10:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:10:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:10:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:10:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:10:51] {3025} INFO - Estimated sufficient time budget=11781s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 09:10:51] {3072} INFO -  at 1.2s,	estimator xgboost's best error=0.1448,	best estimator xgboost's best error=0.1448
[flaml.automl: 09-17 09:10:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:10:53] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.0942,	best estimator xgboost's best error=0.0942
[flaml.automl: 09-17 09:10:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:10:54] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0942,	best estimator xgboost's best error=0.0942
[flaml.automl: 09-17 09:10:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:11:01] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.0942,	best estimator xgboost's best error=0.0942
[flaml.automl: 09-17 09:11:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:11:02] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-17 09:11:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:11:04] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-17 09:11:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:11:05] {3072} INFO -  at 15.6s,	estimator xgboost's best error=0.0563,	best estimator xgboost's best error=0.0563
[flaml.automl: 09-17 09:11:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:11:08] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0563,	best estimator xgboost's best error=0.0563
[flaml.automl: 09-17 09:11:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:11:09] {3072} INFO -  at 19.5s,	estimator xgboost's best error=0.0548,	best estimator xgboost's best error=0.0548
[flaml.automl: 09-17 09:11:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:11:12] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0548,	best estimator xgboost's best error=0.0548
[flaml.automl: 09-17 09:11:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:11:13] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.0521,	best estimator xgboost's best error=0.0521
[flaml.automl: 09-17 09:11:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:11:15] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.0521,	best estimator xgboost's best error=0.0521
[flaml.automl: 09-17 09:11:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:11:20] {3072} INFO -  at 30.8s,	estimator xgboost's best error=0.0421,	best estimator xgboost's best error=0.0421
[flaml.automl: 09-17 09:11:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:11:29] {3072} INFO -  at 39.7s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-17 09:11:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 09:11:35] {3072} INFO -  at 44.9s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-17 09:11:35] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 09:11:49] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-17 09:11:58] {3335} INFO - retrain xgboost for 8.8s
[flaml.automl: 09-17 09:11:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 09:11:58] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:11:58] {2637} INFO - Time taken to find the best model: 39.693747997283936
CO(0)最佳参数：{'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}
CO(0)最佳损失：0.95891000338473
CO(0)最好结果：{'pred_time': 3.347781013412729e-05, 'wall_clock_time': 39.693747997283936, 'metric_for_logging': {'pred_time': 3.347781013412729e-05}, 'val_loss': 0.04108999661527005, 'training_iteration': 1, 'config': {'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}, 'config/n_estimators': 25, 'config/max_leaves': 6, 'config/min_child_weight': 2.978767810916454, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532448312341252, 'config/reg_alpha': 0.0015779591896894877, 'config/reg_lambda': 1.3500086298278042, 'experiment_tag': 'exp', 'time_total_s': 8.92493224143982}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9471926716886332
CO(0)的mse=0.0037454729327792095
CO(0)的mae=0.03921521732237842
CO(0)的mar=0.09887634302524
总共花费的时间为：68.25
大同市
1721A
1725A
1726A
3565A
3566A
3567A
[flaml.automl: 09-17 09:30:50] {2390} INFO - task = regression
[flaml.automl: 09-17 09:30:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:30:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:30:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:30:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:30:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:30:52] {3025} INFO - Estimated sufficient time budget=142270s. Estimated necessary time budget=142s.
[flaml.automl: 09-17 09:30:52] {3072} INFO -  at 2.6s,	estimator xgboost's best error=0.1830,	best estimator xgboost's best error=0.1830
[flaml.automl: 09-17 09:30:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:30:56] {3072} INFO -  at 6.6s,	estimator xgboost's best error=0.1253,	best estimator xgboost's best error=0.1253
[flaml.automl: 09-17 09:30:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:30:58] {3072} INFO -  at 8.8s,	estimator xgboost's best error=0.1253,	best estimator xgboost's best error=0.1253
[flaml.automl: 09-17 09:30:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:31:01] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.1253,	best estimator xgboost's best error=0.1253
[flaml.automl: 09-17 09:31:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:31:04] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.1126,	best estimator xgboost's best error=0.1126
[flaml.automl: 09-17 09:31:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:31:07] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.1017,	best estimator xgboost's best error=0.1017
[flaml.automl: 09-17 09:31:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:31:10] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.1017,	best estimator xgboost's best error=0.1017
[flaml.automl: 09-17 09:31:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:31:12] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.1017,	best estimator xgboost's best error=0.1017
[flaml.automl: 09-17 09:31:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:31:14] {3072} INFO -  at 24.7s,	estimator xgboost's best error=0.1017,	best estimator xgboost's best error=0.1017
[flaml.automl: 09-17 09:31:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:31:17] {3072} INFO -  at 27.3s,	estimator xgboost's best error=0.1017,	best estimator xgboost's best error=0.1017
[flaml.automl: 09-17 09:31:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:31:20] {3072} INFO -  at 30.3s,	estimator xgboost's best error=0.1017,	best estimator xgboost's best error=0.1017
[flaml.automl: 09-17 09:31:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:31:22] {3072} INFO -  at 32.5s,	estimator xgboost's best error=0.1017,	best estimator xgboost's best error=0.1017
[flaml.automl: 09-17 09:31:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:31:34] {3072} INFO -  at 44.6s,	estimator xgboost's best error=0.0955,	best estimator xgboost's best error=0.0955
[flaml.automl: 09-17 09:31:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:31:49] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0944,	best estimator xgboost's best error=0.0944
[flaml.automl: 09-17 09:32:01] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-17 09:32:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:32:01] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:32:01] {2637} INFO - Time taken to find the best model: 59.15637826919556
[flaml.automl: 09-17 09:32:01] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 63883}
CO(0)最佳损失：0.9055876260907957
CO(0)最好结果：{'pred_time': 5.715631199380112e-06, 'wall_clock_time': 59.15637826919556, 'metric_for_logging': {'pred_time': 5.715631199380112e-06}, 'val_loss': 0.09441237390920433, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 63883}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 63883, 'experiment_tag': 'exp', 'time_total_s': 14.596423149108887}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7425979393110262
CO(0)的mse=0.025773400248103154
CO(0)的mae=0.09322005904496238
CO(0)的mar=0.1883379139823473
总共花费的时间为：72.22
长治市
1728A
1731A
2845A
3568A
3569A
3570A
[flaml.automl: 09-17 09:51:59] {2390} INFO - task = regression
[flaml.automl: 09-17 09:51:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:51:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:51:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:51:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:51:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:52:00] {3025} INFO - Estimated sufficient time budget=77352s. Estimated necessary time budget=77s.
[flaml.automl: 09-17 09:52:00] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.3442,	best estimator xgboost's best error=0.3442
[flaml.automl: 09-17 09:52:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:52:02] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.2018,	best estimator xgboost's best error=0.2018
[flaml.automl: 09-17 09:52:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:52:03] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.2018,	best estimator xgboost's best error=0.2018
[flaml.automl: 09-17 09:52:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:52:07] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.2018,	best estimator xgboost's best error=0.2018
[flaml.automl: 09-17 09:52:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:52:08] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.1628,	best estimator xgboost's best error=0.1628
[flaml.automl: 09-17 09:52:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:52:10] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.1458,	best estimator xgboost's best error=0.1458
[flaml.automl: 09-17 09:52:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:52:11] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.1458,	best estimator xgboost's best error=0.1458
[flaml.automl: 09-17 09:52:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:52:14] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.1458,	best estimator xgboost's best error=0.1458
[flaml.automl: 09-17 09:52:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:52:15] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.1458,	best estimator xgboost's best error=0.1458
[flaml.automl: 09-17 09:52:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:52:18] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.1458,	best estimator xgboost's best error=0.1458
[flaml.automl: 09-17 09:52:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:52:19] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.1446,	best estimator xgboost's best error=0.1446
[flaml.automl: 09-17 09:52:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:52:20] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.1446,	best estimator xgboost's best error=0.1446
[flaml.automl: 09-17 09:52:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:52:27] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.1376,	best estimator xgboost's best error=0.1376
[flaml.automl: 09-17 09:52:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:52:39] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.1338,	best estimator xgboost's best error=0.1338
[flaml.automl: 09-17 09:52:39] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 09:52:46] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.1338,	best estimator xgboost's best error=0.1338
[flaml.automl: 09-17 09:52:46] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 09:52:58] {3072} INFO -  at 60.0s,	estimator xgboost's best error=0.1338,	best estimator xgboost's best error=0.1338
[flaml.automl: 09-17 09:53:21] {3335} INFO - retrain xgboost for 22.4s
[flaml.automl: 09-17 09:53:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:53:21] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:53:21] {2637} INFO - Time taken to find the best model: 40.6511869430542
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64945}
CO(0)最佳损失：0.8662300408804484
CO(0)最好结果：{'pred_time': 6.0810447482499814e-06, 'wall_clock_time': 40.6511869430542, 'metric_for_logging': {'pred_time': 6.0810447482499814e-06}, 'val_loss': 0.13376995911955156, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64945}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 64945, 'experiment_tag': 'exp', 'time_total_s': 12.126401424407959}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7760174644367979
CO(0)的mse=0.046187586293390225
CO(0)的mae=0.13491745992451032
CO(0)的mar=0.15159683289001563
总共花费的时间为：83.47
临汾市
3668A
[flaml.automl: 09-17 09:56:57] {2390} INFO - task = regression
[flaml.automl: 09-17 09:56:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:56:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:56:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:56:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:56:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:56:58] {3025} INFO - Estimated sufficient time budget=12115s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 09:56:58] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.4117,	best estimator xgboost's best error=0.4117
[flaml.automl: 09-17 09:56:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:57:00] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.2528,	best estimator xgboost's best error=0.2528
[flaml.automl: 09-17 09:57:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:57:01] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.2528,	best estimator xgboost's best error=0.2528
[flaml.automl: 09-17 09:57:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:57:08] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.2528,	best estimator xgboost's best error=0.2528
[flaml.automl: 09-17 09:57:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:57:09] {3072} INFO -  at 12.6s,	estimator xgboost's best error=0.1681,	best estimator xgboost's best error=0.1681
[flaml.automl: 09-17 09:57:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:57:11] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.1448,	best estimator xgboost's best error=0.1448
[flaml.automl: 09-17 09:57:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:57:13] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.1448,	best estimator xgboost's best error=0.1448
[flaml.automl: 09-17 09:57:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:57:15] {3072} INFO -  at 18.0s,	estimator xgboost's best error=0.1448,	best estimator xgboost's best error=0.1448
[flaml.automl: 09-17 09:57:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:57:16] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.1448,	best estimator xgboost's best error=0.1448
[flaml.automl: 09-17 09:57:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:57:18] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.1448,	best estimator xgboost's best error=0.1448
[flaml.automl: 09-17 09:57:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:57:19] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.1448,	best estimator xgboost's best error=0.1448
[flaml.automl: 09-17 09:57:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:57:21] {3072} INFO -  at 23.9s,	estimator xgboost's best error=0.1448,	best estimator xgboost's best error=0.1448
[flaml.automl: 09-17 09:57:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:57:26] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.1448,	best estimator xgboost's best error=0.1448
[flaml.automl: 09-17 09:57:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:57:29] {3072} INFO -  at 32.0s,	estimator xgboost's best error=0.1386,	best estimator xgboost's best error=0.1386
[flaml.automl: 09-17 09:57:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 09:57:30] {3072} INFO -  at 33.6s,	estimator xgboost's best error=0.1386,	best estimator xgboost's best error=0.1386
[flaml.automl: 09-17 09:57:30] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 09:57:34] {3072} INFO -  at 37.5s,	estimator xgboost's best error=0.1386,	best estimator xgboost's best error=0.1386
[flaml.automl: 09-17 09:57:34] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 09:57:36] {3072} INFO -  at 39.6s,	estimator xgboost's best error=0.1386,	best estimator xgboost's best error=0.1386
[flaml.automl: 09-17 09:57:36] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 09:57:38] {3072} INFO -  at 41.4s,	estimator xgboost's best error=0.1386,	best estimator xgboost's best error=0.1386
[flaml.automl: 09-17 09:57:38] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 09:57:43] {3072} INFO -  at 46.7s,	estimator xgboost's best error=0.1215,	best estimator xgboost's best error=0.1215
[flaml.automl: 09-17 09:57:43] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 09:57:46] {3072} INFO -  at 49.0s,	estimator xgboost's best error=0.1215,	best estimator xgboost's best error=0.1215
[flaml.automl: 09-17 09:57:46] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 09:57:56] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.1215,	best estimator xgboost's best error=0.1215
[flaml.automl: 09-17 09:58:01] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-17 09:58:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7178642069579442, colsample_bynode=1,
             colsample_bytree=0.7808640703646168, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9481937935550555,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.2628045260160708, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9333220038816206, scale_pos_weight=1,
             subsample=0.8151775216506888, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:58:01] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:58:01] {2637} INFO - Time taken to find the best model: 46.68699288368225
[flaml.automl: 09-17 09:58:01] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 9, 'max_leaves': 10, 'min_child_weight': 0.2628045260160708, 'learning_rate': 0.9481937935550555, 'subsample': 0.8151775216506888, 'colsample_bylevel': 0.7178642069579442, 'colsample_bytree': 0.7808640703646168, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9333220038816206}
CO(0)最佳损失：0.8784586963675566
CO(0)最好结果：{'pred_time': 3.2711349077672766e-05, 'wall_clock_time': 46.68699288368225, 'metric_for_logging': {'pred_time': 3.2711349077672766e-05}, 'val_loss': 0.12154130363244339, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 10, 'min_child_weight': 0.2628045260160708, 'learning_rate': 0.9481937935550555, 'subsample': 0.8151775216506888, 'colsample_bylevel': 0.7178642069579442, 'colsample_bytree': 0.7808640703646168, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9333220038816206}, 'config/n_estimators': 9, 'config/max_leaves': 10, 'config/min_child_weight': 0.2628045260160708, 'config/learning_rate': 0.9481937935550555, 'config/subsample': 0.8151775216506888, 'config/colsample_bylevel': 0.7178642069579442, 'config/colsample_bytree': 0.7808640703646168, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.9333220038816206, 'experiment_tag': 'exp', 'time_total_s': 5.256680727005005}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7178642069579442, colsample_bynode=1,
             colsample_bytree=0.7808640703646168, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9481937935550555,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.2628045260160708, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9333220038816206, scale_pos_weight=1,
             subsample=0.8151775216506888, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8528144442654992
CO(0)的mse=0.03536048021412567
CO(0)的mae=0.1245492548945265
CO(0)的mar=0.1262969384163323
总共花费的时间为：64.65
阳泉市
1738A
1739A
1743A
3619A
[flaml.automl: 09-17 10:11:14] {2390} INFO - task = regression
[flaml.automl: 09-17 10:11:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:11:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:11:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:11:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:11:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:11:16] {3025} INFO - Estimated sufficient time budget=101791s. Estimated necessary time budget=102s.
[flaml.automl: 09-17 10:11:16] {3072} INFO -  at 2.7s,	estimator xgboost's best error=0.2677,	best estimator xgboost's best error=0.2677
[flaml.automl: 09-17 10:11:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:11:20] {3072} INFO -  at 6.4s,	estimator xgboost's best error=0.1406,	best estimator xgboost's best error=0.1406
[flaml.automl: 09-17 10:11:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:11:22] {3072} INFO -  at 8.8s,	estimator xgboost's best error=0.1406,	best estimator xgboost's best error=0.1406
[flaml.automl: 09-17 10:11:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:11:27] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.1406,	best estimator xgboost's best error=0.1406
[flaml.automl: 09-17 10:11:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:11:29] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.1024,	best estimator xgboost's best error=0.1024
[flaml.automl: 09-17 10:11:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:11:32] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0839,	best estimator xgboost's best error=0.0839
[flaml.automl: 09-17 10:11:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:11:35] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0839,	best estimator xgboost's best error=0.0839
[flaml.automl: 09-17 10:11:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:11:39] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0839,	best estimator xgboost's best error=0.0839
[flaml.automl: 09-17 10:11:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:11:41] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.0839,	best estimator xgboost's best error=0.0839
[flaml.automl: 09-17 10:11:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:11:44] {3072} INFO -  at 30.8s,	estimator xgboost's best error=0.0839,	best estimator xgboost's best error=0.0839
[flaml.automl: 09-17 10:11:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:11:47] {3072} INFO -  at 33.3s,	estimator xgboost's best error=0.0839,	best estimator xgboost's best error=0.0839
[flaml.automl: 09-17 10:11:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:11:49] {3072} INFO -  at 35.5s,	estimator xgboost's best error=0.0839,	best estimator xgboost's best error=0.0839
[flaml.automl: 09-17 10:11:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:11:59] {3072} INFO -  at 46.1s,	estimator xgboost's best error=0.0770,	best estimator xgboost's best error=0.0770
[flaml.automl: 09-17 10:11:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:12:12] {3072} INFO -  at 58.5s,	estimator xgboost's best error=0.0734,	best estimator xgboost's best error=0.0734
[flaml.automl: 09-17 10:12:29] {3335} INFO - retrain xgboost for 17.5s
[flaml.automl: 09-17 10:12:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 10:12:29] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:12:29] {2637} INFO - Time taken to find the best model: 58.4783239364624
[flaml.automl: 09-17 10:12:29] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43490}
CO(0)最佳损失：0.9266440190055122
CO(0)最好结果：{'pred_time': 8.59056721803147e-06, 'wall_clock_time': 58.4783239364624, 'metric_for_logging': {'pred_time': 8.59056721803147e-06}, 'val_loss': 0.07335598099448778, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43490}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43490, 'experiment_tag': 'exp', 'time_total_s': 12.344205617904663}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.894712277486905
CO(0)的mse=0.012297929984384377
CO(0)的mae=0.06980655296625815
CO(0)的mar=0.08247276064954388
总共花费的时间为：76.85
赤峰市
1744A
1745A
3286A
[flaml.automl: 09-17 10:21:58] {2390} INFO - task = regression
[flaml.automl: 09-17 10:21:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:21:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:21:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:21:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:21:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:22:00] {3025} INFO - Estimated sufficient time budget=12157s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 10:22:00] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1482,	best estimator xgboost's best error=0.1482
[flaml.automl: 09-17 10:22:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:22:02] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0934,	best estimator xgboost's best error=0.0934
[flaml.automl: 09-17 10:22:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:22:03] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0934,	best estimator xgboost's best error=0.0934
[flaml.automl: 09-17 10:22:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:22:13] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0934,	best estimator xgboost's best error=0.0934
[flaml.automl: 09-17 10:22:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:22:14] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.0788,	best estimator xgboost's best error=0.0788
[flaml.automl: 09-17 10:22:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:22:16] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 10:22:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:22:17] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 10:22:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:22:20] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 10:22:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:22:21] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 10:22:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:22:24] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 10:22:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:22:25] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 10:22:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:22:26] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 10:22:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:22:33] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0631,	best estimator xgboost's best error=0.0631
[flaml.automl: 09-17 10:22:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:22:45] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0605,	best estimator xgboost's best error=0.0605
[flaml.automl: 09-17 10:22:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 10:22:51] {3072} INFO -  at 52.8s,	estimator xgboost's best error=0.0605,	best estimator xgboost's best error=0.0605
[flaml.automl: 09-17 10:23:03] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 10:23:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 10:23:03] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:23:03] {2637} INFO - Time taken to find the best model: 46.25957179069519
[flaml.automl: 09-17 10:23:03] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9395018555216043
CO(0)最好结果：{'pred_time': 1.1168087699583598e-05, 'wall_clock_time': 46.25957179069519, 'metric_for_logging': {'pred_time': 1.1168087699583598e-05}, 'val_loss': 0.06049814447839578, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.0264573097229}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8308282104417217
CO(0)的mse=0.011200997656182717
CO(0)的mae=0.06281515402076293
CO(0)的mar=0.1685271744162708
总共花费的时间为：65.36
鞍山市
1749A
1750A
1751A
1752A
1753A
1754A
[flaml.automl: 09-17 10:43:06] {2390} INFO - task = regression
[flaml.automl: 09-17 10:43:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:43:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:43:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:43:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:43:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:43:09] {3025} INFO - Estimated sufficient time budget=168937s. Estimated necessary time budget=169s.
[flaml.automl: 09-17 10:43:09] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.3751,	best estimator xgboost's best error=0.3751
[flaml.automl: 09-17 10:43:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:43:13] {3072} INFO -  at 7.1s,	estimator xgboost's best error=0.2702,	best estimator xgboost's best error=0.2702
[flaml.automl: 09-17 10:43:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:43:16] {3072} INFO -  at 9.8s,	estimator xgboost's best error=0.2702,	best estimator xgboost's best error=0.2702
[flaml.automl: 09-17 10:43:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:43:20] {3072} INFO -  at 13.4s,	estimator xgboost's best error=0.2702,	best estimator xgboost's best error=0.2702
[flaml.automl: 09-17 10:43:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:43:22] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.2189,	best estimator xgboost's best error=0.2189
[flaml.automl: 09-17 10:43:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:43:25] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.2047,	best estimator xgboost's best error=0.2047
[flaml.automl: 09-17 10:43:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:43:27] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.2047,	best estimator xgboost's best error=0.2047
[flaml.automl: 09-17 10:43:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:43:30] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.2047,	best estimator xgboost's best error=0.2047
[flaml.automl: 09-17 10:43:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:43:33] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.2047,	best estimator xgboost's best error=0.2047
[flaml.automl: 09-17 10:43:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:43:35] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.2047,	best estimator xgboost's best error=0.2047
[flaml.automl: 09-17 10:43:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:43:38] {3072} INFO -  at 31.5s,	estimator xgboost's best error=0.2010,	best estimator xgboost's best error=0.2010
[flaml.automl: 09-17 10:43:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:43:40] {3072} INFO -  at 33.6s,	estimator xgboost's best error=0.2010,	best estimator xgboost's best error=0.2010
[flaml.automl: 09-17 10:43:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:43:50] {3072} INFO -  at 44.1s,	estimator xgboost's best error=0.2009,	best estimator xgboost's best error=0.2009
[flaml.automl: 09-17 10:43:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:44:05] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.1940,	best estimator xgboost's best error=0.1940
[flaml.automl: 09-17 10:44:17] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 10:44:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 10:44:17] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:44:17] {2637} INFO - Time taken to find the best model: 59.0670747756958
[flaml.automl: 09-17 10:44:17] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 63301}
CO(0)最佳损失：0.8059548780837112
CO(0)最好结果：{'pred_time': 5.8074617101144885e-06, 'wall_clock_time': 59.0670747756958, 'metric_for_logging': {'pred_time': 5.8074617101144885e-06}, 'val_loss': 0.19404512191628878, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 63301}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 63301, 'experiment_tag': 'exp', 'time_total_s': 15.000252485275269}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.5829015761233953
CO(0)的mse=0.10365229004417227
CO(0)的mae=0.20035885178489934
CO(0)的mar=0.19717678679543799
总共花费的时间为：72.69
抚顺市
1755A
1756A
1757A
1758A
1760A
[flaml.automl: 09-17 11:00:12] {2390} INFO - task = regression
[flaml.automl: 09-17 11:00:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:00:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:00:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:00:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:00:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:00:14] {3025} INFO - Estimated sufficient time budget=91298s. Estimated necessary time budget=91s.
[flaml.automl: 09-17 11:00:14] {3072} INFO -  at 2.0s,	estimator xgboost's best error=0.2507,	best estimator xgboost's best error=0.2507
[flaml.automl: 09-17 11:00:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:00:17] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.1550,	best estimator xgboost's best error=0.1550
[flaml.automl: 09-17 11:00:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:00:18] {3072} INFO -  at 6.1s,	estimator xgboost's best error=0.1550,	best estimator xgboost's best error=0.1550
[flaml.automl: 09-17 11:00:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:00:23] {3072} INFO -  at 10.9s,	estimator xgboost's best error=0.1550,	best estimator xgboost's best error=0.1550
[flaml.automl: 09-17 11:00:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:00:24] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.1318,	best estimator xgboost's best error=0.1318
[flaml.automl: 09-17 11:00:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:00:25] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.1184,	best estimator xgboost's best error=0.1184
[flaml.automl: 09-17 11:00:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:00:27] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.1184,	best estimator xgboost's best error=0.1184
[flaml.automl: 09-17 11:00:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:00:30] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.1184,	best estimator xgboost's best error=0.1184
[flaml.automl: 09-17 11:00:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:00:31] {3072} INFO -  at 18.7s,	estimator xgboost's best error=0.1184,	best estimator xgboost's best error=0.1184
[flaml.automl: 09-17 11:00:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:00:33] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.1184,	best estimator xgboost's best error=0.1184
[flaml.automl: 09-17 11:00:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:00:35] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.1175,	best estimator xgboost's best error=0.1175
[flaml.automl: 09-17 11:00:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:00:36] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.1175,	best estimator xgboost's best error=0.1175
[flaml.automl: 09-17 11:00:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:00:42] {3072} INFO -  at 30.4s,	estimator xgboost's best error=0.1143,	best estimator xgboost's best error=0.1143
[flaml.automl: 09-17 11:00:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:00:54] {3072} INFO -  at 42.4s,	estimator xgboost's best error=0.1101,	best estimator xgboost's best error=0.1101
[flaml.automl: 09-17 11:00:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 11:01:01] {3072} INFO -  at 48.9s,	estimator xgboost's best error=0.1101,	best estimator xgboost's best error=0.1101
[flaml.automl: 09-17 11:01:13] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 11:01:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:01:13] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:01:13] {2637} INFO - Time taken to find the best model: 42.37409853935242
[flaml.automl: 09-17 11:01:13] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52758}
CO(0)最佳损失：0.8899085223620486
CO(0)最好结果：{'pred_time': 6.898198135066273e-06, 'wall_clock_time': 42.37409853935242, 'metric_for_logging': {'pred_time': 6.898198135066273e-06}, 'val_loss': 0.11009147763795135, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52758}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52758, 'experiment_tag': 'exp', 'time_total_s': 11.95452880859375}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7493295422454556
CO(0)的mse=0.03261211697522049
CO(0)的mae=0.11231856146913238
CO(0)的mar=0.15042964859836092
总共花费的时间为：62.05
本溪市
1761A
1762A
1763A
1764A
1765A
[flaml.automl: 09-17 11:16:44] {2390} INFO - task = regression
[flaml.automl: 09-17 11:16:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:16:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:16:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:16:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:16:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:16:45] {3025} INFO - Estimated sufficient time budget=64654s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 11:16:45] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.3860,	best estimator xgboost's best error=0.3860
[flaml.automl: 09-17 11:16:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:16:47] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.2363,	best estimator xgboost's best error=0.2363
[flaml.automl: 09-17 11:16:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:16:48] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.2363,	best estimator xgboost's best error=0.2363
[flaml.automl: 09-17 11:16:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:16:53] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.2363,	best estimator xgboost's best error=0.2363
[flaml.automl: 09-17 11:16:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:16:54] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.2070,	best estimator xgboost's best error=0.2070
[flaml.automl: 09-17 11:16:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:16:56] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.1888,	best estimator xgboost's best error=0.1888
[flaml.automl: 09-17 11:16:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:16:57] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.1888,	best estimator xgboost's best error=0.1888
[flaml.automl: 09-17 11:16:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:17:00] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.1888,	best estimator xgboost's best error=0.1888
[flaml.automl: 09-17 11:17:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:17:01] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.1888,	best estimator xgboost's best error=0.1888
[flaml.automl: 09-17 11:17:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:17:04] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:17:05] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:17:07] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:17:10] {3072} INFO -  at 26.1s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:17:12] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 11:17:15] {3072} INFO -  at 31.5s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:15] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 11:17:20] {3072} INFO -  at 36.3s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:20] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 11:17:21] {3072} INFO -  at 37.9s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:21] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 11:17:23] {3072} INFO -  at 39.3s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:23] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 11:17:30] {3072} INFO -  at 46.2s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:30] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 11:17:31] {3072} INFO -  at 47.8s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:31] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 11:17:39] {3072} INFO -  at 55.4s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:39] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 11:17:42] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-17 11:17:45] {3335} INFO - retrain xgboost for 2.7s
[flaml.automl: 09-17 11:17:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:17:45] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:17:45] {2637} INFO - Time taken to find the best model: 20.160844564437866
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 11, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846, 'FLAML_sample_size': 10000}
CO(0)最佳损失：0.8146105788847886
CO(0)最好结果：{'pred_time': 7.522116096490229e-06, 'wall_clock_time': 20.160844564437866, 'metric_for_logging': {'pred_time': 7.522116096490229e-06}, 'val_loss': 0.18538942111521137, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 11, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846, 'FLAML_sample_size': 10000}, 'config/n_estimators': 4, 'config/max_leaves': 11, 'config/min_child_weight': 0.06372323163910634, 'config/learning_rate': 0.4086634770112844, 'config/subsample': 0.770412309752864, 'config/colsample_bylevel': 0.6720329282771347, 'config/colsample_bytree': 0.752099095598003, 'config/reg_alpha': 0.00271725862127768, 'config/reg_lambda': 3.9335607770117846, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 2.67425274848938}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.43590632793631867
CO(0)的mse=0.13090791503666002
CO(0)的mae=0.20846797533147488
CO(0)的mar=0.183930231243395
总共花费的时间为：62.57
锦州市
1767A
1768A
1770A
1771A
[flaml.automl: 09-17 11:30:34] {2390} INFO - task = regression
[flaml.automl: 09-17 11:30:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:30:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:30:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:30:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:30:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:30:36] {3025} INFO - Estimated sufficient time budget=83279s. Estimated necessary time budget=83s.
[flaml.automl: 09-17 11:30:36] {3072} INFO -  at 2.2s,	estimator xgboost's best error=0.2336,	best estimator xgboost's best error=0.2336
[flaml.automl: 09-17 11:30:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:30:39] {3072} INFO -  at 5.6s,	estimator xgboost's best error=0.1395,	best estimator xgboost's best error=0.1395
[flaml.automl: 09-17 11:30:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:30:41] {3072} INFO -  at 7.8s,	estimator xgboost's best error=0.1395,	best estimator xgboost's best error=0.1395
[flaml.automl: 09-17 11:30:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:30:47] {3072} INFO -  at 13.1s,	estimator xgboost's best error=0.1395,	best estimator xgboost's best error=0.1395
[flaml.automl: 09-17 11:30:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:30:49] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.1151,	best estimator xgboost's best error=0.1151
[flaml.automl: 09-17 11:30:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:30:51] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.1023,	best estimator xgboost's best error=0.1023
[flaml.automl: 09-17 11:30:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:30:54] {3072} INFO -  at 20.0s,	estimator xgboost's best error=0.1023,	best estimator xgboost's best error=0.1023
[flaml.automl: 09-17 11:30:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:30:57] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.1023,	best estimator xgboost's best error=0.1023
[flaml.automl: 09-17 11:30:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:30:59] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.1023,	best estimator xgboost's best error=0.1023
[flaml.automl: 09-17 11:30:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:31:03] {3072} INFO -  at 29.3s,	estimator xgboost's best error=0.1023,	best estimator xgboost's best error=0.1023
[flaml.automl: 09-17 11:31:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:31:04] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.1017,	best estimator xgboost's best error=0.1017
[flaml.automl: 09-17 11:31:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:31:06] {3072} INFO -  at 32.1s,	estimator xgboost's best error=0.1017,	best estimator xgboost's best error=0.1017
[flaml.automl: 09-17 11:31:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:31:12] {3072} INFO -  at 38.6s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-17 11:31:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:31:24] {3072} INFO -  at 50.6s,	estimator xgboost's best error=0.0924,	best estimator xgboost's best error=0.0924
[flaml.automl: 09-17 11:31:36] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-17 11:31:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:31:36] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:31:36] {2637} INFO - Time taken to find the best model: 50.580079078674316
[flaml.automl: 09-17 11:31:36] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42660}
CO(0)最佳损失：0.9075707484498557
CO(0)最好结果：{'pred_time': 9.462350531469418e-06, 'wall_clock_time': 50.580079078674316, 'metric_for_logging': {'pred_time': 9.462350531469418e-06}, 'val_loss': 0.09242925155014428, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42660}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42660, 'experiment_tag': 'exp', 'time_total_s': 11.985320329666138}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7933759913155437
CO(0)的mse=0.024262922483589577
CO(0)的mae=0.09362429019613558
CO(0)的mar=0.11337684504409298
总共花费的时间为：63.28
吉林市
1772A
1774A
1775A
1776A
2868A
[flaml.automl: 09-17 11:48:05] {2390} INFO - task = regression
[flaml.automl: 09-17 11:48:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:48:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:48:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:48:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:48:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:48:06] {3025} INFO - Estimated sufficient time budget=64740s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 11:48:06] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1496,	best estimator xgboost's best error=0.1496
[flaml.automl: 09-17 11:48:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:48:08] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0863,	best estimator xgboost's best error=0.0863
[flaml.automl: 09-17 11:48:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:48:09] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0863,	best estimator xgboost's best error=0.0863
[flaml.automl: 09-17 11:48:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:48:14] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0863,	best estimator xgboost's best error=0.0863
[flaml.automl: 09-17 11:48:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:48:15] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0714,	best estimator xgboost's best error=0.0714
[flaml.automl: 09-17 11:48:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:48:17] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-17 11:48:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:48:18] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-17 11:48:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:48:21] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-17 11:48:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:48:22] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-17 11:48:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:48:25] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-17 11:48:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:48:26] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-17 11:48:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:48:27] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-17 11:48:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:48:34] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0594,	best estimator xgboost's best error=0.0594
[flaml.automl: 09-17 11:48:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:48:46] {3072} INFO -  at 41.6s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-17 11:48:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 11:48:52] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-17 11:49:04] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 11:49:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:49:04] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:49:04] {2637} INFO - Time taken to find the best model: 41.58815264701843
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53932}
CO(0)最佳损失：0.943799689028869
CO(0)最好结果：{'pred_time': 6.9599686292740294e-06, 'wall_clock_time': 41.58815264701843, 'metric_for_logging': {'pred_time': 6.9599686292740294e-06}, 'val_loss': 0.05620031097113102, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53932}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53932, 'experiment_tag': 'exp', 'time_total_s': 12.062129259109497}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8063596057645812
CO(0)的mse=0.008409153208772937
CO(0)的mae=0.05506706595571214
CO(0)的mar=0.07526912387216984
总共花费的时间为：60.91
齐齐哈尔市
1779A
1781A
3662A
[flaml.automl: 09-17 11:58:57] {2390} INFO - task = regression
[flaml.automl: 09-17 11:58:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:58:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:58:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:58:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:58:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:58:59] {3025} INFO - Estimated sufficient time budget=22436s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 11:58:59] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1309,	best estimator xgboost's best error=0.1309
[flaml.automl: 09-17 11:58:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:59:03] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.0887,	best estimator xgboost's best error=0.0887
[flaml.automl: 09-17 11:59:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:59:05] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.0887,	best estimator xgboost's best error=0.0887
[flaml.automl: 09-17 11:59:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:59:31] {3072} INFO -  at 34.7s,	estimator xgboost's best error=0.0887,	best estimator xgboost's best error=0.0887
[flaml.automl: 09-17 11:59:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:59:35] {3072} INFO -  at 38.0s,	estimator xgboost's best error=0.0799,	best estimator xgboost's best error=0.0799
[flaml.automl: 09-17 11:59:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:59:39] {3072} INFO -  at 42.5s,	estimator xgboost's best error=0.0707,	best estimator xgboost's best error=0.0707
[flaml.automl: 09-17 11:59:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:59:43] {3072} INFO -  at 46.1s,	estimator xgboost's best error=0.0707,	best estimator xgboost's best error=0.0707
[flaml.automl: 09-17 11:59:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:59:47] {3072} INFO -  at 50.0s,	estimator xgboost's best error=0.0707,	best estimator xgboost's best error=0.0707
[flaml.automl: 09-17 11:59:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:59:49] {3072} INFO -  at 52.1s,	estimator xgboost's best error=0.0707,	best estimator xgboost's best error=0.0707
[flaml.automl: 09-17 11:59:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:59:53] {3072} INFO -  at 56.6s,	estimator xgboost's best error=0.0707,	best estimator xgboost's best error=0.0707
[flaml.automl: 09-17 11:59:56] {3335} INFO - retrain xgboost for 2.5s
[flaml.automl: 09-17 11:59:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 11:59:56] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:59:56] {2637} INFO - Time taken to find the best model: 42.534889936447144
[flaml.automl: 09-17 11:59:56] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
CO(0)最佳损失：0.9292513424532638
CO(0)最好结果：{'pred_time': 3.85639509559525e-05, 'wall_clock_time': 42.534889936447144, 'metric_for_logging': {'pred_time': 3.85639509559525e-05}, 'val_loss': 0.0707486575467362, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 4.504833221435547}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.6320183628365432
CO(0)的mse=0.015681495804202743
CO(0)的mae=0.07177667439075452
CO(0)的mar=0.142004807039459
总共花费的时间为：59.78
牡丹江市
1784A
1785A
1786A
1787A
[flaml.automl: 09-17 12:12:34] {2390} INFO - task = regression
[flaml.automl: 09-17 12:12:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:12:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:12:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:12:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:12:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:12:36] {3025} INFO - Estimated sufficient time budget=12146s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 12:12:36] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1405,	best estimator xgboost's best error=0.1405
[flaml.automl: 09-17 12:12:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:12:38] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0912,	best estimator xgboost's best error=0.0912
[flaml.automl: 09-17 12:12:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:12:39] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0912,	best estimator xgboost's best error=0.0912
[flaml.automl: 09-17 12:12:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:12:49] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0912,	best estimator xgboost's best error=0.0912
[flaml.automl: 09-17 12:12:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:12:50] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.0743,	best estimator xgboost's best error=0.0743
[flaml.automl: 09-17 12:12:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:12:52] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-17 12:12:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:12:53] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-17 12:12:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:12:56] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-17 12:12:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:12:57] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-17 12:12:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:13:00] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-17 12:13:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:13:01] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-17 12:13:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:13:02] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-17 12:13:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:13:09] {3072} INFO -  at 34.5s,	estimator xgboost's best error=0.0582,	best estimator xgboost's best error=0.0582
[flaml.automl: 09-17 12:13:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:13:21] {3072} INFO -  at 46.5s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-17 12:13:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 12:13:27] {3072} INFO -  at 53.0s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-17 12:13:39] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 12:13:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:13:39] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:13:39] {2637} INFO - Time taken to find the best model: 46.50550389289856
[flaml.automl: 09-17 12:13:39] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.943786738885516
CO(0)最好结果：{'pred_time': 1.0003211911899726e-05, 'wall_clock_time': 46.50550389289856, 'metric_for_logging': {'pred_time': 1.0003211911899726e-05}, 'val_loss': 0.05621326111448398, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.050252914428711}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8428195615001162
CO(0)的mse=0.008605084038161476
CO(0)的mae=0.055137380688351896
CO(0)的mar=0.11662526062613698
总共花费的时间为：65.65
大庆市
1789A
1790A
1792A
1793A
3481A
[flaml.automl: 09-17 12:29:21] {2390} INFO - task = regression
[flaml.automl: 09-17 12:29:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:29:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:29:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:29:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:29:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:29:22] {3025} INFO - Estimated sufficient time budget=64609s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 12:29:22] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.1300,	best estimator xgboost's best error=0.1300
[flaml.automl: 09-17 12:29:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:29:24] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.0812,	best estimator xgboost's best error=0.0812
[flaml.automl: 09-17 12:29:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:29:25] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.0812,	best estimator xgboost's best error=0.0812
[flaml.automl: 09-17 12:29:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:29:30] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0812,	best estimator xgboost's best error=0.0812
[flaml.automl: 09-17 12:29:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:29:31] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-17 12:29:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:29:33] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 12:29:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:29:34] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 12:29:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:29:37] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 12:29:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:29:38] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 12:29:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:29:40] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-17 12:29:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:29:42] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0564,	best estimator xgboost's best error=0.0564
[flaml.automl: 09-17 12:29:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:29:43] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0564,	best estimator xgboost's best error=0.0564
[flaml.automl: 09-17 12:29:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:29:49] {3072} INFO -  at 29.2s,	estimator xgboost's best error=0.0520,	best estimator xgboost's best error=0.0520
[flaml.automl: 09-17 12:29:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:30:01] {3072} INFO -  at 41.3s,	estimator xgboost's best error=0.0497,	best estimator xgboost's best error=0.0497
[flaml.automl: 09-17 12:30:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 12:30:08] {3072} INFO -  at 47.8s,	estimator xgboost's best error=0.0497,	best estimator xgboost's best error=0.0497
[flaml.automl: 09-17 12:30:08] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 12:30:19] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0497,	best estimator xgboost's best error=0.0497
[flaml.automl: 09-17 12:30:31] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 12:30:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:30:31] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:30:31] {2637} INFO - Time taken to find the best model: 41.29957699775696
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53791}
CO(0)最佳损失：0.9502548308695901
CO(0)最好结果：{'pred_time': 6.584253960555665e-06, 'wall_clock_time': 41.29957699775696, 'metric_for_logging': {'pred_time': 6.584253960555665e-06}, 'val_loss': 0.049745169130409966, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53791}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53791, 'experiment_tag': 'exp', 'time_total_s': 12.05318808555603}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8092601934216656
CO(0)的mse=0.009567523977363318
CO(0)的mae=0.05245656254834179
CO(0)的mar=0.1290247625288244
总共花费的时间为：72.21
芜湖市
1795A
1796A
3328A
3465A
3466A
[flaml.automl: 09-17 12:46:42] {2390} INFO - task = regression
[flaml.automl: 09-17 12:46:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:46:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:46:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:46:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:46:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:46:43] {3025} INFO - Estimated sufficient time budget=65205s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 12:46:43] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1630,	best estimator xgboost's best error=0.1630
[flaml.automl: 09-17 12:46:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:46:45] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0936,	best estimator xgboost's best error=0.0936
[flaml.automl: 09-17 12:46:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:46:46] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0936,	best estimator xgboost's best error=0.0936
[flaml.automl: 09-17 12:46:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:46:51] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0936,	best estimator xgboost's best error=0.0936
[flaml.automl: 09-17 12:46:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:46:52] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0745,	best estimator xgboost's best error=0.0745
[flaml.automl: 09-17 12:46:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:46:54] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0653,	best estimator xgboost's best error=0.0653
[flaml.automl: 09-17 12:46:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:46:55] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0653,	best estimator xgboost's best error=0.0653
[flaml.automl: 09-17 12:46:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:46:58] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0653,	best estimator xgboost's best error=0.0653
[flaml.automl: 09-17 12:46:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:46:59] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0653,	best estimator xgboost's best error=0.0653
[flaml.automl: 09-17 12:46:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:47:02] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0653,	best estimator xgboost's best error=0.0653
[flaml.automl: 09-17 12:47:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:47:03] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0645,	best estimator xgboost's best error=0.0645
[flaml.automl: 09-17 12:47:03] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:47:05] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0645,	best estimator xgboost's best error=0.0645
[flaml.automl: 09-17 12:47:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:47:11] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0624,	best estimator xgboost's best error=0.0624
[flaml.automl: 09-17 12:47:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:47:29] {3072} INFO -  at 47.8s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-17 12:47:51] {3335} INFO - retrain xgboost for 22.2s
[flaml.automl: 09-17 12:47:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:47:51] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:47:51] {2637} INFO - Time taken to find the best model: 47.76286768913269
[flaml.automl: 09-17 12:47:51] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54561}
CO(0)最佳损失：0.9394403181839516
CO(0)最好结果：{'pred_time': 1.4249000064850483e-05, 'wall_clock_time': 47.76286768913269, 'metric_for_logging': {'pred_time': 1.4249000064850483e-05}, 'val_loss': 0.060559681816048386, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54561}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54561, 'experiment_tag': 'exp', 'time_total_s': 18.164591073989868}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8024730242190194
CO(0)的mse=0.008044629242907091
CO(0)的mae=0.056636192635816665
CO(0)的mar=0.07967762500711721
总共花费的时间为：71.02
马鞍山市
1798A
1800A
3633A
[flaml.automl: 09-17 12:57:06] {2390} INFO - task = regression
[flaml.automl: 09-17 12:57:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:57:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:57:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:57:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:57:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:57:07] {3025} INFO - Estimated sufficient time budget=12234s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 12:57:07] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2177,	best estimator xgboost's best error=0.2177
[flaml.automl: 09-17 12:57:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:57:09] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1226,	best estimator xgboost's best error=0.1226
[flaml.automl: 09-17 12:57:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:57:10] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1226,	best estimator xgboost's best error=0.1226
[flaml.automl: 09-17 12:57:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:57:20] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.1226,	best estimator xgboost's best error=0.1226
[flaml.automl: 09-17 12:57:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:57:22] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.0981,	best estimator xgboost's best error=0.0981
[flaml.automl: 09-17 12:57:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:57:23] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0886,	best estimator xgboost's best error=0.0886
[flaml.automl: 09-17 12:57:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:57:25] {3072} INFO -  at 19.3s,	estimator xgboost's best error=0.0886,	best estimator xgboost's best error=0.0886
[flaml.automl: 09-17 12:57:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:57:27] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0886,	best estimator xgboost's best error=0.0886
[flaml.automl: 09-17 12:57:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:57:29] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0886,	best estimator xgboost's best error=0.0886
[flaml.automl: 09-17 12:57:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:57:31] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0886,	best estimator xgboost's best error=0.0886
[flaml.automl: 09-17 12:57:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:57:32] {3072} INFO -  at 26.8s,	estimator xgboost's best error=0.0886,	best estimator xgboost's best error=0.0886
[flaml.automl: 09-17 12:57:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:57:33] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.0886,	best estimator xgboost's best error=0.0886
[flaml.automl: 09-17 12:57:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:57:40] {3072} INFO -  at 34.4s,	estimator xgboost's best error=0.0869,	best estimator xgboost's best error=0.0869
[flaml.automl: 09-17 12:57:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:57:52] {3072} INFO -  at 46.5s,	estimator xgboost's best error=0.0833,	best estimator xgboost's best error=0.0833
[flaml.automl: 09-17 12:57:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 12:57:59] {3072} INFO -  at 53.1s,	estimator xgboost's best error=0.0833,	best estimator xgboost's best error=0.0833
[flaml.automl: 09-17 12:58:11] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 12:58:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:58:11] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:58:11] {2637} INFO - Time taken to find the best model: 46.53973031044006
[flaml.automl: 09-17 12:58:11] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.916723408718837
CO(0)最好结果：{'pred_time': 1.158142999634063e-05, 'wall_clock_time': 46.53973031044006, 'metric_for_logging': {'pred_time': 1.158142999634063e-05}, 'val_loss': 0.08327659128116303, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.109973192214966}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7082171215080777
CO(0)的mse=0.0178967180817301
CO(0)的mae=0.08635461380078335
CO(0)的mar=0.1042388024801518
总共花费的时间为：65.65
九江市
1803A
1804A
1805A
1806A
1810A
[flaml.automl: 09-17 13:13:32] {2390} INFO - task = regression
[flaml.automl: 09-17 13:13:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:13:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:13:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:13:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:13:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:13:33] {3025} INFO - Estimated sufficient time budget=62245s. Estimated necessary time budget=62s.
[flaml.automl: 09-17 13:13:33] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1519,	best estimator xgboost's best error=0.1519
[flaml.automl: 09-17 13:13:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:13:36] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0989,	best estimator xgboost's best error=0.0989
[flaml.automl: 09-17 13:13:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:13:37] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0989,	best estimator xgboost's best error=0.0989
[flaml.automl: 09-17 13:13:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:13:42] {3072} INFO -  at 9.5s,	estimator xgboost's best error=0.0989,	best estimator xgboost's best error=0.0989
[flaml.automl: 09-17 13:13:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:13:43] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0849,	best estimator xgboost's best error=0.0849
[flaml.automl: 09-17 13:13:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:13:44] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-17 13:13:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:13:46] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-17 13:13:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:13:48] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-17 13:13:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:13:49] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-17 13:13:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:13:52] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-17 13:13:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:13:54] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-17 13:13:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:13:55] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-17 13:13:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:14:02] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0699,	best estimator xgboost's best error=0.0699
[flaml.automl: 09-17 13:14:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:14:22] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0678,	best estimator xgboost's best error=0.0678
[flaml.automl: 09-17 13:14:42] {3335} INFO - retrain xgboost for 20.6s
[flaml.automl: 09-17 13:14:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:14:42] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:14:42] {2637} INFO - Time taken to find the best model: 49.64364671707153
[flaml.automl: 09-17 13:14:42] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51856}
CO(0)最佳损失：0.9321959016800693
CO(0)最好结果：{'pred_time': 1.5513220829750176e-05, 'wall_clock_time': 49.64364671707153, 'metric_for_logging': {'pred_time': 1.5513220829750176e-05}, 'val_loss': 0.06780409831993077, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51856}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51856, 'experiment_tag': 'exp', 'time_total_s': 20.122608184814453}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8224031578956958
CO(0)的mse=0.009571691772803452
CO(0)的mae=0.06783658960630133
CO(0)的mar=0.2063246157434011
总共花费的时间为：71.18
洛阳市
1815A
1817A
3341A
3593A
3635A
3636A
[flaml.automl: 09-17 13:33:39] {2390} INFO - task = regression
[flaml.automl: 09-17 13:33:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:33:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:33:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:33:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:33:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:33:41] {3025} INFO - Estimated sufficient time budget=77256s. Estimated necessary time budget=77s.
[flaml.automl: 09-17 13:33:41] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1746,	best estimator xgboost's best error=0.1746
[flaml.automl: 09-17 13:33:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:33:43] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1000,	best estimator xgboost's best error=0.1000
[flaml.automl: 09-17 13:33:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:33:44] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1000,	best estimator xgboost's best error=0.1000
[flaml.automl: 09-17 13:33:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:33:48] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.1000,	best estimator xgboost's best error=0.1000
[flaml.automl: 09-17 13:33:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:33:49] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0756,	best estimator xgboost's best error=0.0756
[flaml.automl: 09-17 13:33:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:33:50] {3072} INFO -  at 11.2s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-17 13:33:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:33:52] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-17 13:33:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:33:54] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-17 13:33:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:33:56] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-17 13:33:56] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:33:58] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-17 13:33:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:34:00] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-17 13:34:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:34:01] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-17 13:34:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:34:08] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.0526,	best estimator xgboost's best error=0.0526
[flaml.automl: 09-17 13:34:08] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:34:20] {3072} INFO -  at 40.6s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-17 13:34:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 13:34:26] {3072} INFO -  at 47.1s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-17 13:34:26] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 13:34:39] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.0506,	best estimator xgboost's best error=0.0506
[flaml.automl: 09-17 13:35:00] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-17 13:35:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:35:00] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:35:00] {2637} INFO - Time taken to find the best model: 59.73665523529053
[flaml.automl: 09-17 13:35:00] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 65200}
CO(0)最佳损失：0.9494480628409494
CO(0)最好结果：{'pred_time': 5.584227125590385e-06, 'wall_clock_time': 59.73665523529053, 'metric_for_logging': {'pred_time': 5.584227125590385e-06}, 'val_loss': 0.050551937159050576, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 65200}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 65200, 'experiment_tag': 'exp', 'time_total_s': 12.6009361743927}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9252008232362914
CO(0)的mse=0.005789918435848735
CO(0)的mae=0.05048077282336187
CO(0)的mar=0.14947792708485957
总共花费的时间为：82.35
安阳市
1818A
1819A
3141A
3669A
[flaml.automl: 09-17 13:48:18] {2390} INFO - task = regression
[flaml.automl: 09-17 13:48:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:48:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:48:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:48:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:48:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:48:20] {3025} INFO - Estimated sufficient time budget=49222s. Estimated necessary time budget=49s.
[flaml.automl: 09-17 13:48:20] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2310,	best estimator xgboost's best error=0.2310
[flaml.automl: 09-17 13:48:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:48:22] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1436,	best estimator xgboost's best error=0.1436
[flaml.automl: 09-17 13:48:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:48:23] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1436,	best estimator xgboost's best error=0.1436
[flaml.automl: 09-17 13:48:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:48:29] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.1436,	best estimator xgboost's best error=0.1436
[flaml.automl: 09-17 13:48:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:48:30] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.1189,	best estimator xgboost's best error=0.1189
[flaml.automl: 09-17 13:48:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:48:32] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.1022,	best estimator xgboost's best error=0.1022
[flaml.automl: 09-17 13:48:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:48:34] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.1022,	best estimator xgboost's best error=0.1022
[flaml.automl: 09-17 13:48:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:48:36] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.1022,	best estimator xgboost's best error=0.1022
[flaml.automl: 09-17 13:48:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:48:37] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.1022,	best estimator xgboost's best error=0.1022
[flaml.automl: 09-17 13:48:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:48:40] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.1022,	best estimator xgboost's best error=0.1022
[flaml.automl: 09-17 13:48:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:48:41] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.1022,	best estimator xgboost's best error=0.1022
[flaml.automl: 09-17 13:48:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:48:43] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.1022,	best estimator xgboost's best error=0.1022
[flaml.automl: 09-17 13:48:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:48:49] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.0971,	best estimator xgboost's best error=0.0971
[flaml.automl: 09-17 13:48:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:49:01] {3072} INFO -  at 42.5s,	estimator xgboost's best error=0.0910,	best estimator xgboost's best error=0.0910
[flaml.automl: 09-17 13:49:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 13:49:07] {3072} INFO -  at 48.9s,	estimator xgboost's best error=0.0910,	best estimator xgboost's best error=0.0910
[flaml.automl: 09-17 13:49:19] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-17 13:49:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:49:19] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:49:19] {2637} INFO - Time taken to find the best model: 42.51347994804382
[flaml.automl: 09-17 13:49:19] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41263}
CO(0)最佳损失：0.9090268946319136
CO(0)最好结果：{'pred_time': 8.768135744716765e-06, 'wall_clock_time': 42.51347994804382, 'metric_for_logging': {'pred_time': 8.768135744716765e-06}, 'val_loss': 0.0909731053680864, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41263}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41263, 'experiment_tag': 'exp', 'time_total_s': 11.845683813095093}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.809483558784918
CO(0)的mse=0.027238044373437076
CO(0)的mae=0.09594198967852467
CO(0)的mar=0.13468757650655291
总共花费的时间为：61.53
开封市
3210A
3473A
3592A
[flaml.automl: 09-17 13:58:52] {2390} INFO - task = regression
[flaml.automl: 09-17 13:58:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:58:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:58:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:58:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:58:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:58:55] {3025} INFO - Estimated sufficient time budget=27165s. Estimated necessary time budget=27s.
[flaml.automl: 09-17 13:58:55] {3072} INFO -  at 2.9s,	estimator xgboost's best error=0.1390,	best estimator xgboost's best error=0.1390
[flaml.automl: 09-17 13:58:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:58:59] {3072} INFO -  at 7.7s,	estimator xgboost's best error=0.0821,	best estimator xgboost's best error=0.0821
[flaml.automl: 09-17 13:58:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:59:02] {3072} INFO -  at 10.5s,	estimator xgboost's best error=0.0821,	best estimator xgboost's best error=0.0821
[flaml.automl: 09-17 13:59:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:59:25] {3072} INFO -  at 33.1s,	estimator xgboost's best error=0.0821,	best estimator xgboost's best error=0.0821
[flaml.automl: 09-17 13:59:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:59:27] {3072} INFO -  at 35.6s,	estimator xgboost's best error=0.0655,	best estimator xgboost's best error=0.0655
[flaml.automl: 09-17 13:59:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:59:31] {3072} INFO -  at 39.4s,	estimator xgboost's best error=0.0531,	best estimator xgboost's best error=0.0531
[flaml.automl: 09-17 13:59:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:59:35] {3072} INFO -  at 43.3s,	estimator xgboost's best error=0.0531,	best estimator xgboost's best error=0.0531
[flaml.automl: 09-17 13:59:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:59:41] {3072} INFO -  at 49.0s,	estimator xgboost's best error=0.0531,	best estimator xgboost's best error=0.0531
[flaml.automl: 09-17 13:59:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:59:43] {3072} INFO -  at 51.5s,	estimator xgboost's best error=0.0531,	best estimator xgboost's best error=0.0531
[flaml.automl: 09-17 13:59:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:59:49] {3072} INFO -  at 57.7s,	estimator xgboost's best error=0.0531,	best estimator xgboost's best error=0.0531
[flaml.automl: 09-17 13:59:53] {3335} INFO - retrain xgboost for 3.8s
[flaml.automl: 09-17 13:59:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 13:59:53] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:59:53] {2637} INFO - Time taken to find the best model: 39.406336069107056
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
CO(0)最佳损失：0.9468763346230542
CO(0)最好结果：{'pred_time': 2.4499882565537922e-05, 'wall_clock_time': 39.406336069107056, 'metric_for_logging': {'pred_time': 2.4499882565537922e-05}, 'val_loss': 0.05312366537694578, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 3.7983412742614746}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8626228989710926
CO(0)的mse=0.006161563416703202
CO(0)的mae=0.05157250359327273
CO(0)的mar=0.08962747931506108
总共花费的时间为：62.12
焦作市
1830A
3169A
3335A
3336A
3477A
[flaml.automl: 09-17 14:16:24] {2390} INFO - task = regression
[flaml.automl: 09-17 14:16:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:16:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:16:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:16:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:16:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:16:25] {3025} INFO - Estimated sufficient time budget=62213s. Estimated necessary time budget=62s.
[flaml.automl: 09-17 14:16:25] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.2377,	best estimator xgboost's best error=0.2377
[flaml.automl: 09-17 14:16:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:16:27] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1499,	best estimator xgboost's best error=0.1499
[flaml.automl: 09-17 14:16:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:16:28] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1499,	best estimator xgboost's best error=0.1499
[flaml.automl: 09-17 14:16:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:16:33] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.1499,	best estimator xgboost's best error=0.1499
[flaml.automl: 09-17 14:16:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:16:34] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.1277,	best estimator xgboost's best error=0.1277
[flaml.automl: 09-17 14:16:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:16:36] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.1147,	best estimator xgboost's best error=0.1147
[flaml.automl: 09-17 14:16:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:16:37] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.1147,	best estimator xgboost's best error=0.1147
[flaml.automl: 09-17 14:16:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:16:40] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.1147,	best estimator xgboost's best error=0.1147
[flaml.automl: 09-17 14:16:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:16:41] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.1147,	best estimator xgboost's best error=0.1147
[flaml.automl: 09-17 14:16:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:16:44] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.1147,	best estimator xgboost's best error=0.1147
[flaml.automl: 09-17 14:16:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:16:45] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.1144,	best estimator xgboost's best error=0.1144
[flaml.automl: 09-17 14:16:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:16:47] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.1144,	best estimator xgboost's best error=0.1144
[flaml.automl: 09-17 14:16:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:16:53] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.1102,	best estimator xgboost's best error=0.1102
[flaml.automl: 09-17 14:16:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:17:14] {3072} INFO -  at 50.7s,	estimator xgboost's best error=0.1068,	best estimator xgboost's best error=0.1068
[flaml.automl: 09-17 14:17:36] {3335} INFO - retrain xgboost for 22.4s
[flaml.automl: 09-17 14:17:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:17:36] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:17:36] {2637} INFO - Time taken to find the best model: 50.682146310806274
[flaml.automl: 09-17 14:17:36] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51889}
CO(0)最佳损失：0.8932385782809502
CO(0)最好结果：{'pred_time': 1.4247267435624953e-05, 'wall_clock_time': 50.682146310806274, 'metric_for_logging': {'pred_time': 1.4247267435624953e-05}, 'val_loss': 0.10676142171904977, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51889}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51889, 'experiment_tag': 'exp', 'time_total_s': 21.114352464675903}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7895025094007525
CO(0)的mse=0.026052310451346622
CO(0)的mae=0.10232619617194454
CO(0)的mar=0.1697035484327958
总共花费的时间为：74.00
平顶山市
1833A
3204A
3594A
[flaml.automl: 09-17 14:27:18] {2390} INFO - task = regression
[flaml.automl: 09-17 14:27:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:27:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:27:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:27:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:27:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:27:19] {3025} INFO - Estimated sufficient time budget=12559s. Estimated necessary time budget=13s.
[flaml.automl: 09-17 14:27:19] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1734,	best estimator xgboost's best error=0.1734
[flaml.automl: 09-17 14:27:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:27:21] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-17 14:27:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:27:22] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-17 14:27:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:27:32] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-17 14:27:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:27:34] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-17 14:27:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:27:35] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0582,	best estimator xgboost's best error=0.0582
[flaml.automl: 09-17 14:27:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:27:37] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0582,	best estimator xgboost's best error=0.0582
[flaml.automl: 09-17 14:27:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:27:39] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0582,	best estimator xgboost's best error=0.0582
[flaml.automl: 09-17 14:27:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:27:40] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0582,	best estimator xgboost's best error=0.0582
[flaml.automl: 09-17 14:27:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:27:43] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.0582,	best estimator xgboost's best error=0.0582
[flaml.automl: 09-17 14:27:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:27:44] {3072} INFO -  at 26.3s,	estimator xgboost's best error=0.0582,	best estimator xgboost's best error=0.0582
[flaml.automl: 09-17 14:27:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:27:45] {3072} INFO -  at 27.5s,	estimator xgboost's best error=0.0582,	best estimator xgboost's best error=0.0582
[flaml.automl: 09-17 14:27:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:27:51] {3072} INFO -  at 33.5s,	estimator xgboost's best error=0.0497,	best estimator xgboost's best error=0.0497
[flaml.automl: 09-17 14:27:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:28:02] {3072} INFO -  at 44.6s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-17 14:28:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 14:28:08] {3072} INFO -  at 50.7s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-17 14:28:19] {3335} INFO - retrain xgboost for 10.9s
[flaml.automl: 09-17 14:28:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:28:19] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:28:19] {2637} INFO - Time taken to find the best model: 44.5834321975708
[flaml.automl: 09-17 14:28:19] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9517342860701756
CO(0)最好结果：{'pred_time': 1.4613606990912022e-05, 'wall_clock_time': 44.5834321975708, 'metric_for_logging': {'pred_time': 1.4613606990912022e-05}, 'val_loss': 0.048265713929824344, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.039793968200684}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9398206009605543
CO(0)的mse=0.005122135228921842
CO(0)的mae=0.04695270054441309
CO(0)的mar=0.15333530042811025
总共花费的时间为：62.04
三门峡市
1835A
1836A
1838A
3598A
[flaml.automl: 09-17 14:40:15] {2390} INFO - task = regression
[flaml.automl: 09-17 14:40:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:40:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:40:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:40:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:40:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:40:16] {3025} INFO - Estimated sufficient time budget=50932s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 14:40:16] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1699,	best estimator xgboost's best error=0.1699
[flaml.automl: 09-17 14:40:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:40:18] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0956,	best estimator xgboost's best error=0.0956
[flaml.automl: 09-17 14:40:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:40:19] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0956,	best estimator xgboost's best error=0.0956
[flaml.automl: 09-17 14:40:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:40:26] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0956,	best estimator xgboost's best error=0.0956
[flaml.automl: 09-17 14:40:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:40:27] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0735,	best estimator xgboost's best error=0.0735
[flaml.automl: 09-17 14:40:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:40:28] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0630,	best estimator xgboost's best error=0.0630
[flaml.automl: 09-17 14:40:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:40:30] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0630,	best estimator xgboost's best error=0.0630
[flaml.automl: 09-17 14:40:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:40:33] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0630,	best estimator xgboost's best error=0.0630
[flaml.automl: 09-17 14:40:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:40:34] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0630,	best estimator xgboost's best error=0.0630
[flaml.automl: 09-17 14:40:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:40:36] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0630,	best estimator xgboost's best error=0.0630
[flaml.automl: 09-17 14:40:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:40:38] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0630,	best estimator xgboost's best error=0.0630
[flaml.automl: 09-17 14:40:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:40:39] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0630,	best estimator xgboost's best error=0.0630
[flaml.automl: 09-17 14:40:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:40:46] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0574,	best estimator xgboost's best error=0.0574
[flaml.automl: 09-17 14:40:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:40:58] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.0553,	best estimator xgboost's best error=0.0553
[flaml.automl: 09-17 14:40:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 14:41:04] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0553,	best estimator xgboost's best error=0.0553
[flaml.automl: 09-17 14:41:16] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 14:41:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:41:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:41:16] {2637} INFO - Time taken to find the best model: 43.0276620388031
[flaml.automl: 09-17 14:41:16] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41881}
CO(0)最佳损失：0.9446699744732686
CO(0)最好结果：{'pred_time': 8.581224612837244e-06, 'wall_clock_time': 43.0276620388031, 'metric_for_logging': {'pred_time': 8.581224612837244e-06}, 'val_loss': 0.05533002552673137, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41881}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41881, 'experiment_tag': 'exp', 'time_total_s': 12.038800239562988}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8874690462713452
CO(0)的mse=0.007248543161635498
CO(0)的mae=0.05405651687579836
CO(0)的mar=0.08081365242431676
总共花费的时间为：62.36
宜昌市
1840A
1841A
1842A
1843A
3546A
3653A
[flaml.automl: 09-17 15:00:31] {2390} INFO - task = regression
[flaml.automl: 09-17 15:00:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:00:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:00:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:00:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:00:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:00:33] {3025} INFO - Estimated sufficient time budget=78150s. Estimated necessary time budget=78s.
[flaml.automl: 09-17 15:00:33] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1346,	best estimator xgboost's best error=0.1346
[flaml.automl: 09-17 15:00:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:00:35] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0766,	best estimator xgboost's best error=0.0766
[flaml.automl: 09-17 15:00:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:00:36] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0766,	best estimator xgboost's best error=0.0766
[flaml.automl: 09-17 15:00:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:00:40] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.0766,	best estimator xgboost's best error=0.0766
[flaml.automl: 09-17 15:00:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:00:41] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0591,	best estimator xgboost's best error=0.0591
[flaml.automl: 09-17 15:00:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:00:42] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 15:00:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:00:44] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 15:00:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:00:46] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 15:00:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:00:48] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 15:00:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:00:50] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 15:00:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:00:52] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-17 15:00:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:00:53] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-17 15:00:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:01:00] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.0434,	best estimator xgboost's best error=0.0434
[flaml.automl: 09-17 15:01:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:01:12] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-17 15:01:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 15:01:18] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-17 15:01:18] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 15:01:31] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.0414,	best estimator xgboost's best error=0.0414
[flaml.automl: 09-17 15:01:52] {3335} INFO - retrain xgboost for 21.4s
[flaml.automl: 09-17 15:01:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:01:52] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:01:52] {2637} INFO - Time taken to find the best model: 59.85462951660156
[flaml.automl: 09-17 15:01:52] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 64840}
CO(0)最佳损失：0.9586453177855381
CO(0)最好结果：{'pred_time': 5.632303887816621e-06, 'wall_clock_time': 59.85462951660156, 'metric_for_logging': {'pred_time': 5.632303887816621e-06}, 'val_loss': 0.04135468221446195, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 64840}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 64840, 'experiment_tag': 'exp', 'time_total_s': 12.558202028274536}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.916568124989187
CO(0)的mse=0.004133637324255967
CO(0)的mae=0.04190936726269173
CO(0)的mar=0.07909384451328103
总共花费的时间为：82.39
荆州市
1845A
3548A
[flaml.automl: 09-17 15:08:26] {2390} INFO - task = regression
[flaml.automl: 09-17 15:08:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:08:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:08:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:08:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:08:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:08:28] {3025} INFO - Estimated sufficient time budget=21454s. Estimated necessary time budget=21s.
[flaml.automl: 09-17 15:08:28] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.1993,	best estimator xgboost's best error=0.1993
[flaml.automl: 09-17 15:08:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:08:32] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.0967,	best estimator xgboost's best error=0.0967
[flaml.automl: 09-17 15:08:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:08:34] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.0967,	best estimator xgboost's best error=0.0967
[flaml.automl: 09-17 15:08:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:08:52] {3072} INFO -  at 26.8s,	estimator xgboost's best error=0.0967,	best estimator xgboost's best error=0.0967
[flaml.automl: 09-17 15:08:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:08:54] {3072} INFO -  at 28.9s,	estimator xgboost's best error=0.0568,	best estimator xgboost's best error=0.0568
[flaml.automl: 09-17 15:08:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:08:57] {3072} INFO -  at 31.8s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-17 15:08:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:09:00] {3072} INFO -  at 34.9s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-17 15:09:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:09:05] {3072} INFO -  at 39.4s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-17 15:09:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:09:07] {3072} INFO -  at 41.6s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-17 15:09:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:09:12] {3072} INFO -  at 46.2s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-17 15:09:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:09:14] {3072} INFO -  at 48.4s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-17 15:09:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:09:16] {3072} INFO -  at 50.5s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-17 15:09:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:09:24] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0383,	best estimator xgboost's best error=0.0383
[flaml.automl: 09-17 15:09:35] {3335} INFO - retrain xgboost for 11.0s
[flaml.automl: 09-17 15:09:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 15:09:35] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:09:35] {2637} INFO - Time taken to find the best model: 59.084720611572266
[flaml.automl: 09-17 15:09:35] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9617436912589603
CO(0)最好结果：{'pred_time': 2.7454719341621197e-05, 'wall_clock_time': 59.084720611572266, 'metric_for_logging': {'pred_time': 2.7454719341621197e-05}, 'val_loss': 0.0382563087410397, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 8.60076093673706}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9370431225015878
CO(0)的mse=0.0031195715930075955
CO(0)的mae=0.03695714967731154
CO(0)的mar=0.046519681132698255
总共花费的时间为：70.54
岳阳市
1847A
1848A
1850A
1851A
1852A
[flaml.automl: 09-17 15:25:20] {2390} INFO - task = regression
[flaml.automl: 09-17 15:25:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:25:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:25:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:25:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:25:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:25:22] {3025} INFO - Estimated sufficient time budget=62861s. Estimated necessary time budget=63s.
[flaml.automl: 09-17 15:25:22] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1683,	best estimator xgboost's best error=0.1683
[flaml.automl: 09-17 15:25:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:25:24] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0889,	best estimator xgboost's best error=0.0889
[flaml.automl: 09-17 15:25:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:25:25] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0889,	best estimator xgboost's best error=0.0889
[flaml.automl: 09-17 15:25:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:25:30] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0889,	best estimator xgboost's best error=0.0889
[flaml.automl: 09-17 15:25:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:25:31] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0607,	best estimator xgboost's best error=0.0607
[flaml.automl: 09-17 15:25:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:25:32] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-17 15:25:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:25:34] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-17 15:25:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:25:36] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-17 15:25:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:25:38] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-17 15:25:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:25:40] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0508,	best estimator xgboost's best error=0.0508
[flaml.automl: 09-17 15:25:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:25:42] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0507,	best estimator xgboost's best error=0.0507
[flaml.automl: 09-17 15:25:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:25:43] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0507,	best estimator xgboost's best error=0.0507
[flaml.automl: 09-17 15:25:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:25:50] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0443,	best estimator xgboost's best error=0.0443
[flaml.automl: 09-17 15:25:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:26:02] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0435,	best estimator xgboost's best error=0.0435
[flaml.automl: 09-17 15:26:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 15:26:08] {3072} INFO -  at 48.3s,	estimator xgboost's best error=0.0435,	best estimator xgboost's best error=0.0435
[flaml.automl: 09-17 15:26:20] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 15:26:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:26:20] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:26:20] {2637} INFO - Time taken to find the best model: 41.72741222381592
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52331}
CO(0)最佳损失：0.9564595038445367
CO(0)最好结果：{'pred_time': 6.987119889525842e-06, 'wall_clock_time': 41.72741222381592, 'metric_for_logging': {'pred_time': 6.987119889525842e-06}, 'val_loss': 0.04354049615546331, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52331}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52331, 'experiment_tag': 'exp', 'time_total_s': 12.112592220306396}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8979130670192853
CO(0)的mse=0.0046547269775575545
CO(0)的mae=0.04370111134365568
CO(0)的mar=0.06105671761498596
总共花费的时间为：61.23
常德市
1854A
1857A
3138A
3139A
3140A
[flaml.automl: 09-17 15:42:28] {2390} INFO - task = regression
[flaml.automl: 09-17 15:42:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:42:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:42:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:42:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:42:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:42:29] {3025} INFO - Estimated sufficient time budget=62743s. Estimated necessary time budget=63s.
[flaml.automl: 09-17 15:42:29] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1791,	best estimator xgboost's best error=0.1791
[flaml.automl: 09-17 15:42:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:42:31] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0898,	best estimator xgboost's best error=0.0898
[flaml.automl: 09-17 15:42:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:42:33] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0898,	best estimator xgboost's best error=0.0898
[flaml.automl: 09-17 15:42:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:42:37] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0898,	best estimator xgboost's best error=0.0898
[flaml.automl: 09-17 15:42:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:42:39] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0554,	best estimator xgboost's best error=0.0554
[flaml.automl: 09-17 15:42:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:42:40] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-17 15:42:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:42:42] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-17 15:42:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:42:44] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-17 15:42:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:42:45] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-17 15:42:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:42:48] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-17 15:42:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:42:50] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-17 15:42:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:42:51] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0449,	best estimator xgboost's best error=0.0449
[flaml.automl: 09-17 15:42:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:42:57] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.0382,	best estimator xgboost's best error=0.0382
[flaml.automl: 09-17 15:42:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:43:09] {3072} INFO -  at 41.5s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-17 15:43:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 15:43:16] {3072} INFO -  at 48.0s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-17 15:43:28] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 15:43:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:43:28] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:43:28] {2637} INFO - Time taken to find the best model: 41.49983096122742
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52336}
CO(0)最佳损失：0.9635373543770843
CO(0)最好结果：{'pred_time': 7.83124684959527e-06, 'wall_clock_time': 41.49983096122742, 'metric_for_logging': {'pred_time': 7.83124684959527e-06}, 'val_loss': 0.036462645622915714, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52336}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52336, 'experiment_tag': 'exp', 'time_total_s': 12.114094972610474}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.928122982001246
CO(0)的mse=0.003244308992210018
CO(0)的mae=0.035495983822727235
CO(0)的mar=0.049719585538658635
总共花费的时间为：61.10
张家界市
1858A
1859A
1861A
[flaml.automl: 09-17 15:53:07] {2390} INFO - task = regression
[flaml.automl: 09-17 15:53:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:53:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:53:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:53:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:53:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:53:08] {3025} INFO - Estimated sufficient time budget=12062s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 15:53:08] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1369,	best estimator xgboost's best error=0.1369
[flaml.automl: 09-17 15:53:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:53:10] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0748,	best estimator xgboost's best error=0.0748
[flaml.automl: 09-17 15:53:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:53:11] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0748,	best estimator xgboost's best error=0.0748
[flaml.automl: 09-17 15:53:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:53:21] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0748,	best estimator xgboost's best error=0.0748
[flaml.automl: 09-17 15:53:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:53:22] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0528,	best estimator xgboost's best error=0.0528
[flaml.automl: 09-17 15:53:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:53:24] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0433,	best estimator xgboost's best error=0.0433
[flaml.automl: 09-17 15:53:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:53:27] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0433,	best estimator xgboost's best error=0.0433
[flaml.automl: 09-17 15:53:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:53:31] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0433,	best estimator xgboost's best error=0.0433
[flaml.automl: 09-17 15:53:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:53:33] {3072} INFO -  at 26.3s,	estimator xgboost's best error=0.0433,	best estimator xgboost's best error=0.0433
[flaml.automl: 09-17 15:53:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:53:37] {3072} INFO -  at 30.8s,	estimator xgboost's best error=0.0433,	best estimator xgboost's best error=0.0433
[flaml.automl: 09-17 15:53:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:53:39] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0433,	best estimator xgboost's best error=0.0433
[flaml.automl: 09-17 15:53:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:53:41] {3072} INFO -  at 35.0s,	estimator xgboost's best error=0.0433,	best estimator xgboost's best error=0.0433
[flaml.automl: 09-17 15:53:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:53:53] {3072} INFO -  at 46.9s,	estimator xgboost's best error=0.0339,	best estimator xgboost's best error=0.0339
[flaml.automl: 09-17 15:53:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:54:06] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0329,	best estimator xgboost's best error=0.0329
[flaml.automl: 09-17 15:54:27] {3335} INFO - retrain xgboost for 21.0s
[flaml.automl: 09-17 15:54:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:54:27] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:54:27] {2637} INFO - Time taken to find the best model: 59.20670294761658
[flaml.automl: 09-17 15:54:27] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9670710578952912
CO(0)最好结果：{'pred_time': 2.4629537274162505e-05, 'wall_clock_time': 59.20670294761658, 'metric_for_logging': {'pred_time': 2.4629537274162505e-05}, 'val_loss': 0.032928942104708744, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.347652673721313}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9374588282826288
CO(0)的mse=0.003232423053972724
CO(0)的mae=0.0339501228348488
CO(0)的mar=0.24534875571544376
总共花费的时间为：80.87
桂林市
1862A
1864A
1865A
3403A
3531A
[flaml.automl: 09-17 16:10:47] {2390} INFO - task = regression
[flaml.automl: 09-17 16:10:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:10:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:10:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:10:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:10:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:10:48] {3025} INFO - Estimated sufficient time budget=62720s. Estimated necessary time budget=63s.
[flaml.automl: 09-17 16:10:48] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1524,	best estimator xgboost's best error=0.1524
[flaml.automl: 09-17 16:10:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:10:50] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0787,	best estimator xgboost's best error=0.0787
[flaml.automl: 09-17 16:10:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:10:51] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0787,	best estimator xgboost's best error=0.0787
[flaml.automl: 09-17 16:10:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:10:56] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0787,	best estimator xgboost's best error=0.0787
[flaml.automl: 09-17 16:10:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:10:57] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0481,	best estimator xgboost's best error=0.0481
[flaml.automl: 09-17 16:10:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:10:59] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-17 16:10:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:11:00] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-17 16:11:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:11:03] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-17 16:11:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:11:04] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-17 16:11:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:11:07] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-17 16:11:07] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:11:08] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-17 16:11:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:11:10] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-17 16:11:10] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:11:16] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0295,	best estimator xgboost's best error=0.0295
[flaml.automl: 09-17 16:11:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:11:28] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0275,	best estimator xgboost's best error=0.0275
[flaml.automl: 09-17 16:11:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 16:11:35] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.0275,	best estimator xgboost's best error=0.0275
[flaml.automl: 09-17 16:11:47] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 16:11:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:11:47] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:11:47] {2637} INFO - Time taken to find the best model: 41.70166540145874
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52111}
CO(0)最佳损失：0.9724987285672188
CO(0)最好结果：{'pred_time': 6.996562208789265e-06, 'wall_clock_time': 41.70166540145874, 'metric_for_logging': {'pred_time': 6.996562208789265e-06}, 'val_loss': 0.027501271432781235, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52111}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52111, 'experiment_tag': 'exp', 'time_total_s': 12.130183935165405}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9676738354242903
CO(0)的mse=0.0017317855069812163
CO(0)的mae=0.026767478670824157
CO(0)的mar=0.04098370027784101
总共花费的时间为：61.10
北海市
1866A
1868A
1869A
3400A
[flaml.automl: 09-17 16:24:04] {2390} INFO - task = regression
[flaml.automl: 09-17 16:24:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:24:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:24:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:24:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:24:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:24:06] {3025} INFO - Estimated sufficient time budget=88400s. Estimated necessary time budget=88s.
[flaml.automl: 09-17 16:24:06] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1251,	best estimator xgboost's best error=0.1251
[flaml.automl: 09-17 16:24:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:24:10] {3072} INFO -  at 6.5s,	estimator xgboost's best error=0.0703,	best estimator xgboost's best error=0.0703
[flaml.automl: 09-17 16:24:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:24:12] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.0703,	best estimator xgboost's best error=0.0703
[flaml.automl: 09-17 16:24:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:24:17] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0703,	best estimator xgboost's best error=0.0703
[flaml.automl: 09-17 16:24:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:24:19] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0520,	best estimator xgboost's best error=0.0520
[flaml.automl: 09-17 16:24:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:24:22] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-17 16:24:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:24:25] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-17 16:24:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:24:29] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-17 16:24:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:24:31] {3072} INFO -  at 27.5s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-17 16:24:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:24:34] {3072} INFO -  at 30.2s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-17 16:24:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:24:37] {3072} INFO -  at 33.1s,	estimator xgboost's best error=0.0424,	best estimator xgboost's best error=0.0424
[flaml.automl: 09-17 16:24:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:24:39] {3072} INFO -  at 35.3s,	estimator xgboost's best error=0.0424,	best estimator xgboost's best error=0.0424
[flaml.automl: 09-17 16:24:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:24:51] {3072} INFO -  at 47.4s,	estimator xgboost's best error=0.0378,	best estimator xgboost's best error=0.0378
[flaml.automl: 09-17 16:24:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:25:03] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.0368,	best estimator xgboost's best error=0.0368
[flaml.automl: 09-17 16:25:14] {3335} INFO - retrain xgboost for 11.8s
[flaml.automl: 09-17 16:25:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:25:14] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:25:14] {2637} INFO - Time taken to find the best model: 59.017194986343384
[flaml.automl: 09-17 16:25:14] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41634}
CO(0)最佳损失：0.9631898151241336
CO(0)最好结果：{'pred_time': 8.685528020862887e-06, 'wall_clock_time': 59.017194986343384, 'metric_for_logging': {'pred_time': 8.685528020862887e-06}, 'val_loss': 0.03681018487586637, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41634}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41634, 'experiment_tag': 'exp', 'time_total_s': 11.604182481765747}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9166504603277845
CO(0)的mse=0.0030708793386373546
CO(0)的mae=0.03504506577250791
CO(0)的mar=0.05681776087127056
总共花费的时间为：71.62
柳州市
1870A
1872A
1873A
1874A
1875A
3402A
[flaml.automl: 09-17 16:44:56] {2390} INFO - task = regression
[flaml.automl: 09-17 16:44:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:44:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:44:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:44:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:44:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:44:58] {3025} INFO - Estimated sufficient time budget=139654s. Estimated necessary time budget=140s.
[flaml.automl: 09-17 16:44:58] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.2044,	best estimator xgboost's best error=0.2044
[flaml.automl: 09-17 16:44:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:45:02] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.1056,	best estimator xgboost's best error=0.1056
[flaml.automl: 09-17 16:45:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:45:04] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.1056,	best estimator xgboost's best error=0.1056
[flaml.automl: 09-17 16:45:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:45:08] {3072} INFO -  at 11.9s,	estimator xgboost's best error=0.1056,	best estimator xgboost's best error=0.1056
[flaml.automl: 09-17 16:45:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:45:10] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0693,	best estimator xgboost's best error=0.0693
[flaml.automl: 09-17 16:45:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:45:12] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0601,	best estimator xgboost's best error=0.0601
[flaml.automl: 09-17 16:45:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:45:15] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0601,	best estimator xgboost's best error=0.0601
[flaml.automl: 09-17 16:45:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:45:17] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.0601,	best estimator xgboost's best error=0.0601
[flaml.automl: 09-17 16:45:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:45:19] {3072} INFO -  at 23.5s,	estimator xgboost's best error=0.0601,	best estimator xgboost's best error=0.0601
[flaml.automl: 09-17 16:45:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:45:22] {3072} INFO -  at 26.1s,	estimator xgboost's best error=0.0601,	best estimator xgboost's best error=0.0601
[flaml.automl: 09-17 16:45:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:45:25] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.0596,	best estimator xgboost's best error=0.0596
[flaml.automl: 09-17 16:45:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:45:27] {3072} INFO -  at 31.2s,	estimator xgboost's best error=0.0596,	best estimator xgboost's best error=0.0596
[flaml.automl: 09-17 16:45:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:45:38] {3072} INFO -  at 42.5s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-17 16:45:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:45:52] {3072} INFO -  at 56.7s,	estimator xgboost's best error=0.0531,	best estimator xgboost's best error=0.0531
[flaml.automl: 09-17 16:46:04] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-17 16:46:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:46:04] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:46:04] {2637} INFO - Time taken to find the best model: 56.69249129295349
[flaml.automl: 09-17 16:46:04] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64964}
CO(0)最佳损失：0.9469495548868727
CO(0)最好结果：{'pred_time': 5.494360944243543e-06, 'wall_clock_time': 56.69249129295349, 'metric_for_logging': {'pred_time': 5.494360944243543e-06}, 'val_loss': 0.05305044511312724, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64964}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 64964, 'experiment_tag': 'exp', 'time_total_s': 14.181510925292969}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.813113475140416
CO(0)的mse=0.011467699803084349
CO(0)的mae=0.055319365316251166
CO(0)的mar=0.06523054948299521
总共花费的时间为：69.73
三亚市
1876A
3540A
[flaml.automl: 09-17 16:52:45] {2390} INFO - task = regression
[flaml.automl: 09-17 16:52:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:52:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:52:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:52:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:52:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:52:47] {3025} INFO - Estimated sufficient time budget=22167s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 16:52:47] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.0798,	best estimator xgboost's best error=0.0798
[flaml.automl: 09-17 16:52:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:52:51] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.0420,	best estimator xgboost's best error=0.0420
[flaml.automl: 09-17 16:52:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:52:53] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.0420,	best estimator xgboost's best error=0.0420
[flaml.automl: 09-17 16:52:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:53:11] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.0420,	best estimator xgboost's best error=0.0420
[flaml.automl: 09-17 16:53:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:53:13] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.0274,	best estimator xgboost's best error=0.0274
[flaml.automl: 09-17 16:53:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:53:16] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0216,	best estimator xgboost's best error=0.0216
[flaml.automl: 09-17 16:53:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:53:19] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0216,	best estimator xgboost's best error=0.0216
[flaml.automl: 09-17 16:53:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:53:23] {3072} INFO -  at 38.8s,	estimator xgboost's best error=0.0216,	best estimator xgboost's best error=0.0216
[flaml.automl: 09-17 16:53:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:53:26] {3072} INFO -  at 40.9s,	estimator xgboost's best error=0.0216,	best estimator xgboost's best error=0.0216
[flaml.automl: 09-17 16:53:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:53:30] {3072} INFO -  at 45.4s,	estimator xgboost's best error=0.0216,	best estimator xgboost's best error=0.0216
[flaml.automl: 09-17 16:53:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:53:32] {3072} INFO -  at 47.5s,	estimator xgboost's best error=0.0216,	best estimator xgboost's best error=0.0216
[flaml.automl: 09-17 16:53:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:53:34] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0216,	best estimator xgboost's best error=0.0216
[flaml.automl: 09-17 16:53:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:53:44] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.0185,	best estimator xgboost's best error=0.0185
[flaml.automl: 09-17 16:53:55] {3335} INFO - retrain xgboost for 10.9s
[flaml.automl: 09-17 16:53:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 16:53:55] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:53:55] {2637} INFO - Time taken to find the best model: 59.467902183532715
[flaml.automl: 09-17 16:53:55] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9815170570357351
CO(0)最好结果：{'pred_time': 2.6174629760893763e-05, 'wall_clock_time': 59.467902183532715, 'metric_for_logging': {'pred_time': 2.6174629760893763e-05}, 'val_loss': 0.01848294296426489, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.878827810287476}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9358347741778087
CO(0)的mse=0.0005986496608777437
CO(0)的mae=0.017389037708500636
CO(0)的mar=0.043597017748915896
总共花费的时间为：70.75
德阳市
1902A
3639A
[flaml.automl: 09-17 17:01:33] {2390} INFO - task = regression
[flaml.automl: 09-17 17:01:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:01:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:01:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:01:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:01:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:01:36] {3025} INFO - Estimated sufficient time budget=33960s. Estimated necessary time budget=34s.
[flaml.automl: 09-17 17:01:36] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1166,	best estimator xgboost's best error=0.1166
[flaml.automl: 09-17 17:01:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:01:42] {3072} INFO -  at 9.5s,	estimator xgboost's best error=0.0701,	best estimator xgboost's best error=0.0701
[flaml.automl: 09-17 17:01:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:01:46] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.0701,	best estimator xgboost's best error=0.0701
[flaml.automl: 09-17 17:01:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:02:07] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.0701,	best estimator xgboost's best error=0.0701
[flaml.automl: 09-17 17:02:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:02:09] {3072} INFO -  at 36.0s,	estimator xgboost's best error=0.0589,	best estimator xgboost's best error=0.0589
[flaml.automl: 09-17 17:02:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:02:11] {3072} INFO -  at 38.7s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 17:02:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:02:14] {3072} INFO -  at 41.3s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 17:02:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:02:18] {3072} INFO -  at 45.3s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 17:02:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:02:20] {3072} INFO -  at 47.1s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 17:02:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:02:24] {3072} INFO -  at 51.2s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 17:02:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:02:25] {3072} INFO -  at 52.7s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 17:02:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:02:27] {3072} INFO -  at 53.9s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 17:02:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:02:32] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0450,	best estimator xgboost's best error=0.0450
[flaml.automl: 09-17 17:02:38] {3335} INFO - retrain xgboost for 6.0s
[flaml.automl: 09-17 17:02:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 17:02:38] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:02:38] {2637} INFO - Time taken to find the best model: 59.18948030471802
[flaml.automl: 09-17 17:02:38] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9549775292034142
CO(0)最好结果：{'pred_time': 1.678501887099234e-05, 'wall_clock_time': 59.18948030471802, 'metric_for_logging': {'pred_time': 1.678501887099234e-05}, 'val_loss': 0.04502247079658586, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 5.324289083480835}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.859703944335364
CO(0)的mse=0.004555895656093123
CO(0)的mae=0.04700802582504558
CO(0)的mar=0.07639590309985984
总共花费的时间为：65.65
南充市
1905A
1906A
1907A
1908A
1909A
[flaml.automl: 09-17 17:18:23] {2390} INFO - task = regression
[flaml.automl: 09-17 17:18:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:18:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:18:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:18:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:18:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:18:25] {3025} INFO - Estimated sufficient time budget=105635s. Estimated necessary time budget=106s.
[flaml.automl: 09-17 17:18:25] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.1309,	best estimator xgboost's best error=0.1309
[flaml.automl: 09-17 17:18:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:18:28] {3072} INFO -  at 5.6s,	estimator xgboost's best error=0.0740,	best estimator xgboost's best error=0.0740
[flaml.automl: 09-17 17:18:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:18:30] {3072} INFO -  at 7.6s,	estimator xgboost's best error=0.0740,	best estimator xgboost's best error=0.0740
[flaml.automl: 09-17 17:18:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:18:34] {3072} INFO -  at 11.5s,	estimator xgboost's best error=0.0740,	best estimator xgboost's best error=0.0740
[flaml.automl: 09-17 17:18:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:18:36] {3072} INFO -  at 13.5s,	estimator xgboost's best error=0.0554,	best estimator xgboost's best error=0.0554
[flaml.automl: 09-17 17:18:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:18:39] {3072} INFO -  at 16.2s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-17 17:18:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:18:42] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-17 17:18:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:18:45] {3072} INFO -  at 22.2s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-17 17:18:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:18:47] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-17 17:18:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:18:49] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-17 17:18:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:18:52] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0472,	best estimator xgboost's best error=0.0472
[flaml.automl: 09-17 17:18:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:18:54] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.0472,	best estimator xgboost's best error=0.0472
[flaml.automl: 09-17 17:18:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:19:03] {3072} INFO -  at 39.8s,	estimator xgboost's best error=0.0413,	best estimator xgboost's best error=0.0413
[flaml.automl: 09-17 17:19:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:19:15] {3072} INFO -  at 51.9s,	estimator xgboost's best error=0.0391,	best estimator xgboost's best error=0.0391
[flaml.automl: 09-17 17:19:27] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 17:19:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:19:27] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:19:27] {2637} INFO - Time taken to find the best model: 51.936140298843384
[flaml.automl: 09-17 17:19:27] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52673}
CO(0)最佳损失：0.9608626898536718
CO(0)最好结果：{'pred_time': 7.0101498294728e-06, 'wall_clock_time': 51.936140298843384, 'metric_for_logging': {'pred_time': 7.0101498294728e-06}, 'val_loss': 0.0391373101463282, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52673}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52673, 'experiment_tag': 'exp', 'time_total_s': 12.104401350021362}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9056946116195861
CO(0)的mse=0.00393484210359081
CO(0)的mae=0.039884307784189464
CO(0)的mar=0.0637331920370583
总共花费的时间为：65.00
遵义市
1911A
1912A
1913A
1914A
3536A
[flaml.automl: 09-17 17:35:04] {2390} INFO - task = regression
[flaml.automl: 09-17 17:35:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:35:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:35:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:35:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:35:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:35:06] {3025} INFO - Estimated sufficient time budget=65170s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 17:35:06] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1141,	best estimator xgboost's best error=0.1141
[flaml.automl: 09-17 17:35:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:35:08] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0688,	best estimator xgboost's best error=0.0688
[flaml.automl: 09-17 17:35:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:35:09] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0688,	best estimator xgboost's best error=0.0688
[flaml.automl: 09-17 17:35:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:35:14] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0688,	best estimator xgboost's best error=0.0688
[flaml.automl: 09-17 17:35:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:35:15] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0547,	best estimator xgboost's best error=0.0547
[flaml.automl: 09-17 17:35:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:35:16] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-17 17:35:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:35:18] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-17 17:35:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:35:20] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-17 17:35:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:35:22] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-17 17:35:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:35:24] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-17 17:35:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:35:26] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-17 17:35:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:35:27] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-17 17:35:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:35:34] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0448,	best estimator xgboost's best error=0.0448
[flaml.automl: 09-17 17:35:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:35:46] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0416,	best estimator xgboost's best error=0.0416
[flaml.automl: 09-17 17:35:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 17:35:52] {3072} INFO -  at 48.3s,	estimator xgboost's best error=0.0416,	best estimator xgboost's best error=0.0416
[flaml.automl: 09-17 17:36:04] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 17:36:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:36:04] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:36:04] {2637} INFO - Time taken to find the best model: 41.71214938163757
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54429}
CO(0)最佳损失：0.9583593007081321
CO(0)最好结果：{'pred_time': 7.0372901896320325e-06, 'wall_clock_time': 41.71214938163757, 'metric_for_logging': {'pred_time': 7.0372901896320325e-06}, 'val_loss': 0.04164069929186787, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54429}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54429, 'experiment_tag': 'exp', 'time_total_s': 12.136971712112427}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8811051244026745
CO(0)的mse=0.004060091928728776
CO(0)的mae=0.0415986007440303
CO(0)的mar=0.07329578487549342
总共花费的时间为：61.20
曲靖市
1917A
3376A
3377A
[flaml.automl: 09-17 17:45:59] {2390} INFO - task = regression
[flaml.automl: 09-17 17:45:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:45:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:45:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:45:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:45:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:46:02] {3025} INFO - Estimated sufficient time budget=22629s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 17:46:02] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.1419,	best estimator xgboost's best error=0.1419
[flaml.automl: 09-17 17:46:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:46:05] {3072} INFO -  at 6.1s,	estimator xgboost's best error=0.0954,	best estimator xgboost's best error=0.0954
[flaml.automl: 09-17 17:46:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:46:07] {3072} INFO -  at 8.0s,	estimator xgboost's best error=0.0954,	best estimator xgboost's best error=0.0954
[flaml.automl: 09-17 17:46:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:46:25] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0954,	best estimator xgboost's best error=0.0954
[flaml.automl: 09-17 17:46:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:46:27] {3072} INFO -  at 27.5s,	estimator xgboost's best error=0.0844,	best estimator xgboost's best error=0.0844
[flaml.automl: 09-17 17:46:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:46:30] {3072} INFO -  at 30.3s,	estimator xgboost's best error=0.0768,	best estimator xgboost's best error=0.0768
[flaml.automl: 09-17 17:46:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:46:33] {3072} INFO -  at 33.5s,	estimator xgboost's best error=0.0768,	best estimator xgboost's best error=0.0768
[flaml.automl: 09-17 17:46:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:46:37] {3072} INFO -  at 37.6s,	estimator xgboost's best error=0.0768,	best estimator xgboost's best error=0.0768
[flaml.automl: 09-17 17:46:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:46:39] {3072} INFO -  at 39.6s,	estimator xgboost's best error=0.0768,	best estimator xgboost's best error=0.0768
[flaml.automl: 09-17 17:46:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:46:43] {3072} INFO -  at 43.8s,	estimator xgboost's best error=0.0765,	best estimator xgboost's best error=0.0765
[flaml.automl: 09-17 17:46:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:46:46] {3072} INFO -  at 46.6s,	estimator xgboost's best error=0.0765,	best estimator xgboost's best error=0.0765
[flaml.automl: 09-17 17:46:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:46:48] {3072} INFO -  at 49.1s,	estimator xgboost's best error=0.0765,	best estimator xgboost's best error=0.0765
[flaml.automl: 09-17 17:46:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:46:58] {3072} INFO -  at 58.4s,	estimator xgboost's best error=0.0746,	best estimator xgboost's best error=0.0746
[flaml.automl: 09-17 17:47:10] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 17:47:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8068591840934569, colsample_bynode=1,
             colsample_bytree=0.653516717901374, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9977579754449118,
             max_delta_step=0, max_depth=0, max_leaves=25,
             min_child_weight=0.013548610495542106, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00546465520560431,
             reg_lambda=1.128463058774389, scale_pos_weight=1,
             subsample=0.8083911828355876, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:47:10] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:47:10] {2637} INFO - Time taken to find the best model: 58.405373096466064
[flaml.automl: 09-17 17:47:10] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 25, 'min_child_weight': 0.013548610495542106, 'learning_rate': 0.9977579754449118, 'subsample': 0.8083911828355876, 'colsample_bylevel': 0.8068591840934569, 'colsample_bytree': 0.653516717901374, 'reg_alpha': 0.00546465520560431, 'reg_lambda': 1.128463058774389}
CO(0)最佳损失：0.9254261519019576
CO(0)最好结果：{'pred_time': 2.2825995972047974e-05, 'wall_clock_time': 58.405373096466064, 'metric_for_logging': {'pred_time': 2.2825995972047974e-05}, 'val_loss': 0.07457384809804236, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 25, 'min_child_weight': 0.013548610495542106, 'learning_rate': 0.9977579754449118, 'subsample': 0.8083911828355876, 'colsample_bylevel': 0.8068591840934569, 'colsample_bytree': 0.653516717901374, 'reg_alpha': 0.00546465520560431, 'reg_lambda': 1.128463058774389}, 'config/n_estimators': 8, 'config/max_leaves': 25, 'config/min_child_weight': 0.013548610495542106, 'config/learning_rate': 0.9977579754449118, 'config/subsample': 0.8083911828355876, 'config/colsample_bylevel': 0.8068591840934569, 'config/colsample_bytree': 0.653516717901374, 'config/reg_alpha': 0.00546465520560431, 'config/reg_lambda': 1.128463058774389, 'experiment_tag': 'exp', 'time_total_s': 9.3164381980896}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8068591840934569, colsample_bynode=1,
             colsample_bytree=0.653516717901374, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9977579754449118,
             max_delta_step=0, max_depth=0, max_leaves=25,
             min_child_weight=0.013548610495542106, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00546465520560431,
             reg_lambda=1.128463058774389, scale_pos_weight=1,
             subsample=0.8083911828355876, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7525399862615029
CO(0)的mse=0.012481808335629776
CO(0)的mae=0.07375938239910666
CO(0)的mar=0.1165792370077136
总共花费的时间为：71.18
咸阳市
1918A
1919A
1920A
3525A
[flaml.automl: 09-17 18:00:15] {2390} INFO - task = regression
[flaml.automl: 09-17 18:00:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:00:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:00:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:00:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:00:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:00:16] {3025} INFO - Estimated sufficient time budget=48468s. Estimated necessary time budget=48s.
[flaml.automl: 09-17 18:00:16] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2026,	best estimator xgboost's best error=0.2026
[flaml.automl: 09-17 18:00:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:00:18] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1114,	best estimator xgboost's best error=0.1114
[flaml.automl: 09-17 18:00:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:00:19] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1114,	best estimator xgboost's best error=0.1114
[flaml.automl: 09-17 18:00:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:00:26] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.1114,	best estimator xgboost's best error=0.1114
[flaml.automl: 09-17 18:00:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:00:27] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0795,	best estimator xgboost's best error=0.0795
[flaml.automl: 09-17 18:00:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:00:28] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0650,	best estimator xgboost's best error=0.0650
[flaml.automl: 09-17 18:00:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:00:30] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0650,	best estimator xgboost's best error=0.0650
[flaml.automl: 09-17 18:00:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:00:33] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0650,	best estimator xgboost's best error=0.0650
[flaml.automl: 09-17 18:00:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:00:34] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0650,	best estimator xgboost's best error=0.0650
[flaml.automl: 09-17 18:00:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:00:36] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0650,	best estimator xgboost's best error=0.0650
[flaml.automl: 09-17 18:00:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:00:38] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-17 18:00:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:00:39] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-17 18:00:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:00:46] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-17 18:00:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:00:58] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-17 18:00:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 18:01:04] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-17 18:01:16] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 18:01:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:01:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:01:16] {2637} INFO - Time taken to find the best model: 42.96373987197876
[flaml.automl: 09-17 18:01:16] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40811}
CO(0)最佳损失：0.9461076775433735
CO(0)最好结果：{'pred_time': 9.021738004000998e-06, 'wall_clock_time': 42.96373987197876, 'metric_for_logging': {'pred_time': 9.021738004000998e-06}, 'val_loss': 0.05389232245662642, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40811}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40811, 'experiment_tag': 'exp', 'time_total_s': 12.064782619476318}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9235715176425989
CO(0)的mse=0.006744320077864654
CO(0)的mae=0.05314787671116047
CO(0)的mar=0.0816621875372794
总共花费的时间为：62.26
铜川市
1922A
1923A
[flaml.automl: 09-17 18:08:11] {2390} INFO - task = regression
[flaml.automl: 09-17 18:08:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:08:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:08:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:08:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:08:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:08:13] {3025} INFO - Estimated sufficient time budget=21728s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 18:08:13] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.1524,	best estimator xgboost's best error=0.1524
[flaml.automl: 09-17 18:08:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:08:17] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.1014,	best estimator xgboost's best error=0.1014
[flaml.automl: 09-17 18:08:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:08:19] {3072} INFO -  at 8.0s,	estimator xgboost's best error=0.1014,	best estimator xgboost's best error=0.1014
[flaml.automl: 09-17 18:08:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:08:34] {3072} INFO -  at 23.6s,	estimator xgboost's best error=0.1014,	best estimator xgboost's best error=0.1014
[flaml.automl: 09-17 18:08:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:08:37] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0746,	best estimator xgboost's best error=0.0746
[flaml.automl: 09-17 18:08:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:08:39] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.0634,	best estimator xgboost's best error=0.0634
[flaml.automl: 09-17 18:08:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:08:42] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.0634,	best estimator xgboost's best error=0.0634
[flaml.automl: 09-17 18:08:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:08:47] {3072} INFO -  at 36.1s,	estimator xgboost's best error=0.0634,	best estimator xgboost's best error=0.0634
[flaml.automl: 09-17 18:08:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:08:49] {3072} INFO -  at 38.3s,	estimator xgboost's best error=0.0634,	best estimator xgboost's best error=0.0634
[flaml.automl: 09-17 18:08:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:08:54] {3072} INFO -  at 42.7s,	estimator xgboost's best error=0.0634,	best estimator xgboost's best error=0.0634
[flaml.automl: 09-17 18:08:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:08:56] {3072} INFO -  at 44.8s,	estimator xgboost's best error=0.0634,	best estimator xgboost's best error=0.0634
[flaml.automl: 09-17 18:08:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:08:58] {3072} INFO -  at 47.0s,	estimator xgboost's best error=0.0634,	best estimator xgboost's best error=0.0634
[flaml.automl: 09-17 18:08:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:09:09] {3072} INFO -  at 58.4s,	estimator xgboost's best error=0.0573,	best estimator xgboost's best error=0.0573
[flaml.automl: 09-17 18:09:20] {3335} INFO - retrain xgboost for 11.1s
[flaml.automl: 09-17 18:09:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 18:09:20] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:09:20] {2637} INFO - Time taken to find the best model: 58.436931848526
[flaml.automl: 09-17 18:09:20] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9426661748804287
CO(0)最好结果：{'pred_time': 2.888863736932928e-05, 'wall_clock_time': 58.436931848526, 'metric_for_logging': {'pred_time': 2.888863736932928e-05}, 'val_loss': 0.057333825119571254, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 11.427188634872437}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8668787477860909
CO(0)的mse=0.007716541202556892
CO(0)的mae=0.0561633737317136
CO(0)的mar=0.09627926683892947
总共花费的时间为：69.96
延安市
1926A
1927A
1929A
3652A
[flaml.automl: 09-17 18:21:52] {2390} INFO - task = regression
[flaml.automl: 09-17 18:21:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:21:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:21:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:21:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:21:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:21:54] {3025} INFO - Estimated sufficient time budget=90542s. Estimated necessary time budget=91s.
[flaml.automl: 09-17 18:21:54] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.2114,	best estimator xgboost's best error=0.2114
[flaml.automl: 09-17 18:21:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:21:58] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.1313,	best estimator xgboost's best error=0.1313
[flaml.automl: 09-17 18:21:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:22:00] {3072} INFO -  at 8.4s,	estimator xgboost's best error=0.1313,	best estimator xgboost's best error=0.1313
[flaml.automl: 09-17 18:22:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:22:05] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.1313,	best estimator xgboost's best error=0.1313
[flaml.automl: 09-17 18:22:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:22:08] {3072} INFO -  at 16.7s,	estimator xgboost's best error=0.1096,	best estimator xgboost's best error=0.1096
[flaml.automl: 09-17 18:22:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:22:12] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.0981,	best estimator xgboost's best error=0.0981
[flaml.automl: 09-17 18:22:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:22:16] {3072} INFO -  at 25.0s,	estimator xgboost's best error=0.0981,	best estimator xgboost's best error=0.0981
[flaml.automl: 09-17 18:22:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:22:20] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.0981,	best estimator xgboost's best error=0.0981
[flaml.automl: 09-17 18:22:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:22:23] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.0981,	best estimator xgboost's best error=0.0981
[flaml.automl: 09-17 18:22:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:22:26] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0981,	best estimator xgboost's best error=0.0981
[flaml.automl: 09-17 18:22:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:22:30] {3072} INFO -  at 38.5s,	estimator xgboost's best error=0.0979,	best estimator xgboost's best error=0.0979
[flaml.automl: 09-17 18:22:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:22:33] {3072} INFO -  at 41.8s,	estimator xgboost's best error=0.0979,	best estimator xgboost's best error=0.0979
[flaml.automl: 09-17 18:22:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:22:49] {3072} INFO -  at 57.2s,	estimator xgboost's best error=0.0894,	best estimator xgboost's best error=0.0894
[flaml.automl: 09-17 18:22:59] {3335} INFO - retrain xgboost for 9.9s
[flaml.automl: 09-17 18:22:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 18:22:59] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:22:59] {2637} INFO - Time taken to find the best model: 57.24305057525635
[flaml.automl: 09-17 18:22:59] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 41534}
CO(0)最佳损失：0.9106161671954884
CO(0)最好结果：{'pred_time': 1.7345816962509735e-05, 'wall_clock_time': 57.24305057525635, 'metric_for_logging': {'pred_time': 1.7345816962509735e-05}, 'val_loss': 0.08938383280451158, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 41534}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 41534, 'experiment_tag': 'exp', 'time_total_s': 15.469334602355957}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8552876015798214
CO(0)的mse=0.02037760918891127
CO(0)的mae=0.08820018119957189
CO(0)的mar=0.13669792699591576
总共花费的时间为：68.04
宝鸡市
1930A
1931A
1932A
1933A
1934A
1935A
1937A
[flaml.automl: 09-17 18:45:30] {2390} INFO - task = regression
[flaml.automl: 09-17 18:45:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:45:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:45:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:45:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:45:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:45:32] {3025} INFO - Estimated sufficient time budget=86684s. Estimated necessary time budget=87s.
[flaml.automl: 09-17 18:45:32] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1492,	best estimator xgboost's best error=0.1492
[flaml.automl: 09-17 18:45:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:45:34] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0878,	best estimator xgboost's best error=0.0878
[flaml.automl: 09-17 18:45:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:45:35] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0878,	best estimator xgboost's best error=0.0878
[flaml.automl: 09-17 18:45:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:45:38] {3072} INFO -  at 8.1s,	estimator xgboost's best error=0.0878,	best estimator xgboost's best error=0.0878
[flaml.automl: 09-17 18:45:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:45:39] {3072} INFO -  at 9.2s,	estimator xgboost's best error=0.0677,	best estimator xgboost's best error=0.0677
[flaml.automl: 09-17 18:45:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:45:41] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0609,	best estimator xgboost's best error=0.0609
[flaml.automl: 09-17 18:45:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:45:42] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0609,	best estimator xgboost's best error=0.0609
[flaml.automl: 09-17 18:45:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:45:45] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0609,	best estimator xgboost's best error=0.0609
[flaml.automl: 09-17 18:45:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:45:46] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.0609,	best estimator xgboost's best error=0.0609
[flaml.automl: 09-17 18:45:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:45:49] {3072} INFO -  at 18.7s,	estimator xgboost's best error=0.0609,	best estimator xgboost's best error=0.0609
[flaml.automl: 09-17 18:45:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:45:50] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0600,	best estimator xgboost's best error=0.0600
[flaml.automl: 09-17 18:45:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:45:52] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0600,	best estimator xgboost's best error=0.0600
[flaml.automl: 09-17 18:45:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:45:58] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-17 18:45:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:46:10] {3072} INFO -  at 40.2s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-17 18:46:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 18:46:17] {3072} INFO -  at 46.8s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-17 18:46:17] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 18:46:29] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-17 18:46:41] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 18:46:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:46:41] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:46:41] {2637} INFO - Time taken to find the best model: 40.24820137023926
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 73936}
CO(0)最佳损失：0.9506330799171239
CO(0)最好结果：{'pred_time': 4.885955817975692e-06, 'wall_clock_time': 40.24820137023926, 'metric_for_logging': {'pred_time': 4.885955817975692e-06}, 'val_loss': 0.049366920082876145, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 73936}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 73936, 'experiment_tag': 'exp', 'time_total_s': 12.16033124923706}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9098031011191585
CO(0)的mse=0.006982460136941863
CO(0)的mae=0.04899675314772243
CO(0)的mar=0.08492346981487885
总共花费的时间为：72.55
渭南市
1938A
1939A
1941A
[flaml.automl: 09-17 18:56:13] {2390} INFO - task = regression
[flaml.automl: 09-17 18:56:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:56:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:56:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:56:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:56:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:56:14] {3025} INFO - Estimated sufficient time budget=12051s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 18:56:14] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1758,	best estimator xgboost's best error=0.1758
[flaml.automl: 09-17 18:56:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:56:16] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1042,	best estimator xgboost's best error=0.1042
[flaml.automl: 09-17 18:56:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:56:18] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1042,	best estimator xgboost's best error=0.1042
[flaml.automl: 09-17 18:56:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:56:28] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.1042,	best estimator xgboost's best error=0.1042
[flaml.automl: 09-17 18:56:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:56:29] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0812,	best estimator xgboost's best error=0.0812
[flaml.automl: 09-17 18:56:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:56:30] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 18:56:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:56:32] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 18:56:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:56:34] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 18:56:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:56:36] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 18:56:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:56:38] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 18:56:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:56:39] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 18:56:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:56:41] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.0683,	best estimator xgboost's best error=0.0683
[flaml.automl: 09-17 18:56:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:56:47] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0620,	best estimator xgboost's best error=0.0620
[flaml.automl: 09-17 18:56:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:56:59] {3072} INFO -  at 46.2s,	estimator xgboost's best error=0.0589,	best estimator xgboost's best error=0.0589
[flaml.automl: 09-17 18:56:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 18:57:06] {3072} INFO -  at 52.7s,	estimator xgboost's best error=0.0589,	best estimator xgboost's best error=0.0589
[flaml.automl: 09-17 18:57:18] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 18:57:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:57:18] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:57:18] {2637} INFO - Time taken to find the best model: 46.17194652557373
[flaml.automl: 09-17 18:57:18] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9410789139187639
CO(0)最好结果：{'pred_time': 1.237814708673542e-05, 'wall_clock_time': 46.17194652557373, 'metric_for_logging': {'pred_time': 1.237814708673542e-05}, 'val_loss': 0.05892108608123613, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.95808720588684}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.895355118109043
CO(0)的mse=0.008335104810699654
CO(0)的mae=0.05797221243568585
CO(0)的mar=0.08663883771731336
总共花费的时间为：65.33
金昌市
金昌市没有数据
嘉峪关市
3248A
[flaml.automl: 09-17 19:00:40] {2390} INFO - task = regression
[flaml.automl: 09-17 19:00:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:00:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:00:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:00:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:00:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:00:42] {3025} INFO - Estimated sufficient time budget=12076s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 19:00:42] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1588,	best estimator xgboost's best error=0.1588
[flaml.automl: 09-17 19:00:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:00:43] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.1318,	best estimator xgboost's best error=0.1318
[flaml.automl: 09-17 19:00:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:00:45] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.1318,	best estimator xgboost's best error=0.1318
[flaml.automl: 09-17 19:00:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:00:52] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.1318,	best estimator xgboost's best error=0.1318
[flaml.automl: 09-17 19:00:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:00:53] {3072} INFO -  at 12.6s,	estimator xgboost's best error=0.1147,	best estimator xgboost's best error=0.1147
[flaml.automl: 09-17 19:00:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:00:54] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.1143,	best estimator xgboost's best error=0.1143
[flaml.automl: 09-17 19:00:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:00:56] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.1108,	best estimator xgboost's best error=0.1108
[flaml.automl: 09-17 19:00:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:00:58] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.1108,	best estimator xgboost's best error=0.1108
[flaml.automl: 09-17 19:00:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:01:00] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.1077,	best estimator xgboost's best error=0.1077
[flaml.automl: 09-17 19:01:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:01:03] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.1077,	best estimator xgboost's best error=0.1077
[flaml.automl: 09-17 19:01:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:01:04] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.1077,	best estimator xgboost's best error=0.1077
[flaml.automl: 09-17 19:01:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:01:05] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.1077,	best estimator xgboost's best error=0.1077
[flaml.automl: 09-17 19:01:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:01:11] {3072} INFO -  at 30.8s,	estimator xgboost's best error=0.1077,	best estimator xgboost's best error=0.1077
[flaml.automl: 09-17 19:01:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:01:14] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.1077,	best estimator xgboost's best error=0.1077
[flaml.automl: 09-17 19:01:14] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 19:01:16] {3072} INFO -  at 35.6s,	estimator xgboost's best error=0.1077,	best estimator xgboost's best error=0.1077
[flaml.automl: 09-17 19:01:16] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 19:01:18] {3072} INFO -  at 38.2s,	estimator xgboost's best error=0.1068,	best estimator xgboost's best error=0.1068
[flaml.automl: 09-17 19:01:18] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 19:01:20] {3072} INFO -  at 39.8s,	estimator xgboost's best error=0.1068,	best estimator xgboost's best error=0.1068
[flaml.automl: 09-17 19:01:20] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 19:01:21] {3072} INFO -  at 41.2s,	estimator xgboost's best error=0.1068,	best estimator xgboost's best error=0.1068
[flaml.automl: 09-17 19:01:21] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 19:01:27] {3072} INFO -  at 46.9s,	estimator xgboost's best error=0.1068,	best estimator xgboost's best error=0.1068
[flaml.automl: 09-17 19:01:27] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 19:01:28] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.1068,	best estimator xgboost's best error=0.1068
[flaml.automl: 09-17 19:01:28] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 19:01:35] {3072} INFO -  at 55.0s,	estimator xgboost's best error=0.1068,	best estimator xgboost's best error=0.1068
[flaml.automl: 09-17 19:01:35] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 19:01:40] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.1068,	best estimator xgboost's best error=0.1068
[flaml.automl: 09-17 19:01:43] {3335} INFO - retrain xgboost for 2.5s
[flaml.automl: 09-17 19:01:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.48141033599690336, colsample_bynode=1,
             colsample_bytree=0.7247280147716837, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8186927257821339,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=4.696667157872396, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.004591758016366177, reg_lambda=29.337047760912764,
             scale_pos_weight=1, subsample=0.964553817673497,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 19:01:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:01:43] {2637} INFO - Time taken to find the best model: 38.16695952415466
CO(0)最佳参数：{'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 4.696667157872396, 'learning_rate': 0.8186927257821339, 'subsample': 0.964553817673497, 'colsample_bylevel': 0.48141033599690336, 'colsample_bytree': 0.7247280147716837, 'reg_alpha': 0.004591758016366177, 'reg_lambda': 29.337047760912764}
CO(0)最佳损失：0.8931510449639896
CO(0)最好结果：{'pred_time': 3.3555363127406555e-05, 'wall_clock_time': 38.16695952415466, 'metric_for_logging': {'pred_time': 3.3555363127406555e-05}, 'val_loss': 0.1068489550360104, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 4.696667157872396, 'learning_rate': 0.8186927257821339, 'subsample': 0.964553817673497, 'colsample_bylevel': 0.48141033599690336, 'colsample_bytree': 0.7247280147716837, 'reg_alpha': 0.004591758016366177, 'reg_lambda': 29.337047760912764}, 'config/n_estimators': 6, 'config/max_leaves': 7, 'config/min_child_weight': 4.696667157872396, 'config/learning_rate': 0.8186927257821339, 'config/subsample': 0.964553817673497, 'config/colsample_bylevel': 0.48141033599690336, 'config/colsample_bytree': 0.7247280147716837, 'config/reg_alpha': 0.004591758016366177, 'config/reg_lambda': 29.337047760912764, 'experiment_tag': 'exp', 'time_total_s': 2.5980217456817627}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.48141033599690336, colsample_bynode=1,
             colsample_bytree=0.7247280147716837, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8186927257821339,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=4.696667157872396, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.004591758016366177, reg_lambda=29.337047760912764,
             scale_pos_weight=1, subsample=0.964553817673497,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=-0.36542074924469814
CO(0)的mse=0.030213826031062187
CO(0)的mae=0.11002521585967966
CO(0)的mar=0.5359893788699182
总共花费的时间为：62.67
石嘴山市
1947A
1949A
1950A
3520A
3521A
[flaml.automl: 09-17 19:17:47] {2390} INFO - task = regression
[flaml.automl: 09-17 19:17:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:17:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:17:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:17:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:17:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:17:51] {3025} INFO - Estimated sufficient time budget=171915s. Estimated necessary time budget=172s.
[flaml.automl: 09-17 19:17:51] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.2823,	best estimator xgboost's best error=0.2823
[flaml.automl: 09-17 19:17:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:17:55] {3072} INFO -  at 8.4s,	estimator xgboost's best error=0.2300,	best estimator xgboost's best error=0.2300
[flaml.automl: 09-17 19:17:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:17:59] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.2300,	best estimator xgboost's best error=0.2300
[flaml.automl: 09-17 19:17:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:18:02] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.2300,	best estimator xgboost's best error=0.2300
[flaml.automl: 09-17 19:18:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:18:06] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.1909,	best estimator xgboost's best error=0.1909
[flaml.automl: 09-17 19:18:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:18:09] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.1844,	best estimator xgboost's best error=0.1844
[flaml.automl: 09-17 19:18:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:18:12] {3072} INFO -  at 25.0s,	estimator xgboost's best error=0.1844,	best estimator xgboost's best error=0.1844
[flaml.automl: 09-17 19:18:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:18:14] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.1844,	best estimator xgboost's best error=0.1844
[flaml.automl: 09-17 19:18:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:18:17] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.1844,	best estimator xgboost's best error=0.1844
[flaml.automl: 09-17 19:18:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:18:19] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.1844,	best estimator xgboost's best error=0.1844
[flaml.automl: 09-17 19:18:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:18:23] {3072} INFO -  at 36.3s,	estimator xgboost's best error=0.1776,	best estimator xgboost's best error=0.1776
[flaml.automl: 09-17 19:18:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:18:27] {3072} INFO -  at 39.6s,	estimator xgboost's best error=0.1776,	best estimator xgboost's best error=0.1776
[flaml.automl: 09-17 19:18:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:18:40] {3072} INFO -  at 52.6s,	estimator xgboost's best error=0.1770,	best estimator xgboost's best error=0.1770
[flaml.automl: 09-17 19:18:46] {3335} INFO - retrain xgboost for 6.6s
[flaml.automl: 09-17 19:18:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 19:18:46] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:18:46] {2637} INFO - Time taken to find the best model: 52.56580400466919
[flaml.automl: 09-17 19:18:46] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 50932}
CO(0)最佳损失：0.8230127000722784
CO(0)最好结果：{'pred_time': 1.3698242578405373e-05, 'wall_clock_time': 52.56580400466919, 'metric_for_logging': {'pred_time': 1.3698242578405373e-05}, 'val_loss': 0.1769872999277216, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 50932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 50932, 'experiment_tag': 'exp', 'time_total_s': 12.942084074020386}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.5509081076489724
CO(0)的mse=0.09207548290486381
CO(0)的mae=0.17870918281637704
CO(0)的mar=0.27574616341394514
总共花费的时间为：60.16
克拉玛依市
1951A
1955A
3612A
[flaml.automl: 09-17 19:28:31] {2390} INFO - task = regression
[flaml.automl: 09-17 19:28:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:28:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:28:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:28:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:28:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:28:32] {3025} INFO - Estimated sufficient time budget=12105s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 19:28:32] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1712,	best estimator xgboost's best error=0.1712
[flaml.automl: 09-17 19:28:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:28:35] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0945,	best estimator xgboost's best error=0.0945
[flaml.automl: 09-17 19:28:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:28:36] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0945,	best estimator xgboost's best error=0.0945
[flaml.automl: 09-17 19:28:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:28:46] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0945,	best estimator xgboost's best error=0.0945
[flaml.automl: 09-17 19:28:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:28:47] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0642,	best estimator xgboost's best error=0.0642
[flaml.automl: 09-17 19:28:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:28:49] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-17 19:28:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:28:50] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-17 19:28:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:28:53] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-17 19:28:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:28:54] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-17 19:28:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:28:57] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-17 19:28:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:28:58] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-17 19:28:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:28:59] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-17 19:28:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:29:05] {3072} INFO -  at 34.4s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-17 19:29:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:29:18] {3072} INFO -  at 46.5s,	estimator xgboost's best error=0.0425,	best estimator xgboost's best error=0.0425
[flaml.automl: 09-17 19:29:18] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 19:29:24] {3072} INFO -  at 53.0s,	estimator xgboost's best error=0.0425,	best estimator xgboost's best error=0.0425
[flaml.automl: 09-17 19:29:36] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 19:29:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:29:36] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:29:36] {2637} INFO - Time taken to find the best model: 46.479687213897705
[flaml.automl: 09-17 19:29:36] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9574649861665452
CO(0)最好结果：{'pred_time': 1.1410569249854193e-05, 'wall_clock_time': 46.479687213897705, 'metric_for_logging': {'pred_time': 1.1410569249854193e-05}, 'val_loss': 0.04253501383345479, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.085022926330566}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9198868710837769
CO(0)的mse=0.006741505502714134
CO(0)的mae=0.04560215635841317
CO(0)的mar=0.12565765527056094
总共花费的时间为：65.59
巴音郭楞州
1957A
1958A
[flaml.automl: 09-17 19:35:59] {2390} INFO - task = regression
[flaml.automl: 09-17 19:35:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:35:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:35:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:35:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:35:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:36:00] {3025} INFO - Estimated sufficient time budget=12109s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 19:36:00] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1372,	best estimator xgboost's best error=0.1372
[flaml.automl: 09-17 19:36:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:36:02] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0830,	best estimator xgboost's best error=0.0830
[flaml.automl: 09-17 19:36:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:36:03] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.0830,	best estimator xgboost's best error=0.0830
[flaml.automl: 09-17 19:36:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:36:13] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0830,	best estimator xgboost's best error=0.0830
[flaml.automl: 09-17 19:36:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:36:14] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.0611,	best estimator xgboost's best error=0.0611
[flaml.automl: 09-17 19:36:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:36:15] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-17 19:36:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:36:18] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-17 19:36:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:36:22] {3072} INFO -  at 23.5s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-17 19:36:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:36:24] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-17 19:36:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:36:29] {3072} INFO -  at 30.0s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-17 19:36:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:36:31] {3072} INFO -  at 32.2s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-17 19:36:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:36:33] {3072} INFO -  at 34.4s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-17 19:36:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:36:44] {3072} INFO -  at 45.5s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-17 19:36:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:36:58] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.0485,	best estimator xgboost's best error=0.0485
[flaml.automl: 09-17 19:37:17] {3335} INFO - retrain xgboost for 19.1s
[flaml.automl: 09-17 19:37:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:37:17] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:37:17] {2637} INFO - Time taken to find the best model: 59.330655574798584
[flaml.automl: 09-17 19:37:17] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9515386519297588
CO(0)最好结果：{'pred_time': 3.358303022778724e-05, 'wall_clock_time': 59.330655574798584, 'metric_for_logging': {'pred_time': 3.358303022778724e-05}, 'val_loss': 0.0484613480702412, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 13.77602481842041}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7929563078755828
CO(0)的mse=0.006316633468326761
CO(0)的mae=0.049003623393376376
CO(0)的mar=0.16549273723042976
总共花费的时间为：78.99
信阳市
2054A
2064A
2065A
2066A
[flaml.automl: 09-17 19:50:30] {2390} INFO - task = regression
[flaml.automl: 09-17 19:50:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:50:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:50:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:50:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:50:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:50:31] {3025} INFO - Estimated sufficient time budget=51538s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 19:50:31] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1191,	best estimator xgboost's best error=0.1191
[flaml.automl: 09-17 19:50:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:50:33] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0705,	best estimator xgboost's best error=0.0705
[flaml.automl: 09-17 19:50:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:50:34] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0705,	best estimator xgboost's best error=0.0705
[flaml.automl: 09-17 19:50:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:50:40] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0705,	best estimator xgboost's best error=0.0705
[flaml.automl: 09-17 19:50:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:50:42] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0549,	best estimator xgboost's best error=0.0549
[flaml.automl: 09-17 19:50:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:50:43] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0448,	best estimator xgboost's best error=0.0448
[flaml.automl: 09-17 19:50:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:50:45] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0448,	best estimator xgboost's best error=0.0448
[flaml.automl: 09-17 19:50:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:50:47] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0448,	best estimator xgboost's best error=0.0448
[flaml.automl: 09-17 19:50:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:50:48] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0448,	best estimator xgboost's best error=0.0448
[flaml.automl: 09-17 19:50:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:50:51] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0448,	best estimator xgboost's best error=0.0448
[flaml.automl: 09-17 19:50:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:50:53] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-17 19:50:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:50:54] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-17 19:50:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:51:00] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0388,	best estimator xgboost's best error=0.0388
[flaml.automl: 09-17 19:51:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:51:12] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.0371,	best estimator xgboost's best error=0.0371
[flaml.automl: 09-17 19:51:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 19:51:19] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0371,	best estimator xgboost's best error=0.0371
[flaml.automl: 09-17 19:51:31] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 19:51:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:51:31] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:51:31] {2637} INFO - Time taken to find the best model: 43.03225636482239
[flaml.automl: 09-17 19:51:31] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42953}
CO(0)最佳损失：0.9628537581150272
CO(0)最好结果：{'pred_time': 8.335159480784041e-06, 'wall_clock_time': 43.03225636482239, 'metric_for_logging': {'pred_time': 8.335159480784041e-06}, 'val_loss': 0.03714624188497284, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42953}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42953, 'experiment_tag': 'exp', 'time_total_s': 12.04431939125061}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9249630273891915
CO(0)的mse=0.0032236105447267373
CO(0)的mae=0.03630873551828196
CO(0)的mar=0.07570587252471957
总共花费的时间为：62.24
周口市
2067A
2068A
2069A
2070A
[flaml.automl: 09-17 20:04:17] {2390} INFO - task = regression
[flaml.automl: 09-17 20:04:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:04:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:04:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:04:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:04:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:04:18] {3025} INFO - Estimated sufficient time budget=52412s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 20:04:18] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1597,	best estimator xgboost's best error=0.1597
[flaml.automl: 09-17 20:04:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:04:20] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0869,	best estimator xgboost's best error=0.0869
[flaml.automl: 09-17 20:04:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:04:21] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0869,	best estimator xgboost's best error=0.0869
[flaml.automl: 09-17 20:04:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:04:27] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.0869,	best estimator xgboost's best error=0.0869
[flaml.automl: 09-17 20:04:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:04:28] {3072} INFO -  at 11.7s,	estimator xgboost's best error=0.0609,	best estimator xgboost's best error=0.0609
[flaml.automl: 09-17 20:04:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:04:30] {3072} INFO -  at 13.3s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-17 20:04:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:04:32] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-17 20:04:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:04:34] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-17 20:04:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:04:35] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-17 20:04:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:04:38] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-17 20:04:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:04:40] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-17 20:04:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:04:41] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-17 20:04:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:04:47] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-17 20:04:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:04:59] {3072} INFO -  at 42.6s,	estimator xgboost's best error=0.0400,	best estimator xgboost's best error=0.0400
[flaml.automl: 09-17 20:04:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:05:06] {3072} INFO -  at 49.2s,	estimator xgboost's best error=0.0400,	best estimator xgboost's best error=0.0400
[flaml.automl: 09-17 20:05:18] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 20:05:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:05:18] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:05:18] {2637} INFO - Time taken to find the best model: 42.63154125213623
[flaml.automl: 09-17 20:05:18] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43428}
CO(0)最佳损失：0.9599723958691224
CO(0)最好结果：{'pred_time': 8.364362040842613e-06, 'wall_clock_time': 42.63154125213623, 'metric_for_logging': {'pred_time': 8.364362040842613e-06}, 'val_loss': 0.04002760413087753, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43428}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43428, 'experiment_tag': 'exp', 'time_total_s': 12.097906351089478}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9184011678034498
CO(0)的mse=0.004450549282845237
CO(0)的mae=0.040415590653598246
CO(0)的mar=0.06556481407866416
总共花费的时间为：61.99
漳州市
2075A
2920A
3216A
3530A
[flaml.automl: 09-17 20:18:25] {2390} INFO - task = regression
[flaml.automl: 09-17 20:18:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:18:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:18:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:18:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:18:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:18:26] {3025} INFO - Estimated sufficient time budget=50070s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 20:18:26] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.0956,	best estimator xgboost's best error=0.0956
[flaml.automl: 09-17 20:18:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:18:28] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0587,	best estimator xgboost's best error=0.0587
[flaml.automl: 09-17 20:18:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:18:29] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0587,	best estimator xgboost's best error=0.0587
[flaml.automl: 09-17 20:18:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:18:36] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.0587,	best estimator xgboost's best error=0.0587
[flaml.automl: 09-17 20:18:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:18:37] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-17 20:18:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:18:38] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0401,	best estimator xgboost's best error=0.0401
[flaml.automl: 09-17 20:18:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:18:40] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0401,	best estimator xgboost's best error=0.0401
[flaml.automl: 09-17 20:18:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:18:42] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.0401,	best estimator xgboost's best error=0.0401
[flaml.automl: 09-17 20:18:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:18:44] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0401,	best estimator xgboost's best error=0.0401
[flaml.automl: 09-17 20:18:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:18:46] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0401,	best estimator xgboost's best error=0.0401
[flaml.automl: 09-17 20:18:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:18:48] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-17 20:18:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:18:49] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-17 20:18:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:18:55] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.0350,	best estimator xgboost's best error=0.0350
[flaml.automl: 09-17 20:18:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:19:08] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.0339,	best estimator xgboost's best error=0.0339
[flaml.automl: 09-17 20:19:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:19:14] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0339,	best estimator xgboost's best error=0.0339
[flaml.automl: 09-17 20:19:26] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 20:19:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:19:26] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:19:26] {2637} INFO - Time taken to find the best model: 43.03732085227966
[flaml.automl: 09-17 20:19:26] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41924}
CO(0)最佳损失：0.9660923217500988
CO(0)最好结果：{'pred_time': 8.744931471759758e-06, 'wall_clock_time': 43.03732085227966, 'metric_for_logging': {'pred_time': 8.744931471759758e-06}, 'val_loss': 0.033907678249901166, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41924}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41924, 'experiment_tag': 'exp', 'time_total_s': 12.099026918411255}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9029635274473354
CO(0)的mse=0.002425057108378759
CO(0)的mae=0.03277283246345368
CO(0)的mar=0.07173107083179069
总共花费的时间为：62.48
晋城市
2160A
2161A
2162A
2163A
3620A
[flaml.automl: 09-17 20:34:51] {2390} INFO - task = regression
[flaml.automl: 09-17 20:34:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:34:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:34:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:34:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:34:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:34:52] {3025} INFO - Estimated sufficient time budget=63417s. Estimated necessary time budget=63s.
[flaml.automl: 09-17 20:34:52] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.2960,	best estimator xgboost's best error=0.2960
[flaml.automl: 09-17 20:34:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:34:54] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1715,	best estimator xgboost's best error=0.1715
[flaml.automl: 09-17 20:34:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:34:55] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1715,	best estimator xgboost's best error=0.1715
[flaml.automl: 09-17 20:34:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:35:00] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.1715,	best estimator xgboost's best error=0.1715
[flaml.automl: 09-17 20:35:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:35:01] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.1399,	best estimator xgboost's best error=0.1399
[flaml.automl: 09-17 20:35:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:35:03] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.1205,	best estimator xgboost's best error=0.1205
[flaml.automl: 09-17 20:35:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:35:04] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.1205,	best estimator xgboost's best error=0.1205
[flaml.automl: 09-17 20:35:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:35:07] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.1205,	best estimator xgboost's best error=0.1205
[flaml.automl: 09-17 20:35:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:35:08] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.1205,	best estimator xgboost's best error=0.1205
[flaml.automl: 09-17 20:35:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:35:11] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.1205,	best estimator xgboost's best error=0.1205
[flaml.automl: 09-17 20:35:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:35:12] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.1205,	best estimator xgboost's best error=0.1205
[flaml.automl: 09-17 20:35:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:35:13] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.1205,	best estimator xgboost's best error=0.1205
[flaml.automl: 09-17 20:35:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:35:20] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.1152,	best estimator xgboost's best error=0.1152
[flaml.automl: 09-17 20:35:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:35:32] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.1088,	best estimator xgboost's best error=0.1088
[flaml.automl: 09-17 20:35:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:35:39] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.1088,	best estimator xgboost's best error=0.1088
[flaml.automl: 09-17 20:35:51] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 20:35:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:35:51] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:35:51] {2637} INFO - Time taken to find the best model: 41.68553447723389
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53224}
CO(0)最佳损失：0.8911704370388116
CO(0)最好结果：{'pred_time': 7.10256768111486e-06, 'wall_clock_time': 41.68553447723389, 'metric_for_logging': {'pred_time': 7.10256768111486e-06}, 'val_loss': 0.10882956296118845, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53224}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53224, 'experiment_tag': 'exp', 'time_total_s': 12.12848711013794}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.828040915617558
CO(0)的mse=0.0294458257646149
CO(0)的mae=0.10518388922526364
CO(0)的mar=0.12257134576425466
总共花费的时间为：61.24
朔州市
2166A
2167A
2168A
2169A
2170A
3571A
[flaml.automl: 09-17 20:55:39] {2390} INFO - task = regression
[flaml.automl: 09-17 20:55:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:55:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:55:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:55:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:55:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:55:41] {3025} INFO - Estimated sufficient time budget=77864s. Estimated necessary time budget=78s.
[flaml.automl: 09-17 20:55:41] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1434,	best estimator xgboost's best error=0.1434
[flaml.automl: 09-17 20:55:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:55:43] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0874,	best estimator xgboost's best error=0.0874
[flaml.automl: 09-17 20:55:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:55:44] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0874,	best estimator xgboost's best error=0.0874
[flaml.automl: 09-17 20:55:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:55:48] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.0874,	best estimator xgboost's best error=0.0874
[flaml.automl: 09-17 20:55:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:55:49] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0710,	best estimator xgboost's best error=0.0710
[flaml.automl: 09-17 20:55:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:55:50] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 20:55:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:55:52] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 20:55:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:55:55] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 20:55:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:55:56] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 20:55:56] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:55:58] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-17 20:55:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:56:00] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0615,	best estimator xgboost's best error=0.0615
[flaml.automl: 09-17 20:56:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:56:01] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0615,	best estimator xgboost's best error=0.0615
[flaml.automl: 09-17 20:56:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:56:08] {3072} INFO -  at 28.7s,	estimator xgboost's best error=0.0541,	best estimator xgboost's best error=0.0541
[flaml.automl: 09-17 20:56:08] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:56:20] {3072} INFO -  at 40.8s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-17 20:56:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:56:26] {3072} INFO -  at 47.4s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-17 20:56:26] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 20:56:38] {3072} INFO -  at 58.8s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-17 20:56:50] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 20:56:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:56:50] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:56:50] {2637} INFO - Time taken to find the best model: 40.82268977165222
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65087}
CO(0)最佳损失：0.9478255244656507
CO(0)最好结果：{'pred_time': 5.543891307526985e-06, 'wall_clock_time': 40.82268977165222, 'metric_for_logging': {'pred_time': 5.543891307526985e-06}, 'val_loss': 0.05217447553434928, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65087}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 65087, 'experiment_tag': 'exp', 'time_total_s': 12.151193618774414}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8690671432369065
CO(0)的mse=0.00634093380497466
CO(0)的mae=0.05293991323307179
CO(0)的mar=0.08676671478863374
总共花费的时间为：71.98
晋中市
2171A
2174A
2865A
[flaml.automl: 09-17 21:21:50] {2390} INFO - task = regression
[flaml.automl: 09-17 21:21:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:21:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:21:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:21:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:21:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:21:51] {3025} INFO - Estimated sufficient time budget=11925s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:21:51] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2137,	best estimator xgboost's best error=0.2137
[flaml.automl: 09-17 21:21:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:21:53] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1356,	best estimator xgboost's best error=0.1356
[flaml.automl: 09-17 21:21:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:21:54] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1356,	best estimator xgboost's best error=0.1356
[flaml.automl: 09-17 21:21:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:22:04] {3072} INFO -  at 14.6s,	estimator xgboost's best error=0.1356,	best estimator xgboost's best error=0.1356
[flaml.automl: 09-17 21:22:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:22:06] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.1127,	best estimator xgboost's best error=0.1127
[flaml.automl: 09-17 21:22:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:22:07] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0987,	best estimator xgboost's best error=0.0987
[flaml.automl: 09-17 21:22:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:22:09] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0987,	best estimator xgboost's best error=0.0987
[flaml.automl: 09-17 21:22:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:22:11] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0987,	best estimator xgboost's best error=0.0987
[flaml.automl: 09-17 21:22:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:22:12] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0987,	best estimator xgboost's best error=0.0987
[flaml.automl: 09-17 21:22:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:22:15] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0987,	best estimator xgboost's best error=0.0987
[flaml.automl: 09-17 21:22:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:22:16] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0987,	best estimator xgboost's best error=0.0987
[flaml.automl: 09-17 21:22:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:22:17] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.0987,	best estimator xgboost's best error=0.0987
[flaml.automl: 09-17 21:22:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:22:24] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0902,	best estimator xgboost's best error=0.0902
[flaml.automl: 09-17 21:22:24] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:22:36] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.0855,	best estimator xgboost's best error=0.0855
[flaml.automl: 09-17 21:22:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 21:22:43] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.0855,	best estimator xgboost's best error=0.0855
[flaml.automl: 09-17 21:22:55] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 21:22:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:22:55] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:22:55] {2637} INFO - Time taken to find the best model: 46.35543155670166
[flaml.automl: 09-17 21:22:55] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9145141593645681
CO(0)最好结果：{'pred_time': 1.2117017167434306e-05, 'wall_clock_time': 46.35543155670166, 'metric_for_logging': {'pred_time': 1.2117017167434306e-05}, 'val_loss': 0.08548584063543184, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.12447190284729}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8267754304291939
CO(0)的mse=0.018558558405489055
CO(0)的mae=0.08653113897336107
CO(0)的mar=0.17814245052494976
总共花费的时间为：65.67
运城市
2175A
2178A
2179A
3670A
[flaml.automl: 09-17 21:36:36] {2390} INFO - task = regression
[flaml.automl: 09-17 21:36:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:36:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:36:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:36:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:36:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:36:39] {3025} INFO - Estimated sufficient time budget=123668s. Estimated necessary time budget=124s.
[flaml.automl: 09-17 21:36:39] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.2207,	best estimator xgboost's best error=0.2207
[flaml.automl: 09-17 21:36:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:36:44] {3072} INFO -  at 8.1s,	estimator xgboost's best error=0.1286,	best estimator xgboost's best error=0.1286
[flaml.automl: 09-17 21:36:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:36:46] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.1286,	best estimator xgboost's best error=0.1286
[flaml.automl: 09-17 21:36:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:36:52] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.1286,	best estimator xgboost's best error=0.1286
[flaml.automl: 09-17 21:36:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:36:54] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.1039,	best estimator xgboost's best error=0.1039
[flaml.automl: 09-17 21:36:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:36:58] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0889,	best estimator xgboost's best error=0.0889
[flaml.automl: 09-17 21:36:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:37:02] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.0889,	best estimator xgboost's best error=0.0889
[flaml.automl: 09-17 21:37:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:37:04] {3072} INFO -  at 28.9s,	estimator xgboost's best error=0.0889,	best estimator xgboost's best error=0.0889
[flaml.automl: 09-17 21:37:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:37:07] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.0889,	best estimator xgboost's best error=0.0889
[flaml.automl: 09-17 21:37:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:37:09] {3072} INFO -  at 33.4s,	estimator xgboost's best error=0.0889,	best estimator xgboost's best error=0.0889
[flaml.automl: 09-17 21:37:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:37:13] {3072} INFO -  at 37.4s,	estimator xgboost's best error=0.0885,	best estimator xgboost's best error=0.0885
[flaml.automl: 09-17 21:37:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:37:16] {3072} INFO -  at 40.0s,	estimator xgboost's best error=0.0885,	best estimator xgboost's best error=0.0885
[flaml.automl: 09-17 21:37:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:37:31] {3072} INFO -  at 55.0s,	estimator xgboost's best error=0.0831,	best estimator xgboost's best error=0.0831
[flaml.automl: 09-17 21:37:40] {3335} INFO - retrain xgboost for 9.4s
[flaml.automl: 09-17 21:37:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 21:37:40] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:37:40] {2637} INFO - Time taken to find the best model: 54.993995904922485
[flaml.automl: 09-17 21:37:40] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 42553}
CO(0)最佳损失：0.9169060400703867
CO(0)最好结果：{'pred_time': 1.9984710138376987e-05, 'wall_clock_time': 54.993995904922485, 'metric_for_logging': {'pred_time': 1.9984710138376987e-05}, 'val_loss': 0.08309395992961337, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 42553}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 42553, 'experiment_tag': 'exp', 'time_total_s': 15.004909992218018}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.855454166862172
CO(0)的mse=0.021402580124844986
CO(0)的mae=0.0850020713439948
CO(0)的mar=0.11037448950942993
总共花费的时间为：65.30
忻州市
2182A
3208A
[flaml.automl: 09-17 21:45:05] {2390} INFO - task = regression
[flaml.automl: 09-17 21:45:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:45:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:45:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:45:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:45:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:45:07] {3025} INFO - Estimated sufficient time budget=22948s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 21:45:07] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1788,	best estimator xgboost's best error=0.1788
[flaml.automl: 09-17 21:45:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:45:11] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.1043,	best estimator xgboost's best error=0.1043
[flaml.automl: 09-17 21:45:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:45:13] {3072} INFO -  at 8.4s,	estimator xgboost's best error=0.1043,	best estimator xgboost's best error=0.1043
[flaml.automl: 09-17 21:45:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:45:27] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.1043,	best estimator xgboost's best error=0.1043
[flaml.automl: 09-17 21:45:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:45:28] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0828,	best estimator xgboost's best error=0.0828
[flaml.automl: 09-17 21:45:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:45:30] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.0709,	best estimator xgboost's best error=0.0709
[flaml.automl: 09-17 21:45:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:45:31] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0709,	best estimator xgboost's best error=0.0709
[flaml.automl: 09-17 21:45:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:45:34] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.0709,	best estimator xgboost's best error=0.0709
[flaml.automl: 09-17 21:45:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:45:35] {3072} INFO -  at 30.1s,	estimator xgboost's best error=0.0709,	best estimator xgboost's best error=0.0709
[flaml.automl: 09-17 21:45:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:45:37] {3072} INFO -  at 32.6s,	estimator xgboost's best error=0.0709,	best estimator xgboost's best error=0.0709
[flaml.automl: 09-17 21:45:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:45:38] {3072} INFO -  at 33.7s,	estimator xgboost's best error=0.0709,	best estimator xgboost's best error=0.0709
[flaml.automl: 09-17 21:45:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:45:40] {3072} INFO -  at 34.8s,	estimator xgboost's best error=0.0709,	best estimator xgboost's best error=0.0709
[flaml.automl: 09-17 21:45:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:45:46] {3072} INFO -  at 40.8s,	estimator xgboost's best error=0.0652,	best estimator xgboost's best error=0.0652
[flaml.automl: 09-17 21:45:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 21:45:56] {3072} INFO -  at 51.2s,	estimator xgboost's best error=0.0613,	best estimator xgboost's best error=0.0613
[flaml.automl: 09-17 21:46:06] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-17 21:46:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:46:06] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:46:06] {2637} INFO - Time taken to find the best model: 51.15875720977783
[flaml.automl: 09-17 21:46:06] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.938720466967094
CO(0)最好结果：{'pred_time': 1.6603727063697405e-05, 'wall_clock_time': 51.15875720977783, 'metric_for_logging': {'pred_time': 1.6603727063697405e-05}, 'val_loss': 0.061279533032906, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.357841968536377}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8706950177887012
CO(0)的mse=0.009883672545033981
CO(0)的mae=0.06091765643927336
CO(0)的mar=0.08789694910763923
总共花费的时间为：62.07
吕梁市
2183A
2867A
[flaml.automl: 09-17 21:53:25] {2390} INFO - task = regression
[flaml.automl: 09-17 21:53:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:53:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:53:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:53:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:53:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:53:28] {3025} INFO - Estimated sufficient time budget=32380s. Estimated necessary time budget=32s.
[flaml.automl: 09-17 21:53:28] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1573,	best estimator xgboost's best error=0.1573
[flaml.automl: 09-17 21:53:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:53:34] {3072} INFO -  at 8.9s,	estimator xgboost's best error=0.1071,	best estimator xgboost's best error=0.1071
[flaml.automl: 09-17 21:53:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:53:37] {3072} INFO -  at 12.1s,	estimator xgboost's best error=0.1071,	best estimator xgboost's best error=0.1071
[flaml.automl: 09-17 21:53:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:54:02] {3072} INFO -  at 37.3s,	estimator xgboost's best error=0.1071,	best estimator xgboost's best error=0.1071
[flaml.automl: 09-17 21:54:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:54:05] {3072} INFO -  at 40.4s,	estimator xgboost's best error=0.0942,	best estimator xgboost's best error=0.0942
[flaml.automl: 09-17 21:54:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:54:10] {3072} INFO -  at 44.6s,	estimator xgboost's best error=0.0841,	best estimator xgboost's best error=0.0841
[flaml.automl: 09-17 21:54:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:54:14] {3072} INFO -  at 48.9s,	estimator xgboost's best error=0.0841,	best estimator xgboost's best error=0.0841
[flaml.automl: 09-17 21:54:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:54:20] {3072} INFO -  at 55.6s,	estimator xgboost's best error=0.0841,	best estimator xgboost's best error=0.0841
[flaml.automl: 09-17 21:54:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:54:23] {3072} INFO -  at 58.4s,	estimator xgboost's best error=0.0841,	best estimator xgboost's best error=0.0841
[flaml.automl: 09-17 21:54:26] {3335} INFO - retrain xgboost for 2.8s
[flaml.automl: 09-17 21:54:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 21:54:26] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:54:26] {2637} INFO - Time taken to find the best model: 44.62178325653076
[flaml.automl: 09-17 21:54:26] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
CO(0)最佳损失：0.9159082615062384
CO(0)最好结果：{'pred_time': 5.0617176002001425e-05, 'wall_clock_time': 44.62178325653076, 'metric_for_logging': {'pred_time': 5.0617176002001425e-05}, 'val_loss': 0.08409173849376163, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 4.2700583934783936}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.6612207258538481
CO(0)的mse=0.01759130600809204
CO(0)的mae=0.08746379164515744
CO(0)的mar=0.2507662680755612
总共花费的时间为：61.59
乌海市
2188A
3284A
3621A
[flaml.automl: 09-17 22:04:37] {2390} INFO - task = regression
[flaml.automl: 09-17 22:04:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:04:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:04:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:04:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:04:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:04:40] {3025} INFO - Estimated sufficient time budget=22459s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 22:04:40] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.2431,	best estimator xgboost's best error=0.2431
[flaml.automl: 09-17 22:04:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:04:44] {3072} INFO -  at 6.5s,	estimator xgboost's best error=0.1683,	best estimator xgboost's best error=0.1683
[flaml.automl: 09-17 22:04:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:04:46] {3072} INFO -  at 8.7s,	estimator xgboost's best error=0.1683,	best estimator xgboost's best error=0.1683
[flaml.automl: 09-17 22:04:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:05:03] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.1683,	best estimator xgboost's best error=0.1683
[flaml.automl: 09-17 22:05:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:05:04] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.1480,	best estimator xgboost's best error=0.1480
[flaml.automl: 09-17 22:05:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:05:05] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-17 22:05:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:05:07] {3072} INFO -  at 29.8s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-17 22:05:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:05:09] {3072} INFO -  at 32.2s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-17 22:05:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:05:10] {3072} INFO -  at 33.3s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-17 22:05:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:05:13] {3072} INFO -  at 35.9s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-17 22:05:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:05:14] {3072} INFO -  at 37.0s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-17 22:05:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:05:15] {3072} INFO -  at 38.2s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-17 22:05:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:05:22] {3072} INFO -  at 44.5s,	estimator xgboost's best error=0.1332,	best estimator xgboost's best error=0.1332
[flaml.automl: 09-17 22:05:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:05:34] {3072} INFO -  at 56.4s,	estimator xgboost's best error=0.1262,	best estimator xgboost's best error=0.1262
[flaml.automl: 09-17 22:05:45] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-17 22:05:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:05:45] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:05:45] {2637} INFO - Time taken to find the best model: 56.41163372993469
[flaml.automl: 09-17 22:05:45] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.873821730520897
CO(0)最好结果：{'pred_time': 1.141987426534555e-05, 'wall_clock_time': 56.41163372993469, 'metric_for_logging': {'pred_time': 1.141987426534555e-05}, 'val_loss': 0.12617826947910302, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.869862794876099}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7224961489599264
CO(0)的mse=0.0454138045236601
CO(0)的mae=0.12908876506199743
CO(0)的mar=0.17517296180240535
总共花费的时间为：68.96
通辽市
2191A
3706A
3708A
[flaml.automl: 09-17 22:15:38] {2390} INFO - task = regression
[flaml.automl: 09-17 22:15:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:15:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:15:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:15:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:15:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:15:39] {3025} INFO - Estimated sufficient time budget=12239s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 22:15:39] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1602,	best estimator xgboost's best error=0.1602
[flaml.automl: 09-17 22:15:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:15:41] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0953,	best estimator xgboost's best error=0.0953
[flaml.automl: 09-17 22:15:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:15:43] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0953,	best estimator xgboost's best error=0.0953
[flaml.automl: 09-17 22:15:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:15:53] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0953,	best estimator xgboost's best error=0.0953
[flaml.automl: 09-17 22:15:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:15:54] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0719,	best estimator xgboost's best error=0.0719
[flaml.automl: 09-17 22:15:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:15:55] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0616,	best estimator xgboost's best error=0.0616
[flaml.automl: 09-17 22:15:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:15:57] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0616,	best estimator xgboost's best error=0.0616
[flaml.automl: 09-17 22:15:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:16:00] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0616,	best estimator xgboost's best error=0.0616
[flaml.automl: 09-17 22:16:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:16:01] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0616,	best estimator xgboost's best error=0.0616
[flaml.automl: 09-17 22:16:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:16:03] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0616,	best estimator xgboost's best error=0.0616
[flaml.automl: 09-17 22:16:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:16:05] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0616,	best estimator xgboost's best error=0.0616
[flaml.automl: 09-17 22:16:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:16:06] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0616,	best estimator xgboost's best error=0.0616
[flaml.automl: 09-17 22:16:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:16:12] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0553,	best estimator xgboost's best error=0.0553
[flaml.automl: 09-17 22:16:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:16:24] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0538,	best estimator xgboost's best error=0.0538
[flaml.automl: 09-17 22:16:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:16:31] {3072} INFO -  at 52.8s,	estimator xgboost's best error=0.0538,	best estimator xgboost's best error=0.0538
[flaml.automl: 09-17 22:16:43] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 22:16:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:16:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:16:43] {2637} INFO - Time taken to find the best model: 46.288952112197876
[flaml.automl: 09-17 22:16:43] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9461932460848022
CO(0)最好结果：{'pred_time': 1.1448924146037493e-05, 'wall_clock_time': 46.288952112197876, 'metric_for_logging': {'pred_time': 1.1448924146037493e-05}, 'val_loss': 0.05380675391519781, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.030584335327148}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8274171511535109
CO(0)的mse=0.006695586698067798
CO(0)的mae=0.05462605941512326
CO(0)的mar=0.4465671391694689
总共花费的时间为：65.42
呼伦贝尔市
2192A
[flaml.automl: 09-17 22:19:54] {2390} INFO - task = regression
[flaml.automl: 09-17 22:19:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:19:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:19:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:19:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:19:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:19:55] {3025} INFO - Estimated sufficient time budget=11961s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 22:19:55] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1048,	best estimator xgboost's best error=0.1048
[flaml.automl: 09-17 22:19:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:19:57] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.0715,	best estimator xgboost's best error=0.0715
[flaml.automl: 09-17 22:19:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:19:58] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0715,	best estimator xgboost's best error=0.0715
[flaml.automl: 09-17 22:19:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:20:05] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.0715,	best estimator xgboost's best error=0.0715
[flaml.automl: 09-17 22:20:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:20:06] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-17 22:20:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:20:08] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-17 22:20:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:20:10] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.0445,	best estimator xgboost's best error=0.0445
[flaml.automl: 09-17 22:20:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:20:12] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.0445,	best estimator xgboost's best error=0.0445
[flaml.automl: 09-17 22:20:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:20:14] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0440,	best estimator xgboost's best error=0.0440
[flaml.automl: 09-17 22:20:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:20:16] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.0440,	best estimator xgboost's best error=0.0440
[flaml.automl: 09-17 22:20:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:20:18] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0421,	best estimator xgboost's best error=0.0421
[flaml.automl: 09-17 22:20:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:20:19] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.0421,	best estimator xgboost's best error=0.0421
[flaml.automl: 09-17 22:20:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:20:25] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-17 22:20:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:20:34] {3072} INFO -  at 39.9s,	estimator xgboost's best error=0.0366,	best estimator xgboost's best error=0.0366
[flaml.automl: 09-17 22:20:34] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:20:39] {3072} INFO -  at 45.0s,	estimator xgboost's best error=0.0366,	best estimator xgboost's best error=0.0366
[flaml.automl: 09-17 22:20:39] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 22:20:53] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0366,	best estimator xgboost's best error=0.0366
[flaml.automl: 09-17 22:21:02] {3335} INFO - retrain xgboost for 8.8s
[flaml.automl: 09-17 22:21:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 22:21:02] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:21:02] {2637} INFO - Time taken to find the best model: 39.899224519729614
CO(0)最佳参数：{'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}
CO(0)最佳损失：0.963403327512174
CO(0)最好结果：{'pred_time': 3.674750734052196e-05, 'wall_clock_time': 39.899224519729614, 'metric_for_logging': {'pred_time': 3.674750734052196e-05}, 'val_loss': 0.0365966724878261, 'training_iteration': 1, 'config': {'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}, 'config/n_estimators': 25, 'config/max_leaves': 6, 'config/min_child_weight': 2.978767810916454, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532448312341252, 'config/reg_alpha': 0.0015779591896894877, 'config/reg_lambda': 1.3500086298278042, 'experiment_tag': 'exp', 'time_total_s': 8.841864824295044}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7845860961546208
CO(0)的mse=0.004073533419360773
CO(0)的mae=0.040029062106589644
CO(0)的mar=0.09512098888017634
总共花费的时间为：68.13
巴彦淖尔市
2196A
[flaml.automl: 09-17 22:24:24] {2390} INFO - task = regression
[flaml.automl: 09-17 22:24:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:24:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:24:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:24:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:24:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:24:27] {3025} INFO - Estimated sufficient time budget=21466s. Estimated necessary time budget=21s.
[flaml.automl: 09-17 22:24:27] {3072} INFO -  at 2.2s,	estimator xgboost's best error=0.1017,	best estimator xgboost's best error=0.1017
[flaml.automl: 09-17 22:24:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:24:30] {3072} INFO -  at 5.6s,	estimator xgboost's best error=0.0764,	best estimator xgboost's best error=0.0764
[flaml.automl: 09-17 22:24:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:24:32] {3072} INFO -  at 7.8s,	estimator xgboost's best error=0.0764,	best estimator xgboost's best error=0.0764
[flaml.automl: 09-17 22:24:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:24:45] {3072} INFO -  at 20.7s,	estimator xgboost's best error=0.0764,	best estimator xgboost's best error=0.0764
[flaml.automl: 09-17 22:24:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:24:47] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0626,	best estimator xgboost's best error=0.0626
[flaml.automl: 09-17 22:24:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:24:50] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0573,	best estimator xgboost's best error=0.0573
[flaml.automl: 09-17 22:24:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:24:53] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.0573,	best estimator xgboost's best error=0.0573
[flaml.automl: 09-17 22:24:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:24:57] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.0573,	best estimator xgboost's best error=0.0573
[flaml.automl: 09-17 22:24:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:24:59] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.0573,	best estimator xgboost's best error=0.0573
[flaml.automl: 09-17 22:24:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:25:05] {3072} INFO -  at 40.6s,	estimator xgboost's best error=0.0573,	best estimator xgboost's best error=0.0573
[flaml.automl: 09-17 22:25:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:25:08] {3072} INFO -  at 43.8s,	estimator xgboost's best error=0.0570,	best estimator xgboost's best error=0.0570
[flaml.automl: 09-17 22:25:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:25:11] {3072} INFO -  at 47.0s,	estimator xgboost's best error=0.0570,	best estimator xgboost's best error=0.0570
[flaml.automl: 09-17 22:25:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:25:17] {3072} INFO -  at 53.0s,	estimator xgboost's best error=0.0514,	best estimator xgboost's best error=0.0514
[flaml.automl: 09-17 22:25:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:25:24] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-17 22:25:34] {3335} INFO - retrain xgboost for 10.2s
[flaml.automl: 09-17 22:25:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.6661475860669547, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=3.3357978394446626, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5562381480563631, scale_pos_weight=1,
             subsample=0.8794877198851965, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:25:34] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:25:34] {2637} INFO - Time taken to find the best model: 59.54729652404785
[flaml.automl: 09-17 22:25:34] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 17, 'max_leaves': 6, 'min_child_weight': 3.3357978394446626, 'learning_rate': 0.5408133424638543, 'subsample': 0.8794877198851965, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.6661475860669547, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5562381480563631}
CO(0)最佳损失：0.9505699514281952
CO(0)最好结果：{'pred_time': 4.41211765095339e-05, 'wall_clock_time': 59.54729652404785, 'metric_for_logging': {'pred_time': 4.41211765095339e-05}, 'val_loss': 0.049430048571804816, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 6, 'min_child_weight': 3.3357978394446626, 'learning_rate': 0.5408133424638543, 'subsample': 0.8794877198851965, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.6661475860669547, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5562381480563631}, 'config/n_estimators': 17, 'config/max_leaves': 6, 'config/min_child_weight': 3.3357978394446626, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8794877198851965, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.6661475860669547, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.5562381480563631, 'experiment_tag': 'exp', 'time_total_s': 6.592348337173462}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.6661475860669547, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=3.3357978394446626, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5562381480563631, scale_pos_weight=1,
             subsample=0.8794877198851965, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8033340440930483
CO(0)的mse=0.006108250466340434
CO(0)的mae=0.04818271475871222
CO(0)的mar=0.09138013614566771
总共花费的时间为：69.98
乌兰察布市
2197A
3285A
3421A
[flaml.automl: 09-17 22:34:39] {2390} INFO - task = regression
[flaml.automl: 09-17 22:34:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:34:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:34:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:34:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:34:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:34:42] {3025} INFO - Estimated sufficient time budget=22410s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 22:34:42] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1455,	best estimator xgboost's best error=0.1455
[flaml.automl: 09-17 22:34:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:34:45] {3072} INFO -  at 6.4s,	estimator xgboost's best error=0.0918,	best estimator xgboost's best error=0.0918
[flaml.automl: 09-17 22:34:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:34:48] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.0918,	best estimator xgboost's best error=0.0918
[flaml.automl: 09-17 22:34:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:35:04] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0918,	best estimator xgboost's best error=0.0918
[flaml.automl: 09-17 22:35:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:35:05] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0732,	best estimator xgboost's best error=0.0732
[flaml.automl: 09-17 22:35:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:35:06] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.0641,	best estimator xgboost's best error=0.0641
[flaml.automl: 09-17 22:35:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:35:08] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.0641,	best estimator xgboost's best error=0.0641
[flaml.automl: 09-17 22:35:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:35:10] {3072} INFO -  at 31.3s,	estimator xgboost's best error=0.0641,	best estimator xgboost's best error=0.0641
[flaml.automl: 09-17 22:35:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:35:12] {3072} INFO -  at 32.5s,	estimator xgboost's best error=0.0641,	best estimator xgboost's best error=0.0641
[flaml.automl: 09-17 22:35:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:35:14] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.0641,	best estimator xgboost's best error=0.0641
[flaml.automl: 09-17 22:35:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:35:15] {3072} INFO -  at 36.3s,	estimator xgboost's best error=0.0641,	best estimator xgboost's best error=0.0641
[flaml.automl: 09-17 22:35:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:35:17] {3072} INFO -  at 37.4s,	estimator xgboost's best error=0.0641,	best estimator xgboost's best error=0.0641
[flaml.automl: 09-17 22:35:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:35:23] {3072} INFO -  at 43.9s,	estimator xgboost's best error=0.0606,	best estimator xgboost's best error=0.0606
[flaml.automl: 09-17 22:35:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:35:35] {3072} INFO -  at 55.8s,	estimator xgboost's best error=0.0586,	best estimator xgboost's best error=0.0586
[flaml.automl: 09-17 22:35:47] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 22:35:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:35:47] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:35:47] {2637} INFO - Time taken to find the best model: 55.84928297996521
[flaml.automl: 09-17 22:35:47] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.941416941876069
CO(0)最好结果：{'pred_time': 1.2488991214815472e-05, 'wall_clock_time': 55.84928297996521, 'metric_for_logging': {'pred_time': 1.2488991214815472e-05}, 'val_loss': 0.058583058123931074, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.973731279373169}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8130722507847894
CO(0)的mse=0.008280044843302553
CO(0)的mae=0.059438545594979755
CO(0)的mar=0.1756560036830512
总共花费的时间为：68.46
阜新市
2207A
2208A
2209A
2210A
2211A
[flaml.automl: 09-17 22:52:09] {2390} INFO - task = regression
[flaml.automl: 09-17 22:52:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:52:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:52:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:52:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:52:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:52:11] {3025} INFO - Estimated sufficient time budget=121049s. Estimated necessary time budget=121s.
[flaml.automl: 09-17 22:52:11] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.1743,	best estimator xgboost's best error=0.1743
[flaml.automl: 09-17 22:52:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:52:15] {3072} INFO -  at 6.6s,	estimator xgboost's best error=0.1131,	best estimator xgboost's best error=0.1131
[flaml.automl: 09-17 22:52:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:52:18] {3072} INFO -  at 8.8s,	estimator xgboost's best error=0.1131,	best estimator xgboost's best error=0.1131
[flaml.automl: 09-17 22:52:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:52:21] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.1131,	best estimator xgboost's best error=0.1131
[flaml.automl: 09-17 22:52:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:52:23] {3072} INFO -  at 14.4s,	estimator xgboost's best error=0.0957,	best estimator xgboost's best error=0.0957
[flaml.automl: 09-17 22:52:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:52:26] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-17 22:52:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:52:29] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-17 22:52:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:52:32] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-17 22:52:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:52:34] {3072} INFO -  at 25.0s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-17 22:52:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:52:36] {3072} INFO -  at 27.3s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-17 22:52:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:52:38] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-17 22:52:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:52:39] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-17 22:52:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:52:46] {3072} INFO -  at 37.1s,	estimator xgboost's best error=0.0772,	best estimator xgboost's best error=0.0772
[flaml.automl: 09-17 22:52:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:52:58] {3072} INFO -  at 49.2s,	estimator xgboost's best error=0.0734,	best estimator xgboost's best error=0.0734
[flaml.automl: 09-17 22:53:10] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 22:53:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:53:10] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:53:10] {2637} INFO - Time taken to find the best model: 49.1649272441864
[flaml.automl: 09-17 22:53:10] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54365}
CO(0)最佳损失：0.9265919627942524
CO(0)最好结果：{'pred_time': 7.2341745923122336e-06, 'wall_clock_time': 49.1649272441864, 'metric_for_logging': {'pred_time': 7.2341745923122336e-06}, 'val_loss': 0.07340803720574765, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54365}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54365, 'experiment_tag': 'exp', 'time_total_s': 12.076791048049927}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8305361435132357
CO(0)的mse=0.01421248336053143
CO(0)的mae=0.0735276990086373
CO(0)的mar=0.1239881241637314
总共花费的时间为：62.19
辽阳市
2212A
2213A
2214A
2215A
[flaml.automl: 09-17 23:05:48] {2390} INFO - task = regression
[flaml.automl: 09-17 23:05:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:05:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:05:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:05:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:05:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:05:50] {3025} INFO - Estimated sufficient time budget=90870s. Estimated necessary time budget=91s.
[flaml.automl: 09-17 23:05:50] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.2765,	best estimator xgboost's best error=0.2765
[flaml.automl: 09-17 23:05:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:05:53] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.1769,	best estimator xgboost's best error=0.1769
[flaml.automl: 09-17 23:05:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:05:55] {3072} INFO -  at 7.6s,	estimator xgboost's best error=0.1769,	best estimator xgboost's best error=0.1769
[flaml.automl: 09-17 23:05:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:06:01] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.1769,	best estimator xgboost's best error=0.1769
[flaml.automl: 09-17 23:06:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:06:04] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.1602,	best estimator xgboost's best error=0.1602
[flaml.automl: 09-17 23:06:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:06:08] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.1419,	best estimator xgboost's best error=0.1419
[flaml.automl: 09-17 23:06:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:06:12] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.1419,	best estimator xgboost's best error=0.1419
[flaml.automl: 09-17 23:06:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:06:15] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.1419,	best estimator xgboost's best error=0.1419
[flaml.automl: 09-17 23:06:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:06:17] {3072} INFO -  at 29.7s,	estimator xgboost's best error=0.1419,	best estimator xgboost's best error=0.1419
[flaml.automl: 09-17 23:06:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:06:20] {3072} INFO -  at 32.5s,	estimator xgboost's best error=0.1419,	best estimator xgboost's best error=0.1419
[flaml.automl: 09-17 23:06:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:06:23] {3072} INFO -  at 35.2s,	estimator xgboost's best error=0.1409,	best estimator xgboost's best error=0.1409
[flaml.automl: 09-17 23:06:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:06:25] {3072} INFO -  at 37.4s,	estimator xgboost's best error=0.1409,	best estimator xgboost's best error=0.1409
[flaml.automl: 09-17 23:06:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:06:37] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.1367,	best estimator xgboost's best error=0.1367
[flaml.automl: 09-17 23:06:49] {3335} INFO - retrain xgboost for 12.3s
[flaml.automl: 09-17 23:06:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 23:06:49] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:06:49] {2637} INFO - Time taken to find the best model: 49.425292015075684
[flaml.automl: 09-17 23:06:49] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 43380}
CO(0)最佳损失：0.8633245672277582
CO(0)最好结果：{'pred_time': 1.9270709579205073e-05, 'wall_clock_time': 49.425292015075684, 'metric_for_logging': {'pred_time': 1.9270709579205073e-05}, 'val_loss': 0.13667543277224178, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 43380}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 43380, 'experiment_tag': 'exp', 'time_total_s': 12.006393909454346}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7283688744181566
CO(0)的mse=0.051066872189993846
CO(0)的mae=0.13441599890391626
CO(0)的mar=0.16003790967041426
总共花费的时间为：62.73
铁岭市
2216A
2217A
2218A
2219A
[flaml.automl: 09-17 23:20:12] {2390} INFO - task = regression
[flaml.automl: 09-17 23:20:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:20:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:20:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:20:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:20:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:20:13] {3025} INFO - Estimated sufficient time budget=52556s. Estimated necessary time budget=53s.
[flaml.automl: 09-17 23:20:13] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1833,	best estimator xgboost's best error=0.1833
[flaml.automl: 09-17 23:20:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:20:15] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1124,	best estimator xgboost's best error=0.1124
[flaml.automl: 09-17 23:20:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:20:16] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1124,	best estimator xgboost's best error=0.1124
[flaml.automl: 09-17 23:20:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:20:22] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.1124,	best estimator xgboost's best error=0.1124
[flaml.automl: 09-17 23:20:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:20:23] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-17 23:20:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:20:25] {3072} INFO -  at 13.4s,	estimator xgboost's best error=0.0789,	best estimator xgboost's best error=0.0789
[flaml.automl: 09-17 23:20:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:20:27] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0789,	best estimator xgboost's best error=0.0789
[flaml.automl: 09-17 23:20:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:20:29] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0789,	best estimator xgboost's best error=0.0789
[flaml.automl: 09-17 23:20:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:20:30] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.0789,	best estimator xgboost's best error=0.0789
[flaml.automl: 09-17 23:20:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:20:33] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0789,	best estimator xgboost's best error=0.0789
[flaml.automl: 09-17 23:20:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:20:34] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0785,	best estimator xgboost's best error=0.0785
[flaml.automl: 09-17 23:20:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:20:36] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0785,	best estimator xgboost's best error=0.0785
[flaml.automl: 09-17 23:20:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:20:42] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0743,	best estimator xgboost's best error=0.0743
[flaml.automl: 09-17 23:20:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:20:54] {3072} INFO -  at 42.6s,	estimator xgboost's best error=0.0700,	best estimator xgboost's best error=0.0700
[flaml.automl: 09-17 23:20:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 23:21:01] {3072} INFO -  at 49.1s,	estimator xgboost's best error=0.0700,	best estimator xgboost's best error=0.0700
[flaml.automl: 09-17 23:21:25] {3335} INFO - retrain xgboost for 24.3s
[flaml.automl: 09-17 23:21:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:21:25] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:21:25] {2637} INFO - Time taken to find the best model: 42.59073996543884
[flaml.automl: 09-17 23:21:25] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43399}
CO(0)最佳损失：0.9299673653036485
CO(0)最好结果：{'pred_time': 8.203863183072363e-06, 'wall_clock_time': 42.59073996543884, 'metric_for_logging': {'pred_time': 8.203863183072363e-06}, 'val_loss': 0.07003263469635142, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43399}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43399, 'experiment_tag': 'exp', 'time_total_s': 12.069681644439697}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8825155581287113
CO(0)的mse=0.01088365459900209
CO(0)的mae=0.06901971866231864
CO(0)的mar=0.17673415402159606
总共花费的时间为：74.25
朝阳市
2220A
2221A
2222A
2223A
[flaml.automl: 09-17 23:34:32] {2390} INFO - task = regression
[flaml.automl: 09-17 23:34:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:34:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:34:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:34:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:34:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:34:35] {3025} INFO - Estimated sufficient time budget=96576s. Estimated necessary time budget=97s.
[flaml.automl: 09-17 23:34:35] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.1971,	best estimator xgboost's best error=0.1971
[flaml.automl: 09-17 23:34:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:34:39] {3072} INFO -  at 6.5s,	estimator xgboost's best error=0.1225,	best estimator xgboost's best error=0.1225
[flaml.automl: 09-17 23:34:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:34:41] {3072} INFO -  at 8.7s,	estimator xgboost's best error=0.1225,	best estimator xgboost's best error=0.1225
[flaml.automl: 09-17 23:34:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:34:46] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.1225,	best estimator xgboost's best error=0.1225
[flaml.automl: 09-17 23:34:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:34:48] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.1024,	best estimator xgboost's best error=0.1024
[flaml.automl: 09-17 23:34:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:34:51] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0895,	best estimator xgboost's best error=0.0895
[flaml.automl: 09-17 23:34:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:34:54] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0895,	best estimator xgboost's best error=0.0895
[flaml.automl: 09-17 23:34:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:34:57] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.0895,	best estimator xgboost's best error=0.0895
[flaml.automl: 09-17 23:34:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:35:00] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.0895,	best estimator xgboost's best error=0.0895
[flaml.automl: 09-17 23:35:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:35:02] {3072} INFO -  at 30.1s,	estimator xgboost's best error=0.0895,	best estimator xgboost's best error=0.0895
[flaml.automl: 09-17 23:35:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:35:05] {3072} INFO -  at 33.1s,	estimator xgboost's best error=0.0881,	best estimator xgboost's best error=0.0881
[flaml.automl: 09-17 23:35:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:35:07] {3072} INFO -  at 35.2s,	estimator xgboost's best error=0.0881,	best estimator xgboost's best error=0.0881
[flaml.automl: 09-17 23:35:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:35:19] {3072} INFO -  at 47.1s,	estimator xgboost's best error=0.0829,	best estimator xgboost's best error=0.0829
[flaml.automl: 09-17 23:35:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:35:31] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0784,	best estimator xgboost's best error=0.0784
[flaml.automl: 09-17 23:35:43] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 23:35:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:35:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:35:43] {2637} INFO - Time taken to find the best model: 59.127636432647705
[flaml.automl: 09-17 23:35:43] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43288}
CO(0)最佳损失：0.9216383680946614
CO(0)最好结果：{'pred_time': 8.579450436788389e-06, 'wall_clock_time': 59.127636432647705, 'metric_for_logging': {'pred_time': 8.579450436788389e-06}, 'val_loss': 0.07836163190533861, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43288}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43288, 'experiment_tag': 'exp', 'time_total_s': 12.022629737854004}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.806931579638527
CO(0)的mse=0.01827875765127387
CO(0)的mae=0.08072746930051007
CO(0)的mar=0.10512471767735869
总共花费的时间为：72.07
四平市
2226A
3486A
3713A
[flaml.automl: 09-17 23:45:24] {2390} INFO - task = regression
[flaml.automl: 09-17 23:45:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:45:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:45:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:45:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:45:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:45:26] {3025} INFO - Estimated sufficient time budget=22624s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 23:45:26] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1584,	best estimator xgboost's best error=0.1584
[flaml.automl: 09-17 23:45:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:45:30] {3072} INFO -  at 6.4s,	estimator xgboost's best error=0.1120,	best estimator xgboost's best error=0.1120
[flaml.automl: 09-17 23:45:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:45:32] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.1120,	best estimator xgboost's best error=0.1120
[flaml.automl: 09-17 23:45:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:45:51] {3072} INFO -  at 27.3s,	estimator xgboost's best error=0.1120,	best estimator xgboost's best error=0.1120
[flaml.automl: 09-17 23:45:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:45:53] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.0980,	best estimator xgboost's best error=0.0980
[flaml.automl: 09-17 23:45:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:45:56] {3072} INFO -  at 32.4s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 23:45:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:45:59] {3072} INFO -  at 35.5s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 23:45:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:46:04] {3072} INFO -  at 40.1s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 23:46:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:46:06] {3072} INFO -  at 42.3s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 23:46:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:46:11] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 23:46:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:46:13] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 23:46:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:46:15] {3072} INFO -  at 51.6s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 23:46:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:46:23] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0838,	best estimator xgboost's best error=0.0838
[flaml.automl: 09-17 23:46:35] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-17 23:46:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 23:46:35] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:46:35] {2637} INFO - Time taken to find the best model: 59.16779685020447
[flaml.automl: 09-17 23:46:35] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9161658382207896
CO(0)最好结果：{'pred_time': 2.0472042716652762e-05, 'wall_clock_time': 59.16779685020447, 'metric_for_logging': {'pred_time': 2.0472042716652762e-05}, 'val_loss': 0.08383416177921046, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 7.596313953399658}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7526371944435077
CO(0)的mse=0.014783667477811357
CO(0)的mae=0.0794497485954093
CO(0)的mar=0.20676534518624354
总共花费的时间为：72.56
辽源市
2227A
[flaml.automl: 09-17 23:50:35] {2390} INFO - task = regression
[flaml.automl: 09-17 23:50:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:50:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:50:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:50:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:50:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:50:37] {3025} INFO - Estimated sufficient time budget=22578s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 23:50:37] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.1738,	best estimator xgboost's best error=0.1738
[flaml.automl: 09-17 23:50:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:50:40] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.1158,	best estimator xgboost's best error=0.1158
[flaml.automl: 09-17 23:50:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:50:43] {3072} INFO -  at 8.0s,	estimator xgboost's best error=0.1158,	best estimator xgboost's best error=0.1158
[flaml.automl: 09-17 23:50:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:50:56] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.1158,	best estimator xgboost's best error=0.1158
[flaml.automl: 09-17 23:50:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:50:58] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0873,	best estimator xgboost's best error=0.0873
[flaml.automl: 09-17 23:50:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:51:01] {3072} INFO -  at 26.2s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-17 23:51:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:51:04] {3072} INFO -  at 29.2s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-17 23:51:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:51:08] {3072} INFO -  at 33.1s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-17 23:51:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:51:10] {3072} INFO -  at 35.3s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-17 23:51:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:51:14] {3072} INFO -  at 39.8s,	estimator xgboost's best error=0.0733,	best estimator xgboost's best error=0.0733
[flaml.automl: 09-17 23:51:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:51:17] {3072} INFO -  at 42.7s,	estimator xgboost's best error=0.0733,	best estimator xgboost's best error=0.0733
[flaml.automl: 09-17 23:51:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:51:20] {3072} INFO -  at 45.2s,	estimator xgboost's best error=0.0733,	best estimator xgboost's best error=0.0733
[flaml.automl: 09-17 23:51:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:51:33] {3072} INFO -  at 58.6s,	estimator xgboost's best error=0.0733,	best estimator xgboost's best error=0.0733
[flaml.automl: 09-17 23:51:38] {3335} INFO - retrain xgboost for 4.5s
[flaml.automl: 09-17 23:51:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:51:38] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:51:38] {2637} INFO - Time taken to find the best model: 39.831660747528076
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}
CO(0)最佳损失：0.9266853397359018
CO(0)最好结果：{'pred_time': 6.193022265482108e-05, 'wall_clock_time': 39.831660747528076, 'metric_for_logging': {'pred_time': 6.193022265482108e-05}, 'val_loss': 0.07331466026409812, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 10, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846}, 'config/n_estimators': 4, 'config/max_leaves': 10, 'config/min_child_weight': 0.06372323163910634, 'config/learning_rate': 0.4086634770112844, 'config/subsample': 0.770412309752864, 'config/colsample_bylevel': 0.6720329282771347, 'config/colsample_bytree': 0.752099095598003, 'config/reg_alpha': 0.00271725862127768, 'config/reg_lambda': 3.9335607770117846, 'experiment_tag': 'exp', 'time_total_s': 4.497117519378662}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.6918233510995938
CO(0)的mse=0.015350072871009509
CO(0)的mae=0.07885559401393745
CO(0)的mar=0.10792734310265055
总共花费的时间为：63.53
通化市
2229A
2230A
[flaml.automl: 09-17 23:57:54] {2390} INFO - task = regression
[flaml.automl: 09-17 23:57:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:57:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:57:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:57:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:57:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:57:55] {3025} INFO - Estimated sufficient time budget=12207s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 23:57:55] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1930,	best estimator xgboost's best error=0.1930
[flaml.automl: 09-17 23:57:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:57:58] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1252,	best estimator xgboost's best error=0.1252
[flaml.automl: 09-17 23:57:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:57:59] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1252,	best estimator xgboost's best error=0.1252
[flaml.automl: 09-17 23:57:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:58:08] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.1252,	best estimator xgboost's best error=0.1252
[flaml.automl: 09-17 23:58:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:58:09] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.1096,	best estimator xgboost's best error=0.1096
[flaml.automl: 09-17 23:58:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:58:11] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0963,	best estimator xgboost's best error=0.0963
[flaml.automl: 09-17 23:58:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:58:13] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0963,	best estimator xgboost's best error=0.0963
[flaml.automl: 09-17 23:58:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:58:15] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0963,	best estimator xgboost's best error=0.0963
[flaml.automl: 09-17 23:58:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:58:16] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0963,	best estimator xgboost's best error=0.0963
[flaml.automl: 09-17 23:58:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:58:19] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.0963,	best estimator xgboost's best error=0.0963
[flaml.automl: 09-17 23:58:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:58:20] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0963,	best estimator xgboost's best error=0.0963
[flaml.automl: 09-17 23:58:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:58:21] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.0963,	best estimator xgboost's best error=0.0963
[flaml.automl: 09-17 23:58:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:58:27] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0930,	best estimator xgboost's best error=0.0930
[flaml.automl: 09-17 23:58:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:58:37] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 23:58:37] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 23:58:43] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 23:58:43] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 23:58:53] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-17 23:59:03] {3335} INFO - retrain xgboost for 10.2s
[flaml.automl: 09-17 23:59:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:59:03] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:59:03] {2637} INFO - Time taken to find the best model: 43.40827822685242
[flaml.automl: 09-17 23:59:03] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9111629173294716
CO(0)最好结果：{'pred_time': 1.6994920931278485e-05, 'wall_clock_time': 43.40827822685242, 'metric_for_logging': {'pred_time': 1.6994920931278485e-05}, 'val_loss': 0.08883708267052833, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.44278621673584}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7231265124602873
CO(0)的mse=0.029948849781723744
CO(0)的mae=0.09220913622109041
CO(0)的mar=0.12378836530345043
总共花费的时间为：69.60
白山市
2231A
2232A
[flaml.automl: 09-18 00:06:28] {2390} INFO - task = regression
[flaml.automl: 09-18 00:06:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:06:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:06:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:06:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:06:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:06:29] {3025} INFO - Estimated sufficient time budget=12185s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 00:06:29] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.2301,	best estimator xgboost's best error=0.2301
[flaml.automl: 09-18 00:06:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:06:31] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1245,	best estimator xgboost's best error=0.1245
[flaml.automl: 09-18 00:06:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:06:32] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.1245,	best estimator xgboost's best error=0.1245
[flaml.automl: 09-18 00:06:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:06:42] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.1245,	best estimator xgboost's best error=0.1245
[flaml.automl: 09-18 00:06:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:06:43] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.0948,	best estimator xgboost's best error=0.0948
[flaml.automl: 09-18 00:06:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:06:44] {3072} INFO -  at 16.7s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-18 00:06:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:06:46] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-18 00:06:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:06:48] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-18 00:06:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:06:50] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-18 00:06:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:06:52] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-18 00:06:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:06:53] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-18 00:06:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:06:54] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.0774,	best estimator xgboost's best error=0.0774
[flaml.automl: 09-18 00:06:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:07:00] {3072} INFO -  at 32.8s,	estimator xgboost's best error=0.0688,	best estimator xgboost's best error=0.0688
[flaml.automl: 09-18 00:07:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:07:11] {3072} INFO -  at 43.2s,	estimator xgboost's best error=0.0659,	best estimator xgboost's best error=0.0659
[flaml.automl: 09-18 00:07:11] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 00:07:17] {3072} INFO -  at 49.2s,	estimator xgboost's best error=0.0659,	best estimator xgboost's best error=0.0659
[flaml.automl: 09-18 00:07:17] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 00:07:27] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.0654,	best estimator xgboost's best error=0.0654
[flaml.automl: 09-18 00:07:45] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-18 00:07:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:07:45] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:07:45] {2637} INFO - Time taken to find the best model: 59.83509874343872
[flaml.automl: 09-18 00:07:45] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.9345809196810438
CO(0)最好结果：{'pred_time': 1.635595697332967e-05, 'wall_clock_time': 59.83509874343872, 'metric_for_logging': {'pred_time': 1.635595697332967e-05}, 'val_loss': 0.06541908031895613, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 10.612075567245483}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.893276598409914
CO(0)的mse=0.011455213436689706
CO(0)的mae=0.06692960792689406
CO(0)的mar=0.08338506250641528
总共花费的时间为：77.51
松原市
2233A
2234A
[flaml.automl: 09-18 00:14:14] {2390} INFO - task = regression
[flaml.automl: 09-18 00:14:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:14:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:14:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:14:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:14:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:14:15] {3025} INFO - Estimated sufficient time budget=12046s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 00:14:15] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1309,	best estimator xgboost's best error=0.1309
[flaml.automl: 09-18 00:14:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:14:17] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.0875,	best estimator xgboost's best error=0.0875
[flaml.automl: 09-18 00:14:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:14:18] {3072} INFO -  at 4.4s,	estimator xgboost's best error=0.0875,	best estimator xgboost's best error=0.0875
[flaml.automl: 09-18 00:14:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:14:28] {3072} INFO -  at 14.4s,	estimator xgboost's best error=0.0875,	best estimator xgboost's best error=0.0875
[flaml.automl: 09-18 00:14:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:14:30] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-18 00:14:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:14:33] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0558,	best estimator xgboost's best error=0.0558
[flaml.automl: 09-18 00:14:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:14:36] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0558,	best estimator xgboost's best error=0.0558
[flaml.automl: 09-18 00:14:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:14:40] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0558,	best estimator xgboost's best error=0.0558
[flaml.automl: 09-18 00:14:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:14:42] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0558,	best estimator xgboost's best error=0.0558
[flaml.automl: 09-18 00:14:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:14:46] {3072} INFO -  at 32.2s,	estimator xgboost's best error=0.0558,	best estimator xgboost's best error=0.0558
[flaml.automl: 09-18 00:14:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:14:48] {3072} INFO -  at 34.5s,	estimator xgboost's best error=0.0558,	best estimator xgboost's best error=0.0558
[flaml.automl: 09-18 00:14:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:14:50] {3072} INFO -  at 36.4s,	estimator xgboost's best error=0.0558,	best estimator xgboost's best error=0.0558
[flaml.automl: 09-18 00:14:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:15:01] {3072} INFO -  at 46.7s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-18 00:15:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:15:13] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0490,	best estimator xgboost's best error=0.0490
[flaml.automl: 09-18 00:15:29] {3335} INFO - retrain xgboost for 15.7s
[flaml.automl: 09-18 00:15:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:15:29] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:15:29] {2637} INFO - Time taken to find the best model: 59.1759135723114
[flaml.automl: 09-18 00:15:29] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.951030854284429
CO(0)最好结果：{'pred_time': 2.547149724030515e-05, 'wall_clock_time': 59.1759135723114, 'metric_for_logging': {'pred_time': 2.547149724030515e-05}, 'val_loss': 0.04896914571557094, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.46798825263977}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8765220539724234
CO(0)的mse=0.006038749918130383
CO(0)的mae=0.0487718581434361
CO(0)的mar=0.17223304547366486
总共花费的时间为：75.26
白城市
2235A
2236A
[flaml.automl: 09-18 00:21:41] {2390} INFO - task = regression
[flaml.automl: 09-18 00:21:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:21:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:21:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:21:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:21:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:21:42] {3025} INFO - Estimated sufficient time budget=11938s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 00:21:42] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1446,	best estimator xgboost's best error=0.1446
[flaml.automl: 09-18 00:21:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:21:44] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0821,	best estimator xgboost's best error=0.0821
[flaml.automl: 09-18 00:21:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:21:46] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.0821,	best estimator xgboost's best error=0.0821
[flaml.automl: 09-18 00:21:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:21:55] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0821,	best estimator xgboost's best error=0.0821
[flaml.automl: 09-18 00:21:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:21:56] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.0580,	best estimator xgboost's best error=0.0580
[flaml.automl: 09-18 00:21:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:21:58] {3072} INFO -  at 16.7s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-18 00:21:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:21:59] {3072} INFO -  at 18.3s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-18 00:21:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:22:02] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-18 00:22:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:22:03] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-18 00:22:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:22:05] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-18 00:22:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:22:07] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-18 00:22:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:22:08] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-18 00:22:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:22:14] {3072} INFO -  at 32.8s,	estimator xgboost's best error=0.0425,	best estimator xgboost's best error=0.0425
[flaml.automl: 09-18 00:22:14] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:22:24] {3072} INFO -  at 43.3s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-18 00:22:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 00:22:30] {3072} INFO -  at 49.3s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-18 00:22:30] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 00:22:40] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-18 00:22:51] {3335} INFO - retrain xgboost for 10.5s
[flaml.automl: 09-18 00:22:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:22:51] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:22:51] {2637} INFO - Time taken to find the best model: 43.25288152694702
[flaml.automl: 09-18 00:22:51] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9606672501251814
CO(0)最好结果：{'pred_time': 1.684091872873105e-05, 'wall_clock_time': 43.25288152694702, 'metric_for_logging': {'pred_time': 1.684091872873105e-05}, 'val_loss': 0.039332749874818586, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.468019008636475}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8902031125486469
CO(0)的mse=0.004037120944427013
CO(0)的mae=0.038548537528173395
CO(0)的mar=0.18667503470427235
总共花费的时间为：69.91
延边朝鲜族自治州
2237A
2238A
2239A
[flaml.automl: 09-18 00:32:07] {2390} INFO - task = regression
[flaml.automl: 09-18 00:32:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:32:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:32:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:32:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:32:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:32:08] {3025} INFO - Estimated sufficient time budget=12125s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 00:32:08] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.0882,	best estimator xgboost's best error=0.0882
[flaml.automl: 09-18 00:32:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:32:10] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0560,	best estimator xgboost's best error=0.0560
[flaml.automl: 09-18 00:32:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:32:11] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0560,	best estimator xgboost's best error=0.0560
[flaml.automl: 09-18 00:32:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:32:21] {3072} INFO -  at 14.6s,	estimator xgboost's best error=0.0560,	best estimator xgboost's best error=0.0560
[flaml.automl: 09-18 00:32:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:32:22] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.0471,	best estimator xgboost's best error=0.0471
[flaml.automl: 09-18 00:32:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:32:24] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0418,	best estimator xgboost's best error=0.0418
[flaml.automl: 09-18 00:32:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:32:26] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0418,	best estimator xgboost's best error=0.0418
[flaml.automl: 09-18 00:32:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:32:28] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0418,	best estimator xgboost's best error=0.0418
[flaml.automl: 09-18 00:32:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:32:29] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.0418,	best estimator xgboost's best error=0.0418
[flaml.automl: 09-18 00:32:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:32:32] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.0418,	best estimator xgboost's best error=0.0418
[flaml.automl: 09-18 00:32:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:32:33] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0418,	best estimator xgboost's best error=0.0418
[flaml.automl: 09-18 00:32:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:32:34] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.0418,	best estimator xgboost's best error=0.0418
[flaml.automl: 09-18 00:32:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:32:41] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.0386,	best estimator xgboost's best error=0.0386
[flaml.automl: 09-18 00:32:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:32:53] {3072} INFO -  at 46.2s,	estimator xgboost's best error=0.0373,	best estimator xgboost's best error=0.0373
[flaml.automl: 09-18 00:32:53] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 00:32:59] {3072} INFO -  at 52.8s,	estimator xgboost's best error=0.0373,	best estimator xgboost's best error=0.0373
[flaml.automl: 09-18 00:33:11] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 00:33:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:33:11] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:33:11] {2637} INFO - Time taken to find the best model: 46.22597694396973
[flaml.automl: 09-18 00:33:11] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9626962448970592
CO(0)最好结果：{'pred_time': 1.1148309855937014e-05, 'wall_clock_time': 46.22597694396973, 'metric_for_logging': {'pred_time': 1.1148309855937014e-05}, 'val_loss': 0.03730375510294086, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.11461091041565}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8639038652045173
CO(0)的mse=0.003341119101669397
CO(0)的mae=0.03475639484089446
CO(0)的mar=0.06138288146827102
总共花费的时间为：65.32
鸡西市
2240A
2243A
3707A
3709A
[flaml.automl: 09-18 00:45:56] {2390} INFO - task = regression
[flaml.automl: 09-18 00:45:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:45:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:45:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:45:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:45:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:45:58] {3025} INFO - Estimated sufficient time budget=51292s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 00:45:58] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1659,	best estimator xgboost's best error=0.1659
[flaml.automl: 09-18 00:45:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:46:00] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1053,	best estimator xgboost's best error=0.1053
[flaml.automl: 09-18 00:46:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:46:01] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1053,	best estimator xgboost's best error=0.1053
[flaml.automl: 09-18 00:46:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:46:07] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.1053,	best estimator xgboost's best error=0.1053
[flaml.automl: 09-18 00:46:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:46:08] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0863,	best estimator xgboost's best error=0.0863
[flaml.automl: 09-18 00:46:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:46:10] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0779,	best estimator xgboost's best error=0.0779
[flaml.automl: 09-18 00:46:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:46:12] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0779,	best estimator xgboost's best error=0.0779
[flaml.automl: 09-18 00:46:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:46:14] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0779,	best estimator xgboost's best error=0.0779
[flaml.automl: 09-18 00:46:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:46:15] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0779,	best estimator xgboost's best error=0.0779
[flaml.automl: 09-18 00:46:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:46:18] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0779,	best estimator xgboost's best error=0.0779
[flaml.automl: 09-18 00:46:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:46:19] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.0755,	best estimator xgboost's best error=0.0755
[flaml.automl: 09-18 00:46:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:46:21] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0755,	best estimator xgboost's best error=0.0755
[flaml.automl: 09-18 00:46:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:46:27] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.0691,	best estimator xgboost's best error=0.0691
[flaml.automl: 09-18 00:46:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:46:39] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.0660,	best estimator xgboost's best error=0.0660
[flaml.automl: 09-18 00:46:39] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 00:46:46] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.0660,	best estimator xgboost's best error=0.0660
[flaml.automl: 09-18 00:46:58] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 00:46:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:46:58] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:46:58] {2637} INFO - Time taken to find the best model: 42.95455551147461
[flaml.automl: 09-18 00:46:58] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42605}
CO(0)最佳损失：0.9339689999089765
CO(0)最好结果：{'pred_time': 8.481688070357676e-06, 'wall_clock_time': 42.95455551147461, 'metric_for_logging': {'pred_time': 8.481688070357676e-06}, 'val_loss': 0.06603100009102345, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42605}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42605, 'experiment_tag': 'exp', 'time_total_s': 12.064555168151855}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8235469402342626
CO(0)的mse=0.013541921814731501
CO(0)的mae=0.06215895036663506
CO(0)的mar=0.20962606668698305
总共花费的时间为：62.17
鹤岗市
2244A
2245A
2246A
2247A
[flaml.automl: 09-18 00:59:44] {2390} INFO - task = regression
[flaml.automl: 09-18 00:59:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:59:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:59:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:59:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:59:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:59:45] {3025} INFO - Estimated sufficient time budget=12133s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 00:59:45] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1247,	best estimator xgboost's best error=0.1247
[flaml.automl: 09-18 00:59:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:59:47] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0826,	best estimator xgboost's best error=0.0826
[flaml.automl: 09-18 00:59:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:59:49] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0826,	best estimator xgboost's best error=0.0826
[flaml.automl: 09-18 00:59:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:59:59] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0826,	best estimator xgboost's best error=0.0826
[flaml.automl: 09-18 00:59:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:00:00] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0712,	best estimator xgboost's best error=0.0712
[flaml.automl: 09-18 01:00:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:00:01] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0629,	best estimator xgboost's best error=0.0629
[flaml.automl: 09-18 01:00:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:00:03] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0629,	best estimator xgboost's best error=0.0629
[flaml.automl: 09-18 01:00:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:00:05] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0629,	best estimator xgboost's best error=0.0629
[flaml.automl: 09-18 01:00:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:00:07] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0629,	best estimator xgboost's best error=0.0629
[flaml.automl: 09-18 01:00:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:00:09] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0629,	best estimator xgboost's best error=0.0629
[flaml.automl: 09-18 01:00:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:00:10] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0629,	best estimator xgboost's best error=0.0629
[flaml.automl: 09-18 01:00:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:00:12] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0629,	best estimator xgboost's best error=0.0629
[flaml.automl: 09-18 01:00:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:00:18] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0570,	best estimator xgboost's best error=0.0570
[flaml.automl: 09-18 01:00:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:00:30] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.0559,	best estimator xgboost's best error=0.0559
[flaml.automl: 09-18 01:00:30] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:00:37] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.0559,	best estimator xgboost's best error=0.0559
[flaml.automl: 09-18 01:00:49] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 01:00:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:00:49] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:00:49] {2637} INFO - Time taken to find the best model: 46.35155010223389
[flaml.automl: 09-18 01:00:49] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9441096074218329
CO(0)最好结果：{'pred_time': 9.104367110873962e-06, 'wall_clock_time': 46.35155010223389, 'metric_for_logging': {'pred_time': 9.104367110873962e-06}, 'val_loss': 0.05589039257816705, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.077430009841919}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7742982750391872
CO(0)的mse=0.009541199801117981
CO(0)的mae=0.056112217116699996
CO(0)的mar=0.1196417688178897
总共花费的时间为：65.59
双鸭山市
2248A
2249A
2250A
2251A
[flaml.automl: 09-18 01:13:19] {2390} INFO - task = regression
[flaml.automl: 09-18 01:13:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:13:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:13:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:13:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:13:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:13:20] {3025} INFO - Estimated sufficient time budget=48593s. Estimated necessary time budget=49s.
[flaml.automl: 09-18 01:13:20] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1740,	best estimator xgboost's best error=0.1740
[flaml.automl: 09-18 01:13:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:13:22] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1232,	best estimator xgboost's best error=0.1232
[flaml.automl: 09-18 01:13:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:13:24] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1232,	best estimator xgboost's best error=0.1232
[flaml.automl: 09-18 01:13:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:13:30] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.1232,	best estimator xgboost's best error=0.1232
[flaml.automl: 09-18 01:13:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:13:31] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.1113,	best estimator xgboost's best error=0.1113
[flaml.automl: 09-18 01:13:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:13:33] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.0995,	best estimator xgboost's best error=0.0995
[flaml.automl: 09-18 01:13:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:13:34] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0995,	best estimator xgboost's best error=0.0995
[flaml.automl: 09-18 01:13:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:13:37] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.0995,	best estimator xgboost's best error=0.0995
[flaml.automl: 09-18 01:13:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:13:38] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0995,	best estimator xgboost's best error=0.0995
[flaml.automl: 09-18 01:13:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:13:40] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0995,	best estimator xgboost's best error=0.0995
[flaml.automl: 09-18 01:13:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:13:42] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0993,	best estimator xgboost's best error=0.0993
[flaml.automl: 09-18 01:13:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:13:43] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.0993,	best estimator xgboost's best error=0.0993
[flaml.automl: 09-18 01:13:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:13:50] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.0923,	best estimator xgboost's best error=0.0923
[flaml.automl: 09-18 01:13:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:14:02] {3072} INFO -  at 42.8s,	estimator xgboost's best error=0.0912,	best estimator xgboost's best error=0.0912
[flaml.automl: 09-18 01:14:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:14:08] {3072} INFO -  at 49.3s,	estimator xgboost's best error=0.0912,	best estimator xgboost's best error=0.0912
[flaml.automl: 09-18 01:14:20] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-18 01:14:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:14:20] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:14:20] {2637} INFO - Time taken to find the best model: 42.801748752593994
[flaml.automl: 09-18 01:14:20] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40779}
CO(0)最佳损失：0.9087789351720601
CO(0)最好结果：{'pred_time': 9.960118994442366e-06, 'wall_clock_time': 42.801748752593994, 'metric_for_logging': {'pred_time': 9.960118994442366e-06}, 'val_loss': 0.09122106482793994, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40779}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40779, 'experiment_tag': 'exp', 'time_total_s': 12.05319857597351}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.6276265904966299
CO(0)的mse=0.029052634018017796
CO(0)的mae=0.08844825585385657
CO(0)的mar=0.2870901357119007
总共花费的时间为：61.96
伊春市
2252A
2253A
2254A
3342A
3343A
3344A
3480A
[flaml.automl: 09-18 01:37:50] {2390} INFO - task = regression
[flaml.automl: 09-18 01:37:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:37:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:37:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:37:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:37:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:37:51] {3025} INFO - Estimated sufficient time budget=93496s. Estimated necessary time budget=93s.
[flaml.automl: 09-18 01:37:51] {3072} INFO -  at 1.7s,	estimator xgboost's best error=0.1416,	best estimator xgboost's best error=0.1416
[flaml.automl: 09-18 01:37:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:37:53] {3072} INFO -  at 3.8s,	estimator xgboost's best error=0.0868,	best estimator xgboost's best error=0.0868
[flaml.automl: 09-18 01:37:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:37:54] {3072} INFO -  at 5.0s,	estimator xgboost's best error=0.0868,	best estimator xgboost's best error=0.0868
[flaml.automl: 09-18 01:37:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:37:58] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.0868,	best estimator xgboost's best error=0.0868
[flaml.automl: 09-18 01:37:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:37:59] {3072} INFO -  at 9.4s,	estimator xgboost's best error=0.0752,	best estimator xgboost's best error=0.0752
[flaml.automl: 09-18 01:37:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:38:00] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-18 01:38:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:38:02] {3072} INFO -  at 12.6s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-18 01:38:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:38:04] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-18 01:38:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:38:05] {3072} INFO -  at 16.2s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-18 01:38:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:38:08] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-18 01:38:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:38:10] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-18 01:38:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:38:11] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-18 01:38:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:38:17] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.0583,	best estimator xgboost's best error=0.0583
[flaml.automl: 09-18 01:38:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:38:30] {3072} INFO -  at 40.2s,	estimator xgboost's best error=0.0548,	best estimator xgboost's best error=0.0548
[flaml.automl: 09-18 01:38:30] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:38:36] {3072} INFO -  at 46.8s,	estimator xgboost's best error=0.0548,	best estimator xgboost's best error=0.0548
[flaml.automl: 09-18 01:38:36] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 01:38:49] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.0548,	best estimator xgboost's best error=0.0548
[flaml.automl: 09-18 01:39:01] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 01:39:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:39:01] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:39:01] {2637} INFO - Time taken to find the best model: 40.2073118686676
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 72976}
CO(0)最佳损失：0.9451577742473385
CO(0)最好结果：{'pred_time': 5.1014637120894076e-06, 'wall_clock_time': 40.2073118686676, 'metric_for_logging': {'pred_time': 5.1014637120894076e-06}, 'val_loss': 0.05484222575266156, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 72976}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 72976, 'experiment_tag': 'exp', 'time_total_s': 12.073749542236328}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8622196971857823
CO(0)的mse=0.009377750492067892
CO(0)的mae=0.05441310617991875
CO(0)的mar=0.12491281289700402
总共花费的时间为：72.50
佳木斯市
2255A
2256A
2257A
2258A
2259A
[flaml.automl: 09-18 01:54:53] {2390} INFO - task = regression
[flaml.automl: 09-18 01:54:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:54:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:54:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:54:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:54:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:54:54] {3025} INFO - Estimated sufficient time budget=63367s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 01:54:54] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1174,	best estimator xgboost's best error=0.1174
[flaml.automl: 09-18 01:54:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:54:56] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0785,	best estimator xgboost's best error=0.0785
[flaml.automl: 09-18 01:54:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:54:57] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0785,	best estimator xgboost's best error=0.0785
[flaml.automl: 09-18 01:54:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:55:02] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0785,	best estimator xgboost's best error=0.0785
[flaml.automl: 09-18 01:55:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:55:03] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0656,	best estimator xgboost's best error=0.0656
[flaml.automl: 09-18 01:55:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:55:05] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0590,	best estimator xgboost's best error=0.0590
[flaml.automl: 09-18 01:55:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:55:06] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0590,	best estimator xgboost's best error=0.0590
[flaml.automl: 09-18 01:55:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:55:09] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0590,	best estimator xgboost's best error=0.0590
[flaml.automl: 09-18 01:55:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:55:10] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0590,	best estimator xgboost's best error=0.0590
[flaml.automl: 09-18 01:55:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:55:12] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0590,	best estimator xgboost's best error=0.0590
[flaml.automl: 09-18 01:55:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:55:14] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0586,	best estimator xgboost's best error=0.0586
[flaml.automl: 09-18 01:55:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:55:15] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0586,	best estimator xgboost's best error=0.0586
[flaml.automl: 09-18 01:55:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:55:22] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 01:55:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:55:34] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-18 01:55:34] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:55:40] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-18 01:55:52] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 01:55:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:55:52] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:55:52] {2637} INFO - Time taken to find the best model: 41.686893463134766
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52992}
CO(0)最佳损失：0.9477977885472023
CO(0)最好结果：{'pred_time': 6.875718139917908e-06, 'wall_clock_time': 41.686893463134766, 'metric_for_logging': {'pred_time': 6.875718139917908e-06}, 'val_loss': 0.052202211452797645, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52992}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52992, 'experiment_tag': 'exp', 'time_total_s': 12.130865573883057}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.809048743940564
CO(0)的mse=0.008339923127971343
CO(0)的mae=0.0527356686547829
CO(0)的mar=0.09646420815399653
总共花费的时间为：61.14
七台河市
2262A
3345A
3637A
3684A
[flaml.automl: 09-18 02:08:04] {2390} INFO - task = regression
[flaml.automl: 09-18 02:08:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:08:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:08:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:08:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:08:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:08:05] {3025} INFO - Estimated sufficient time budget=49584s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 02:08:05] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1363,	best estimator xgboost's best error=0.1363
[flaml.automl: 09-18 02:08:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:08:07] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0885,	best estimator xgboost's best error=0.0885
[flaml.automl: 09-18 02:08:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:08:08] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0885,	best estimator xgboost's best error=0.0885
[flaml.automl: 09-18 02:08:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:08:15] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0885,	best estimator xgboost's best error=0.0885
[flaml.automl: 09-18 02:08:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:08:16] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0764,	best estimator xgboost's best error=0.0764
[flaml.automl: 09-18 02:08:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:08:17] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0680,	best estimator xgboost's best error=0.0680
[flaml.automl: 09-18 02:08:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:08:19] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0680,	best estimator xgboost's best error=0.0680
[flaml.automl: 09-18 02:08:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:08:21] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0680,	best estimator xgboost's best error=0.0680
[flaml.automl: 09-18 02:08:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:08:23] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0680,	best estimator xgboost's best error=0.0680
[flaml.automl: 09-18 02:08:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:08:25] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0680,	best estimator xgboost's best error=0.0680
[flaml.automl: 09-18 02:08:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:08:27] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 02:08:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:08:28] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 02:08:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:08:34] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.0632,	best estimator xgboost's best error=0.0632
[flaml.automl: 09-18 02:08:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:08:47] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.0632,	best estimator xgboost's best error=0.0632
[flaml.automl: 09-18 02:08:47] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:08:51] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.0632,	best estimator xgboost's best error=0.0632
[flaml.automl: 09-18 02:08:51] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 02:09:02] {3072} INFO -  at 58.6s,	estimator xgboost's best error=0.0632,	best estimator xgboost's best error=0.0632
[flaml.automl: 09-18 02:09:10] {3335} INFO - retrain xgboost for 7.4s
[flaml.automl: 09-18 02:09:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 02:09:10] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:09:10] {2637} INFO - Time taken to find the best model: 30.93947672843933
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 41653}
CO(0)最佳损失：0.9368093198543492
CO(0)最好结果：{'pred_time': 9.151173606241309e-06, 'wall_clock_time': 30.93947672843933, 'metric_for_logging': {'pred_time': 9.151173606241309e-06}, 'val_loss': 0.06319068014565078, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 41653}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 41653, 'experiment_tag': 'exp', 'time_total_s': 6.525296211242676}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7930835718992065
CO(0)的mse=0.009262556558506382
CO(0)的mae=0.06130021269689292
CO(0)的mar=0.10167023252899185
总共花费的时间为：66.76
黑河市
2263A
2264A
2265A
[flaml.automl: 09-18 02:18:41] {2390} INFO - task = regression
[flaml.automl: 09-18 02:18:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:18:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:18:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:18:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:18:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:18:44] {3025} INFO - Estimated sufficient time budget=26532s. Estimated necessary time budget=27s.
[flaml.automl: 09-18 02:18:44] {3072} INFO -  at 2.9s,	estimator xgboost's best error=0.1228,	best estimator xgboost's best error=0.1228
[flaml.automl: 09-18 02:18:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:18:49] {3072} INFO -  at 7.8s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-18 02:18:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:18:51] {3072} INFO -  at 10.4s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-18 02:18:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:19:14] {3072} INFO -  at 32.6s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-18 02:19:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:19:16] {3072} INFO -  at 34.8s,	estimator xgboost's best error=0.0540,	best estimator xgboost's best error=0.0540
[flaml.automl: 09-18 02:19:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:19:19] {3072} INFO -  at 37.9s,	estimator xgboost's best error=0.0469,	best estimator xgboost's best error=0.0469
[flaml.automl: 09-18 02:19:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:19:22] {3072} INFO -  at 40.4s,	estimator xgboost's best error=0.0469,	best estimator xgboost's best error=0.0469
[flaml.automl: 09-18 02:19:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:19:24] {3072} INFO -  at 42.9s,	estimator xgboost's best error=0.0469,	best estimator xgboost's best error=0.0469
[flaml.automl: 09-18 02:19:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:19:25] {3072} INFO -  at 44.0s,	estimator xgboost's best error=0.0469,	best estimator xgboost's best error=0.0469
[flaml.automl: 09-18 02:19:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:19:28] {3072} INFO -  at 46.7s,	estimator xgboost's best error=0.0469,	best estimator xgboost's best error=0.0469
[flaml.automl: 09-18 02:19:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:19:29] {3072} INFO -  at 47.8s,	estimator xgboost's best error=0.0469,	best estimator xgboost's best error=0.0469
[flaml.automl: 09-18 02:19:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:19:30] {3072} INFO -  at 49.0s,	estimator xgboost's best error=0.0469,	best estimator xgboost's best error=0.0469
[flaml.automl: 09-18 02:19:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:19:37] {3072} INFO -  at 55.5s,	estimator xgboost's best error=0.0424,	best estimator xgboost's best error=0.0424
[flaml.automl: 09-18 02:19:43] {3335} INFO - retrain xgboost for 6.5s
[flaml.automl: 09-18 02:19:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 02:19:43] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:19:43] {2637} INFO - Time taken to find the best model: 55.48452353477478
[flaml.automl: 09-18 02:19:43] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9575567245298129
CO(0)最好结果：{'pred_time': 1.2192181055874142e-05, 'wall_clock_time': 55.48452353477478, 'metric_for_logging': {'pred_time': 1.2192181055874142e-05}, 'val_loss': 0.042443275470187135, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 6.493046522140503}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7731134208664757
CO(0)的mse=0.004699854022942114
CO(0)的mae=0.04067638459083973
CO(0)的mar=0.14998775375552292
总共花费的时间为：62.59
绥化市
2266A
2267A
[flaml.automl: 09-18 02:26:04] {2390} INFO - task = regression
[flaml.automl: 09-18 02:26:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:26:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:26:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:26:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:26:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:26:05] {3025} INFO - Estimated sufficient time budget=12089s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 02:26:05] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1376,	best estimator xgboost's best error=0.1376
[flaml.automl: 09-18 02:26:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:26:07] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0749,	best estimator xgboost's best error=0.0749
[flaml.automl: 09-18 02:26:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:26:09] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0749,	best estimator xgboost's best error=0.0749
[flaml.automl: 09-18 02:26:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:26:18] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0749,	best estimator xgboost's best error=0.0749
[flaml.automl: 09-18 02:26:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:26:19] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0534,	best estimator xgboost's best error=0.0534
[flaml.automl: 09-18 02:26:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:26:21] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0428,	best estimator xgboost's best error=0.0428
[flaml.automl: 09-18 02:26:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:26:22] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0428,	best estimator xgboost's best error=0.0428
[flaml.automl: 09-18 02:26:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:26:25] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0428,	best estimator xgboost's best error=0.0428
[flaml.automl: 09-18 02:26:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:26:26] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0428,	best estimator xgboost's best error=0.0428
[flaml.automl: 09-18 02:26:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:26:28] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.0428,	best estimator xgboost's best error=0.0428
[flaml.automl: 09-18 02:26:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:26:30] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0428,	best estimator xgboost's best error=0.0428
[flaml.automl: 09-18 02:26:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:26:31] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.0428,	best estimator xgboost's best error=0.0428
[flaml.automl: 09-18 02:26:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:26:37] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0389,	best estimator xgboost's best error=0.0389
[flaml.automl: 09-18 02:26:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:26:47] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-18 02:26:47] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:26:53] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-18 02:26:53] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 02:27:03] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0356,	best estimator xgboost's best error=0.0356
[flaml.automl: 09-18 02:27:30] {3335} INFO - retrain xgboost for 26.9s
[flaml.automl: 09-18 02:27:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:27:30] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:27:30] {2637} INFO - Time taken to find the best model: 59.19646763801575
[flaml.automl: 09-18 02:27:30] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.9644043408988465
CO(0)最好结果：{'pred_time': 1.6441536108785316e-05, 'wall_clock_time': 59.19646763801575, 'metric_for_logging': {'pred_time': 1.6441536108785316e-05}, 'val_loss': 0.03559565910115353, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.70603060722351}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9471829795134593
CO(0)的mse=0.002694518270152159
CO(0)的mae=0.03335711420541098
CO(0)的mar=0.08895151014824312
总共花费的时间为：86.56
大兴安岭地区
3663A
[flaml.automl: 09-18 02:30:40] {2390} INFO - task = regression
[flaml.automl: 09-18 02:30:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:30:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:30:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:30:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:30:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:30:42] {3025} INFO - Estimated sufficient time budget=11993s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 02:30:42] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1245,	best estimator xgboost's best error=0.1245
[flaml.automl: 09-18 02:30:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:30:43] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.0868,	best estimator xgboost's best error=0.0868
[flaml.automl: 09-18 02:30:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:30:45] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0868,	best estimator xgboost's best error=0.0868
[flaml.automl: 09-18 02:30:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:30:52] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.0868,	best estimator xgboost's best error=0.0868
[flaml.automl: 09-18 02:30:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:30:53] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0659,	best estimator xgboost's best error=0.0659
[flaml.automl: 09-18 02:30:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:30:54] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-18 02:30:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:30:56] {3072} INFO -  at 15.6s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-18 02:30:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:30:58] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-18 02:30:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:30:59] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-18 02:30:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:31:02] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-18 02:31:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:31:03] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-18 02:31:03] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:31:04] {3072} INFO -  at 23.7s,	estimator xgboost's best error=0.0579,	best estimator xgboost's best error=0.0579
[flaml.automl: 09-18 02:31:04] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:31:11] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.0549,	best estimator xgboost's best error=0.0549
[flaml.automl: 09-18 02:31:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:31:29] {3072} INFO -  at 49.0s,	estimator xgboost's best error=0.0462,	best estimator xgboost's best error=0.0462
[flaml.automl: 09-18 02:31:47] {3335} INFO - retrain xgboost for 18.1s
[flaml.automl: 09-18 02:31:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:31:47] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:31:47] {2637} INFO - Time taken to find the best model: 48.973291873931885
[flaml.automl: 09-18 02:31:47] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9537887452999577
CO(0)最好结果：{'pred_time': 6.789578100479325e-05, 'wall_clock_time': 48.973291873931885, 'metric_for_logging': {'pred_time': 6.789578100479325e-05}, 'val_loss': 0.04621125470004227, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 17, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 18.082399368286133}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8029286986880664
CO(0)的mse=0.00791724861027147
CO(0)的mae=0.04642155749294014
CO(0)的mar=0.22464205939748821
总共花费的时间为：67.34
蚌埠市
2270A
2271A
2274A
2275A
3715A
[flaml.automl: 09-18 02:46:43] {2390} INFO - task = regression
[flaml.automl: 09-18 02:46:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:46:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:46:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:46:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:46:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:46:45] {3025} INFO - Estimated sufficient time budget=120190s. Estimated necessary time budget=120s.
[flaml.automl: 09-18 02:46:45] {3072} INFO -  at 2.6s,	estimator xgboost's best error=0.1090,	best estimator xgboost's best error=0.1090
[flaml.automl: 09-18 02:46:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:46:49] {3072} INFO -  at 6.6s,	estimator xgboost's best error=0.0663,	best estimator xgboost's best error=0.0663
[flaml.automl: 09-18 02:46:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:46:51] {3072} INFO -  at 8.9s,	estimator xgboost's best error=0.0663,	best estimator xgboost's best error=0.0663
[flaml.automl: 09-18 02:46:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:46:55] {3072} INFO -  at 12.8s,	estimator xgboost's best error=0.0663,	best estimator xgboost's best error=0.0663
[flaml.automl: 09-18 02:46:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:46:57] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-18 02:46:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:47:00] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-18 02:47:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:47:03] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-18 02:47:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:47:05] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-18 02:47:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:47:07] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-18 02:47:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:47:10] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-18 02:47:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:47:13] {3072} INFO -  at 30.6s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-18 02:47:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:47:15] {3072} INFO -  at 32.6s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-18 02:47:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:47:26] {3072} INFO -  at 43.7s,	estimator xgboost's best error=0.0406,	best estimator xgboost's best error=0.0406
[flaml.automl: 09-18 02:47:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:47:42] {3072} INFO -  at 60.0s,	estimator xgboost's best error=0.0400,	best estimator xgboost's best error=0.0400
[flaml.automl: 09-18 02:47:59] {3335} INFO - retrain xgboost for 16.6s
[flaml.automl: 09-18 02:47:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:47:59] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:47:59] {2637} INFO - Time taken to find the best model: 59.956445932388306
[flaml.automl: 09-18 02:47:59] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 55193}
CO(0)最佳损失：0.9600321181357325
CO(0)最好结果：{'pred_time': 1.38107450949549e-05, 'wall_clock_time': 59.956445932388306, 'metric_for_logging': {'pred_time': 1.38107450949549e-05}, 'val_loss': 0.039967881864267515, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 55193}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 55193, 'experiment_tag': 'exp', 'time_total_s': 16.25033402442932}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8903184643155633
CO(0)的mse=0.003709810193615955
CO(0)的mae=0.04077807025521972
CO(0)的mar=0.08589568438292677
总共花费的时间为：77.43
淮南市
2278A
2279A
2280A
2281A
[flaml.automl: 09-18 03:00:36] {2390} INFO - task = regression
[flaml.automl: 09-18 03:00:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:00:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:00:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:00:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:00:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:00:38] {3025} INFO - Estimated sufficient time budget=49838s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 03:00:38] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1287,	best estimator xgboost's best error=0.1287
[flaml.automl: 09-18 03:00:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:00:40] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0775,	best estimator xgboost's best error=0.0775
[flaml.automl: 09-18 03:00:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:00:41] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0775,	best estimator xgboost's best error=0.0775
[flaml.automl: 09-18 03:00:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:00:47] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0775,	best estimator xgboost's best error=0.0775
[flaml.automl: 09-18 03:00:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:00:48] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0632,	best estimator xgboost's best error=0.0632
[flaml.automl: 09-18 03:00:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:00:50] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 03:00:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:00:52] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 03:00:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:00:54] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 03:00:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:00:55] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 03:00:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:00:58] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 03:00:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:00:59] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-18 03:00:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:01:01] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-18 03:01:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:01:07] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-18 03:01:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:01:19] {3072} INFO -  at 43.1s,	estimator xgboost's best error=0.0471,	best estimator xgboost's best error=0.0471
[flaml.automl: 09-18 03:01:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 03:01:26] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0471,	best estimator xgboost's best error=0.0471
[flaml.automl: 09-18 03:01:38] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 03:01:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:01:38] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:01:38] {2637} INFO - Time taken to find the best model: 43.120808839797974
[flaml.automl: 09-18 03:01:38] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41905}
CO(0)最佳损失：0.9529091838694755
CO(0)最好结果：{'pred_time': 8.475404198343262e-06, 'wall_clock_time': 43.120808839797974, 'metric_for_logging': {'pred_time': 8.475404198343262e-06}, 'val_loss': 0.047090816130524474, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41905}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41905, 'experiment_tag': 'exp', 'time_total_s': 12.110584735870361}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9007373767082667
CO(0)的mse=0.004557673758218418
CO(0)的mae=0.04419838561090578
CO(0)的mar=0.114126867529805
总共花费的时间为：62.32
淮北市
2282A
2283A
2284A
3330A
[flaml.automl: 09-18 03:14:33] {2390} INFO - task = regression
[flaml.automl: 09-18 03:14:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:14:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:14:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:14:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:14:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:14:34] {3025} INFO - Estimated sufficient time budget=52210s. Estimated necessary time budget=52s.
[flaml.automl: 09-18 03:14:34] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1342,	best estimator xgboost's best error=0.1342
[flaml.automl: 09-18 03:14:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:14:36] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0819,	best estimator xgboost's best error=0.0819
[flaml.automl: 09-18 03:14:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:14:37] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0819,	best estimator xgboost's best error=0.0819
[flaml.automl: 09-18 03:14:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:14:43] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.0819,	best estimator xgboost's best error=0.0819
[flaml.automl: 09-18 03:14:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:14:44] {3072} INFO -  at 11.7s,	estimator xgboost's best error=0.0658,	best estimator xgboost's best error=0.0658
[flaml.automl: 09-18 03:14:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:14:46] {3072} INFO -  at 13.3s,	estimator xgboost's best error=0.0566,	best estimator xgboost's best error=0.0566
[flaml.automl: 09-18 03:14:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:14:47] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0566,	best estimator xgboost's best error=0.0566
[flaml.automl: 09-18 03:14:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:14:50] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0566,	best estimator xgboost's best error=0.0566
[flaml.automl: 09-18 03:14:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:14:51] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0566,	best estimator xgboost's best error=0.0566
[flaml.automl: 09-18 03:14:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:14:54] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0566,	best estimator xgboost's best error=0.0566
[flaml.automl: 09-18 03:14:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:14:55] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-18 03:14:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:14:56] {3072} INFO -  at 23.9s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-18 03:14:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:15:04] {3072} INFO -  at 31.5s,	estimator xgboost's best error=0.0519,	best estimator xgboost's best error=0.0519
[flaml.automl: 09-18 03:15:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:15:30] {3072} INFO -  at 57.4s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 03:15:58] {3335} INFO - retrain xgboost for 27.7s
[flaml.automl: 09-18 03:15:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:15:58] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:15:58] {2637} INFO - Time taken to find the best model: 57.36532020568848
[flaml.automl: 09-18 03:15:58] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43445}
CO(0)最佳损失：0.9497918728997664
CO(0)最好结果：{'pred_time': 2.0750613872335288e-05, 'wall_clock_time': 57.36532020568848, 'metric_for_logging': {'pred_time': 2.0750613872335288e-05}, 'val_loss': 0.050208127100233604, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43445}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43445, 'experiment_tag': 'exp', 'time_total_s': 25.814406394958496}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8610630442083052
CO(0)的mse=0.005721419956049718
CO(0)的mae=0.049670111063120526
CO(0)的mar=0.07908103591555322
总共花费的时间为：85.88
铜陵市
2285A
2286A
2287A
2288A
2289A
2290A
[flaml.automl: 09-18 03:35:16] {2390} INFO - task = regression
[flaml.automl: 09-18 03:35:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:35:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:35:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:35:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:35:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:35:18] {3025} INFO - Estimated sufficient time budget=76631s. Estimated necessary time budget=77s.
[flaml.automl: 09-18 03:35:18] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1933,	best estimator xgboost's best error=0.1933
[flaml.automl: 09-18 03:35:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:35:20] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1222,	best estimator xgboost's best error=0.1222
[flaml.automl: 09-18 03:35:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:35:21] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1222,	best estimator xgboost's best error=0.1222
[flaml.automl: 09-18 03:35:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:35:25] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.1222,	best estimator xgboost's best error=0.1222
[flaml.automl: 09-18 03:35:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:35:26] {3072} INFO -  at 9.8s,	estimator xgboost's best error=0.1024,	best estimator xgboost's best error=0.1024
[flaml.automl: 09-18 03:35:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:35:27] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.0931,	best estimator xgboost's best error=0.0931
[flaml.automl: 09-18 03:35:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:35:29] {3072} INFO -  at 13.0s,	estimator xgboost's best error=0.0931,	best estimator xgboost's best error=0.0931
[flaml.automl: 09-18 03:35:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:35:32] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0931,	best estimator xgboost's best error=0.0931
[flaml.automl: 09-18 03:35:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:35:33] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.0931,	best estimator xgboost's best error=0.0931
[flaml.automl: 09-18 03:35:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:35:35] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0931,	best estimator xgboost's best error=0.0931
[flaml.automl: 09-18 03:35:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:35:37] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0928,	best estimator xgboost's best error=0.0928
[flaml.automl: 09-18 03:35:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:35:38] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0928,	best estimator xgboost's best error=0.0928
[flaml.automl: 09-18 03:35:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:35:45] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.0889,	best estimator xgboost's best error=0.0889
[flaml.automl: 09-18 03:35:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:35:57] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.0874,	best estimator xgboost's best error=0.0874
[flaml.automl: 09-18 03:35:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 03:36:03] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.0874,	best estimator xgboost's best error=0.0874
[flaml.automl: 09-18 03:36:03] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 03:36:16] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.0874,	best estimator xgboost's best error=0.0874
[flaml.automl: 09-18 03:36:28] {3335} INFO - retrain xgboost for 12.2s
[flaml.automl: 09-18 03:36:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:36:28] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:36:28] {2637} INFO - Time taken to find the best model: 40.73959565162659
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64077}
CO(0)最佳损失：0.9125636135977222
CO(0)最好结果：{'pred_time': 5.774685506070598e-06, 'wall_clock_time': 40.73959565162659, 'metric_for_logging': {'pred_time': 5.774685506070598e-06}, 'val_loss': 0.0874363864022778, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 64077}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 64077, 'experiment_tag': 'exp', 'time_total_s': 12.126597881317139}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7330791432371885
CO(0)的mse=0.021140677736207432
CO(0)的mae=0.08738336959834979
CO(0)的mar=0.12768932769642533
总共花费的时间为：73.00
安庆市
2291A
2292A
3173A
[flaml.automl: 09-18 03:46:09] {2390} INFO - task = regression
[flaml.automl: 09-18 03:46:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:46:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:46:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:46:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:46:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:46:10] {3025} INFO - Estimated sufficient time budget=12181s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 03:46:10] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1475,	best estimator xgboost's best error=0.1475
[flaml.automl: 09-18 03:46:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:46:13] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0821,	best estimator xgboost's best error=0.0821
[flaml.automl: 09-18 03:46:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:46:14] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0821,	best estimator xgboost's best error=0.0821
[flaml.automl: 09-18 03:46:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:46:24] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0821,	best estimator xgboost's best error=0.0821
[flaml.automl: 09-18 03:46:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:46:25] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0619,	best estimator xgboost's best error=0.0619
[flaml.automl: 09-18 03:46:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:46:26] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0512,	best estimator xgboost's best error=0.0512
[flaml.automl: 09-18 03:46:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:46:28] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0512,	best estimator xgboost's best error=0.0512
[flaml.automl: 09-18 03:46:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:46:31] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0512,	best estimator xgboost's best error=0.0512
[flaml.automl: 09-18 03:46:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:46:32] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0512,	best estimator xgboost's best error=0.0512
[flaml.automl: 09-18 03:46:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:46:34] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0512,	best estimator xgboost's best error=0.0512
[flaml.automl: 09-18 03:46:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:46:36] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0512,	best estimator xgboost's best error=0.0512
[flaml.automl: 09-18 03:46:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:46:37] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.0512,	best estimator xgboost's best error=0.0512
[flaml.automl: 09-18 03:46:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:46:43] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0458,	best estimator xgboost's best error=0.0458
[flaml.automl: 09-18 03:46:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:46:55] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0441,	best estimator xgboost's best error=0.0441
[flaml.automl: 09-18 03:46:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 03:47:02] {3072} INFO -  at 52.7s,	estimator xgboost's best error=0.0441,	best estimator xgboost's best error=0.0441
[flaml.automl: 09-18 03:47:14] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-18 03:47:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:47:14] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:47:14] {2637} INFO - Time taken to find the best model: 46.29739761352539
[flaml.automl: 09-18 03:47:14] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9558670830202234
CO(0)最好结果：{'pred_time': 1.0909994825864265e-05, 'wall_clock_time': 46.29739761352539, 'metric_for_logging': {'pred_time': 1.0909994825864265e-05}, 'val_loss': 0.044132916979776615, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.06526231765747}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9035906978697347
CO(0)的mse=0.004456548111384555
CO(0)的mae=0.04306243576354749
CO(0)的mar=0.14339036509122452
总共花费的时间为：65.32
黄山市
2295A
2296A
2297A
[flaml.automl: 09-18 03:56:40] {2390} INFO - task = regression
[flaml.automl: 09-18 03:56:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:56:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:56:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:56:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:56:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:56:41] {3025} INFO - Estimated sufficient time budget=12322s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 03:56:41] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1069,	best estimator xgboost's best error=0.1069
[flaml.automl: 09-18 03:56:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:56:43] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0601,	best estimator xgboost's best error=0.0601
[flaml.automl: 09-18 03:56:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:56:45] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0601,	best estimator xgboost's best error=0.0601
[flaml.automl: 09-18 03:56:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:56:55] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0601,	best estimator xgboost's best error=0.0601
[flaml.automl: 09-18 03:56:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:56:56] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 03:56:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:56:57] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-18 03:56:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:56:59] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-18 03:56:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:57:02] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-18 03:57:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:57:03] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-18 03:57:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:57:05] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-18 03:57:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:57:07] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-18 03:57:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:57:08] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-18 03:57:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:57:14] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0301,	best estimator xgboost's best error=0.0301
[flaml.automl: 09-18 03:57:14] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:57:26] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.0293,	best estimator xgboost's best error=0.0293
[flaml.automl: 09-18 03:57:26] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 03:57:33] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.0293,	best estimator xgboost's best error=0.0293
[flaml.automl: 09-18 03:57:45] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 03:57:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:57:45] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:57:45] {2637} INFO - Time taken to find the best model: 46.371859550476074
[flaml.automl: 09-18 03:57:45] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9707341123659464
CO(0)最好结果：{'pred_time': 1.1418549294184141e-05, 'wall_clock_time': 46.371859550476074, 'metric_for_logging': {'pred_time': 1.1418549294184141e-05}, 'val_loss': 0.029265887634053567, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.073343992233276}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9529834866610799
CO(0)的mse=0.0017664576496992362
CO(0)的mae=0.02864088382133025
CO(0)的mar=0.07270889789967333
总共花费的时间为：65.56
滁州市
2298A
2299A
3331A
[flaml.automl: 09-18 04:07:33] {2390} INFO - task = regression
[flaml.automl: 09-18 04:07:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:07:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:07:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:07:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:07:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:07:36] {3025} INFO - Estimated sufficient time budget=27163s. Estimated necessary time budget=27s.
[flaml.automl: 09-18 04:07:36] {3072} INFO -  at 2.9s,	estimator xgboost's best error=0.1392,	best estimator xgboost's best error=0.1392
[flaml.automl: 09-18 04:07:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:07:41] {3072} INFO -  at 8.0s,	estimator xgboost's best error=0.0832,	best estimator xgboost's best error=0.0832
[flaml.automl: 09-18 04:07:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:07:44] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.0832,	best estimator xgboost's best error=0.0832
[flaml.automl: 09-18 04:07:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:08:05] {3072} INFO -  at 32.4s,	estimator xgboost's best error=0.0832,	best estimator xgboost's best error=0.0832
[flaml.automl: 09-18 04:08:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:08:08] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.0670,	best estimator xgboost's best error=0.0670
[flaml.automl: 09-18 04:08:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:08:12] {3072} INFO -  at 38.6s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-18 04:08:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:08:15] {3072} INFO -  at 42.2s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-18 04:08:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:08:19] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-18 04:08:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:08:21] {3072} INFO -  at 47.9s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-18 04:08:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:08:24] {3072} INFO -  at 50.6s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-18 04:08:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:08:25] {3072} INFO -  at 51.7s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-18 04:08:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:08:26] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-18 04:08:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:08:33] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.0505,	best estimator xgboost's best error=0.0505
[flaml.automl: 09-18 04:08:45] {3335} INFO - retrain xgboost for 11.7s
[flaml.automl: 09-18 04:08:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 04:08:45] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:08:45] {2637} INFO - Time taken to find the best model: 59.8554801940918
[flaml.automl: 09-18 04:08:45] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9495240585812631
CO(0)最好结果：{'pred_time': 1.3320904059950353e-05, 'wall_clock_time': 59.8554801940918, 'metric_for_logging': {'pred_time': 1.3320904059950353e-05}, 'val_loss': 0.050475941418736896, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 6.944796085357666}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.886457129189485
CO(0)的mse=0.006296147766840581
CO(0)的mae=0.05242209100448733
CO(0)的mar=0.10130064138162705
总共花费的时间为：72.27
阜阳市
2301A
2875A
3468A
3469A
[flaml.automl: 09-18 04:21:35] {2390} INFO - task = regression
[flaml.automl: 09-18 04:21:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:21:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:21:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:21:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:21:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:21:36] {3025} INFO - Estimated sufficient time budget=52918s. Estimated necessary time budget=53s.
[flaml.automl: 09-18 04:21:36] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1166,	best estimator xgboost's best error=0.1166
[flaml.automl: 09-18 04:21:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:21:39] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0721,	best estimator xgboost's best error=0.0721
[flaml.automl: 09-18 04:21:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:21:40] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0721,	best estimator xgboost's best error=0.0721
[flaml.automl: 09-18 04:21:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:21:46] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.0721,	best estimator xgboost's best error=0.0721
[flaml.automl: 09-18 04:21:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:21:47] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.0583,	best estimator xgboost's best error=0.0583
[flaml.automl: 09-18 04:21:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:21:48] {3072} INFO -  at 13.4s,	estimator xgboost's best error=0.0515,	best estimator xgboost's best error=0.0515
[flaml.automl: 09-18 04:21:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:21:50] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0515,	best estimator xgboost's best error=0.0515
[flaml.automl: 09-18 04:21:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:21:52] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0515,	best estimator xgboost's best error=0.0515
[flaml.automl: 09-18 04:21:52] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:21:53] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.0515,	best estimator xgboost's best error=0.0515
[flaml.automl: 09-18 04:21:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:21:56] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.0515,	best estimator xgboost's best error=0.0515
[flaml.automl: 09-18 04:21:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:21:58] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0511,	best estimator xgboost's best error=0.0511
[flaml.automl: 09-18 04:21:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:21:59] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0511,	best estimator xgboost's best error=0.0511
[flaml.automl: 09-18 04:21:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:22:05] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 04:22:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:22:17] {3072} INFO -  at 42.5s,	estimator xgboost's best error=0.0450,	best estimator xgboost's best error=0.0450
[flaml.automl: 09-18 04:22:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 04:22:24] {3072} INFO -  at 49.0s,	estimator xgboost's best error=0.0450,	best estimator xgboost's best error=0.0450
[flaml.automl: 09-18 04:22:36] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 04:22:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:22:36] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:22:36] {2637} INFO - Time taken to find the best model: 42.481810331344604
[flaml.automl: 09-18 04:22:36] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43659}
CO(0)最佳损失：0.9549771856361015
CO(0)最好结果：{'pred_time': 8.268077160692343e-06, 'wall_clock_time': 42.481810331344604, 'metric_for_logging': {'pred_time': 8.268077160692343e-06}, 'val_loss': 0.04502281436389855, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43659}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43659, 'experiment_tag': 'exp', 'time_total_s': 11.974249362945557}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8616449466794773
CO(0)的mse=0.005019775167022023
CO(0)的mae=0.0439666639703103
CO(0)的mar=0.07491110504234831
总共花费的时间为：61.81
宿州市
3463A
3634A
3701A
[flaml.automl: 09-18 04:32:58] {2390} INFO - task = regression
[flaml.automl: 09-18 04:32:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:32:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:32:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:32:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:32:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:33:00] {3025} INFO - Estimated sufficient time budget=20967s. Estimated necessary time budget=21s.
[flaml.automl: 09-18 04:33:00] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.1043,	best estimator xgboost's best error=0.1043
[flaml.automl: 09-18 04:33:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:33:03] {3072} INFO -  at 5.9s,	estimator xgboost's best error=0.0667,	best estimator xgboost's best error=0.0667
[flaml.automl: 09-18 04:33:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:33:06] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.0667,	best estimator xgboost's best error=0.0667
[flaml.automl: 09-18 04:33:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:33:22] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0667,	best estimator xgboost's best error=0.0667
[flaml.automl: 09-18 04:33:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:33:24] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.0555,	best estimator xgboost's best error=0.0555
[flaml.automl: 09-18 04:33:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:33:26] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 04:33:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:33:29] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 04:33:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:33:33] {3072} INFO -  at 35.7s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 04:33:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:33:35] {3072} INFO -  at 37.7s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 04:33:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:33:40] {3072} INFO -  at 42.4s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 04:33:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:33:42] {3072} INFO -  at 44.5s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 04:33:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:33:44] {3072} INFO -  at 46.5s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 04:33:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:33:55] {3072} INFO -  at 57.2s,	estimator xgboost's best error=0.0433,	best estimator xgboost's best error=0.0433
[flaml.automl: 09-18 04:34:03] {3335} INFO - retrain xgboost for 8.0s
[flaml.automl: 09-18 04:34:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 04:34:03] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:34:03] {2637} INFO - Time taken to find the best model: 57.21639037132263
[flaml.automl: 09-18 04:34:03] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9567275907975062
CO(0)最好结果：{'pred_time': 1.9219262631890682e-05, 'wall_clock_time': 57.21639037132263, 'metric_for_logging': {'pred_time': 1.9219262631890682e-05}, 'val_loss': 0.043272409202493804, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 10.669546127319336}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8591083289579057
CO(0)的mse=0.004475600286352468
CO(0)的mae=0.04494057421940636
CO(0)的mar=0.0953300058563349
总共花费的时间为：65.86
六安市
2307A
2308A
2309A
2310A
[flaml.automl: 09-18 04:47:03] {2390} INFO - task = regression
[flaml.automl: 09-18 04:47:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:47:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:47:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:47:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:47:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:47:04] {3025} INFO - Estimated sufficient time budget=53203s. Estimated necessary time budget=53s.
[flaml.automl: 09-18 04:47:04] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1260,	best estimator xgboost's best error=0.1260
[flaml.automl: 09-18 04:47:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:47:06] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0704,	best estimator xgboost's best error=0.0704
[flaml.automl: 09-18 04:47:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:47:07] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0704,	best estimator xgboost's best error=0.0704
[flaml.automl: 09-18 04:47:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:47:13] {3072} INFO -  at 10.5s,	estimator xgboost's best error=0.0704,	best estimator xgboost's best error=0.0704
[flaml.automl: 09-18 04:47:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:47:14] {3072} INFO -  at 11.7s,	estimator xgboost's best error=0.0510,	best estimator xgboost's best error=0.0510
[flaml.automl: 09-18 04:47:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:47:16] {3072} INFO -  at 13.3s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-18 04:47:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:47:17] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-18 04:47:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:47:20] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-18 04:47:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:47:21] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-18 04:47:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:47:24] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-18 04:47:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:47:25] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0405,	best estimator xgboost's best error=0.0405
[flaml.automl: 09-18 04:47:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:47:27] {3072} INFO -  at 23.9s,	estimator xgboost's best error=0.0405,	best estimator xgboost's best error=0.0405
[flaml.automl: 09-18 04:47:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:47:33] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0364,	best estimator xgboost's best error=0.0364
[flaml.automl: 09-18 04:47:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:47:45] {3072} INFO -  at 42.6s,	estimator xgboost's best error=0.0339,	best estimator xgboost's best error=0.0339
[flaml.automl: 09-18 04:47:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 04:47:52] {3072} INFO -  at 49.1s,	estimator xgboost's best error=0.0339,	best estimator xgboost's best error=0.0339
[flaml.automl: 09-18 04:48:04] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 04:48:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:48:04] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:48:04] {2637} INFO - Time taken to find the best model: 42.57412910461426
[flaml.automl: 09-18 04:48:04] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 44287}
CO(0)最佳损失：0.9660831656064524
CO(0)最好结果：{'pred_time': 8.060592530437951e-06, 'wall_clock_time': 42.57412910461426, 'metric_for_logging': {'pred_time': 8.060592530437951e-06}, 'val_loss': 0.0339168343935476, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 44287}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 44287, 'experiment_tag': 'exp', 'time_total_s': 12.101658821105957}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9465868350301451
CO(0)的mse=0.002476735452392652
CO(0)的mae=0.03372793012008278
CO(0)的mar=0.07091807363891046
总共花费的时间为：61.97
亳州市
2311A
2312A
3332A
[flaml.automl: 09-18 04:57:20] {2390} INFO - task = regression
[flaml.automl: 09-18 04:57:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:57:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:57:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:57:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:57:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:57:21] {3025} INFO - Estimated sufficient time budget=11917s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 04:57:21] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1260,	best estimator xgboost's best error=0.1260
[flaml.automl: 09-18 04:57:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:57:23] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0738,	best estimator xgboost's best error=0.0738
[flaml.automl: 09-18 04:57:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:57:24] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0738,	best estimator xgboost's best error=0.0738
[flaml.automl: 09-18 04:57:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:57:34] {3072} INFO -  at 14.6s,	estimator xgboost's best error=0.0738,	best estimator xgboost's best error=0.0738
[flaml.automl: 09-18 04:57:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:57:36] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.0583,	best estimator xgboost's best error=0.0583
[flaml.automl: 09-18 04:57:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:57:37] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 04:57:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:57:39] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 04:57:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:57:41] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 04:57:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:57:42] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 04:57:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:57:47] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 04:57:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:57:49] {3072} INFO -  at 29.2s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 04:57:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:57:51] {3072} INFO -  at 31.2s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 04:57:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:58:02] {3072} INFO -  at 42.3s,	estimator xgboost's best error=0.0407,	best estimator xgboost's best error=0.0407
[flaml.automl: 09-18 04:58:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:58:19] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.0392,	best estimator xgboost's best error=0.0392
[flaml.automl: 09-18 04:58:40] {3335} INFO - retrain xgboost for 21.1s
[flaml.automl: 09-18 04:58:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:58:40] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:58:40] {2637} INFO - Time taken to find the best model: 58.96424412727356
[flaml.automl: 09-18 04:58:40] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9607882946800492
CO(0)最好结果：{'pred_time': 2.417353051521111e-05, 'wall_clock_time': 58.96424412727356, 'metric_for_logging': {'pred_time': 2.417353051521111e-05}, 'val_loss': 0.03921170531995079, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 16.637566566467285}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9250052513753694
CO(0)的mse=0.0033630801256626236
CO(0)的mae=0.03722541679363189
CO(0)的mar=0.09348698005622375
总共花费的时间为：80.64
池州市
3237A
3333A
3334A
[flaml.automl: 09-18 05:08:23] {2390} INFO - task = regression
[flaml.automl: 09-18 05:08:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:08:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:08:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:08:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:08:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:08:24] {3025} INFO - Estimated sufficient time budget=12157s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 05:08:24] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1672,	best estimator xgboost's best error=0.1672
[flaml.automl: 09-18 05:08:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:08:27] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0974,	best estimator xgboost's best error=0.0974
[flaml.automl: 09-18 05:08:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:08:28] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0974,	best estimator xgboost's best error=0.0974
[flaml.automl: 09-18 05:08:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:08:38] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0974,	best estimator xgboost's best error=0.0974
[flaml.automl: 09-18 05:08:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:08:39] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0780,	best estimator xgboost's best error=0.0780
[flaml.automl: 09-18 05:08:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:08:41] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 05:08:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:08:42] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 05:08:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:08:45] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 05:08:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:08:46] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 05:08:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:08:48] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 05:08:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:08:50] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 05:08:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:08:51] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 05:08:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:08:57] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0642,	best estimator xgboost's best error=0.0642
[flaml.automl: 09-18 05:08:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:09:09] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.0620,	best estimator xgboost's best error=0.0620
[flaml.automl: 09-18 05:09:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:09:16] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.0620,	best estimator xgboost's best error=0.0620
[flaml.automl: 09-18 05:09:28] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 05:09:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:09:28] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:09:28] {2637} INFO - Time taken to find the best model: 46.371185541152954
[flaml.automl: 09-18 05:09:28] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9380257210636126
CO(0)最好结果：{'pred_time': 1.1112180842134485e-05, 'wall_clock_time': 46.371185541152954, 'metric_for_logging': {'pred_time': 1.1112180842134485e-05}, 'val_loss': 0.061974278936387406, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.087214946746826}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8346109931865999
CO(0)的mse=0.008895946620174877
CO(0)的mae=0.06333389387695583
CO(0)的mar=0.08984563903218226
总共花费的时间为：65.46
宣城市
2316A
2317A
2318A
3470A
[flaml.automl: 09-18 05:21:54] {2390} INFO - task = regression
[flaml.automl: 09-18 05:21:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:21:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:21:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:21:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:21:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:21:56] {3025} INFO - Estimated sufficient time budget=51438s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 05:21:56] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1115,	best estimator xgboost's best error=0.1115
[flaml.automl: 09-18 05:21:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:21:58] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0658,	best estimator xgboost's best error=0.0658
[flaml.automl: 09-18 05:21:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:21:59] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0658,	best estimator xgboost's best error=0.0658
[flaml.automl: 09-18 05:21:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:22:05] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0658,	best estimator xgboost's best error=0.0658
[flaml.automl: 09-18 05:22:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:22:06] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0529,	best estimator xgboost's best error=0.0529
[flaml.automl: 09-18 05:22:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:22:08] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0427,	best estimator xgboost's best error=0.0427
[flaml.automl: 09-18 05:22:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:22:10] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0427,	best estimator xgboost's best error=0.0427
[flaml.automl: 09-18 05:22:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:22:12] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0427,	best estimator xgboost's best error=0.0427
[flaml.automl: 09-18 05:22:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:22:13] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0427,	best estimator xgboost's best error=0.0427
[flaml.automl: 09-18 05:22:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:22:16] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0427,	best estimator xgboost's best error=0.0427
[flaml.automl: 09-18 05:22:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:22:17] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0427,	best estimator xgboost's best error=0.0427
[flaml.automl: 09-18 05:22:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:22:19] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0427,	best estimator xgboost's best error=0.0427
[flaml.automl: 09-18 05:22:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:22:25] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0379,	best estimator xgboost's best error=0.0379
[flaml.automl: 09-18 05:22:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:22:37] {3072} INFO -  at 43.1s,	estimator xgboost's best error=0.0372,	best estimator xgboost's best error=0.0372
[flaml.automl: 09-18 05:22:37] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:22:44] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0372,	best estimator xgboost's best error=0.0372
[flaml.automl: 09-18 05:22:56] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 05:22:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:22:56] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:22:56] {2637} INFO - Time taken to find the best model: 43.06369662284851
[flaml.automl: 09-18 05:22:56] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42816}
CO(0)最佳损失：0.9628144198280766
CO(0)最好结果：{'pred_time': 9.256433468498369e-06, 'wall_clock_time': 43.06369662284851, 'metric_for_logging': {'pred_time': 9.256433468498369e-06}, 'val_loss': 0.03718558017192342, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42816}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42816, 'experiment_tag': 'exp', 'time_total_s': 12.106691837310791}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9124575518182747
CO(0)的mse=0.0030892090910175822
CO(0)的mae=0.036986416888742164
CO(0)的mar=0.06900760317890936
总共花费的时间为：62.36
莆田市
2319A
2320A
2321A
2322A
2323A
[flaml.automl: 09-18 05:38:39] {2390} INFO - task = regression
[flaml.automl: 09-18 05:38:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:38:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:38:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:38:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:38:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:38:41] {3025} INFO - Estimated sufficient time budget=116566s. Estimated necessary time budget=117s.
[flaml.automl: 09-18 05:38:41] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.0879,	best estimator xgboost's best error=0.0879
[flaml.automl: 09-18 05:38:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:38:44] {3072} INFO -  at 5.5s,	estimator xgboost's best error=0.0511,	best estimator xgboost's best error=0.0511
[flaml.automl: 09-18 05:38:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:38:46] {3072} INFO -  at 7.3s,	estimator xgboost's best error=0.0511,	best estimator xgboost's best error=0.0511
[flaml.automl: 09-18 05:38:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:38:50] {3072} INFO -  at 11.7s,	estimator xgboost's best error=0.0511,	best estimator xgboost's best error=0.0511
[flaml.automl: 09-18 05:38:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:38:52] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.0420,	best estimator xgboost's best error=0.0420
[flaml.automl: 09-18 05:38:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:38:54] {3072} INFO -  at 16.1s,	estimator xgboost's best error=0.0344,	best estimator xgboost's best error=0.0344
[flaml.automl: 09-18 05:38:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:38:57] {3072} INFO -  at 18.3s,	estimator xgboost's best error=0.0344,	best estimator xgboost's best error=0.0344
[flaml.automl: 09-18 05:38:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:39:00] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0344,	best estimator xgboost's best error=0.0344
[flaml.automl: 09-18 05:39:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:39:02] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0344,	best estimator xgboost's best error=0.0344
[flaml.automl: 09-18 05:39:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:39:04] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0344,	best estimator xgboost's best error=0.0344
[flaml.automl: 09-18 05:39:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:39:07] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.0344,	best estimator xgboost's best error=0.0344
[flaml.automl: 09-18 05:39:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:39:08] {3072} INFO -  at 29.7s,	estimator xgboost's best error=0.0344,	best estimator xgboost's best error=0.0344
[flaml.automl: 09-18 05:39:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:39:17] {3072} INFO -  at 38.9s,	estimator xgboost's best error=0.0310,	best estimator xgboost's best error=0.0310
[flaml.automl: 09-18 05:39:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:39:37] {3072} INFO -  at 58.7s,	estimator xgboost's best error=0.0296,	best estimator xgboost's best error=0.0296
[flaml.automl: 09-18 05:40:04] {3335} INFO - retrain xgboost for 26.7s
[flaml.automl: 09-18 05:40:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:40:04] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:40:04] {2637} INFO - Time taken to find the best model: 58.73917198181152
[flaml.automl: 09-18 05:40:04] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53719}
CO(0)最佳损失：0.970373258259193
CO(0)最好结果：{'pred_time': 6.827902286610705e-06, 'wall_clock_time': 58.73917198181152, 'metric_for_logging': {'pred_time': 6.827902286610705e-06}, 'val_loss': 0.02962674174080703, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53719}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53719, 'experiment_tag': 'exp', 'time_total_s': 19.84425926208496}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9006386277423472
CO(0)的mse=0.0021536176315097378
CO(0)的mae=0.03050769540468492
CO(0)的mar=0.05569245647566396
总共花费的时间为：86.72
三明市
2324A
2325A
2326A
2327A
[flaml.automl: 09-18 05:52:21] {2390} INFO - task = regression
[flaml.automl: 09-18 05:52:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:52:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:52:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:52:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:52:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:52:22] {3025} INFO - Estimated sufficient time budget=49094s. Estimated necessary time budget=49s.
[flaml.automl: 09-18 05:52:22] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1868,	best estimator xgboost's best error=0.1868
[flaml.automl: 09-18 05:52:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:52:24] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1196,	best estimator xgboost's best error=0.1196
[flaml.automl: 09-18 05:52:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:52:25] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1196,	best estimator xgboost's best error=0.1196
[flaml.automl: 09-18 05:52:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:52:32] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.1196,	best estimator xgboost's best error=0.1196
[flaml.automl: 09-18 05:52:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:52:33] {3072} INFO -  at 12.1s,	estimator xgboost's best error=0.1045,	best estimator xgboost's best error=0.1045
[flaml.automl: 09-18 05:52:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:52:34] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.0985,	best estimator xgboost's best error=0.0985
[flaml.automl: 09-18 05:52:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:52:36] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0985,	best estimator xgboost's best error=0.0985
[flaml.automl: 09-18 05:52:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:52:38] {3072} INFO -  at 17.7s,	estimator xgboost's best error=0.0985,	best estimator xgboost's best error=0.0985
[flaml.automl: 09-18 05:52:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:52:40] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0985,	best estimator xgboost's best error=0.0985
[flaml.automl: 09-18 05:52:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:52:42] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0985,	best estimator xgboost's best error=0.0985
[flaml.automl: 09-18 05:52:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:52:44] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0980,	best estimator xgboost's best error=0.0980
[flaml.automl: 09-18 05:52:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:52:45] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.0980,	best estimator xgboost's best error=0.0980
[flaml.automl: 09-18 05:52:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:52:51] {3072} INFO -  at 30.8s,	estimator xgboost's best error=0.0956,	best estimator xgboost's best error=0.0956
[flaml.automl: 09-18 05:52:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:53:04] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.0934,	best estimator xgboost's best error=0.0934
[flaml.automl: 09-18 05:53:04] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:53:10] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0934,	best estimator xgboost's best error=0.0934
[flaml.automl: 09-18 05:53:22] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 05:53:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:53:22] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:53:22] {2637} INFO - Time taken to find the best model: 42.970672607421875
[flaml.automl: 09-18 05:53:22] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41468}
CO(0)最佳损失：0.9065566694403274
CO(0)最好结果：{'pred_time': 8.878918985525766e-06, 'wall_clock_time': 42.970672607421875, 'metric_for_logging': {'pred_time': 8.878918985525766e-06}, 'val_loss': 0.09344333055967258, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41468}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41468, 'experiment_tag': 'exp', 'time_total_s': 12.141719341278076}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.6037716178246804
CO(0)的mse=0.026179974817525747
CO(0)的mae=0.09217214326893446
CO(0)的mar=0.12457390900453973
总共花费的时间为：62.26
南平市
2331A
2332A
2333A
2334A
[flaml.automl: 09-18 06:06:11] {2390} INFO - task = regression
[flaml.automl: 09-18 06:06:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:06:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:06:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:06:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:06:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:06:12] {3025} INFO - Estimated sufficient time budget=50731s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 06:06:12] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1044,	best estimator xgboost's best error=0.1044
[flaml.automl: 09-18 06:06:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:06:14] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0641,	best estimator xgboost's best error=0.0641
[flaml.automl: 09-18 06:06:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:06:16] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0641,	best estimator xgboost's best error=0.0641
[flaml.automl: 09-18 06:06:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:06:22] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0641,	best estimator xgboost's best error=0.0641
[flaml.automl: 09-18 06:06:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:06:23] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0518,	best estimator xgboost's best error=0.0518
[flaml.automl: 09-18 06:06:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:06:25] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-18 06:06:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:06:26] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-18 06:06:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:06:29] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-18 06:06:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:06:30] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-18 06:06:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:06:33] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-18 06:06:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:06:34] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-18 06:06:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:06:35] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0454,	best estimator xgboost's best error=0.0454
[flaml.automl: 09-18 06:06:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:06:42] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0388,	best estimator xgboost's best error=0.0388
[flaml.automl: 09-18 06:06:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:06:54] {3072} INFO -  at 43.1s,	estimator xgboost's best error=0.0374,	best estimator xgboost's best error=0.0374
[flaml.automl: 09-18 06:06:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:07:01] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0374,	best estimator xgboost's best error=0.0374
[flaml.automl: 09-18 06:07:13] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 06:07:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:07:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:07:13] {2637} INFO - Time taken to find the best model: 43.08229422569275
[flaml.automl: 09-18 06:07:13] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42381}
CO(0)最佳损失：0.9626129483351029
CO(0)最好结果：{'pred_time': 8.61015036354146e-06, 'wall_clock_time': 43.08229422569275, 'metric_for_logging': {'pred_time': 8.61015036354146e-06}, 'val_loss': 0.037387051664897086, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42381}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42381, 'experiment_tag': 'exp', 'time_total_s': 12.075499534606934}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8881727323853508
CO(0)的mse=0.0035250481178930996
CO(0)的mae=0.03841219987837142
CO(0)的mar=0.0751115471078707
总共花费的时间为：62.27
龙岩市
2335A
2336A
2337A
2338A
[flaml.automl: 09-18 06:20:51] {2390} INFO - task = regression
[flaml.automl: 09-18 06:20:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:20:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:20:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:20:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:20:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:20:52] {3025} INFO - Estimated sufficient time budget=52235s. Estimated necessary time budget=52s.
[flaml.automl: 09-18 06:20:52] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.0928,	best estimator xgboost's best error=0.0928
[flaml.automl: 09-18 06:20:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:20:54] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0605,	best estimator xgboost's best error=0.0605
[flaml.automl: 09-18 06:20:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:20:55] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0605,	best estimator xgboost's best error=0.0605
[flaml.automl: 09-18 06:20:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:21:01] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.0605,	best estimator xgboost's best error=0.0605
[flaml.automl: 09-18 06:21:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:21:03] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.0496,	best estimator xgboost's best error=0.0496
[flaml.automl: 09-18 06:21:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:21:06] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-18 06:21:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:21:09] {3072} INFO -  at 18.2s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-18 06:21:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:21:13] {3072} INFO -  at 22.2s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-18 06:21:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:21:14] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-18 06:21:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:21:18] {3072} INFO -  at 27.5s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-18 06:21:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:21:21] {3072} INFO -  at 30.1s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-18 06:21:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:21:23] {3072} INFO -  at 32.3s,	estimator xgboost's best error=0.0437,	best estimator xgboost's best error=0.0437
[flaml.automl: 09-18 06:21:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:21:34] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-18 06:21:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:21:50] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0376,	best estimator xgboost's best error=0.0376
[flaml.automl: 09-18 06:22:07] {3335} INFO - retrain xgboost for 17.2s
[flaml.automl: 09-18 06:22:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:22:07] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:22:07] {2637} INFO - Time taken to find the best model: 59.21210479736328
[flaml.automl: 09-18 06:22:07] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43315}
CO(0)最佳损失：0.9623846546622415
CO(0)最好结果：{'pred_time': 1.5723886099521518e-05, 'wall_clock_time': 59.21210479736328, 'metric_for_logging': {'pred_time': 1.5723886099521518e-05}, 'val_loss': 0.03761534533775846, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43315}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43315, 'experiment_tag': 'exp', 'time_total_s': 15.782862901687622}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8526465847726639
CO(0)的mse=0.0031417476276710244
CO(0)的mae=0.03796707808291546
CO(0)的mar=0.08086580139810645
总共花费的时间为：77.24
宁德市
2339A
3209A
[flaml.automl: 09-18 06:28:28] {2390} INFO - task = regression
[flaml.automl: 09-18 06:28:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:28:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:28:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:28:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:28:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:28:31] {3025} INFO - Estimated sufficient time budget=33255s. Estimated necessary time budget=33s.
[flaml.automl: 09-18 06:28:31] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1132,	best estimator xgboost's best error=0.1132
[flaml.automl: 09-18 06:28:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:28:36] {3072} INFO -  at 8.9s,	estimator xgboost's best error=0.0757,	best estimator xgboost's best error=0.0757
[flaml.automl: 09-18 06:28:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:28:40] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0757,	best estimator xgboost's best error=0.0757
[flaml.automl: 09-18 06:28:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:28:55] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.0757,	best estimator xgboost's best error=0.0757
[flaml.automl: 09-18 06:28:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:28:56] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.0593,	best estimator xgboost's best error=0.0593
[flaml.automl: 09-18 06:28:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:28:57] {3072} INFO -  at 29.7s,	estimator xgboost's best error=0.0528,	best estimator xgboost's best error=0.0528
[flaml.automl: 09-18 06:28:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:28:59] {3072} INFO -  at 31.3s,	estimator xgboost's best error=0.0528,	best estimator xgboost's best error=0.0528
[flaml.automl: 09-18 06:28:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:29:01] {3072} INFO -  at 33.7s,	estimator xgboost's best error=0.0528,	best estimator xgboost's best error=0.0528
[flaml.automl: 09-18 06:29:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:29:02] {3072} INFO -  at 34.8s,	estimator xgboost's best error=0.0528,	best estimator xgboost's best error=0.0528
[flaml.automl: 09-18 06:29:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:29:05] {3072} INFO -  at 37.2s,	estimator xgboost's best error=0.0528,	best estimator xgboost's best error=0.0528
[flaml.automl: 09-18 06:29:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:29:06] {3072} INFO -  at 38.3s,	estimator xgboost's best error=0.0528,	best estimator xgboost's best error=0.0528
[flaml.automl: 09-18 06:29:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:29:07] {3072} INFO -  at 39.4s,	estimator xgboost's best error=0.0528,	best estimator xgboost's best error=0.0528
[flaml.automl: 09-18 06:29:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:29:13] {3072} INFO -  at 45.4s,	estimator xgboost's best error=0.0486,	best estimator xgboost's best error=0.0486
[flaml.automl: 09-18 06:29:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:29:23] {3072} INFO -  at 55.7s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-18 06:29:34] {3335} INFO - retrain xgboost for 10.3s
[flaml.automl: 09-18 06:29:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:29:34] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:29:34] {2637} INFO - Time taken to find the best model: 55.70787453651428
[flaml.automl: 09-18 06:29:34] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9529818935353968
CO(0)最好结果：{'pred_time': 1.7557878413622678e-05, 'wall_clock_time': 55.70787453651428, 'metric_for_logging': {'pred_time': 1.7557878413622678e-05}, 'val_loss': 0.04701810646460329, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.334681034088135}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.841191464760797
CO(0)的mse=0.004964967619444367
CO(0)的mae=0.04595418427570085
CO(0)的mar=0.07624575170690669
总共花费的时间为：66.60
景德镇市
2342A
2343A
2344A
2345A
2346A
[flaml.automl: 09-18 06:45:17] {2390} INFO - task = regression
[flaml.automl: 09-18 06:45:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:45:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:45:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:45:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:45:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:45:18] {3025} INFO - Estimated sufficient time budget=63102s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 06:45:18] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1167,	best estimator xgboost's best error=0.1167
[flaml.automl: 09-18 06:45:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:45:20] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0728,	best estimator xgboost's best error=0.0728
[flaml.automl: 09-18 06:45:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:45:21] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0728,	best estimator xgboost's best error=0.0728
[flaml.automl: 09-18 06:45:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:45:26] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0728,	best estimator xgboost's best error=0.0728
[flaml.automl: 09-18 06:45:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:45:27] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0611,	best estimator xgboost's best error=0.0611
[flaml.automl: 09-18 06:45:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:45:29] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0533,	best estimator xgboost's best error=0.0533
[flaml.automl: 09-18 06:45:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:45:30] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.0533,	best estimator xgboost's best error=0.0533
[flaml.automl: 09-18 06:45:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:45:33] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0533,	best estimator xgboost's best error=0.0533
[flaml.automl: 09-18 06:45:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:45:34] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0533,	best estimator xgboost's best error=0.0533
[flaml.automl: 09-18 06:45:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:45:37] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0533,	best estimator xgboost's best error=0.0533
[flaml.automl: 09-18 06:45:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:45:38] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-18 06:45:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:45:40] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-18 06:45:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:45:46] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0473,	best estimator xgboost's best error=0.0473
[flaml.automl: 09-18 06:45:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:45:58] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0473,	best estimator xgboost's best error=0.0473
[flaml.automl: 09-18 06:45:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:46:02] {3072} INFO -  at 45.9s,	estimator xgboost's best error=0.0473,	best estimator xgboost's best error=0.0473
[flaml.automl: 09-18 06:46:02] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 06:46:14] {3072} INFO -  at 57.3s,	estimator xgboost's best error=0.0473,	best estimator xgboost's best error=0.0473
[flaml.automl: 09-18 06:46:20] {3335} INFO - retrain xgboost for 6.5s
[flaml.automl: 09-18 06:46:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 06:46:20] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:46:20] {2637} INFO - Time taken to find the best model: 29.512300729751587
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 52387}
CO(0)最佳损失：0.9526844302689705
CO(0)最好结果：{'pred_time': 7.1808186373622985e-06, 'wall_clock_time': 29.512300729751587, 'metric_for_logging': {'pred_time': 7.1808186373622985e-06}, 'val_loss': 0.04731556973102944, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 52387}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 52387, 'experiment_tag': 'exp', 'time_total_s': 6.532557249069214}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8573129576324727
CO(0)的mse=0.005446269329633226
CO(0)的mae=0.0486204581684309
CO(0)的mar=0.10209806722767924
总共花费的时间为：64.73
萍乡市
2347A
2348A
2349A
2350A
2351A
[flaml.automl: 09-18 07:01:58] {2390} INFO - task = regression
[flaml.automl: 09-18 07:01:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:01:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:01:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:01:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:01:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:01:59] {3025} INFO - Estimated sufficient time budget=61214s. Estimated necessary time budget=61s.
[flaml.automl: 09-18 07:01:59] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.3623,	best estimator xgboost's best error=0.3623
[flaml.automl: 09-18 07:01:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:02:01] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.2226,	best estimator xgboost's best error=0.2226
[flaml.automl: 09-18 07:02:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:02:03] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.2226,	best estimator xgboost's best error=0.2226
[flaml.automl: 09-18 07:02:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:02:07] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.2226,	best estimator xgboost's best error=0.2226
[flaml.automl: 09-18 07:02:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:02:08] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.1911,	best estimator xgboost's best error=0.1911
[flaml.automl: 09-18 07:02:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:02:10] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.1801,	best estimator xgboost's best error=0.1801
[flaml.automl: 09-18 07:02:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:02:12] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.1801,	best estimator xgboost's best error=0.1801
[flaml.automl: 09-18 07:02:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:02:14] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.1801,	best estimator xgboost's best error=0.1801
[flaml.automl: 09-18 07:02:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:02:15] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.1801,	best estimator xgboost's best error=0.1801
[flaml.automl: 09-18 07:02:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:02:18] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:02:20] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:02:21] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:02:24] {3072} INFO -  at 26.1s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:24] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:02:27] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:02:29] {3072} INFO -  at 31.5s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:29] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 07:02:34] {3072} INFO -  at 36.2s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:34] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 07:02:36] {3072} INFO -  at 37.8s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:36] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 07:02:37] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:37] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 07:02:44] {3072} INFO -  at 46.1s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:44] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 07:02:45] {3072} INFO -  at 47.7s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:45] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 07:02:53] {3072} INFO -  at 55.3s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:53] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 07:02:57] {3072} INFO -  at 58.8s,	estimator xgboost's best error=0.1797,	best estimator xgboost's best error=0.1797
[flaml.automl: 09-18 07:02:59] {3335} INFO - retrain xgboost for 2.7s
[flaml.automl: 09-18 07:02:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:02:59] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:02:59] {2637} INFO - Time taken to find the best model: 20.173502683639526
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 11, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846, 'FLAML_sample_size': 10000}
CO(0)最佳损失：0.8203103217113238
CO(0)最好结果：{'pred_time': 6.876777495134305e-06, 'wall_clock_time': 20.173502683639526, 'metric_for_logging': {'pred_time': 6.876777495134305e-06}, 'val_loss': 0.17968967828867624, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 11, 'min_child_weight': 0.06372323163910634, 'learning_rate': 0.4086634770112844, 'subsample': 0.770412309752864, 'colsample_bylevel': 0.6720329282771347, 'colsample_bytree': 0.752099095598003, 'reg_alpha': 0.00271725862127768, 'reg_lambda': 3.9335607770117846, 'FLAML_sample_size': 10000}, 'config/n_estimators': 4, 'config/max_leaves': 11, 'config/min_child_weight': 0.06372323163910634, 'config/learning_rate': 0.4086634770112844, 'config/subsample': 0.770412309752864, 'config/colsample_bylevel': 0.6720329282771347, 'config/colsample_bytree': 0.752099095598003, 'config/reg_alpha': 0.00271725862127768, 'config/reg_lambda': 3.9335607770117846, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 2.67071795463562}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6720329282771347, colsample_bynode=1,
             colsample_bytree=0.752099095598003, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4086634770112844,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.06372323163910634, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00271725862127768,
             reg_lambda=3.9335607770117846, scale_pos_weight=1,
             subsample=0.770412309752864, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.2357604978165494
CO(0)的mse=0.11520299697739067
CO(0)的mae=0.18433547605485037
CO(0)的mar=0.19660302512739664
总共花费的时间为：62.43
新余市
2352A
2353A
2354A
2355A
[flaml.automl: 09-18 07:15:40] {2390} INFO - task = regression
[flaml.automl: 09-18 07:15:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:15:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:15:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:15:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:15:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:15:42] {3025} INFO - Estimated sufficient time budget=12138s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 07:15:42] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2111,	best estimator xgboost's best error=0.2111
[flaml.automl: 09-18 07:15:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:15:44] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1429,	best estimator xgboost's best error=0.1429
[flaml.automl: 09-18 07:15:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:15:45] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1429,	best estimator xgboost's best error=0.1429
[flaml.automl: 09-18 07:15:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:15:55] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.1429,	best estimator xgboost's best error=0.1429
[flaml.automl: 09-18 07:15:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:15:56] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.1263,	best estimator xgboost's best error=0.1263
[flaml.automl: 09-18 07:15:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:15:58] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.1161,	best estimator xgboost's best error=0.1161
[flaml.automl: 09-18 07:15:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:15:59] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.1161,	best estimator xgboost's best error=0.1161
[flaml.automl: 09-18 07:15:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:16:02] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.1161,	best estimator xgboost's best error=0.1161
[flaml.automl: 09-18 07:16:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:16:03] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.1161,	best estimator xgboost's best error=0.1161
[flaml.automl: 09-18 07:16:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:16:06] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.1161,	best estimator xgboost's best error=0.1161
[flaml.automl: 09-18 07:16:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:16:07] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.1161,	best estimator xgboost's best error=0.1161
[flaml.automl: 09-18 07:16:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:16:08] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.1161,	best estimator xgboost's best error=0.1161
[flaml.automl: 09-18 07:16:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:16:14] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.1161,	best estimator xgboost's best error=0.1161
[flaml.automl: 09-18 07:16:14] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:16:17] {3072} INFO -  at 37.2s,	estimator xgboost's best error=0.1146,	best estimator xgboost's best error=0.1146
[flaml.automl: 09-18 07:16:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:16:19] {3072} INFO -  at 38.8s,	estimator xgboost's best error=0.1146,	best estimator xgboost's best error=0.1146
[flaml.automl: 09-18 07:16:19] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 07:16:24] {3072} INFO -  at 43.6s,	estimator xgboost's best error=0.1127,	best estimator xgboost's best error=0.1127
[flaml.automl: 09-18 07:16:24] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 07:16:27] {3072} INFO -  at 46.5s,	estimator xgboost's best error=0.1127,	best estimator xgboost's best error=0.1127
[flaml.automl: 09-18 07:16:27] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 07:16:29] {3072} INFO -  at 48.7s,	estimator xgboost's best error=0.1127,	best estimator xgboost's best error=0.1127
[flaml.automl: 09-18 07:16:29] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 07:16:39] {3072} INFO -  at 58.9s,	estimator xgboost's best error=0.1127,	best estimator xgboost's best error=0.1127
[flaml.automl: 09-18 07:16:45] {3335} INFO - retrain xgboost for 5.9s
[flaml.automl: 09-18 07:16:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:16:45] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:16:45] {2637} INFO - Time taken to find the best model: 43.61011290550232
[flaml.automl: 09-18 07:16:45] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}
CO(0)最佳损失：0.8872917758122936
CO(0)最好结果：{'pred_time': 9.256122258044174e-06, 'wall_clock_time': 43.61011290550232, 'metric_for_logging': {'pred_time': 9.256122258044174e-06}, 'val_loss': 0.11270822418770639, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 9, 'min_child_weight': 3.029166996333365, 'learning_rate': 0.28782051713764356, 'subsample': 0.8051874633193139, 'colsample_bylevel': 0.656637827405616, 'colsample_bytree': 0.7193335366419935, 'reg_alpha': 0.0009765625, 'reg_lambda': 2.3848121277079324}, 'config/n_estimators': 9, 'config/max_leaves': 9, 'config/min_child_weight': 3.029166996333365, 'config/learning_rate': 0.28782051713764356, 'config/subsample': 0.8051874633193139, 'config/colsample_bylevel': 0.656637827405616, 'config/colsample_bytree': 0.7193335366419935, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 2.3848121277079324, 'experiment_tag': 'exp', 'time_total_s': 4.826171398162842}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.656637827405616, colsample_bynode=1,
             colsample_bytree=0.7193335366419935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.28782051713764356,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=3.029166996333365, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=2.3848121277079324, scale_pos_weight=1,
             subsample=0.8051874633193139, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.41901654387364085
CO(0)的mse=0.047779234301804994
CO(0)的mae=0.1156827411550878
CO(0)的mar=0.24026849761626862
总共花费的时间为：65.69
鹰潭市
2357A
2358A
2359A
2360A
2361A
[flaml.automl: 09-18 07:32:17] {2390} INFO - task = regression
[flaml.automl: 09-18 07:32:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:32:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:32:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:32:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:32:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:32:18] {3025} INFO - Estimated sufficient time budget=65021s. Estimated necessary time budget=65s.
[flaml.automl: 09-18 07:32:18] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1278,	best estimator xgboost's best error=0.1278
[flaml.automl: 09-18 07:32:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:32:20] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0834,	best estimator xgboost's best error=0.0834
[flaml.automl: 09-18 07:32:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:32:22] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0834,	best estimator xgboost's best error=0.0834
[flaml.automl: 09-18 07:32:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:32:26] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0834,	best estimator xgboost's best error=0.0834
[flaml.automl: 09-18 07:32:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:32:28] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0706,	best estimator xgboost's best error=0.0706
[flaml.automl: 09-18 07:32:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:32:29] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0633,	best estimator xgboost's best error=0.0633
[flaml.automl: 09-18 07:32:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:32:31] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0633,	best estimator xgboost's best error=0.0633
[flaml.automl: 09-18 07:32:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:32:33] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0633,	best estimator xgboost's best error=0.0633
[flaml.automl: 09-18 07:32:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:32:34] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0633,	best estimator xgboost's best error=0.0633
[flaml.automl: 09-18 07:32:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:32:37] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0633,	best estimator xgboost's best error=0.0633
[flaml.automl: 09-18 07:32:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:32:39] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0633,	best estimator xgboost's best error=0.0633
[flaml.automl: 09-18 07:32:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:32:40] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0633,	best estimator xgboost's best error=0.0633
[flaml.automl: 09-18 07:32:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:32:46] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0591,	best estimator xgboost's best error=0.0591
[flaml.automl: 09-18 07:32:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:32:58] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0587,	best estimator xgboost's best error=0.0587
[flaml.automl: 09-18 07:32:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:33:05] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.0587,	best estimator xgboost's best error=0.0587
[flaml.automl: 09-18 07:33:17] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 07:33:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:33:17] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:33:17] {2637} INFO - Time taken to find the best model: 41.669206380844116
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52023}
CO(0)最佳损失：0.9413259798332989
CO(0)最好结果：{'pred_time': 7.1041807957616076e-06, 'wall_clock_time': 41.669206380844116, 'metric_for_logging': {'pred_time': 7.1041807957616076e-06}, 'val_loss': 0.05867402016670104, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52023}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52023, 'experiment_tag': 'exp', 'time_total_s': 12.067093849182129}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8296768582233317
CO(0)的mse=0.007442236082698806
CO(0)的mae=0.05738392576443969
CO(0)的mar=0.20468852261322815
总共花费的时间为：61.07
赣州市
2362A
2363A
2364A
2365A
2366A
3109A
[flaml.automl: 09-18 07:52:07] {2390} INFO - task = regression
[flaml.automl: 09-18 07:52:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:52:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:52:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:52:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:52:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:52:10] {3025} INFO - Estimated sufficient time budget=145189s. Estimated necessary time budget=145s.
[flaml.automl: 09-18 07:52:10] {3072} INFO -  at 2.7s,	estimator xgboost's best error=0.2871,	best estimator xgboost's best error=0.2871
[flaml.automl: 09-18 07:52:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:52:14] {3072} INFO -  at 6.6s,	estimator xgboost's best error=0.1347,	best estimator xgboost's best error=0.1347
[flaml.automl: 09-18 07:52:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:52:15] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.1347,	best estimator xgboost's best error=0.1347
[flaml.automl: 09-18 07:52:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:52:19] {3072} INFO -  at 11.6s,	estimator xgboost's best error=0.1347,	best estimator xgboost's best error=0.1347
[flaml.automl: 09-18 07:52:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:52:21] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0642,	best estimator xgboost's best error=0.0642
[flaml.automl: 09-18 07:52:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:52:23] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0448,	best estimator xgboost's best error=0.0448
[flaml.automl: 09-18 07:52:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:52:26] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0447,	best estimator xgboost's best error=0.0447
[flaml.automl: 09-18 07:52:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:52:29] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0447,	best estimator xgboost's best error=0.0447
[flaml.automl: 09-18 07:52:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:52:31] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0447,	best estimator xgboost's best error=0.0447
[flaml.automl: 09-18 07:52:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:52:34] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.0447,	best estimator xgboost's best error=0.0447
[flaml.automl: 09-18 07:52:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:52:36] {3072} INFO -  at 29.2s,	estimator xgboost's best error=0.0447,	best estimator xgboost's best error=0.0447
[flaml.automl: 09-18 07:52:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:52:39] {3072} INFO -  at 32.1s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-18 07:52:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:52:41] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-18 07:52:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:52:52] {3072} INFO -  at 45.3s,	estimator xgboost's best error=0.0330,	best estimator xgboost's best error=0.0330
[flaml.automl: 09-18 07:52:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:53:06] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0323,	best estimator xgboost's best error=0.0323
[flaml.automl: 09-18 07:53:25] {3335} INFO - retrain xgboost for 18.8s
[flaml.automl: 09-18 07:53:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:53:25] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:53:25] {2637} INFO - Time taken to find the best model: 59.36110758781433
[flaml.automl: 09-18 07:53:25] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 62710}
CO(0)最佳损失：0.9677034604721173
CO(0)最好结果：{'pred_time': 1.00047183228415e-05, 'wall_clock_time': 59.36110758781433, 'metric_for_logging': {'pred_time': 1.00047183228415e-05}, 'val_loss': 0.03229653952788266, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 62710}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 62710, 'experiment_tag': 'exp', 'time_total_s': 14.030520677566528}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9630925312793697
CO(0)的mse=0.0022498467324862047
CO(0)的mae=0.030977711020391197
CO(0)的mar=0.036734257985640936
总共花费的时间为：79.35
吉安市
2367A
2368A
2369A
2370A
[flaml.automl: 09-18 08:06:12] {2390} INFO - task = regression
[flaml.automl: 09-18 08:06:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:06:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:06:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:06:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:06:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:06:14] {3025} INFO - Estimated sufficient time budget=88152s. Estimated necessary time budget=88s.
[flaml.automl: 09-18 08:06:14] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1277,	best estimator xgboost's best error=0.1277
[flaml.automl: 09-18 08:06:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:06:18] {3072} INFO -  at 6.2s,	estimator xgboost's best error=0.0802,	best estimator xgboost's best error=0.0802
[flaml.automl: 09-18 08:06:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:06:20] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.0802,	best estimator xgboost's best error=0.0802
[flaml.automl: 09-18 08:06:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:06:25] {3072} INFO -  at 13.2s,	estimator xgboost's best error=0.0802,	best estimator xgboost's best error=0.0802
[flaml.automl: 09-18 08:06:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:06:27] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.0649,	best estimator xgboost's best error=0.0649
[flaml.automl: 09-18 08:06:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:06:29] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0581,	best estimator xgboost's best error=0.0581
[flaml.automl: 09-18 08:06:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:06:32] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.0581,	best estimator xgboost's best error=0.0581
[flaml.automl: 09-18 08:06:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:06:36] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.0581,	best estimator xgboost's best error=0.0581
[flaml.automl: 09-18 08:06:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:06:38] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.0581,	best estimator xgboost's best error=0.0581
[flaml.automl: 09-18 08:06:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:06:41] {3072} INFO -  at 29.3s,	estimator xgboost's best error=0.0581,	best estimator xgboost's best error=0.0581
[flaml.automl: 09-18 08:06:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:06:44] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.0581,	best estimator xgboost's best error=0.0581
[flaml.automl: 09-18 08:06:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:06:46] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.0581,	best estimator xgboost's best error=0.0581
[flaml.automl: 09-18 08:06:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:06:54] {3072} INFO -  at 42.6s,	estimator xgboost's best error=0.0554,	best estimator xgboost's best error=0.0554
[flaml.automl: 09-18 08:06:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:07:06] {3072} INFO -  at 54.7s,	estimator xgboost's best error=0.0538,	best estimator xgboost's best error=0.0538
[flaml.automl: 09-18 08:07:19] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 08:07:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:07:19] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:07:19] {2637} INFO - Time taken to find the best model: 54.678542375564575
[flaml.automl: 09-18 08:07:19] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43189}
CO(0)最佳损失：0.946170721076831
CO(0)最好结果：{'pred_time': 9.275521057599484e-06, 'wall_clock_time': 54.678542375564575, 'metric_for_logging': {'pred_time': 9.275521057599484e-06}, 'val_loss': 0.053829278923169, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43189}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43189, 'experiment_tag': 'exp', 'time_total_s': 12.101483345031738}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8507613210036575
CO(0)的mse=0.005653721493758357
CO(0)的mae=0.05429673416576078
CO(0)的mar=0.09717213714344394
总共花费的时间为：67.58
宜春市
2371A
2374A
2375A
3151A
3415A
[flaml.automl: 09-18 08:22:26] {2390} INFO - task = regression
[flaml.automl: 09-18 08:22:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:22:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:22:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:22:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:22:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:22:28] {3025} INFO - Estimated sufficient time budget=108387s. Estimated necessary time budget=108s.
[flaml.automl: 09-18 08:22:28] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.2171,	best estimator xgboost's best error=0.2171
[flaml.automl: 09-18 08:22:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:22:32] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.1250,	best estimator xgboost's best error=0.1250
[flaml.automl: 09-18 08:22:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:22:35] {3072} INFO -  at 8.7s,	estimator xgboost's best error=0.1250,	best estimator xgboost's best error=0.1250
[flaml.automl: 09-18 08:22:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:22:39] {3072} INFO -  at 13.0s,	estimator xgboost's best error=0.1250,	best estimator xgboost's best error=0.1250
[flaml.automl: 09-18 08:22:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:22:41] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.0954,	best estimator xgboost's best error=0.0954
[flaml.automl: 09-18 08:22:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:22:44] {3072} INFO -  at 17.7s,	estimator xgboost's best error=0.0868,	best estimator xgboost's best error=0.0868
[flaml.automl: 09-18 08:22:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:22:46] {3072} INFO -  at 20.5s,	estimator xgboost's best error=0.0868,	best estimator xgboost's best error=0.0868
[flaml.automl: 09-18 08:22:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:22:50] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.0868,	best estimator xgboost's best error=0.0868
[flaml.automl: 09-18 08:22:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:22:52] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0868,	best estimator xgboost's best error=0.0868
[flaml.automl: 09-18 08:22:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:22:54] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.0868,	best estimator xgboost's best error=0.0868
[flaml.automl: 09-18 08:22:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:22:57] {3072} INFO -  at 31.3s,	estimator xgboost's best error=0.0866,	best estimator xgboost's best error=0.0866
[flaml.automl: 09-18 08:22:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:22:59] {3072} INFO -  at 33.6s,	estimator xgboost's best error=0.0866,	best estimator xgboost's best error=0.0866
[flaml.automl: 09-18 08:22:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:23:11] {3072} INFO -  at 45.0s,	estimator xgboost's best error=0.0827,	best estimator xgboost's best error=0.0827
[flaml.automl: 09-18 08:23:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:23:25] {3072} INFO -  at 58.8s,	estimator xgboost's best error=0.0803,	best estimator xgboost's best error=0.0803
[flaml.automl: 09-18 08:23:37] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 08:23:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:23:37] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:23:37] {2637} INFO - Time taken to find the best model: 58.788113594055176
[flaml.automl: 09-18 08:23:37] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51352}
CO(0)最佳损失：0.9197144957031653
CO(0)最好结果：{'pred_time': 6.947315161244058e-06, 'wall_clock_time': 58.788113594055176, 'metric_for_logging': {'pred_time': 6.947315161244058e-06}, 'val_loss': 0.08028550429683462, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51352}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51352, 'experiment_tag': 'exp', 'time_total_s': 13.794349908828735}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7516993494689921
CO(0)的mse=0.013969899736345647
CO(0)的mae=0.08132511885624955
CO(0)的mar=0.10854910945642833
总共花费的时间为：71.77
抚州市
2376A
2377A
2378A
2379A
2380A
[flaml.automl: 09-18 08:39:30] {2390} INFO - task = regression
[flaml.automl: 09-18 08:39:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:39:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:39:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:39:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:39:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:39:33] {3025} INFO - Estimated sufficient time budget=119578s. Estimated necessary time budget=120s.
[flaml.automl: 09-18 08:39:33] {3072} INFO -  at 2.7s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-18 08:39:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:39:36] {3072} INFO -  at 6.4s,	estimator xgboost's best error=0.0714,	best estimator xgboost's best error=0.0714
[flaml.automl: 09-18 08:39:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:39:38] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.0714,	best estimator xgboost's best error=0.0714
[flaml.automl: 09-18 08:39:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:39:42] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.0714,	best estimator xgboost's best error=0.0714
[flaml.automl: 09-18 08:39:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:39:44] {3072} INFO -  at 14.5s,	estimator xgboost's best error=0.0479,	best estimator xgboost's best error=0.0479
[flaml.automl: 09-18 08:39:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:39:47] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.0386,	best estimator xgboost's best error=0.0386
[flaml.automl: 09-18 08:39:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:39:50] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.0386,	best estimator xgboost's best error=0.0386
[flaml.automl: 09-18 08:39:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:39:53] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0386,	best estimator xgboost's best error=0.0386
[flaml.automl: 09-18 08:39:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:39:55] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.0386,	best estimator xgboost's best error=0.0386
[flaml.automl: 09-18 08:39:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:39:58] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0386,	best estimator xgboost's best error=0.0386
[flaml.automl: 09-18 08:39:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:40:00] {3072} INFO -  at 30.2s,	estimator xgboost's best error=0.0381,	best estimator xgboost's best error=0.0381
[flaml.automl: 09-18 08:40:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:40:02] {3072} INFO -  at 32.3s,	estimator xgboost's best error=0.0381,	best estimator xgboost's best error=0.0381
[flaml.automl: 09-18 08:40:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:40:11] {3072} INFO -  at 41.2s,	estimator xgboost's best error=0.0337,	best estimator xgboost's best error=0.0337
[flaml.automl: 09-18 08:40:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:40:23] {3072} INFO -  at 53.2s,	estimator xgboost's best error=0.0312,	best estimator xgboost's best error=0.0312
[flaml.automl: 09-18 08:40:43] {3335} INFO - retrain xgboost for 19.7s
[flaml.automl: 09-18 08:40:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:40:43] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:40:43] {2637} INFO - Time taken to find the best model: 53.207756996154785
[flaml.automl: 09-18 08:40:43] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53257}
CO(0)最佳损失：0.9687887629420623
CO(0)最好结果：{'pred_time': 6.9126884612573e-06, 'wall_clock_time': 53.207756996154785, 'metric_for_logging': {'pred_time': 6.9126884612573e-06}, 'val_loss': 0.031211237057937723, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53257}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53257, 'experiment_tag': 'exp', 'time_total_s': 12.055377006530762}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9301007020319343
CO(0)的mse=0.0024655452495842894
CO(0)的mae=0.03163963202161027
CO(0)的mar=0.048425910343227664
总共花费的时间为：73.95
上饶市
2381A
2382A
2383A
3685A
[flaml.automl: 09-18 08:53:17] {2390} INFO - task = regression
[flaml.automl: 09-18 08:53:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:53:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:53:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:53:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:53:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:53:18] {3025} INFO - Estimated sufficient time budget=50023s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 08:53:18] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1349,	best estimator xgboost's best error=0.1349
[flaml.automl: 09-18 08:53:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:53:20] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0756,	best estimator xgboost's best error=0.0756
[flaml.automl: 09-18 08:53:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:53:21] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0756,	best estimator xgboost's best error=0.0756
[flaml.automl: 09-18 08:53:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:53:28] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0756,	best estimator xgboost's best error=0.0756
[flaml.automl: 09-18 08:53:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:53:29] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0563,	best estimator xgboost's best error=0.0563
[flaml.automl: 09-18 08:53:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:53:30] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0455,	best estimator xgboost's best error=0.0455
[flaml.automl: 09-18 08:53:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:53:32] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0455,	best estimator xgboost's best error=0.0455
[flaml.automl: 09-18 08:53:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:53:34] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0455,	best estimator xgboost's best error=0.0455
[flaml.automl: 09-18 08:53:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:53:36] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0455,	best estimator xgboost's best error=0.0455
[flaml.automl: 09-18 08:53:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:53:38] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0455,	best estimator xgboost's best error=0.0455
[flaml.automl: 09-18 08:53:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:53:40] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0455,	best estimator xgboost's best error=0.0455
[flaml.automl: 09-18 08:53:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:53:41] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0455,	best estimator xgboost's best error=0.0455
[flaml.automl: 09-18 08:53:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:53:47] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0398,	best estimator xgboost's best error=0.0398
[flaml.automl: 09-18 08:53:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:54:00] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.0381,	best estimator xgboost's best error=0.0381
[flaml.automl: 09-18 08:54:00] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:54:06] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0381,	best estimator xgboost's best error=0.0381
[flaml.automl: 09-18 08:54:18] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 08:54:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:54:18] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:54:18] {2637} INFO - Time taken to find the best model: 43.0496461391449
[flaml.automl: 09-18 08:54:18] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41346}
CO(0)最佳损失：0.961889633277719
CO(0)最好结果：{'pred_time': 8.511138470939932e-06, 'wall_clock_time': 43.0496461391449, 'metric_for_logging': {'pred_time': 8.511138470939932e-06}, 'val_loss': 0.038110366722281, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41346}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41346, 'experiment_tag': 'exp', 'time_total_s': 12.083298206329346}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9353626302926273
CO(0)的mse=0.003016329723766573
CO(0)的mae=0.03862952007384538
CO(0)的mar=0.0699470692040148
总共花费的时间为：62.31
鹤壁市
2385A
2386A
2387A
3474A
3596A
[flaml.automl: 09-18 09:10:22] {2390} INFO - task = regression
[flaml.automl: 09-18 09:10:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:10:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:10:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:10:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:10:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:10:23] {3025} INFO - Estimated sufficient time budget=64364s. Estimated necessary time budget=64s.
[flaml.automl: 09-18 09:10:24] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.2723,	best estimator xgboost's best error=0.2723
[flaml.automl: 09-18 09:10:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:10:26] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.1698,	best estimator xgboost's best error=0.1698
[flaml.automl: 09-18 09:10:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:10:27] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.1698,	best estimator xgboost's best error=0.1698
[flaml.automl: 09-18 09:10:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:10:32] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.1698,	best estimator xgboost's best error=0.1698
[flaml.automl: 09-18 09:10:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:10:33] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.1430,	best estimator xgboost's best error=0.1430
[flaml.automl: 09-18 09:10:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:10:34] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.1263,	best estimator xgboost's best error=0.1263
[flaml.automl: 09-18 09:10:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:10:36] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.1263,	best estimator xgboost's best error=0.1263
[flaml.automl: 09-18 09:10:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:10:38] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.1263,	best estimator xgboost's best error=0.1263
[flaml.automl: 09-18 09:10:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:10:40] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.1263,	best estimator xgboost's best error=0.1263
[flaml.automl: 09-18 09:10:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:10:42] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.1263,	best estimator xgboost's best error=0.1263
[flaml.automl: 09-18 09:10:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:10:44] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.1260,	best estimator xgboost's best error=0.1260
[flaml.automl: 09-18 09:10:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:10:45] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.1260,	best estimator xgboost's best error=0.1260
[flaml.automl: 09-18 09:10:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:10:52] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.1210,	best estimator xgboost's best error=0.1210
[flaml.automl: 09-18 09:10:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:11:03] {3072} INFO -  at 41.5s,	estimator xgboost's best error=0.1177,	best estimator xgboost's best error=0.1177
[flaml.automl: 09-18 09:11:03] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 09:11:10] {3072} INFO -  at 47.9s,	estimator xgboost's best error=0.1177,	best estimator xgboost's best error=0.1177
[flaml.automl: 09-18 09:11:10] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 09:11:21] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.1167,	best estimator xgboost's best error=0.1167
[flaml.automl: 09-18 09:11:42] {3335} INFO - retrain xgboost for 21.3s
[flaml.automl: 09-18 09:11:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 09:11:42] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:11:42] {2637} INFO - Time taken to find the best model: 59.06830143928528
[flaml.automl: 09-18 09:11:42] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 52848}
CO(0)最佳损失：0.8832796699001703
CO(0)最好结果：{'pred_time': 6.860024718354918e-06, 'wall_clock_time': 59.06830143928528, 'metric_for_logging': {'pred_time': 6.860024718354918e-06}, 'val_loss': 0.11672033009982974, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 52848}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 52848, 'experiment_tag': 'exp', 'time_total_s': 11.188965559005737}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7417095655120347
CO(0)的mse=0.040894906627175126
CO(0)的mae=0.12353427519879581
CO(0)的mar=0.1920873154124242
总共花费的时间为：81.25
新乡市
2390A
2391A
3054A
3475A
3476A
[flaml.automl: 09-18 09:28:12] {2390} INFO - task = regression
[flaml.automl: 09-18 09:28:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:28:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:28:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:28:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:28:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:28:13] {3025} INFO - Estimated sufficient time budget=61419s. Estimated necessary time budget=61s.
[flaml.automl: 09-18 09:28:13] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.2362,	best estimator xgboost's best error=0.2362
[flaml.automl: 09-18 09:28:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:28:15] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1277,	best estimator xgboost's best error=0.1277
[flaml.automl: 09-18 09:28:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:28:16] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1277,	best estimator xgboost's best error=0.1277
[flaml.automl: 09-18 09:28:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:28:22] {3072} INFO -  at 10.1s,	estimator xgboost's best error=0.1277,	best estimator xgboost's best error=0.1277
[flaml.automl: 09-18 09:28:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:28:23] {3072} INFO -  at 11.2s,	estimator xgboost's best error=0.0952,	best estimator xgboost's best error=0.0952
[flaml.automl: 09-18 09:28:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:28:24] {3072} INFO -  at 12.8s,	estimator xgboost's best error=0.0791,	best estimator xgboost's best error=0.0791
[flaml.automl: 09-18 09:28:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:28:26] {3072} INFO -  at 14.4s,	estimator xgboost's best error=0.0791,	best estimator xgboost's best error=0.0791
[flaml.automl: 09-18 09:28:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:28:30] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.0791,	best estimator xgboost's best error=0.0791
[flaml.automl: 09-18 09:28:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:28:32] {3072} INFO -  at 19.9s,	estimator xgboost's best error=0.0791,	best estimator xgboost's best error=0.0791
[flaml.automl: 09-18 09:28:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:28:35] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0791,	best estimator xgboost's best error=0.0791
[flaml.automl: 09-18 09:28:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:28:38] {3072} INFO -  at 26.3s,	estimator xgboost's best error=0.0791,	best estimator xgboost's best error=0.0791
[flaml.automl: 09-18 09:28:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:28:40] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.0791,	best estimator xgboost's best error=0.0791
[flaml.automl: 09-18 09:28:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:28:56] {3072} INFO -  at 44.6s,	estimator xgboost's best error=0.0704,	best estimator xgboost's best error=0.0704
[flaml.automl: 09-18 09:29:15] {3335} INFO - retrain xgboost for 18.3s
[flaml.automl: 09-18 09:29:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 09:29:15] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:29:15] {2637} INFO - Time taken to find the best model: 44.61488747596741
[flaml.automl: 09-18 09:29:15] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 51238}
CO(0)最佳损失：0.9296205620613796
CO(0)最好结果：{'pred_time': 1.801701650562561e-05, 'wall_clock_time': 44.61488747596741, 'metric_for_logging': {'pred_time': 1.801701650562561e-05}, 'val_loss': 0.07037943793862043, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 51238}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 51238, 'experiment_tag': 'exp', 'time_total_s': 16.238038063049316}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8988966119339176
CO(0)的mse=0.01117636405268587
CO(0)的mae=0.06957911906767562
CO(0)的mar=0.11038125157788944
总共花费的时间为：63.98
濮阳市
2392A
2395A
3021A
[flaml.automl: 09-18 09:38:06] {2390} INFO - task = regression
[flaml.automl: 09-18 09:38:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:38:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:38:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:38:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:38:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:38:07] {3025} INFO - Estimated sufficient time budget=12117s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 09:38:07] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1506,	best estimator xgboost's best error=0.1506
[flaml.automl: 09-18 09:38:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:38:09] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0861,	best estimator xgboost's best error=0.0861
[flaml.automl: 09-18 09:38:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:38:10] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0861,	best estimator xgboost's best error=0.0861
[flaml.automl: 09-18 09:38:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:38:20] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0861,	best estimator xgboost's best error=0.0861
[flaml.automl: 09-18 09:38:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:38:21] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0663,	best estimator xgboost's best error=0.0663
[flaml.automl: 09-18 09:38:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:38:23] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0520,	best estimator xgboost's best error=0.0520
[flaml.automl: 09-18 09:38:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:38:25] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0520,	best estimator xgboost's best error=0.0520
[flaml.automl: 09-18 09:38:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:38:27] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0520,	best estimator xgboost's best error=0.0520
[flaml.automl: 09-18 09:38:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:38:28] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0520,	best estimator xgboost's best error=0.0520
[flaml.automl: 09-18 09:38:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:38:31] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0520,	best estimator xgboost's best error=0.0520
[flaml.automl: 09-18 09:38:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:38:32] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0520,	best estimator xgboost's best error=0.0520
[flaml.automl: 09-18 09:38:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:38:33] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0520,	best estimator xgboost's best error=0.0520
[flaml.automl: 09-18 09:38:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:38:40] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0458,	best estimator xgboost's best error=0.0458
[flaml.automl: 09-18 09:38:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:38:52] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.0433,	best estimator xgboost's best error=0.0433
[flaml.automl: 09-18 09:38:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 09:38:58] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.0433,	best estimator xgboost's best error=0.0433
[flaml.automl: 09-18 09:39:10] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 09:39:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 09:39:10] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:39:10] {2637} INFO - Time taken to find the best model: 46.367453813552856
[flaml.automl: 09-18 09:39:10] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9566934368755697
CO(0)最好结果：{'pred_time': 1.1448190876166204e-05, 'wall_clock_time': 46.367453813552856, 'metric_for_logging': {'pred_time': 1.1448190876166204e-05}, 'val_loss': 0.043306563124430215, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.096192836761475}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9309765184066872
CO(0)的mse=0.004467157147130945
CO(0)的mae=0.04212803027631666
CO(0)的mar=0.06950470846826755
总共花费的时间为：65.55
许昌市
2396A
3134A
3337A
3338A
3597A
[flaml.automl: 09-18 09:55:55] {2390} INFO - task = regression
[flaml.automl: 09-18 09:55:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:55:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:55:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:55:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:55:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:55:58] {3025} INFO - Estimated sufficient time budget=160953s. Estimated necessary time budget=161s.
[flaml.automl: 09-18 09:55:58] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.1832,	best estimator xgboost's best error=0.1832
[flaml.automl: 09-18 09:55:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:56:02] {3072} INFO -  at 6.8s,	estimator xgboost's best error=0.0951,	best estimator xgboost's best error=0.0951
[flaml.automl: 09-18 09:56:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:56:03] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.0951,	best estimator xgboost's best error=0.0951
[flaml.automl: 09-18 09:56:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:56:08] {3072} INFO -  at 13.2s,	estimator xgboost's best error=0.0951,	best estimator xgboost's best error=0.0951
[flaml.automl: 09-18 09:56:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:56:10] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0646,	best estimator xgboost's best error=0.0646
[flaml.automl: 09-18 09:56:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:56:13] {3072} INFO -  at 18.2s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-18 09:56:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:56:16] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-18 09:56:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:56:18] {3072} INFO -  at 23.6s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-18 09:56:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:56:21] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-18 09:56:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:56:23] {3072} INFO -  at 28.2s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-18 09:56:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:56:25] {3072} INFO -  at 29.8s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-18 09:56:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:56:26] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-18 09:56:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:56:32] {3072} INFO -  at 37.5s,	estimator xgboost's best error=0.0430,	best estimator xgboost's best error=0.0430
[flaml.automl: 09-18 09:56:32] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:56:44] {3072} INFO -  at 49.7s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 09:56:57] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 09:56:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 09:56:57] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:56:57] {2637} INFO - Time taken to find the best model: 49.708253383636475
[flaml.automl: 09-18 09:56:57] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54506}
CO(0)最佳损失：0.957449903181309
CO(0)最好结果：{'pred_time': 6.641396126850261e-06, 'wall_clock_time': 49.708253383636475, 'metric_for_logging': {'pred_time': 6.641396126850261e-06}, 'val_loss': 0.04255009681869108, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54506}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54506, 'experiment_tag': 'exp', 'time_total_s': 12.163549184799194}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9344687144892488
CO(0)的mse=0.0038577979915639113
CO(0)的mae=0.040634804023197815
CO(0)的mar=0.05557052020027177
总共花费的时间为：63.00
漯河市
2399A
2400A
2401A
2402A
3478A
3479A
[flaml.automl: 09-18 10:16:19] {2390} INFO - task = regression
[flaml.automl: 09-18 10:16:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:16:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:16:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:16:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:16:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:16:20] {3025} INFO - Estimated sufficient time budget=77565s. Estimated necessary time budget=78s.
[flaml.automl: 09-18 10:16:20] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.1422,	best estimator xgboost's best error=0.1422
[flaml.automl: 09-18 10:16:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:16:22] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.0780,	best estimator xgboost's best error=0.0780
[flaml.automl: 09-18 10:16:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:16:24] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.0780,	best estimator xgboost's best error=0.0780
[flaml.automl: 09-18 10:16:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:16:27] {3072} INFO -  at 8.7s,	estimator xgboost's best error=0.0780,	best estimator xgboost's best error=0.0780
[flaml.automl: 09-18 10:16:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:16:29] {3072} INFO -  at 9.8s,	estimator xgboost's best error=0.0565,	best estimator xgboost's best error=0.0565
[flaml.automl: 09-18 10:16:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:16:30] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-18 10:16:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:16:32] {3072} INFO -  at 13.0s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-18 10:16:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:16:34] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-18 10:16:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:16:35] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-18 10:16:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:16:38] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-18 10:16:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:16:40] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-18 10:16:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:16:41] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-18 10:16:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:16:47] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.0378,	best estimator xgboost's best error=0.0378
[flaml.automl: 09-18 10:16:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:16:59] {3072} INFO -  at 40.5s,	estimator xgboost's best error=0.0374,	best estimator xgboost's best error=0.0374
[flaml.automl: 09-18 10:16:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 10:17:06] {3072} INFO -  at 47.1s,	estimator xgboost's best error=0.0374,	best estimator xgboost's best error=0.0374
[flaml.automl: 09-18 10:17:06] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 10:17:18] {3072} INFO -  at 59.6s,	estimator xgboost's best error=0.0370,	best estimator xgboost's best error=0.0370
[flaml.automl: 09-18 10:17:40] {3335} INFO - retrain xgboost for 21.4s
[flaml.automl: 09-18 10:17:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:17:40] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:17:40] {2637} INFO - Time taken to find the best model: 59.613025426864624
[flaml.automl: 09-18 10:17:40] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 64332}
CO(0)最佳损失：0.9629582534762288
CO(0)最好结果：{'pred_time': 5.699398134697998e-06, 'wall_clock_time': 59.613025426864624, 'metric_for_logging': {'pred_time': 5.699398134697998e-06}, 'val_loss': 0.03704174652377119, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 64332}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 64332, 'experiment_tag': 'exp', 'time_total_s': 12.533207893371582}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.937296152073707
CO(0)的mse=0.0028821189283111653
CO(0)的mae=0.036390909033084
CO(0)的mar=0.0578346089805363
总共花费的时间为：82.19
南阳市
2403A
2404A
2405A
2406A
2407A
[flaml.automl: 09-18 10:33:16] {2390} INFO - task = regression
[flaml.automl: 09-18 10:33:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:33:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:33:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:33:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:33:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:33:18] {3025} INFO - Estimated sufficient time budget=65286s. Estimated necessary time budget=65s.
[flaml.automl: 09-18 10:33:18] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.2168,	best estimator xgboost's best error=0.2168
[flaml.automl: 09-18 10:33:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:33:20] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.1069,	best estimator xgboost's best error=0.1069
[flaml.automl: 09-18 10:33:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:33:21] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.1069,	best estimator xgboost's best error=0.1069
[flaml.automl: 09-18 10:33:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:33:26] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.1069,	best estimator xgboost's best error=0.1069
[flaml.automl: 09-18 10:33:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:33:27] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0629,	best estimator xgboost's best error=0.0629
[flaml.automl: 09-18 10:33:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:33:29] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0515,	best estimator xgboost's best error=0.0515
[flaml.automl: 09-18 10:33:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:33:30] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0515,	best estimator xgboost's best error=0.0515
[flaml.automl: 09-18 10:33:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:33:33] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0515,	best estimator xgboost's best error=0.0515
[flaml.automl: 09-18 10:33:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:33:34] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0515,	best estimator xgboost's best error=0.0515
[flaml.automl: 09-18 10:33:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:33:36] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0515,	best estimator xgboost's best error=0.0515
[flaml.automl: 09-18 10:33:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:33:38] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 10:33:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:33:39] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 10:33:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:33:46] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0418,	best estimator xgboost's best error=0.0418
[flaml.automl: 09-18 10:33:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:34:01] {3072} INFO -  at 44.7s,	estimator xgboost's best error=0.0404,	best estimator xgboost's best error=0.0404
[flaml.automl: 09-18 10:34:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 10:34:13] {3072} INFO -  at 56.4s,	estimator xgboost's best error=0.0404,	best estimator xgboost's best error=0.0404
[flaml.automl: 09-18 10:34:35] {3335} INFO - retrain xgboost for 22.2s
[flaml.automl: 09-18 10:34:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:34:35] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:34:35] {2637} INFO - Time taken to find the best model: 44.67897653579712
[flaml.automl: 09-18 10:34:35] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54220}
CO(0)最佳损失：0.9595504397779283
CO(0)最好结果：{'pred_time': 1.2050407061438342e-05, 'wall_clock_time': 44.67897653579712, 'metric_for_logging': {'pred_time': 1.2050407061438342e-05}, 'val_loss': 0.04044956022207173, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 54220}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 54220, 'experiment_tag': 'exp', 'time_total_s': 15.11719799041748}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9440660175390306
CO(0)的mse=0.0038665346528167716
CO(0)的mae=0.03934976744796042
CO(0)的mar=0.05425698006721774
总共花费的时间为：79.60
商丘市
2408A
2409A
2410A
[flaml.automl: 09-18 10:44:23] {2390} INFO - task = regression
[flaml.automl: 09-18 10:44:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:44:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:44:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:44:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:44:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:44:27] {3025} INFO - Estimated sufficient time budget=34818s. Estimated necessary time budget=35s.
[flaml.automl: 09-18 10:44:27] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.1386,	best estimator xgboost's best error=0.1386
[flaml.automl: 09-18 10:44:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:44:33] {3072} INFO -  at 9.8s,	estimator xgboost's best error=0.0779,	best estimator xgboost's best error=0.0779
[flaml.automl: 09-18 10:44:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:44:36] {3072} INFO -  at 13.2s,	estimator xgboost's best error=0.0779,	best estimator xgboost's best error=0.0779
[flaml.automl: 09-18 10:44:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:44:53] {3072} INFO -  at 30.2s,	estimator xgboost's best error=0.0779,	best estimator xgboost's best error=0.0779
[flaml.automl: 09-18 10:44:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:44:55] {3072} INFO -  at 31.4s,	estimator xgboost's best error=0.0591,	best estimator xgboost's best error=0.0591
[flaml.automl: 09-18 10:44:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:44:56] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 10:44:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:44:58] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 10:44:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:45:00] {3072} INFO -  at 37.0s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 10:45:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:45:01] {3072} INFO -  at 38.2s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 10:45:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:45:04] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 10:45:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:45:05] {3072} INFO -  at 41.8s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 10:45:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:45:06] {3072} INFO -  at 42.9s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 10:45:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:45:12] {3072} INFO -  at 49.2s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-18 10:45:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:45:23] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.0396,	best estimator xgboost's best error=0.0396
[flaml.automl: 09-18 10:45:35] {3335} INFO - retrain xgboost for 11.8s
[flaml.automl: 09-18 10:45:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:45:35] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:45:35] {2637} INFO - Time taken to find the best model: 59.80996823310852
[flaml.automl: 09-18 10:45:35] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9604053474145882
CO(0)最好结果：{'pred_time': 1.1084646508223858e-05, 'wall_clock_time': 59.80996823310852, 'metric_for_logging': {'pred_time': 1.1084646508223858e-05}, 'val_loss': 0.039594652585411796, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.559500217437744}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9355245036507877
CO(0)的mse=0.003615403551603916
CO(0)的mae=0.03987759893646386
CO(0)的mar=0.07391692446919075
总共花费的时间为：72.30
驻马店市
2420A
2421A
2422A
3339A
[flaml.automl: 09-18 10:58:38] {2390} INFO - task = regression
[flaml.automl: 09-18 10:58:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:58:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:58:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:58:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:58:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:58:41] {3025} INFO - Estimated sufficient time budget=117122s. Estimated necessary time budget=117s.
[flaml.automl: 09-18 10:58:41] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.1354,	best estimator xgboost's best error=0.1354
[flaml.automl: 09-18 10:58:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:58:45] {3072} INFO -  at 7.8s,	estimator xgboost's best error=0.0807,	best estimator xgboost's best error=0.0807
[flaml.automl: 09-18 10:58:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:58:48] {3072} INFO -  at 10.3s,	estimator xgboost's best error=0.0807,	best estimator xgboost's best error=0.0807
[flaml.automl: 09-18 10:58:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:58:53] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0807,	best estimator xgboost's best error=0.0807
[flaml.automl: 09-18 10:58:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:58:55] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0623,	best estimator xgboost's best error=0.0623
[flaml.automl: 09-18 10:58:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:58:59] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.0532,	best estimator xgboost's best error=0.0532
[flaml.automl: 09-18 10:58:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:59:02] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0532,	best estimator xgboost's best error=0.0532
[flaml.automl: 09-18 10:59:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:59:06] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.0532,	best estimator xgboost's best error=0.0532
[flaml.automl: 09-18 10:59:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:59:09] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0532,	best estimator xgboost's best error=0.0532
[flaml.automl: 09-18 10:59:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:59:12] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0532,	best estimator xgboost's best error=0.0532
[flaml.automl: 09-18 10:59:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:59:16] {3072} INFO -  at 38.1s,	estimator xgboost's best error=0.0527,	best estimator xgboost's best error=0.0527
[flaml.automl: 09-18 10:59:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:59:18] {3072} INFO -  at 40.5s,	estimator xgboost's best error=0.0527,	best estimator xgboost's best error=0.0527
[flaml.automl: 09-18 10:59:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:59:31] {3072} INFO -  at 53.6s,	estimator xgboost's best error=0.0456,	best estimator xgboost's best error=0.0456
[flaml.automl: 09-18 10:59:38] {3335} INFO - retrain xgboost for 6.4s
[flaml.automl: 09-18 10:59:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 10:59:38] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:59:38] {2637} INFO - Time taken to find the best model: 53.638099670410156
[flaml.automl: 09-18 10:59:38] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 42746}
CO(0)最佳损失：0.9543741185612459
CO(0)最好结果：{'pred_time': 8.369897541246916e-06, 'wall_clock_time': 53.638099670410156, 'metric_for_logging': {'pred_time': 8.369897541246916e-06}, 'val_loss': 0.04562588143875411, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 42746}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 42746, 'experiment_tag': 'exp', 'time_total_s': 13.115580558776855}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8917791320858057
CO(0)的mse=0.005945525944861646
CO(0)的mae=0.046427203069303744
CO(0)的mar=0.0932647695547855
总共花费的时间为：60.97
黄石市
2423A
2424A
2427A
3149A
[flaml.automl: 09-18 11:13:04] {2390} INFO - task = regression
[flaml.automl: 09-18 11:13:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:13:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:13:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:13:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:13:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:13:05] {3025} INFO - Estimated sufficient time budget=51235s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 11:13:05] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2963,	best estimator xgboost's best error=0.2963
[flaml.automl: 09-18 11:13:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:13:07] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1573,	best estimator xgboost's best error=0.1573
[flaml.automl: 09-18 11:13:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:13:08] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1573,	best estimator xgboost's best error=0.1573
[flaml.automl: 09-18 11:13:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:13:14] {3072} INFO -  at 10.4s,	estimator xgboost's best error=0.1573,	best estimator xgboost's best error=0.1573
[flaml.automl: 09-18 11:13:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:13:15] {3072} INFO -  at 11.6s,	estimator xgboost's best error=0.1146,	best estimator xgboost's best error=0.1146
[flaml.automl: 09-18 11:13:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:13:17] {3072} INFO -  at 13.1s,	estimator xgboost's best error=0.1029,	best estimator xgboost's best error=0.1029
[flaml.automl: 09-18 11:13:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:13:18] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.1029,	best estimator xgboost's best error=0.1029
[flaml.automl: 09-18 11:13:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:13:21] {3072} INFO -  at 17.2s,	estimator xgboost's best error=0.1029,	best estimator xgboost's best error=0.1029
[flaml.automl: 09-18 11:13:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:13:22] {3072} INFO -  at 18.3s,	estimator xgboost's best error=0.1029,	best estimator xgboost's best error=0.1029
[flaml.automl: 09-18 11:13:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:13:25] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.1029,	best estimator xgboost's best error=0.1029
[flaml.automl: 09-18 11:13:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:13:26] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.1023,	best estimator xgboost's best error=0.1023
[flaml.automl: 09-18 11:13:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:13:28] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.1023,	best estimator xgboost's best error=0.1023
[flaml.automl: 09-18 11:13:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:13:34] {3072} INFO -  at 30.3s,	estimator xgboost's best error=0.0961,	best estimator xgboost's best error=0.0961
[flaml.automl: 09-18 11:13:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:13:46] {3072} INFO -  at 42.3s,	estimator xgboost's best error=0.0941,	best estimator xgboost's best error=0.0941
[flaml.automl: 09-18 11:13:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 11:13:53] {3072} INFO -  at 48.9s,	estimator xgboost's best error=0.0941,	best estimator xgboost's best error=0.0941
[flaml.automl: 09-18 11:14:05] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 11:14:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:14:05] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:14:05] {2637} INFO - Time taken to find the best model: 42.346877336502075
[flaml.automl: 09-18 11:14:05] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43538}
CO(0)最佳损失：0.9058691351029833
CO(0)最好结果：{'pred_time': 8.625745674754627e-06, 'wall_clock_time': 42.346877336502075, 'metric_for_logging': {'pred_time': 8.625745674754627e-06}, 'val_loss': 0.09413086489701675, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43538}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43538, 'experiment_tag': 'exp', 'time_total_s': 12.088553428649902}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7870691306258686
CO(0)的mse=0.022342715962946395
CO(0)的mae=0.09387449020785946
CO(0)的mar=0.10497288980329875
总共花费的时间为：61.59
十堰市
2428A
2429A
2430A
2431A
3545A
[flaml.automl: 09-18 11:29:12] {2390} INFO - task = regression
[flaml.automl: 09-18 11:29:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:29:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:29:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:29:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:29:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:29:15] {3025} INFO - Estimated sufficient time budget=153046s. Estimated necessary time budget=153s.
[flaml.automl: 09-18 11:29:15] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.1382,	best estimator xgboost's best error=0.1382
[flaml.automl: 09-18 11:29:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:29:20] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.0789,	best estimator xgboost's best error=0.0789
[flaml.automl: 09-18 11:29:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:29:22] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.0789,	best estimator xgboost's best error=0.0789
[flaml.automl: 09-18 11:29:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:29:26] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0789,	best estimator xgboost's best error=0.0789
[flaml.automl: 09-18 11:29:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:29:29] {3072} INFO -  at 17.7s,	estimator xgboost's best error=0.0600,	best estimator xgboost's best error=0.0600
[flaml.automl: 09-18 11:29:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:29:33] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0523,	best estimator xgboost's best error=0.0523
[flaml.automl: 09-18 11:29:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:29:35] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0523,	best estimator xgboost's best error=0.0523
[flaml.automl: 09-18 11:29:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:29:37] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0523,	best estimator xgboost's best error=0.0523
[flaml.automl: 09-18 11:29:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:29:38] {3072} INFO -  at 26.8s,	estimator xgboost's best error=0.0523,	best estimator xgboost's best error=0.0523
[flaml.automl: 09-18 11:29:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:29:41] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0523,	best estimator xgboost's best error=0.0523
[flaml.automl: 09-18 11:29:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:29:42] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-18 11:29:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:29:44] {3072} INFO -  at 32.3s,	estimator xgboost's best error=0.0516,	best estimator xgboost's best error=0.0516
[flaml.automl: 09-18 11:29:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:29:50] {3072} INFO -  at 38.8s,	estimator xgboost's best error=0.0456,	best estimator xgboost's best error=0.0456
[flaml.automl: 09-18 11:29:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:30:02] {3072} INFO -  at 50.8s,	estimator xgboost's best error=0.0431,	best estimator xgboost's best error=0.0431
[flaml.automl: 09-18 11:30:14] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 11:30:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:30:14] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:30:14] {2637} INFO - Time taken to find the best model: 50.80722117424011
[flaml.automl: 09-18 11:30:14] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53134}
CO(0)最佳损失：0.9569079440057803
CO(0)最好结果：{'pred_time': 6.910060156328568e-06, 'wall_clock_time': 50.80722117424011, 'metric_for_logging': {'pred_time': 6.910060156328568e-06}, 'val_loss': 0.04309205599421974, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53134}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53134, 'experiment_tag': 'exp', 'time_total_s': 12.04546070098877}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8930467381984367
CO(0)的mse=0.0045429293020038865
CO(0)的mae=0.042556628903274135
CO(0)的mar=0.06820767876496138
总共花费的时间为：63.97
襄阳市
2432A
2433A
2434A
2435A
3396A
3397A
[flaml.automl: 09-18 11:49:09] {2390} INFO - task = regression
[flaml.automl: 09-18 11:49:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:49:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:49:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:49:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:49:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:49:11] {3025} INFO - Estimated sufficient time budget=144765s. Estimated necessary time budget=145s.
[flaml.automl: 09-18 11:49:11] {3072} INFO -  at 2.7s,	estimator xgboost's best error=0.1862,	best estimator xgboost's best error=0.1862
[flaml.automl: 09-18 11:49:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:49:15] {3072} INFO -  at 6.6s,	estimator xgboost's best error=0.0929,	best estimator xgboost's best error=0.0929
[flaml.automl: 09-18 11:49:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:49:17] {3072} INFO -  at 8.8s,	estimator xgboost's best error=0.0929,	best estimator xgboost's best error=0.0929
[flaml.automl: 09-18 11:49:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:49:20] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.0929,	best estimator xgboost's best error=0.0929
[flaml.automl: 09-18 11:49:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:49:22] {3072} INFO -  at 13.3s,	estimator xgboost's best error=0.0574,	best estimator xgboost's best error=0.0574
[flaml.automl: 09-18 11:49:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:49:23] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-18 11:49:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:49:25] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-18 11:49:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:49:27] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-18 11:49:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:49:28] {3072} INFO -  at 20.0s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-18 11:49:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:49:31] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0453,	best estimator xgboost's best error=0.0453
[flaml.automl: 09-18 11:49:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:49:34] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0451,	best estimator xgboost's best error=0.0451
[flaml.automl: 09-18 11:49:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:49:36] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.0451,	best estimator xgboost's best error=0.0451
[flaml.automl: 09-18 11:49:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:49:47] {3072} INFO -  at 38.6s,	estimator xgboost's best error=0.0377,	best estimator xgboost's best error=0.0377
[flaml.automl: 09-18 11:49:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:50:07] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.0369,	best estimator xgboost's best error=0.0369
[flaml.automl: 09-18 11:50:28] {3335} INFO - retrain xgboost for 20.1s
[flaml.automl: 09-18 11:50:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:50:28] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:50:28] {2637} INFO - Time taken to find the best model: 59.01671648025513
[flaml.automl: 09-18 11:50:28] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65864}
CO(0)最佳损失：0.9630811916904414
CO(0)最好结果：{'pred_time': 1.1635145898040551e-05, 'wall_clock_time': 59.01671648025513, 'metric_for_logging': {'pred_time': 1.1635145898040551e-05}, 'val_loss': 0.03691880830955857, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 65864}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 65864, 'experiment_tag': 'exp', 'time_total_s': 20.46079397201538}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9376304089977491
CO(0)的mse=0.0027767509722618463
CO(0)的mae=0.03529514704270135
CO(0)的mar=0.048281128871033745
总共花费的时间为：80.22
鄂州市
2436A
2437A
[flaml.automl: 09-18 11:56:40] {2390} INFO - task = regression
[flaml.automl: 09-18 11:56:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:56:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:56:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:56:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:56:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:56:41] {3025} INFO - Estimated sufficient time budget=12125s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 11:56:41] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.2090,	best estimator xgboost's best error=0.2090
[flaml.automl: 09-18 11:56:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:56:43] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1172,	best estimator xgboost's best error=0.1172
[flaml.automl: 09-18 11:56:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:56:44] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.1172,	best estimator xgboost's best error=0.1172
[flaml.automl: 09-18 11:56:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:56:54] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.1172,	best estimator xgboost's best error=0.1172
[flaml.automl: 09-18 11:56:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:56:55] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0885,	best estimator xgboost's best error=0.0885
[flaml.automl: 09-18 11:56:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:56:56] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0790,	best estimator xgboost's best error=0.0790
[flaml.automl: 09-18 11:56:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:56:58] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0790,	best estimator xgboost's best error=0.0790
[flaml.automl: 09-18 11:56:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:57:01] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0790,	best estimator xgboost's best error=0.0790
[flaml.automl: 09-18 11:57:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:57:02] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0790,	best estimator xgboost's best error=0.0790
[flaml.automl: 09-18 11:57:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:57:04] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.0790,	best estimator xgboost's best error=0.0790
[flaml.automl: 09-18 11:57:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:57:05] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0790,	best estimator xgboost's best error=0.0790
[flaml.automl: 09-18 11:57:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:57:07] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.0790,	best estimator xgboost's best error=0.0790
[flaml.automl: 09-18 11:57:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:57:13] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0770,	best estimator xgboost's best error=0.0770
[flaml.automl: 09-18 11:57:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:57:23] {3072} INFO -  at 43.5s,	estimator xgboost's best error=0.0736,	best estimator xgboost's best error=0.0736
[flaml.automl: 09-18 11:57:23] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 11:57:29] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0736,	best estimator xgboost's best error=0.0736
[flaml.automl: 09-18 11:57:40] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 11:57:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:57:40] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:57:40] {2637} INFO - Time taken to find the best model: 43.47868013381958
[flaml.automl: 09-18 11:57:40] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9264347829627209
CO(0)最好结果：{'pred_time': 1.8642378635093814e-05, 'wall_clock_time': 43.47868013381958, 'metric_for_logging': {'pred_time': 1.8642378635093814e-05}, 'val_loss': 0.0735652170372791, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.470948457717896}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7884054212034385
CO(0)的mse=0.012634837616349152
CO(0)的mae=0.07209882277619521
CO(0)的mar=0.09936620685818882
总共花费的时间为：60.35
荆门市
2439A
2440A
2441A
3547A
[flaml.automl: 09-18 12:10:18] {2390} INFO - task = regression
[flaml.automl: 09-18 12:10:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:10:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:10:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:10:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:10:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:10:21] {3025} INFO - Estimated sufficient time budget=112581s. Estimated necessary time budget=113s.
[flaml.automl: 09-18 12:10:21] {3072} INFO -  at 2.9s,	estimator xgboost's best error=0.1531,	best estimator xgboost's best error=0.1531
[flaml.automl: 09-18 12:10:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:10:26] {3072} INFO -  at 7.6s,	estimator xgboost's best error=0.0930,	best estimator xgboost's best error=0.0930
[flaml.automl: 09-18 12:10:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:10:29] {3072} INFO -  at 10.5s,	estimator xgboost's best error=0.0930,	best estimator xgboost's best error=0.0930
[flaml.automl: 09-18 12:10:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:10:33] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.0930,	best estimator xgboost's best error=0.0930
[flaml.automl: 09-18 12:10:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:10:35] {3072} INFO -  at 17.2s,	estimator xgboost's best error=0.0747,	best estimator xgboost's best error=0.0747
[flaml.automl: 09-18 12:10:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:10:38] {3072} INFO -  at 19.9s,	estimator xgboost's best error=0.0670,	best estimator xgboost's best error=0.0670
[flaml.automl: 09-18 12:10:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:10:41] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0670,	best estimator xgboost's best error=0.0670
[flaml.automl: 09-18 12:10:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:10:45] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.0670,	best estimator xgboost's best error=0.0670
[flaml.automl: 09-18 12:10:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:10:47] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.0670,	best estimator xgboost's best error=0.0670
[flaml.automl: 09-18 12:10:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:10:51] {3072} INFO -  at 32.6s,	estimator xgboost's best error=0.0670,	best estimator xgboost's best error=0.0670
[flaml.automl: 09-18 12:10:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:10:54] {3072} INFO -  at 35.7s,	estimator xgboost's best error=0.0670,	best estimator xgboost's best error=0.0670
[flaml.automl: 09-18 12:10:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:10:56] {3072} INFO -  at 37.6s,	estimator xgboost's best error=0.0670,	best estimator xgboost's best error=0.0670
[flaml.automl: 09-18 12:10:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:11:07] {3072} INFO -  at 48.9s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-18 12:11:18] {3335} INFO - retrain xgboost for 11.4s
[flaml.automl: 09-18 12:11:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 12:11:18] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:11:18] {2637} INFO - Time taken to find the best model: 48.93457341194153
[flaml.automl: 09-18 12:11:18] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 42601}
CO(0)最佳损失：0.9375446873890936
CO(0)最好结果：{'pred_time': 1.3647776492094258e-05, 'wall_clock_time': 48.93457341194153, 'metric_for_logging': {'pred_time': 1.3647776492094258e-05}, 'val_loss': 0.06245531261090644, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 42601}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 42601, 'experiment_tag': 'exp', 'time_total_s': 11.324187517166138}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7758698283174146
CO(0)的mse=0.010196028185252531
CO(0)的mae=0.0620751403799755
CO(0)的mar=0.09718534496238307
总共花费的时间为：61.13
孝感市
2443A
2444A
[flaml.automl: 09-18 12:17:42] {2390} INFO - task = regression
[flaml.automl: 09-18 12:17:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:17:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:17:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:17:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:17:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:17:44] {3025} INFO - Estimated sufficient time budget=20471s. Estimated necessary time budget=20s.
[flaml.automl: 09-18 12:17:44] {3072} INFO -  at 2.2s,	estimator xgboost's best error=0.2468,	best estimator xgboost's best error=0.2468
[flaml.automl: 09-18 12:17:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:17:48] {3072} INFO -  at 5.9s,	estimator xgboost's best error=0.1214,	best estimator xgboost's best error=0.1214
[flaml.automl: 09-18 12:17:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:17:50] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.1214,	best estimator xgboost's best error=0.1214
[flaml.automl: 09-18 12:17:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:18:07] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.1214,	best estimator xgboost's best error=0.1214
[flaml.automl: 09-18 12:18:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:18:09] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.0765,	best estimator xgboost's best error=0.0765
[flaml.automl: 09-18 12:18:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:18:12] {3072} INFO -  at 30.4s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-18 12:18:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:18:15] {3072} INFO -  at 33.5s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-18 12:18:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:18:20] {3072} INFO -  at 38.0s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-18 12:18:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:18:22] {3072} INFO -  at 40.2s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-18 12:18:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:18:26] {3072} INFO -  at 44.7s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-18 12:18:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:18:29] {3072} INFO -  at 46.9s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-18 12:18:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:18:31] {3072} INFO -  at 49.0s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-18 12:18:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:18:41] {3072} INFO -  at 58.9s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-18 12:18:47] {3335} INFO - retrain xgboost for 6.5s
[flaml.automl: 09-18 12:18:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 12:18:47] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:18:47] {2637} INFO - Time taken to find the best model: 58.88552165031433
[flaml.automl: 09-18 12:18:47] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.942815052883896
CO(0)最好结果：{'pred_time': 3.128478688874015e-05, 'wall_clock_time': 58.88552165031433, 'metric_for_logging': {'pred_time': 3.128478688874015e-05}, 'val_loss': 0.057184947116104, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.861940145492554}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.883687751092442
CO(0)的mse=0.008658056939622717
CO(0)的mae=0.05793610823895671
CO(0)的mar=0.06715654819502119
总共花费的时间为：65.77
黄冈市
2929A
3398A
[flaml.automl: 09-18 12:24:55] {2390} INFO - task = regression
[flaml.automl: 09-18 12:24:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:24:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:24:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:24:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:24:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:24:56] {3025} INFO - Estimated sufficient time budget=12081s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 12:24:56] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1661,	best estimator xgboost's best error=0.1661
[flaml.automl: 09-18 12:24:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:24:59] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0922,	best estimator xgboost's best error=0.0922
[flaml.automl: 09-18 12:24:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:25:00] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0922,	best estimator xgboost's best error=0.0922
[flaml.automl: 09-18 12:25:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:25:09] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0922,	best estimator xgboost's best error=0.0922
[flaml.automl: 09-18 12:25:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:25:10] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0700,	best estimator xgboost's best error=0.0700
[flaml.automl: 09-18 12:25:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:25:12] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0595,	best estimator xgboost's best error=0.0595
[flaml.automl: 09-18 12:25:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:25:14] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0595,	best estimator xgboost's best error=0.0595
[flaml.automl: 09-18 12:25:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:25:16] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0595,	best estimator xgboost's best error=0.0595
[flaml.automl: 09-18 12:25:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:25:17] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0595,	best estimator xgboost's best error=0.0595
[flaml.automl: 09-18 12:25:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:25:20] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.0595,	best estimator xgboost's best error=0.0595
[flaml.automl: 09-18 12:25:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:25:21] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0595,	best estimator xgboost's best error=0.0595
[flaml.automl: 09-18 12:25:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:25:22] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.0595,	best estimator xgboost's best error=0.0595
[flaml.automl: 09-18 12:25:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:25:28] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.0562,	best estimator xgboost's best error=0.0562
[flaml.automl: 09-18 12:25:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:25:39] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0540,	best estimator xgboost's best error=0.0540
[flaml.automl: 09-18 12:25:39] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 12:25:45] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0540,	best estimator xgboost's best error=0.0540
[flaml.automl: 09-18 12:25:45] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 12:25:54] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.0531,	best estimator xgboost's best error=0.0531
[flaml.automl: 09-18 12:26:12] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-18 12:26:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 12:26:12] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:26:12] {2637} INFO - Time taken to find the best model: 59.256349325180054
[flaml.automl: 09-18 12:26:12] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.9469111286491305
CO(0)最好结果：{'pred_time': 1.6614769854410263e-05, 'wall_clock_time': 59.256349325180054, 'metric_for_logging': {'pred_time': 1.6614769854410263e-05}, 'val_loss': 0.05308887135086951, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.740323781967163}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.834983988640756
CO(0)的mse=0.0069641705519459494
CO(0)的mae=0.05334989009438199
CO(0)的mar=0.07834294335023374
总共花费的时间为：76.95
咸宁市
2447A
2448A
2449A
[flaml.automl: 09-18 12:36:05] {2390} INFO - task = regression
[flaml.automl: 09-18 12:36:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:36:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:36:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:36:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:36:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:36:06] {3025} INFO - Estimated sufficient time budget=12061s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 12:36:06] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1225,	best estimator xgboost's best error=0.1225
[flaml.automl: 09-18 12:36:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:36:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0738,	best estimator xgboost's best error=0.0738
[flaml.automl: 09-18 12:36:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:36:09] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0738,	best estimator xgboost's best error=0.0738
[flaml.automl: 09-18 12:36:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:36:19] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0738,	best estimator xgboost's best error=0.0738
[flaml.automl: 09-18 12:36:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:36:21] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0594,	best estimator xgboost's best error=0.0594
[flaml.automl: 09-18 12:36:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:36:22] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0498,	best estimator xgboost's best error=0.0498
[flaml.automl: 09-18 12:36:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:36:24] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0498,	best estimator xgboost's best error=0.0498
[flaml.automl: 09-18 12:36:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:36:26] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0498,	best estimator xgboost's best error=0.0498
[flaml.automl: 09-18 12:36:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:36:27] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0498,	best estimator xgboost's best error=0.0498
[flaml.automl: 09-18 12:36:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:36:30] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0498,	best estimator xgboost's best error=0.0498
[flaml.automl: 09-18 12:36:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:36:31] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0498,	best estimator xgboost's best error=0.0498
[flaml.automl: 09-18 12:36:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:36:32] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0498,	best estimator xgboost's best error=0.0498
[flaml.automl: 09-18 12:36:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:36:39] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0451,	best estimator xgboost's best error=0.0451
[flaml.automl: 09-18 12:36:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:36:51] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0435,	best estimator xgboost's best error=0.0435
[flaml.automl: 09-18 12:36:51] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 12:36:58] {3072} INFO -  at 52.8s,	estimator xgboost's best error=0.0435,	best estimator xgboost's best error=0.0435
[flaml.automl: 09-18 12:37:10] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 12:37:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 12:37:10] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:37:10] {2637} INFO - Time taken to find the best model: 46.34774327278137
[flaml.automl: 09-18 12:37:10] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9564917569251811
CO(0)最好结果：{'pred_time': 1.1217425295043599e-05, 'wall_clock_time': 46.34774327278137, 'metric_for_logging': {'pred_time': 1.1217425295043599e-05}, 'val_loss': 0.04350824307481898, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.082335472106934}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8899824415599044
CO(0)的mse=0.004256251492070401
CO(0)的mae=0.041548207483885546
CO(0)的mar=0.07914229146941791
总共花费的时间为：65.47
随州市
2451A
2452A
2453A
[flaml.automl: 09-18 12:47:00] {2390} INFO - task = regression
[flaml.automl: 09-18 12:47:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:47:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:47:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:47:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:47:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:47:01] {3025} INFO - Estimated sufficient time budget=12319s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 12:47:01] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1780,	best estimator xgboost's best error=0.1780
[flaml.automl: 09-18 12:47:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:47:03] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0920,	best estimator xgboost's best error=0.0920
[flaml.automl: 09-18 12:47:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:47:04] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0920,	best estimator xgboost's best error=0.0920
[flaml.automl: 09-18 12:47:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:47:14] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0920,	best estimator xgboost's best error=0.0920
[flaml.automl: 09-18 12:47:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:47:15] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.0613,	best estimator xgboost's best error=0.0613
[flaml.automl: 09-18 12:47:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:47:17] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0513,	best estimator xgboost's best error=0.0513
[flaml.automl: 09-18 12:47:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:47:19] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0513,	best estimator xgboost's best error=0.0513
[flaml.automl: 09-18 12:47:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:47:21] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0513,	best estimator xgboost's best error=0.0513
[flaml.automl: 09-18 12:47:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:47:22] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0513,	best estimator xgboost's best error=0.0513
[flaml.automl: 09-18 12:47:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:47:25] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.0513,	best estimator xgboost's best error=0.0513
[flaml.automl: 09-18 12:47:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:47:26] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0513,	best estimator xgboost's best error=0.0513
[flaml.automl: 09-18 12:47:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:47:27] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.0513,	best estimator xgboost's best error=0.0513
[flaml.automl: 09-18 12:47:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:47:34] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0458,	best estimator xgboost's best error=0.0458
[flaml.automl: 09-18 12:47:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:47:46] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0435,	best estimator xgboost's best error=0.0435
[flaml.automl: 09-18 12:47:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 12:47:52] {3072} INFO -  at 52.8s,	estimator xgboost's best error=0.0435,	best estimator xgboost's best error=0.0435
[flaml.automl: 09-18 12:48:04] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 12:48:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 12:48:04] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:48:04] {2637} INFO - Time taken to find the best model: 46.303333044052124
[flaml.automl: 09-18 12:48:04] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9565473015028079
CO(0)最好结果：{'pred_time': 1.0607458151393437e-05, 'wall_clock_time': 46.303333044052124, 'metric_for_logging': {'pred_time': 1.0607458151393437e-05}, 'val_loss': 0.04345269849719219, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.112935781478882}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9033281357632598
CO(0)的mse=0.004395436203621749
CO(0)的mae=0.043369755789564736
CO(0)的mar=0.059624717135407665
总共花费的时间为：65.37
恩施土家族苗族自治州
2454A
2455A
3549A
[flaml.automl: 09-18 12:58:10] {2390} INFO - task = regression
[flaml.automl: 09-18 12:58:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:58:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:58:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:58:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:58:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:58:11] {3025} INFO - Estimated sufficient time budget=12143s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 12:58:11] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2160,	best estimator xgboost's best error=0.2160
[flaml.automl: 09-18 12:58:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:58:13] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1196,	best estimator xgboost's best error=0.1196
[flaml.automl: 09-18 12:58:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:58:14] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1196,	best estimator xgboost's best error=0.1196
[flaml.automl: 09-18 12:58:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:58:24] {3072} INFO -  at 14.6s,	estimator xgboost's best error=0.1196,	best estimator xgboost's best error=0.1196
[flaml.automl: 09-18 12:58:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:58:25] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.0828,	best estimator xgboost's best error=0.0828
[flaml.automl: 09-18 12:58:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:58:27] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 12:58:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:58:29] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 12:58:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:58:31] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 12:58:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:58:32] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 12:58:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:58:35] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 12:58:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:58:36] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 12:58:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:58:37] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 12:58:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:58:44] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.0648,	best estimator xgboost's best error=0.0648
[flaml.automl: 09-18 12:58:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:58:56] {3072} INFO -  at 46.2s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-18 12:58:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 12:59:02] {3072} INFO -  at 52.8s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-18 12:59:14] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 12:59:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 12:59:14] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:59:14] {2637} INFO - Time taken to find the best model: 46.22946286201477
[flaml.automl: 09-18 12:59:14] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9383266606713108
CO(0)最好结果：{'pred_time': 1.08827792267802e-05, 'wall_clock_time': 46.22946286201477, 'metric_for_logging': {'pred_time': 1.08827792267802e-05}, 'val_loss': 0.061673339328689195, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.097691059112549}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8921156965532979
CO(0)的mse=0.007667821619863696
CO(0)的mae=0.0597080328079629
CO(0)的mar=0.1723152164989305
总共花费的时间为：65.41
衡阳市
2456A
2457A
2458A
2459A
2460A
2461A
3399A
[flaml.automl: 09-18 13:20:17] {2390} INFO - task = regression
[flaml.automl: 09-18 13:20:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:20:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:20:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:20:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:20:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:20:18] {3025} INFO - Estimated sufficient time budget=88955s. Estimated necessary time budget=89s.
[flaml.automl: 09-18 13:20:18] {3072} INFO -  at 1.6s,	estimator xgboost's best error=0.1861,	best estimator xgboost's best error=0.1861
[flaml.automl: 09-18 13:20:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:20:20] {3072} INFO -  at 3.7s,	estimator xgboost's best error=0.1048,	best estimator xgboost's best error=0.1048
[flaml.automl: 09-18 13:20:20] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:20:22] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.1048,	best estimator xgboost's best error=0.1048
[flaml.automl: 09-18 13:20:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:20:25] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.1048,	best estimator xgboost's best error=0.1048
[flaml.automl: 09-18 13:20:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:20:26] {3072} INFO -  at 9.3s,	estimator xgboost's best error=0.0771,	best estimator xgboost's best error=0.0771
[flaml.automl: 09-18 13:20:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:20:28] {3072} INFO -  at 10.9s,	estimator xgboost's best error=0.0690,	best estimator xgboost's best error=0.0690
[flaml.automl: 09-18 13:20:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:20:29] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.0690,	best estimator xgboost's best error=0.0690
[flaml.automl: 09-18 13:20:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:20:32] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0690,	best estimator xgboost's best error=0.0690
[flaml.automl: 09-18 13:20:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:20:33] {3072} INFO -  at 16.1s,	estimator xgboost's best error=0.0690,	best estimator xgboost's best error=0.0690
[flaml.automl: 09-18 13:20:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:20:35] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0690,	best estimator xgboost's best error=0.0690
[flaml.automl: 09-18 13:20:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:20:37] {3072} INFO -  at 20.5s,	estimator xgboost's best error=0.0690,	best estimator xgboost's best error=0.0690
[flaml.automl: 09-18 13:20:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:20:38] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0690,	best estimator xgboost's best error=0.0690
[flaml.automl: 09-18 13:20:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:20:45] {3072} INFO -  at 28.2s,	estimator xgboost's best error=0.0634,	best estimator xgboost's best error=0.0634
[flaml.automl: 09-18 13:20:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:20:57] {3072} INFO -  at 40.3s,	estimator xgboost's best error=0.0620,	best estimator xgboost's best error=0.0620
[flaml.automl: 09-18 13:20:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:21:04] {3072} INFO -  at 46.9s,	estimator xgboost's best error=0.0620,	best estimator xgboost's best error=0.0620
[flaml.automl: 09-18 13:21:04] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 13:21:16] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0615,	best estimator xgboost's best error=0.0615
[flaml.automl: 09-18 13:21:39] {3335} INFO - retrain xgboost for 22.6s
[flaml.automl: 09-18 13:21:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:21:39] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:21:39] {2637} INFO - Time taken to find the best model: 59.4354407787323
[flaml.automl: 09-18 13:21:39] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 74466}
CO(0)最佳损失：0.9384863068979961
CO(0)最好结果：{'pred_time': 5.593861698023862e-06, 'wall_clock_time': 59.4354407787323, 'metric_for_logging': {'pred_time': 5.593861698023862e-06}, 'val_loss': 0.061513693102003934, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 74466}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 74466, 'experiment_tag': 'exp', 'time_total_s': 12.57798457145691}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.827807776515199
CO(0)的mse=0.009820074871421192
CO(0)的mae=0.06008172213157312
CO(0)的mar=0.0812340457611788
总共花费的时间为：83.37
邵阳市
2462A
2463A
2464A
2465A
2466A
[flaml.automl: 09-18 13:37:16] {2390} INFO - task = regression
[flaml.automl: 09-18 13:37:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:37:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:37:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:37:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:37:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:37:17] {3025} INFO - Estimated sufficient time budget=63333s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 13:37:17] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1450,	best estimator xgboost's best error=0.1450
[flaml.automl: 09-18 13:37:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:37:19] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0797,	best estimator xgboost's best error=0.0797
[flaml.automl: 09-18 13:37:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:37:20] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.0797,	best estimator xgboost's best error=0.0797
[flaml.automl: 09-18 13:37:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:37:25] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0797,	best estimator xgboost's best error=0.0797
[flaml.automl: 09-18 13:37:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:37:26] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-18 13:37:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:37:28] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 13:37:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:37:29] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 13:37:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:37:32] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 13:37:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:37:33] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 13:37:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:37:36] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 13:37:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:37:37] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0456,	best estimator xgboost's best error=0.0456
[flaml.automl: 09-18 13:37:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:37:39] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0456,	best estimator xgboost's best error=0.0456
[flaml.automl: 09-18 13:37:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:37:45] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-18 13:37:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:37:57] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-18 13:37:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:38:07] {3072} INFO -  at 51.5s,	estimator xgboost's best error=0.0380,	best estimator xgboost's best error=0.0380
[flaml.automl: 09-18 13:38:27] {3335} INFO - retrain xgboost for 19.8s
[flaml.automl: 09-18 13:38:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:38:27] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:38:27] {2637} INFO - Time taken to find the best model: 41.69985556602478
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52052}
CO(0)最佳损失：0.962016190525943
CO(0)最好结果：{'pred_time': 7.018591193572449e-06, 'wall_clock_time': 41.69985556602478, 'metric_for_logging': {'pred_time': 7.018591193572449e-06}, 'val_loss': 0.037983809474057016, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52052}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52052, 'experiment_tag': 'exp', 'time_total_s': 12.090794563293457}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9411659995194397
CO(0)的mse=0.003199421141862323
CO(0)的mae=0.03810630571960902
CO(0)的mar=0.06580236199632114
总共花费的时间为：72.24
益阳市
2467A
2468A
2469A
2470A
2471A
[flaml.automl: 09-18 13:53:26] {2390} INFO - task = regression
[flaml.automl: 09-18 13:53:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:53:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:53:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:53:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:53:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:53:28] {3025} INFO - Estimated sufficient time budget=115611s. Estimated necessary time budget=116s.
[flaml.automl: 09-18 13:53:28] {3072} INFO -  at 2.5s,	estimator xgboost's best error=0.2771,	best estimator xgboost's best error=0.2771
[flaml.automl: 09-18 13:53:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:53:32] {3072} INFO -  at 6.4s,	estimator xgboost's best error=0.1367,	best estimator xgboost's best error=0.1367
[flaml.automl: 09-18 13:53:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:53:34] {3072} INFO -  at 8.7s,	estimator xgboost's best error=0.1367,	best estimator xgboost's best error=0.1367
[flaml.automl: 09-18 13:53:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:53:39] {3072} INFO -  at 12.8s,	estimator xgboost's best error=0.1367,	best estimator xgboost's best error=0.1367
[flaml.automl: 09-18 13:53:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:53:41] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0786,	best estimator xgboost's best error=0.0786
[flaml.automl: 09-18 13:53:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:53:44] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0622,	best estimator xgboost's best error=0.0622
[flaml.automl: 09-18 13:53:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:53:47] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0622,	best estimator xgboost's best error=0.0622
[flaml.automl: 09-18 13:53:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:53:50] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.0622,	best estimator xgboost's best error=0.0622
[flaml.automl: 09-18 13:53:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:53:52] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0622,	best estimator xgboost's best error=0.0622
[flaml.automl: 09-18 13:53:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:53:55] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.0622,	best estimator xgboost's best error=0.0622
[flaml.automl: 09-18 13:53:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:53:58] {3072} INFO -  at 32.1s,	estimator xgboost's best error=0.0616,	best estimator xgboost's best error=0.0616
[flaml.automl: 09-18 13:53:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:54:00] {3072} INFO -  at 33.9s,	estimator xgboost's best error=0.0616,	best estimator xgboost's best error=0.0616
[flaml.automl: 09-18 13:54:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:54:06] {3072} INFO -  at 40.4s,	estimator xgboost's best error=0.0524,	best estimator xgboost's best error=0.0524
[flaml.automl: 09-18 13:54:06] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:54:23] {3072} INFO -  at 56.7s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 13:54:42] {3335} INFO - retrain xgboost for 19.8s
[flaml.automl: 09-18 13:54:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:54:42] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:54:42] {2637} INFO - Time taken to find the best model: 56.743447065353394
[flaml.automl: 09-18 13:54:42] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52299}
CO(0)最佳损失：0.9498292179847289
CO(0)最好结果：{'pred_time': 1.3955034884763273e-05, 'wall_clock_time': 56.743447065353394, 'metric_for_logging': {'pred_time': 1.3955034884763273e-05}, 'val_loss': 0.05017078201527113, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52299}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52299, 'experiment_tag': 'exp', 'time_total_s': 16.331610918045044}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.935424879071003
CO(0)的mse=0.006922106186897337
CO(0)的mae=0.05076082847835442
CO(0)的mar=0.058994697902527565
总共花费的时间为：77.56
郴州市
2472A
2473A
2474A
2475A
2476A
[flaml.automl: 09-18 14:10:00] {2390} INFO - task = regression
[flaml.automl: 09-18 14:10:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:10:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:10:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:10:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:10:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:10:01] {3025} INFO - Estimated sufficient time budget=63373s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 14:10:01] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1391,	best estimator xgboost's best error=0.1391
[flaml.automl: 09-18 14:10:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:10:03] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0759,	best estimator xgboost's best error=0.0759
[flaml.automl: 09-18 14:10:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:10:04] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0759,	best estimator xgboost's best error=0.0759
[flaml.automl: 09-18 14:10:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:10:09] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0759,	best estimator xgboost's best error=0.0759
[flaml.automl: 09-18 14:10:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:10:10] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0533,	best estimator xgboost's best error=0.0533
[flaml.automl: 09-18 14:10:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:10:12] {3072} INFO -  at 12.6s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 14:10:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:10:15] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 14:10:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:10:18] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 14:10:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:10:20] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 14:10:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:10:23] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 14:10:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:10:25] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 14:10:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:10:27] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 14:10:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:10:38] {3072} INFO -  at 39.0s,	estimator xgboost's best error=0.0355,	best estimator xgboost's best error=0.0355
[flaml.automl: 09-18 14:10:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:10:59] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0341,	best estimator xgboost's best error=0.0341
[flaml.automl: 09-18 14:11:16] {3335} INFO - retrain xgboost for 16.9s
[flaml.automl: 09-18 14:11:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:11:16] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:11:16] {2637} INFO - Time taken to find the best model: 59.41721057891846
[flaml.automl: 09-18 14:11:16] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53132}
CO(0)最佳损失：0.9658767968575928
CO(0)最好结果：{'pred_time': 1.313305159571371e-05, 'wall_clock_time': 59.41721057891846, 'metric_for_logging': {'pred_time': 1.313305159571371e-05}, 'val_loss': 0.0341232031424073, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 53132}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 53132, 'experiment_tag': 'exp', 'time_total_s': 20.458561182022095}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9486630613441462
CO(0)的mse=0.002533929321021166
CO(0)的mae=0.033702355496639885
CO(0)的mar=0.05834361779595543
总共花费的时间为：77.24
永州市
2477A
2478A
2479A
2480A
2481A
[flaml.automl: 09-18 14:27:23] {2390} INFO - task = regression
[flaml.automl: 09-18 14:27:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:27:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:27:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:27:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:27:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:27:24] {3025} INFO - Estimated sufficient time budget=60710s. Estimated necessary time budget=61s.
[flaml.automl: 09-18 14:27:24] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1441,	best estimator xgboost's best error=0.1441
[flaml.automl: 09-18 14:27:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:27:26] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0792,	best estimator xgboost's best error=0.0792
[flaml.automl: 09-18 14:27:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:27:27] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.0792,	best estimator xgboost's best error=0.0792
[flaml.automl: 09-18 14:27:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:27:32] {3072} INFO -  at 9.4s,	estimator xgboost's best error=0.0792,	best estimator xgboost's best error=0.0792
[flaml.automl: 09-18 14:27:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:27:33] {3072} INFO -  at 10.5s,	estimator xgboost's best error=0.0565,	best estimator xgboost's best error=0.0565
[flaml.automl: 09-18 14:27:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:27:35] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 14:27:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:27:36] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 14:27:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:27:39] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 14:27:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:27:40] {3072} INFO -  at 17.2s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 14:27:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:27:43] {3072} INFO -  at 19.8s,	estimator xgboost's best error=0.0467,	best estimator xgboost's best error=0.0467
[flaml.automl: 09-18 14:27:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:27:44] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.0462,	best estimator xgboost's best error=0.0462
[flaml.automl: 09-18 14:27:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:27:45] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.0462,	best estimator xgboost's best error=0.0462
[flaml.automl: 09-18 14:27:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:27:52] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.0412,	best estimator xgboost's best error=0.0412
[flaml.automl: 09-18 14:27:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:28:04] {3072} INFO -  at 41.2s,	estimator xgboost's best error=0.0398,	best estimator xgboost's best error=0.0398
[flaml.automl: 09-18 14:28:04] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 14:28:10] {3072} INFO -  at 47.7s,	estimator xgboost's best error=0.0398,	best estimator xgboost's best error=0.0398
[flaml.automl: 09-18 14:28:10] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 14:28:22] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0398,	best estimator xgboost's best error=0.0398
[flaml.automl: 09-18 14:28:34] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 14:28:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:28:34] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:28:34] {2637} INFO - Time taken to find the best model: 41.187732219696045
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52647}
CO(0)最佳损失：0.9602151763186495
CO(0)最好结果：{'pred_time': 8.016366225022536e-06, 'wall_clock_time': 41.187732219696045, 'metric_for_logging': {'pred_time': 8.016366225022536e-06}, 'val_loss': 0.03978482368135045, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52647}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52647, 'experiment_tag': 'exp', 'time_total_s': 12.08698296546936}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9098889755501577
CO(0)的mse=0.0036956983237298956
CO(0)的mae=0.03965304368877411
CO(0)的mar=0.06190126182000644
总共花费的时间为：72.12
怀化市
2482A
2483A
2484A
2485A
2486A
[flaml.automl: 09-18 14:43:27] {2390} INFO - task = regression
[flaml.automl: 09-18 14:43:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:43:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:43:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:43:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:43:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:43:28] {3025} INFO - Estimated sufficient time budget=63248s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 14:43:28] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.2112,	best estimator xgboost's best error=0.2112
[flaml.automl: 09-18 14:43:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:43:30] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1093,	best estimator xgboost's best error=0.1093
[flaml.automl: 09-18 14:43:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:43:32] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1093,	best estimator xgboost's best error=0.1093
[flaml.automl: 09-18 14:43:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:43:36] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.1093,	best estimator xgboost's best error=0.1093
[flaml.automl: 09-18 14:43:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:43:38] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0695,	best estimator xgboost's best error=0.0695
[flaml.automl: 09-18 14:43:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:43:39] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0554,	best estimator xgboost's best error=0.0554
[flaml.automl: 09-18 14:43:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:43:41] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0554,	best estimator xgboost's best error=0.0554
[flaml.automl: 09-18 14:43:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:43:43] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0554,	best estimator xgboost's best error=0.0554
[flaml.automl: 09-18 14:43:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:43:44] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0554,	best estimator xgboost's best error=0.0554
[flaml.automl: 09-18 14:43:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:43:47] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0554,	best estimator xgboost's best error=0.0554
[flaml.automl: 09-18 14:43:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:43:49] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0540,	best estimator xgboost's best error=0.0540
[flaml.automl: 09-18 14:43:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:43:50] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0540,	best estimator xgboost's best error=0.0540
[flaml.automl: 09-18 14:43:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:43:56] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0435,	best estimator xgboost's best error=0.0435
[flaml.automl: 09-18 14:43:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:44:08] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0428,	best estimator xgboost's best error=0.0428
[flaml.automl: 09-18 14:44:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 14:44:19] {3072} INFO -  at 52.6s,	estimator xgboost's best error=0.0428,	best estimator xgboost's best error=0.0428
[flaml.automl: 09-18 14:44:40] {3335} INFO - retrain xgboost for 21.0s
[flaml.automl: 09-18 14:44:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:44:40] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:44:40] {2637} INFO - Time taken to find the best model: 41.6912522315979
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52666}
CO(0)最佳损失：0.9571915352322818
CO(0)最好结果：{'pred_time': 6.975861990688113e-06, 'wall_clock_time': 41.6912522315979, 'metric_for_logging': {'pred_time': 6.975861990688113e-06}, 'val_loss': 0.04280846476771818, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52666}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52666, 'experiment_tag': 'exp', 'time_total_s': 12.095491886138916}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9547818532984983
CO(0)的mse=0.004011160826129224
CO(0)的mae=0.04257184310899592
CO(0)的mar=0.07431206258932135
总共花费的时间为：74.41
娄底市
2487A
2488A
2489A
2490A
2491A
[flaml.automl: 09-18 15:00:34] {2390} INFO - task = regression
[flaml.automl: 09-18 15:00:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:00:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:00:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:00:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:00:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:00:36] {3025} INFO - Estimated sufficient time budget=62217s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 15:00:36] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.2126,	best estimator xgboost's best error=0.2126
[flaml.automl: 09-18 15:00:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:00:38] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1347,	best estimator xgboost's best error=0.1347
[flaml.automl: 09-18 15:00:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:00:39] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1347,	best estimator xgboost's best error=0.1347
[flaml.automl: 09-18 15:00:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:00:44] {3072} INFO -  at 9.5s,	estimator xgboost's best error=0.1347,	best estimator xgboost's best error=0.1347
[flaml.automl: 09-18 15:00:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:00:45] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.1201,	best estimator xgboost's best error=0.1201
[flaml.automl: 09-18 15:00:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:00:46] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.1116,	best estimator xgboost's best error=0.1116
[flaml.automl: 09-18 15:00:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:00:48] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.1116,	best estimator xgboost's best error=0.1116
[flaml.automl: 09-18 15:00:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:00:50] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.1116,	best estimator xgboost's best error=0.1116
[flaml.automl: 09-18 15:00:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:00:52] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.1116,	best estimator xgboost's best error=0.1116
[flaml.automl: 09-18 15:00:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:00:54] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.1116,	best estimator xgboost's best error=0.1116
[flaml.automl: 09-18 15:00:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:00:56] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.1099,	best estimator xgboost's best error=0.1099
[flaml.automl: 09-18 15:00:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:00:57] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.1099,	best estimator xgboost's best error=0.1099
[flaml.automl: 09-18 15:00:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:01:04] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.1084,	best estimator xgboost's best error=0.1084
[flaml.automl: 09-18 15:01:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:01:16] {3072} INFO -  at 41.5s,	estimator xgboost's best error=0.1047,	best estimator xgboost's best error=0.1047
[flaml.automl: 09-18 15:01:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:01:24] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.1047,	best estimator xgboost's best error=0.1047
[flaml.automl: 09-18 15:01:43] {3335} INFO - retrain xgboost for 19.8s
[flaml.automl: 09-18 15:01:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:01:43] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:01:43] {2637} INFO - Time taken to find the best model: 41.47110414505005
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51876}
CO(0)最佳损失：0.8952801109331503
CO(0)最好结果：{'pred_time': 7.1291870577810205e-06, 'wall_clock_time': 41.47110414505005, 'metric_for_logging': {'pred_time': 7.1291870577810205e-06}, 'val_loss': 0.10471988906684972, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 51876}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 51876, 'experiment_tag': 'exp', 'time_total_s': 12.094527244567871}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.5803009465257087
CO(0)的mse=0.037869706555557735
CO(0)的mae=0.10356301907339104
CO(0)的mar=0.18200130710590665
总共花费的时间为：70.12
湘西州
2492A
2493A
2494A
[flaml.automl: 09-18 15:10:44] {2390} INFO - task = regression
[flaml.automl: 09-18 15:10:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:10:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:10:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:10:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:10:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:10:45] {3025} INFO - Estimated sufficient time budget=12100s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 15:10:45] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1420,	best estimator xgboost's best error=0.1420
[flaml.automl: 09-18 15:10:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:10:47] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0869,	best estimator xgboost's best error=0.0869
[flaml.automl: 09-18 15:10:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:10:48] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0869,	best estimator xgboost's best error=0.0869
[flaml.automl: 09-18 15:10:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:10:58] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0869,	best estimator xgboost's best error=0.0869
[flaml.automl: 09-18 15:10:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:10:59] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0717,	best estimator xgboost's best error=0.0717
[flaml.automl: 09-18 15:10:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:11:01] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0621,	best estimator xgboost's best error=0.0621
[flaml.automl: 09-18 15:11:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:11:03] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0621,	best estimator xgboost's best error=0.0621
[flaml.automl: 09-18 15:11:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:11:05] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0621,	best estimator xgboost's best error=0.0621
[flaml.automl: 09-18 15:11:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:11:06] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0621,	best estimator xgboost's best error=0.0621
[flaml.automl: 09-18 15:11:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:11:09] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.0621,	best estimator xgboost's best error=0.0621
[flaml.automl: 09-18 15:11:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:11:10] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0621,	best estimator xgboost's best error=0.0621
[flaml.automl: 09-18 15:11:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:11:11] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.0621,	best estimator xgboost's best error=0.0621
[flaml.automl: 09-18 15:11:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:11:18] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.0547,	best estimator xgboost's best error=0.0547
[flaml.automl: 09-18 15:11:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:11:30] {3072} INFO -  at 46.1s,	estimator xgboost's best error=0.0534,	best estimator xgboost's best error=0.0534
[flaml.automl: 09-18 15:11:30] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:11:36] {3072} INFO -  at 52.6s,	estimator xgboost's best error=0.0534,	best estimator xgboost's best error=0.0534
[flaml.automl: 09-18 15:11:48] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 15:11:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:11:48] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:11:48] {2637} INFO - Time taken to find the best model: 46.11265802383423
[flaml.automl: 09-18 15:11:48] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9466290669984244
CO(0)最好结果：{'pred_time': 1.2436001047567397e-05, 'wall_clock_time': 46.11265802383423, 'metric_for_logging': {'pred_time': 1.2436001047567397e-05}, 'val_loss': 0.05337093300157556, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.981931447982788}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8676739016213283
CO(0)的mse=0.00647969955267095
CO(0)的mae=0.05326458160382066
CO(0)的mar=0.18274236487185302
总共花费的时间为：65.15
梧州市
2495A
2496A
2497A
2498A
[flaml.automl: 09-18 15:24:22] {2390} INFO - task = regression
[flaml.automl: 09-18 15:24:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:24:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:24:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:24:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:24:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:24:23] {3025} INFO - Estimated sufficient time budget=51789s. Estimated necessary time budget=52s.
[flaml.automl: 09-18 15:24:23] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.2457,	best estimator xgboost's best error=0.2457
[flaml.automl: 09-18 15:24:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:24:25] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1211,	best estimator xgboost's best error=0.1211
[flaml.automl: 09-18 15:24:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:24:27] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1211,	best estimator xgboost's best error=0.1211
[flaml.automl: 09-18 15:24:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:24:32] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.1211,	best estimator xgboost's best error=0.1211
[flaml.automl: 09-18 15:24:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:24:34] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.0755,	best estimator xgboost's best error=0.0755
[flaml.automl: 09-18 15:24:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:24:35] {3072} INFO -  at 13.3s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-18 15:24:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:24:37] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-18 15:24:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:24:39] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-18 15:24:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:24:40] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-18 15:24:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:24:43] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-18 15:24:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:24:45] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-18 15:24:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:24:46] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-18 15:24:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:24:52] {3072} INFO -  at 30.4s,	estimator xgboost's best error=0.0591,	best estimator xgboost's best error=0.0591
[flaml.automl: 09-18 15:24:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:25:04] {3072} INFO -  at 42.4s,	estimator xgboost's best error=0.0566,	best estimator xgboost's best error=0.0566
[flaml.automl: 09-18 15:25:04] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:25:11] {3072} INFO -  at 48.9s,	estimator xgboost's best error=0.0566,	best estimator xgboost's best error=0.0566
[flaml.automl: 09-18 15:25:23] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 15:25:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:25:23] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:25:23] {2637} INFO - Time taken to find the best model: 42.4098002910614
[flaml.automl: 09-18 15:25:23] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43498}
CO(0)最佳损失：0.9434254782604786
CO(0)最好结果：{'pred_time': 8.344798232091572e-06, 'wall_clock_time': 42.4098002910614, 'metric_for_logging': {'pred_time': 8.344798232091572e-06}, 'val_loss': 0.05657452173952134, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43498}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43498, 'experiment_tag': 'exp', 'time_total_s': 11.959800720214844}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.813488439050438
CO(0)的mse=0.007678639015699313
CO(0)的mae=0.055177472477761794
CO(0)的mar=0.06253393392208091
总共花费的时间为：61.71
防城港市
2499A
2500A
2501A
[flaml.automl: 09-18 15:34:42] {2390} INFO - task = regression
[flaml.automl: 09-18 15:34:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:34:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:34:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:34:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:34:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:34:43] {3025} INFO - Estimated sufficient time budget=12155s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 15:34:43] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1384,	best estimator xgboost's best error=0.1384
[flaml.automl: 09-18 15:34:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:34:45] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0854,	best estimator xgboost's best error=0.0854
[flaml.automl: 09-18 15:34:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:34:46] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0854,	best estimator xgboost's best error=0.0854
[flaml.automl: 09-18 15:34:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:34:56] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0854,	best estimator xgboost's best error=0.0854
[flaml.automl: 09-18 15:34:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:34:58] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-18 15:34:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:34:59] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-18 15:34:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:35:01] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-18 15:35:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:35:03] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-18 15:35:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:35:04] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-18 15:35:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:35:07] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-18 15:35:07] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:35:08] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-18 15:35:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:35:09] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0617,	best estimator xgboost's best error=0.0617
[flaml.automl: 09-18 15:35:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:35:16] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0596,	best estimator xgboost's best error=0.0596
[flaml.automl: 09-18 15:35:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:35:28] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0573,	best estimator xgboost's best error=0.0573
[flaml.automl: 09-18 15:35:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:35:36] {3072} INFO -  at 54.6s,	estimator xgboost's best error=0.0573,	best estimator xgboost's best error=0.0573
[flaml.automl: 09-18 15:35:54] {3335} INFO - retrain xgboost for 17.6s
[flaml.automl: 09-18 15:35:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:35:54] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:35:54] {2637} INFO - Time taken to find the best model: 46.34307885169983
[flaml.automl: 09-18 15:35:54] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.942677345142985
CO(0)最好结果：{'pred_time': 1.1309371017154195e-05, 'wall_clock_time': 46.34307885169983, 'metric_for_logging': {'pred_time': 1.1309371017154195e-05}, 'val_loss': 0.057322654857015036, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.046068668365479}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7510048247822807
CO(0)的mse=0.009713890814615204
CO(0)的mae=0.05509531088518642
CO(0)的mar=0.08254084043106541
总共花费的时间为：72.75
钦州市
2502A
2503A
2504A
3404A
[flaml.automl: 09-18 15:48:40] {2390} INFO - task = regression
[flaml.automl: 09-18 15:48:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:48:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:48:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:48:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:48:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:48:41] {3025} INFO - Estimated sufficient time budget=51719s. Estimated necessary time budget=52s.
[flaml.automl: 09-18 15:48:41] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.2000,	best estimator xgboost's best error=0.2000
[flaml.automl: 09-18 15:48:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:48:43] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.1142,	best estimator xgboost's best error=0.1142
[flaml.automl: 09-18 15:48:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:48:44] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.1142,	best estimator xgboost's best error=0.1142
[flaml.automl: 09-18 15:48:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:48:50] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.1142,	best estimator xgboost's best error=0.1142
[flaml.automl: 09-18 15:48:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:48:51] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.0879,	best estimator xgboost's best error=0.0879
[flaml.automl: 09-18 15:48:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:48:53] {3072} INFO -  at 13.4s,	estimator xgboost's best error=0.0820,	best estimator xgboost's best error=0.0820
[flaml.automl: 09-18 15:48:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:48:55] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0820,	best estimator xgboost's best error=0.0820
[flaml.automl: 09-18 15:48:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:48:57] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0820,	best estimator xgboost's best error=0.0820
[flaml.automl: 09-18 15:48:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:48:58] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.0820,	best estimator xgboost's best error=0.0820
[flaml.automl: 09-18 15:48:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:49:01] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0820,	best estimator xgboost's best error=0.0820
[flaml.automl: 09-18 15:49:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:49:02] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0805,	best estimator xgboost's best error=0.0805
[flaml.automl: 09-18 15:49:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:49:04] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0805,	best estimator xgboost's best error=0.0805
[flaml.automl: 09-18 15:49:04] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:49:10] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0796,	best estimator xgboost's best error=0.0796
[flaml.automl: 09-18 15:49:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:49:22] {3072} INFO -  at 42.6s,	estimator xgboost's best error=0.0781,	best estimator xgboost's best error=0.0781
[flaml.automl: 09-18 15:49:22] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:49:29] {3072} INFO -  at 49.2s,	estimator xgboost's best error=0.0781,	best estimator xgboost's best error=0.0781
[flaml.automl: 09-18 15:49:41] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 15:49:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:49:41] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:49:41] {2637} INFO - Time taken to find the best model: 42.63959789276123
[flaml.automl: 09-18 15:49:41] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43076}
CO(0)最佳损失：0.9218659389185363
CO(0)最好结果：{'pred_time': 8.576793363852867e-06, 'wall_clock_time': 42.63959789276123, 'metric_for_logging': {'pred_time': 8.576793363852867e-06}, 'val_loss': 0.07813406108146374, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43076}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43076, 'experiment_tag': 'exp', 'time_total_s': 12.1123628616333}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.6261824070575344
CO(0)的mse=0.017344485226858934
CO(0)的mae=0.0720838821275854
CO(0)的mar=0.08621556039366145
总共花费的时间为：61.89
贵港市
2505A
2506A
2507A
2508A
3405A
[flaml.automl: 09-18 16:05:52] {2390} INFO - task = regression
[flaml.automl: 09-18 16:05:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:05:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:05:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:05:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:05:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:05:54] {3025} INFO - Estimated sufficient time budget=111053s. Estimated necessary time budget=111s.
[flaml.automl: 09-18 16:05:54] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1783,	best estimator xgboost's best error=0.1783
[flaml.automl: 09-18 16:05:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:05:57] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.1006,	best estimator xgboost's best error=0.1006
[flaml.automl: 09-18 16:05:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:05:59] {3072} INFO -  at 7.6s,	estimator xgboost's best error=0.1006,	best estimator xgboost's best error=0.1006
[flaml.automl: 09-18 16:05:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:06:03] {3072} INFO -  at 12.1s,	estimator xgboost's best error=0.1006,	best estimator xgboost's best error=0.1006
[flaml.automl: 09-18 16:06:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:06:05] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0742,	best estimator xgboost's best error=0.0742
[flaml.automl: 09-18 16:06:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:06:08] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.0652,	best estimator xgboost's best error=0.0652
[flaml.automl: 09-18 16:06:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:06:11] {3072} INFO -  at 19.8s,	estimator xgboost's best error=0.0652,	best estimator xgboost's best error=0.0652
[flaml.automl: 09-18 16:06:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:06:15] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.0652,	best estimator xgboost's best error=0.0652
[flaml.automl: 09-18 16:06:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:06:17] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.0652,	best estimator xgboost's best error=0.0652
[flaml.automl: 09-18 16:06:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:06:19] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.0652,	best estimator xgboost's best error=0.0652
[flaml.automl: 09-18 16:06:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:06:22] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0652,	best estimator xgboost's best error=0.0652
[flaml.automl: 09-18 16:06:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:06:24] {3072} INFO -  at 32.5s,	estimator xgboost's best error=0.0652,	best estimator xgboost's best error=0.0652
[flaml.automl: 09-18 16:06:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:06:35] {3072} INFO -  at 43.3s,	estimator xgboost's best error=0.0605,	best estimator xgboost's best error=0.0605
[flaml.automl: 09-18 16:06:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:06:47] {3072} INFO -  at 55.9s,	estimator xgboost's best error=0.0584,	best estimator xgboost's best error=0.0584
[flaml.automl: 09-18 16:06:59] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 16:06:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:06:59] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:06:59] {2637} INFO - Time taken to find the best model: 55.8721604347229
[flaml.automl: 09-18 16:06:59] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52668}
CO(0)最佳损失：0.941633889064124
CO(0)最好结果：{'pred_time': 7.045629512096661e-06, 'wall_clock_time': 55.8721604347229, 'metric_for_logging': {'pred_time': 7.045629512096661e-06}, 'val_loss': 0.05836611093587597, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52668}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52668, 'experiment_tag': 'exp', 'time_total_s': 12.606832265853882}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8232661263898832
CO(0)的mse=0.007514898137404625
CO(0)的mae=0.056457057633639
CO(0)的mar=0.0819453642616198
总共花费的时间为：68.88
玉林市
2509A
2510A
2511A
3532A
3533A
[flaml.automl: 09-18 16:23:28] {2390} INFO - task = regression
[flaml.automl: 09-18 16:23:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:23:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:23:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:23:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:23:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:23:30] {3025} INFO - Estimated sufficient time budget=106978s. Estimated necessary time budget=107s.
[flaml.automl: 09-18 16:23:30] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.1286,	best estimator xgboost's best error=0.1286
[flaml.automl: 09-18 16:23:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:23:33] {3072} INFO -  at 6.1s,	estimator xgboost's best error=0.0690,	best estimator xgboost's best error=0.0690
[flaml.automl: 09-18 16:23:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:23:36] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.0690,	best estimator xgboost's best error=0.0690
[flaml.automl: 09-18 16:23:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:23:39] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0690,	best estimator xgboost's best error=0.0690
[flaml.automl: 09-18 16:23:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:23:41] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 16:23:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:23:44] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-18 16:23:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:23:47] {3072} INFO -  at 19.5s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-18 16:23:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:23:50] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-18 16:23:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:23:52] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-18 16:23:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:23:55] {3072} INFO -  at 27.5s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-18 16:23:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:23:58] {3072} INFO -  at 30.2s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-18 16:23:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:23:59] {3072} INFO -  at 31.8s,	estimator xgboost's best error=0.0411,	best estimator xgboost's best error=0.0411
[flaml.automl: 09-18 16:23:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:24:10] {3072} INFO -  at 42.7s,	estimator xgboost's best error=0.0355,	best estimator xgboost's best error=0.0355
[flaml.automl: 09-18 16:24:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:24:23] {3072} INFO -  at 55.2s,	estimator xgboost's best error=0.0347,	best estimator xgboost's best error=0.0347
[flaml.automl: 09-18 16:24:35] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 16:24:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:24:35] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:24:35] {2637} INFO - Time taken to find the best model: 55.20632076263428
[flaml.automl: 09-18 16:24:35] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52952}
CO(0)最佳损失：0.9652803062681111
CO(0)最好结果：{'pred_time': 6.883667894644773e-06, 'wall_clock_time': 55.20632076263428, 'metric_for_logging': {'pred_time': 6.883667894644773e-06}, 'val_loss': 0.03471969373188887, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52952}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52952, 'experiment_tag': 'exp', 'time_total_s': 12.474261999130249}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9031396145381221
CO(0)的mse=0.002801189709220301
CO(0)的mae=0.035011358088475546
CO(0)的mar=0.05251715051340038
总共花费的时间为：68.14
百色市
2512A
2513A
3406A
[flaml.automl: 09-18 16:34:17] {2390} INFO - task = regression
[flaml.automl: 09-18 16:34:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:34:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:34:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:34:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:34:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:34:19] {3025} INFO - Estimated sufficient time budget=20260s. Estimated necessary time budget=20s.
[flaml.automl: 09-18 16:34:19] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.2134,	best estimator xgboost's best error=0.2134
[flaml.automl: 09-18 16:34:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:34:23] {3072} INFO -  at 6.2s,	estimator xgboost's best error=0.1025,	best estimator xgboost's best error=0.1025
[flaml.automl: 09-18 16:34:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:34:25] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.1025,	best estimator xgboost's best error=0.1025
[flaml.automl: 09-18 16:34:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:34:42] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.1025,	best estimator xgboost's best error=0.1025
[flaml.automl: 09-18 16:34:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:34:44] {3072} INFO -  at 28.0s,	estimator xgboost's best error=0.0614,	best estimator xgboost's best error=0.0614
[flaml.automl: 09-18 16:34:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:34:47] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-18 16:34:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:34:50] {3072} INFO -  at 33.2s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-18 16:34:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:34:54] {3072} INFO -  at 37.3s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-18 16:34:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:34:55] {3072} INFO -  at 39.1s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-18 16:34:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:34:58] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-18 16:34:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:35:00] {3072} INFO -  at 43.2s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-18 16:35:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:35:01] {3072} INFO -  at 44.3s,	estimator xgboost's best error=0.0494,	best estimator xgboost's best error=0.0494
[flaml.automl: 09-18 16:35:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:35:07] {3072} INFO -  at 50.8s,	estimator xgboost's best error=0.0428,	best estimator xgboost's best error=0.0428
[flaml.automl: 09-18 16:35:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:35:16] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.0413,	best estimator xgboost's best error=0.0413
[flaml.automl: 09-18 16:35:28] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 16:35:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:35:28] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:35:28] {2637} INFO - Time taken to find the best model: 59.691086530685425
[flaml.automl: 09-18 16:35:28] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.958657792498331
CO(0)最好结果：{'pred_time': 1.0574527155592775e-05, 'wall_clock_time': 59.691086530685425, 'metric_for_logging': {'pred_time': 1.0574527155592775e-05}, 'val_loss': 0.04134220750166894, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 8.879531860351562}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8834763830077059
CO(0)的mse=0.0036822803969542156
CO(0)的mae=0.04185037056197468
CO(0)的mar=0.051989256969063466
总共花费的时间为：72.32
贺州市
2514A
2515A
3534A
[flaml.automl: 09-18 16:44:38] {2390} INFO - task = regression
[flaml.automl: 09-18 16:44:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:44:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:44:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:44:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:44:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:44:39] {3025} INFO - Estimated sufficient time budget=12207s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:44:39] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1445,	best estimator xgboost's best error=0.1445
[flaml.automl: 09-18 16:44:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:44:41] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0754,	best estimator xgboost's best error=0.0754
[flaml.automl: 09-18 16:44:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:44:43] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0754,	best estimator xgboost's best error=0.0754
[flaml.automl: 09-18 16:44:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:44:53] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0754,	best estimator xgboost's best error=0.0754
[flaml.automl: 09-18 16:44:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:44:54] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-18 16:44:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:44:55] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 16:44:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:44:57] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 16:44:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:45:00] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 16:45:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:45:01] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 16:45:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:45:06] {3072} INFO -  at 28.2s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 16:45:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:45:08] {3072} INFO -  at 30.3s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 16:45:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:45:10] {3072} INFO -  at 32.0s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 16:45:10] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:45:22] {3072} INFO -  at 43.6s,	estimator xgboost's best error=0.0362,	best estimator xgboost's best error=0.0362
[flaml.automl: 09-18 16:45:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:45:37] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0348,	best estimator xgboost's best error=0.0348
[flaml.automl: 09-18 16:45:58] {3335} INFO - retrain xgboost for 21.2s
[flaml.automl: 09-18 16:45:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:45:58] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:45:58] {2637} INFO - Time taken to find the best model: 59.15042567253113
[flaml.automl: 09-18 16:45:58] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9651757546045211
CO(0)最好结果：{'pred_time': 1.4143495091534555e-05, 'wall_clock_time': 59.15042567253113, 'metric_for_logging': {'pred_time': 1.4143495091534555e-05}, 'val_loss': 0.034824245395478924, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 15.541106462478638}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9130488351999286
CO(0)的mse=0.0026762722394265506
CO(0)的mae=0.03416310737582169
CO(0)的mar=0.049956682667114304
总共花费的时间为：81.02
河池市
2516A
2517A
2518A
[flaml.automl: 09-18 16:55:48] {2390} INFO - task = regression
[flaml.automl: 09-18 16:55:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:55:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:55:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:55:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:55:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:55:49] {3025} INFO - Estimated sufficient time budget=12162s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:55:49] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1295,	best estimator xgboost's best error=0.1295
[flaml.automl: 09-18 16:55:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:55:51] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0636,	best estimator xgboost's best error=0.0636
[flaml.automl: 09-18 16:55:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:55:52] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0636,	best estimator xgboost's best error=0.0636
[flaml.automl: 09-18 16:55:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:56:02] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0636,	best estimator xgboost's best error=0.0636
[flaml.automl: 09-18 16:56:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:56:03] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0386,	best estimator xgboost's best error=0.0386
[flaml.automl: 09-18 16:56:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:56:05] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0311,	best estimator xgboost's best error=0.0311
[flaml.automl: 09-18 16:56:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:56:07] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0311,	best estimator xgboost's best error=0.0311
[flaml.automl: 09-18 16:56:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:56:09] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0311,	best estimator xgboost's best error=0.0311
[flaml.automl: 09-18 16:56:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:56:10] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0311,	best estimator xgboost's best error=0.0311
[flaml.automl: 09-18 16:56:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:56:13] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0311,	best estimator xgboost's best error=0.0311
[flaml.automl: 09-18 16:56:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:56:14] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0311,	best estimator xgboost's best error=0.0311
[flaml.automl: 09-18 16:56:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:56:15] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.0311,	best estimator xgboost's best error=0.0311
[flaml.automl: 09-18 16:56:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:56:22] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0263,	best estimator xgboost's best error=0.0263
[flaml.automl: 09-18 16:56:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:56:34] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0254,	best estimator xgboost's best error=0.0254
[flaml.automl: 09-18 16:56:34] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 16:56:40] {3072} INFO -  at 52.8s,	estimator xgboost's best error=0.0254,	best estimator xgboost's best error=0.0254
[flaml.automl: 09-18 16:56:52] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 16:56:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:56:52] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:56:52] {2637} INFO - Time taken to find the best model: 46.314640045166016
[flaml.automl: 09-18 16:56:52] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9745895916492822
CO(0)最好结果：{'pred_time': 1.2049443759839257e-05, 'wall_clock_time': 46.314640045166016, 'metric_for_logging': {'pred_time': 1.2049443759839257e-05}, 'val_loss': 0.025410408350717804, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.04299807548523}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9332608019392873
CO(0)的mse=0.0016201714020224718
CO(0)的mae=0.026569183718276313
CO(0)的mar=0.03807695276806188
总共花费的时间为：65.49
来宾市
2519A
2520A
3535A
[flaml.automl: 09-18 17:06:32] {2390} INFO - task = regression
[flaml.automl: 09-18 17:06:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:06:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:06:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:06:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:06:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:06:33] {3025} INFO - Estimated sufficient time budget=12199s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 17:06:33] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2043,	best estimator xgboost's best error=0.2043
[flaml.automl: 09-18 17:06:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:06:35] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1021,	best estimator xgboost's best error=0.1021
[flaml.automl: 09-18 17:06:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:06:36] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1021,	best estimator xgboost's best error=0.1021
[flaml.automl: 09-18 17:06:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:06:46] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.1021,	best estimator xgboost's best error=0.1021
[flaml.automl: 09-18 17:06:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:06:47] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0635,	best estimator xgboost's best error=0.0635
[flaml.automl: 09-18 17:06:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:06:49] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-18 17:06:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:06:51] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-18 17:06:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:06:53] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-18 17:06:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:06:54] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-18 17:06:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:06:57] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-18 17:06:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:06:58] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-18 17:06:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:06:59] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0530,	best estimator xgboost's best error=0.0530
[flaml.automl: 09-18 17:06:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:07:06] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0476,	best estimator xgboost's best error=0.0476
[flaml.automl: 09-18 17:07:06] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:07:23] {3072} INFO -  at 51.2s,	estimator xgboost's best error=0.0463,	best estimator xgboost's best error=0.0463
[flaml.automl: 09-18 17:07:45] {3335} INFO - retrain xgboost for 22.2s
[flaml.automl: 09-18 17:07:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:07:45] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:07:45] {2637} INFO - Time taken to find the best model: 51.21513819694519
[flaml.automl: 09-18 17:07:45] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9537281504346891
CO(0)最好结果：{'pred_time': 1.671750483557689e-05, 'wall_clock_time': 51.21513819694519, 'metric_for_logging': {'pred_time': 1.671750483557689e-05}, 'val_loss': 0.04627184956531092, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 16.90731978416443}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8864460614761472
CO(0)的mse=0.004920409342928971
CO(0)的mae=0.04593992584587922
CO(0)的mar=0.056545998167135354
总共花费的时间为：74.04
崇左市
2521A
2522A
[flaml.automl: 09-18 17:13:51] {2390} INFO - task = regression
[flaml.automl: 09-18 17:13:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:13:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:13:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:13:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:13:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:13:52] {3025} INFO - Estimated sufficient time budget=12180s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 17:13:52] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1582,	best estimator xgboost's best error=0.1582
[flaml.automl: 09-18 17:13:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:13:54] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0779,	best estimator xgboost's best error=0.0779
[flaml.automl: 09-18 17:13:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:13:55] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0779,	best estimator xgboost's best error=0.0779
[flaml.automl: 09-18 17:13:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:14:08] {3072} INFO -  at 17.7s,	estimator xgboost's best error=0.0779,	best estimator xgboost's best error=0.0779
[flaml.automl: 09-18 17:14:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:14:10] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0455,	best estimator xgboost's best error=0.0455
[flaml.automl: 09-18 17:14:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:14:13] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0349,	best estimator xgboost's best error=0.0349
[flaml.automl: 09-18 17:14:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:14:16] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.0349,	best estimator xgboost's best error=0.0349
[flaml.automl: 09-18 17:14:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:14:21] {3072} INFO -  at 30.2s,	estimator xgboost's best error=0.0349,	best estimator xgboost's best error=0.0349
[flaml.automl: 09-18 17:14:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:14:23] {3072} INFO -  at 32.0s,	estimator xgboost's best error=0.0349,	best estimator xgboost's best error=0.0349
[flaml.automl: 09-18 17:14:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:14:27] {3072} INFO -  at 36.6s,	estimator xgboost's best error=0.0349,	best estimator xgboost's best error=0.0349
[flaml.automl: 09-18 17:14:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:14:29] {3072} INFO -  at 38.7s,	estimator xgboost's best error=0.0349,	best estimator xgboost's best error=0.0349
[flaml.automl: 09-18 17:14:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:14:31] {3072} INFO -  at 40.4s,	estimator xgboost's best error=0.0349,	best estimator xgboost's best error=0.0349
[flaml.automl: 09-18 17:14:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:14:41] {3072} INFO -  at 50.5s,	estimator xgboost's best error=0.0292,	best estimator xgboost's best error=0.0292
[flaml.automl: 09-18 17:14:51] {3335} INFO - retrain xgboost for 10.0s
[flaml.automl: 09-18 17:14:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 17:14:51] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:14:51] {2637} INFO - Time taken to find the best model: 50.46716356277466
[flaml.automl: 09-18 17:14:51] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9707816641061202
CO(0)最好结果：{'pred_time': 3.4900136122298784e-05, 'wall_clock_time': 50.46716356277466, 'metric_for_logging': {'pred_time': 3.4900136122298784e-05}, 'val_loss': 0.02921833589387977, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 10.07750940322876}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9608144425354872
CO(0)的mse=0.0016342897814928693
CO(0)的mae=0.027916836495369263
CO(0)的mar=0.04019918382021826
总共花费的时间为：60.93
广元市
2523A
2524A
3617A
[flaml.automl: 09-18 17:24:24] {2390} INFO - task = regression
[flaml.automl: 09-18 17:24:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:24:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:24:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:24:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:24:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:24:25] {3025} INFO - Estimated sufficient time budget=12160s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 17:24:25] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1778,	best estimator xgboost's best error=0.1778
[flaml.automl: 09-18 17:24:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:24:27] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1005,	best estimator xgboost's best error=0.1005
[flaml.automl: 09-18 17:24:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:24:29] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1005,	best estimator xgboost's best error=0.1005
[flaml.automl: 09-18 17:24:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:24:39] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.1005,	best estimator xgboost's best error=0.1005
[flaml.automl: 09-18 17:24:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:24:40] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0764,	best estimator xgboost's best error=0.0764
[flaml.automl: 09-18 17:24:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:24:41] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0675,	best estimator xgboost's best error=0.0675
[flaml.automl: 09-18 17:24:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:24:43] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 17:24:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:24:46] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 17:24:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:24:47] {3072} INFO -  at 23.5s,	estimator xgboost's best error=0.0674,	best estimator xgboost's best error=0.0674
[flaml.automl: 09-18 17:24:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:24:50] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0650,	best estimator xgboost's best error=0.0650
[flaml.automl: 09-18 17:24:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:24:52] {3072} INFO -  at 28.2s,	estimator xgboost's best error=0.0650,	best estimator xgboost's best error=0.0650
[flaml.automl: 09-18 17:24:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:24:53] {3072} INFO -  at 29.3s,	estimator xgboost's best error=0.0650,	best estimator xgboost's best error=0.0650
[flaml.automl: 09-18 17:24:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:25:07] {3072} INFO -  at 42.9s,	estimator xgboost's best error=0.0578,	best estimator xgboost's best error=0.0578
[flaml.automl: 09-18 17:25:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:25:23] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0555,	best estimator xgboost's best error=0.0555
[flaml.automl: 09-18 17:25:47] {3335} INFO - retrain xgboost for 24.0s
[flaml.automl: 09-18 17:25:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:25:47] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:25:47] {2637} INFO - Time taken to find the best model: 59.38865303993225
[flaml.automl: 09-18 17:25:47] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
CO(0)最佳损失：0.9444985657076741
CO(0)最好结果：{'pred_time': 1.1309327484553736e-05, 'wall_clock_time': 59.38865303993225, 'metric_for_logging': {'pred_time': 1.1309327484553736e-05}, 'val_loss': 0.05550143429232594, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.449084281921387}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8852115928815547
CO(0)的mse=0.008538054746890818
CO(0)的mae=0.05568387748729597
CO(0)的mar=0.09007663715161046
总共花费的时间为：83.97
遂宁市
2527A
2528A
2529A
2530A
[flaml.automl: 09-18 17:38:52] {2390} INFO - task = regression
[flaml.automl: 09-18 17:38:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:38:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:38:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:38:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:38:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:38:53] {3025} INFO - Estimated sufficient time budget=50957s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 17:38:53] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1144,	best estimator xgboost's best error=0.1144
[flaml.automl: 09-18 17:38:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:38:55] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0686,	best estimator xgboost's best error=0.0686
[flaml.automl: 09-18 17:38:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:38:56] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0686,	best estimator xgboost's best error=0.0686
[flaml.automl: 09-18 17:38:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:39:02] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0686,	best estimator xgboost's best error=0.0686
[flaml.automl: 09-18 17:39:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:39:04] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-18 17:39:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:39:05] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0457,	best estimator xgboost's best error=0.0457
[flaml.automl: 09-18 17:39:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:39:07] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0457,	best estimator xgboost's best error=0.0457
[flaml.automl: 09-18 17:39:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:39:09] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.0457,	best estimator xgboost's best error=0.0457
[flaml.automl: 09-18 17:39:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:39:10] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0457,	best estimator xgboost's best error=0.0457
[flaml.automl: 09-18 17:39:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:39:13] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0457,	best estimator xgboost's best error=0.0457
[flaml.automl: 09-18 17:39:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:39:15] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.0457,	best estimator xgboost's best error=0.0457
[flaml.automl: 09-18 17:39:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:39:16] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0457,	best estimator xgboost's best error=0.0457
[flaml.automl: 09-18 17:39:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:39:22] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-18 17:39:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:39:34] {3072} INFO -  at 43.1s,	estimator xgboost's best error=0.0388,	best estimator xgboost's best error=0.0388
[flaml.automl: 09-18 17:39:34] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 17:39:41] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0388,	best estimator xgboost's best error=0.0388
[flaml.automl: 09-18 17:39:53] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 17:39:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:39:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:39:53] {2637} INFO - Time taken to find the best model: 43.05570387840271
[flaml.automl: 09-18 17:39:53] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42301}
CO(0)最佳损失：0.9611666081426524
CO(0)最好结果：{'pred_time': 8.421181567398392e-06, 'wall_clock_time': 43.05570387840271, 'metric_for_logging': {'pred_time': 8.421181567398392e-06}, 'val_loss': 0.03883339185734754, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42301}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42301, 'experiment_tag': 'exp', 'time_total_s': 12.13848328590393}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8770132659490651
CO(0)的mse=0.004614361727632036
CO(0)的mae=0.0391800828902584
CO(0)的mar=0.08377004829485434
总共花费的时间为：62.36
内江市
2531A
2532A
2533A
2534A
[flaml.automl: 09-18 17:53:00] {2390} INFO - task = regression
[flaml.automl: 09-18 17:53:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:53:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:53:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:53:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:53:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:53:03] {3025} INFO - Estimated sufficient time budget=119684s. Estimated necessary time budget=120s.
[flaml.automl: 09-18 17:53:03] {3072} INFO -  at 3.0s,	estimator xgboost's best error=0.1506,	best estimator xgboost's best error=0.1506
[flaml.automl: 09-18 17:53:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:53:08] {3072} INFO -  at 7.6s,	estimator xgboost's best error=0.0819,	best estimator xgboost's best error=0.0819
[flaml.automl: 09-18 17:53:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:53:10] {3072} INFO -  at 9.8s,	estimator xgboost's best error=0.0819,	best estimator xgboost's best error=0.0819
[flaml.automl: 09-18 17:53:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:53:15] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.0819,	best estimator xgboost's best error=0.0819
[flaml.automl: 09-18 17:53:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:53:16] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0561,	best estimator xgboost's best error=0.0561
[flaml.automl: 09-18 17:53:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:53:18] {3072} INFO -  at 18.0s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 17:53:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:53:20] {3072} INFO -  at 19.6s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 17:53:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:53:22] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 17:53:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:53:23] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 17:53:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:53:26] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 17:53:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:53:27] {3072} INFO -  at 27.3s,	estimator xgboost's best error=0.0457,	best estimator xgboost's best error=0.0457
[flaml.automl: 09-18 17:53:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:53:28] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.0457,	best estimator xgboost's best error=0.0457
[flaml.automl: 09-18 17:53:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:53:35] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.0385,	best estimator xgboost's best error=0.0385
[flaml.automl: 09-18 17:53:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:53:45] {3072} INFO -  at 44.7s,	estimator xgboost's best error=0.0357,	best estimator xgboost's best error=0.0357
[flaml.automl: 09-18 17:53:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 17:53:51] {3072} INFO -  at 51.0s,	estimator xgboost's best error=0.0357,	best estimator xgboost's best error=0.0357
[flaml.automl: 09-18 17:54:03] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-18 17:54:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:54:03] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:54:03] {2637} INFO - Time taken to find the best model: 44.676581621170044
[flaml.automl: 09-18 17:54:03] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43396}
CO(0)最佳损失：0.9642963468637619
CO(0)最好结果：{'pred_time': 8.25115181706546e-06, 'wall_clock_time': 44.676581621170044, 'metric_for_logging': {'pred_time': 8.25115181706546e-06}, 'val_loss': 0.03570365313623813, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43396}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43396, 'experiment_tag': 'exp', 'time_total_s': 10.100497961044312}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9508402427019993
CO(0)的mse=0.002848436356555744
CO(0)的mae=0.03429436872535207
CO(0)的mar=0.058155192369930206
总共花费的时间为：63.77
眉山市
2539A
2540A
3137A
3148A
[flaml.automl: 09-18 18:06:39] {2390} INFO - task = regression
[flaml.automl: 09-18 18:06:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:06:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:06:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:06:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:06:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:06:40] {3025} INFO - Estimated sufficient time budget=49941s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 18:06:40] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1475,	best estimator xgboost's best error=0.1475
[flaml.automl: 09-18 18:06:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:06:42] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0820,	best estimator xgboost's best error=0.0820
[flaml.automl: 09-18 18:06:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:06:43] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0820,	best estimator xgboost's best error=0.0820
[flaml.automl: 09-18 18:06:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:06:50] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0820,	best estimator xgboost's best error=0.0820
[flaml.automl: 09-18 18:06:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:06:51] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0589,	best estimator xgboost's best error=0.0589
[flaml.automl: 09-18 18:06:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:06:52] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0510,	best estimator xgboost's best error=0.0510
[flaml.automl: 09-18 18:06:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:06:54] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0510,	best estimator xgboost's best error=0.0510
[flaml.automl: 09-18 18:06:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:06:57] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0510,	best estimator xgboost's best error=0.0510
[flaml.automl: 09-18 18:06:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:06:58] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0510,	best estimator xgboost's best error=0.0510
[flaml.automl: 09-18 18:06:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:07:00] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0510,	best estimator xgboost's best error=0.0510
[flaml.automl: 09-18 18:07:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:07:02] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0509,	best estimator xgboost's best error=0.0509
[flaml.automl: 09-18 18:07:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:07:03] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0509,	best estimator xgboost's best error=0.0509
[flaml.automl: 09-18 18:07:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:07:10] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0455,	best estimator xgboost's best error=0.0455
[flaml.automl: 09-18 18:07:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:07:22] {3072} INFO -  at 43.1s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 18:07:22] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 18:07:28] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 18:07:40] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 18:07:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:07:40] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:07:40] {2637} INFO - Time taken to find the best model: 43.08617687225342
[flaml.automl: 09-18 18:07:40] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41965}
CO(0)最佳损失：0.9564366764034449
CO(0)最好结果：{'pred_time': 9.22265237885536e-06, 'wall_clock_time': 43.08617687225342, 'metric_for_logging': {'pred_time': 9.22265237885536e-06}, 'val_loss': 0.043563323596555126, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41965}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41965, 'experiment_tag': 'exp', 'time_total_s': 12.079124927520752}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9095292868340703
CO(0)的mse=0.0038992407428589276
CO(0)的mae=0.042006164677169326
CO(0)的mar=0.06393648122967421
总共花费的时间为：62.42
广安市
2543A
2544A
2545A
2902A
[flaml.automl: 09-18 18:19:54] {2390} INFO - task = regression
[flaml.automl: 09-18 18:19:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:19:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:19:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:19:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:19:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:19:55] {3025} INFO - Estimated sufficient time budget=48647s. Estimated necessary time budget=49s.
[flaml.automl: 09-18 18:19:55] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1227,	best estimator xgboost's best error=0.1227
[flaml.automl: 09-18 18:19:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:19:57] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-18 18:19:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:19:58] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-18 18:19:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:20:04] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-18 18:20:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:20:06] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0513,	best estimator xgboost's best error=0.0513
[flaml.automl: 09-18 18:20:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:20:07] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0431,	best estimator xgboost's best error=0.0431
[flaml.automl: 09-18 18:20:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:20:09] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.0431,	best estimator xgboost's best error=0.0431
[flaml.automl: 09-18 18:20:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:20:11] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0431,	best estimator xgboost's best error=0.0431
[flaml.automl: 09-18 18:20:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:20:12] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0431,	best estimator xgboost's best error=0.0431
[flaml.automl: 09-18 18:20:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:20:15] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0431,	best estimator xgboost's best error=0.0431
[flaml.automl: 09-18 18:20:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:20:17] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 18:20:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:20:18] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 18:20:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:20:26] {3072} INFO -  at 32.4s,	estimator xgboost's best error=0.0363,	best estimator xgboost's best error=0.0363
[flaml.automl: 09-18 18:20:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:20:49] {3072} INFO -  at 55.6s,	estimator xgboost's best error=0.0347,	best estimator xgboost's best error=0.0347
[flaml.automl: 09-18 18:21:16] {3335} INFO - retrain xgboost for 27.3s
[flaml.automl: 09-18 18:21:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:21:16] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:21:16] {2637} INFO - Time taken to find the best model: 55.64658188819885
[flaml.automl: 09-18 18:21:16] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40673}
CO(0)最佳损失：0.9653425494441417
CO(0)最好结果：{'pred_time': 2.2006773315699754e-05, 'wall_clock_time': 55.64658188819885, 'metric_for_logging': {'pred_time': 2.2006773315699754e-05}, 'val_loss': 0.034657450555858366, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 40673}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 40673, 'experiment_tag': 'exp', 'time_total_s': 23.254531860351562}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9430709746477968
CO(0)的mse=0.002680879877954562
CO(0)的mae=0.034417431945869145
CO(0)的mar=0.05855431727421729
总共花费的时间为：83.70
达州市
2548A
2549A
2550A
2551A
2552A
[flaml.automl: 09-18 18:36:42] {2390} INFO - task = regression
[flaml.automl: 09-18 18:36:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:36:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:36:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:36:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:36:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:36:46] {3025} INFO - Estimated sufficient time budget=234457s. Estimated necessary time budget=234s.
[flaml.automl: 09-18 18:36:46] {3072} INFO -  at 5.3s,	estimator xgboost's best error=0.1876,	best estimator xgboost's best error=0.1876
[flaml.automl: 09-18 18:36:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:36:50] {3072} INFO -  at 9.5s,	estimator xgboost's best error=0.1859,	best estimator xgboost's best error=0.1859
[flaml.automl: 09-18 18:36:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:36:54] {3072} INFO -  at 13.5s,	estimator xgboost's best error=0.1859,	best estimator xgboost's best error=0.1859
[flaml.automl: 09-18 18:36:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:36:58] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.1859,	best estimator xgboost's best error=0.1859
[flaml.automl: 09-18 18:36:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:37:00] {3072} INFO -  at 19.8s,	estimator xgboost's best error=0.1688,	best estimator xgboost's best error=0.1688
[flaml.automl: 09-18 18:37:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:37:03] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.0916,	best estimator xgboost's best error=0.0916
[flaml.automl: 09-18 18:37:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:37:06] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.0896,	best estimator xgboost's best error=0.0896
[flaml.automl: 09-18 18:37:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:37:07] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.0896,	best estimator xgboost's best error=0.0896
[flaml.automl: 09-18 18:37:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:37:09] {3072} INFO -  at 28.3s,	estimator xgboost's best error=0.0896,	best estimator xgboost's best error=0.0896
[flaml.automl: 09-18 18:37:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:37:12] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0859,	best estimator xgboost's best error=0.0859
[flaml.automl: 09-18 18:37:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:37:13] {3072} INFO -  at 32.6s,	estimator xgboost's best error=0.0859,	best estimator xgboost's best error=0.0859
[flaml.automl: 09-18 18:37:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:37:14] {3072} INFO -  at 33.7s,	estimator xgboost's best error=0.0859,	best estimator xgboost's best error=0.0859
[flaml.automl: 09-18 18:37:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:37:16] {3072} INFO -  at 35.6s,	estimator xgboost's best error=0.0859,	best estimator xgboost's best error=0.0859
[flaml.automl: 09-18 18:37:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:37:18] {3072} INFO -  at 37.7s,	estimator xgboost's best error=0.0859,	best estimator xgboost's best error=0.0859
[flaml.automl: 09-18 18:37:18] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 18:37:21] {3072} INFO -  at 40.6s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-18 18:37:21] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 18:37:26] {3072} INFO -  at 45.3s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-18 18:37:26] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 18:37:28] {3072} INFO -  at 47.1s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-18 18:37:28] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 18:37:29] {3072} INFO -  at 48.6s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-18 18:37:29] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 18:37:34] {3072} INFO -  at 53.5s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-18 18:37:34] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 18:37:34] {3072} INFO -  at 53.5s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-18 18:37:34] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 18:37:34] {3072} INFO -  at 53.6s,	estimator xgboost's best error=0.0794,	best estimator xgboost's best error=0.0794
[flaml.automl: 09-18 18:37:34] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 18:37:35] {3072} INFO -  at 53.9s,	estimator xgboost's best error=0.0757,	best estimator xgboost's best error=0.0757
[flaml.automl: 09-18 18:37:35] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-18 18:37:35] {3072} INFO -  at 54.0s,	estimator xgboost's best error=0.0757,	best estimator xgboost's best error=0.0757
[flaml.automl: 09-18 18:37:35] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-18 18:37:35] {3072} INFO -  at 54.5s,	estimator xgboost's best error=0.0757,	best estimator xgboost's best error=0.0757
[flaml.automl: 09-18 18:37:35] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-18 18:37:35] {3072} INFO -  at 54.7s,	estimator xgboost's best error=0.0756,	best estimator xgboost's best error=0.0756
[flaml.automl: 09-18 18:37:35] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-18 18:37:35] {3072} INFO -  at 54.7s,	estimator xgboost's best error=0.0756,	best estimator xgboost's best error=0.0756
[flaml.automl: 09-18 18:37:35] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-18 18:37:37] {3072} INFO -  at 56.0s,	estimator xgboost's best error=0.0731,	best estimator xgboost's best error=0.0731
[flaml.automl: 09-18 18:37:37] {2897} INFO - iteration 27, current learner xgboost
[flaml.automl: 09-18 18:37:44] {3072} INFO -  at 63.7s,	estimator xgboost's best error=0.0731,	best estimator xgboost's best error=0.0731
[flaml.automl: 09-18 18:38:39] {3335} INFO - retrain xgboost for 55.0s
[flaml.automl: 09-18 18:38:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9007509693439255, colsample_bynode=1,
             colsample_bytree=0.9734771013030347, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4736309451340078,
             max_delta_step=0, max_depth=0, max_leaves=37,
             min_child_weight=0.6006948589187273, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.10948612945352608,
             reg_lambda=0.02530561018934929, scale_pos_weight=1,
             subsample=0.9066759959039968, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:38:39] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:38:39] {2637} INFO - Time taken to find the best model: 55.96275734901428
[flaml.automl: 09-18 18:38:39] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 37, 'max_leaves': 37, 'min_child_weight': 0.6006948589187273, 'learning_rate': 0.4736309451340078, 'subsample': 0.9066759959039968, 'colsample_bylevel': 0.9007509693439255, 'colsample_bytree': 0.9734771013030347, 'reg_alpha': 0.10948612945352608, 'reg_lambda': 0.02530561018934929, 'FLAML_sample_size': 51916}
CO(0)最佳损失：0.9269122498901863
CO(0)最好结果：{'pred_time': 1.0953037964861693e-06, 'wall_clock_time': 55.96275734901428, 'metric_for_logging': {'pred_time': 1.0953037964861693e-06}, 'val_loss': 0.07308775010981373, 'training_iteration': 1, 'config': {'n_estimators': 37, 'max_leaves': 37, 'min_child_weight': 0.6006948589187273, 'learning_rate': 0.4736309451340078, 'subsample': 0.9066759959039968, 'colsample_bylevel': 0.9007509693439255, 'colsample_bytree': 0.9734771013030347, 'reg_alpha': 0.10948612945352608, 'reg_lambda': 0.02530561018934929, 'FLAML_sample_size': 51916}, 'config/n_estimators': 37, 'config/max_leaves': 37, 'config/min_child_weight': 0.6006948589187273, 'config/learning_rate': 0.4736309451340078, 'config/subsample': 0.9066759959039968, 'config/colsample_bylevel': 0.9007509693439255, 'config/colsample_bytree': 0.9734771013030347, 'config/reg_alpha': 0.10948612945352608, 'config/reg_lambda': 0.02530561018934929, 'config/FLAML_sample_size': 51916, 'experiment_tag': 'exp', 'time_total_s': 1.2119898796081543}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9007509693439255, colsample_bynode=1,
             colsample_bytree=0.9734771013030347, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4736309451340078,
             max_delta_step=0, max_depth=0, max_leaves=37,
             min_child_weight=0.6006948589187273, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.10948612945352608,
             reg_lambda=0.02530561018934929, scale_pos_weight=1,
             subsample=0.9066759959039968, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8226593380100314
CO(0)的mse=0.017008203246466996
CO(0)的mae=0.07394095267007206
CO(0)的mar=0.11580347718752425
总共花费的时间为：120.38
雅安市
2555A
2556A
[flaml.automl: 09-18 18:44:41] {2390} INFO - task = regression
[flaml.automl: 09-18 18:44:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:44:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:44:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:44:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:44:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:44:43] {3025} INFO - Estimated sufficient time budget=22742s. Estimated necessary time budget=23s.
[flaml.automl: 09-18 18:44:43] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.0947,	best estimator xgboost's best error=0.0947
[flaml.automl: 09-18 18:44:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:44:47] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.0523,	best estimator xgboost's best error=0.0523
[flaml.automl: 09-18 18:44:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:44:49] {3072} INFO -  at 8.6s,	estimator xgboost's best error=0.0523,	best estimator xgboost's best error=0.0523
[flaml.automl: 09-18 18:44:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:45:06] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0523,	best estimator xgboost's best error=0.0523
[flaml.automl: 09-18 18:45:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:45:09] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.0393,	best estimator xgboost's best error=0.0393
[flaml.automl: 09-18 18:45:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:45:12] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-18 18:45:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:45:15] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-18 18:45:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:45:19] {3072} INFO -  at 38.5s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-18 18:45:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:45:21] {3072} INFO -  at 40.6s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-18 18:45:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:45:26] {3072} INFO -  at 45.2s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-18 18:45:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:45:28] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-18 18:45:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:45:30] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.0318,	best estimator xgboost's best error=0.0318
[flaml.automl: 09-18 18:45:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:45:40] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.0270,	best estimator xgboost's best error=0.0270
[flaml.automl: 09-18 18:45:49] {3335} INFO - retrain xgboost for 8.8s
[flaml.automl: 09-18 18:45:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 18:45:49] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:45:49] {2637} INFO - Time taken to find the best model: 59.260695934295654
[flaml.automl: 09-18 18:45:49] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9730324986814228
CO(0)最好结果：{'pred_time': 3.257071947415017e-05, 'wall_clock_time': 59.260695934295654, 'metric_for_logging': {'pred_time': 3.257071947415017e-05}, 'val_loss': 0.02696750131857713, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.815709114074707}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9505584534253472
CO(0)的mse=0.0013886699247138074
CO(0)的mae=0.02555919149348406
CO(0)的mar=0.05002036697585339
总共花费的时间为：68.49
巴中市
2914A
3183A
3616A
[flaml.automl: 09-18 18:55:35] {2390} INFO - task = regression
[flaml.automl: 09-18 18:55:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:55:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:55:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:55:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:55:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:55:36] {3025} INFO - Estimated sufficient time budget=12111s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 18:55:36] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1005,	best estimator xgboost's best error=0.1005
[flaml.automl: 09-18 18:55:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:55:38] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0612,	best estimator xgboost's best error=0.0612
[flaml.automl: 09-18 18:55:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:55:39] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0612,	best estimator xgboost's best error=0.0612
[flaml.automl: 09-18 18:55:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:55:49] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0612,	best estimator xgboost's best error=0.0612
[flaml.automl: 09-18 18:55:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:55:51] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.0488,	best estimator xgboost's best error=0.0488
[flaml.automl: 09-18 18:55:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:55:52] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0423,	best estimator xgboost's best error=0.0423
[flaml.automl: 09-18 18:55:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:55:54] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0423,	best estimator xgboost's best error=0.0423
[flaml.automl: 09-18 18:55:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:55:56] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0423,	best estimator xgboost's best error=0.0423
[flaml.automl: 09-18 18:55:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:55:57] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0423,	best estimator xgboost's best error=0.0423
[flaml.automl: 09-18 18:55:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:56:00] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0423,	best estimator xgboost's best error=0.0423
[flaml.automl: 09-18 18:56:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:56:01] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0423,	best estimator xgboost's best error=0.0423
[flaml.automl: 09-18 18:56:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:56:02] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0423,	best estimator xgboost's best error=0.0423
[flaml.automl: 09-18 18:56:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:56:13] {3072} INFO -  at 38.2s,	estimator xgboost's best error=0.0359,	best estimator xgboost's best error=0.0359
[flaml.automl: 09-18 18:56:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:56:34] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0341,	best estimator xgboost's best error=0.0341
[flaml.automl: 09-18 18:57:00] {3335} INFO - retrain xgboost for 26.0s
[flaml.automl: 09-18 18:57:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:57:00] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:57:00] {2637} INFO - Time taken to find the best model: 59.43066167831421
[flaml.automl: 09-18 18:57:00] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.965885702305691
CO(0)最好结果：{'pred_time': 2.072420058399649e-05, 'wall_clock_time': 59.43066167831421, 'metric_for_logging': {'pred_time': 2.072420058399649e-05}, 'val_loss': 0.034114297694308945, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 21.216346979141235}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9000724386699633
CO(0)的mse=0.00293844949740614
CO(0)的mae=0.03504817165432092
CO(0)的mar=0.06355576498094485
总共花费的时间为：86.10
资阳市
2561A
2562A
2563A
2564A
2565A
[flaml.automl: 09-18 19:12:15] {2390} INFO - task = regression
[flaml.automl: 09-18 19:12:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:12:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:12:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:12:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:12:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:12:16] {3025} INFO - Estimated sufficient time budget=63289s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 19:12:16] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1104,	best estimator xgboost's best error=0.1104
[flaml.automl: 09-18 19:12:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:12:19] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0651,	best estimator xgboost's best error=0.0651
[flaml.automl: 09-18 19:12:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:12:20] {3072} INFO -  at 4.9s,	estimator xgboost's best error=0.0651,	best estimator xgboost's best error=0.0651
[flaml.automl: 09-18 19:12:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:12:25] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.0651,	best estimator xgboost's best error=0.0651
[flaml.automl: 09-18 19:12:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:12:26] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0497,	best estimator xgboost's best error=0.0497
[flaml.automl: 09-18 19:12:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:12:27] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0445,	best estimator xgboost's best error=0.0445
[flaml.automl: 09-18 19:12:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:12:29] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0445,	best estimator xgboost's best error=0.0445
[flaml.automl: 09-18 19:12:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:12:31] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0445,	best estimator xgboost's best error=0.0445
[flaml.automl: 09-18 19:12:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:12:33] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0445,	best estimator xgboost's best error=0.0445
[flaml.automl: 09-18 19:12:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:12:35] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0445,	best estimator xgboost's best error=0.0445
[flaml.automl: 09-18 19:12:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:12:37] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.0444,	best estimator xgboost's best error=0.0444
[flaml.automl: 09-18 19:12:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:12:38] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0444,	best estimator xgboost's best error=0.0444
[flaml.automl: 09-18 19:12:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:12:44] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0383,	best estimator xgboost's best error=0.0383
[flaml.automl: 09-18 19:12:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:13:05] {3072} INFO -  at 49.8s,	estimator xgboost's best error=0.0368,	best estimator xgboost's best error=0.0368
[flaml.automl: 09-18 19:13:26] {3335} INFO - retrain xgboost for 20.8s
[flaml.automl: 09-18 19:13:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:13:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:13:26] {2637} INFO - Time taken to find the best model: 49.82013559341431
[flaml.automl: 09-18 19:13:26] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52526}
CO(0)最佳损失：0.9631502250203209
CO(0)最好结果：{'pred_time': 1.2587087702134383e-05, 'wall_clock_time': 49.82013559341431, 'metric_for_logging': {'pred_time': 1.2587087702134383e-05}, 'val_loss': 0.036849774979679104, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52526}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52526, 'experiment_tag': 'exp', 'time_total_s': 20.21469283103943}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9104420305166276
CO(0)的mse=0.0029793369480503725
CO(0)的mae=0.03628416709690344
CO(0)的mar=0.060148179221342545
总共花费的时间为：71.75
阿坝藏族羌族自治州
2566A
2567A
2568A
[flaml.automl: 09-18 19:22:47] {2390} INFO - task = regression
[flaml.automl: 09-18 19:22:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:22:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:22:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:22:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:22:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:22:49] {3025} INFO - Estimated sufficient time budget=12777s. Estimated necessary time budget=13s.
[flaml.automl: 09-18 19:22:49] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1207,	best estimator xgboost's best error=0.1207
[flaml.automl: 09-18 19:22:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:22:51] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0749,	best estimator xgboost's best error=0.0749
[flaml.automl: 09-18 19:22:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:22:52] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0749,	best estimator xgboost's best error=0.0749
[flaml.automl: 09-18 19:22:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:23:02] {3072} INFO -  at 14.5s,	estimator xgboost's best error=0.0749,	best estimator xgboost's best error=0.0749
[flaml.automl: 09-18 19:23:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:23:03] {3072} INFO -  at 15.6s,	estimator xgboost's best error=0.0598,	best estimator xgboost's best error=0.0598
[flaml.automl: 09-18 19:23:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:23:04] {3072} INFO -  at 17.2s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-18 19:23:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:23:06] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-18 19:23:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:23:08] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-18 19:23:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:23:10] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-18 19:23:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:23:12] {3072} INFO -  at 24.8s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-18 19:23:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:23:13] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-18 19:23:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:23:14] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.0522,	best estimator xgboost's best error=0.0522
[flaml.automl: 09-18 19:23:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:23:20] {3072} INFO -  at 33.1s,	estimator xgboost's best error=0.0451,	best estimator xgboost's best error=0.0451
[flaml.automl: 09-18 19:23:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:23:31] {3072} INFO -  at 44.1s,	estimator xgboost's best error=0.0439,	best estimator xgboost's best error=0.0439
[flaml.automl: 09-18 19:23:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 19:23:37] {3072} INFO -  at 50.2s,	estimator xgboost's best error=0.0439,	best estimator xgboost's best error=0.0439
[flaml.automl: 09-18 19:23:48] {3335} INFO - retrain xgboost for 11.0s
[flaml.automl: 09-18 19:23:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:23:48] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:23:48] {2637} INFO - Time taken to find the best model: 44.14674425125122
[flaml.automl: 09-18 19:23:48] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9561268928896366
CO(0)最好结果：{'pred_time': 1.2441906485341019e-05, 'wall_clock_time': 44.14674425125122, 'metric_for_logging': {'pred_time': 1.2441906485341019e-05}, 'val_loss': 0.04387310711036339, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.030869960784912}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8884399171967532
CO(0)的mse=0.005050018337329827
CO(0)的mae=0.04465704964017418
CO(0)的mar=0.15140211215459043
总共花费的时间为：61.79
甘孜藏族自治州
3065A
[flaml.automl: 09-18 19:27:23] {2390} INFO - task = regression
[flaml.automl: 09-18 19:27:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:27:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:27:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:27:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:27:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:27:24] {3025} INFO - Estimated sufficient time budget=11451s. Estimated necessary time budget=11s.
[flaml.automl: 09-18 19:27:24] {3072} INFO -  at 1.2s,	estimator xgboost's best error=0.1053,	best estimator xgboost's best error=0.1053
[flaml.automl: 09-18 19:27:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:27:26] {3072} INFO -  at 3.0s,	estimator xgboost's best error=0.0638,	best estimator xgboost's best error=0.0638
[flaml.automl: 09-18 19:27:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:27:27] {3072} INFO -  at 4.0s,	estimator xgboost's best error=0.0638,	best estimator xgboost's best error=0.0638
[flaml.automl: 09-18 19:27:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:27:34] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0638,	best estimator xgboost's best error=0.0638
[flaml.automl: 09-18 19:27:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:27:35] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-18 19:27:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:27:37] {3072} INFO -  at 13.4s,	estimator xgboost's best error=0.0371,	best estimator xgboost's best error=0.0371
[flaml.automl: 09-18 19:27:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:27:38] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0339,	best estimator xgboost's best error=0.0339
[flaml.automl: 09-18 19:27:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:27:40] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.0339,	best estimator xgboost's best error=0.0339
[flaml.automl: 09-18 19:27:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:27:42] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0331,	best estimator xgboost's best error=0.0331
[flaml.automl: 09-18 19:27:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:27:45] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0331,	best estimator xgboost's best error=0.0331
[flaml.automl: 09-18 19:27:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:27:46] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.0302,	best estimator xgboost's best error=0.0302
[flaml.automl: 09-18 19:27:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:27:47] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.0302,	best estimator xgboost's best error=0.0302
[flaml.automl: 09-18 19:27:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:27:53] {3072} INFO -  at 30.1s,	estimator xgboost's best error=0.0259,	best estimator xgboost's best error=0.0259
[flaml.automl: 09-18 19:27:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:28:03] {3072} INFO -  at 39.4s,	estimator xgboost's best error=0.0256,	best estimator xgboost's best error=0.0256
[flaml.automl: 09-18 19:28:03] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 19:28:08] {3072} INFO -  at 44.6s,	estimator xgboost's best error=0.0256,	best estimator xgboost's best error=0.0256
[flaml.automl: 09-18 19:28:08] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 19:28:23] {3072} INFO -  at 59.6s,	estimator xgboost's best error=0.0256,	best estimator xgboost's best error=0.0256
[flaml.automl: 09-18 19:28:32] {3335} INFO - retrain xgboost for 9.3s
[flaml.automl: 09-18 19:28:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 19:28:32] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:28:32] {2637} INFO - Time taken to find the best model: 39.40092611312866
CO(0)最佳参数：{'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}
CO(0)最佳损失：0.9743763318369211
CO(0)最好结果：{'pred_time': 3.3553375685510555e-05, 'wall_clock_time': 39.40092611312866, 'metric_for_logging': {'pred_time': 3.3553375685510555e-05}, 'val_loss': 0.02562366816307888, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}, 'config/n_estimators': 26, 'config/max_leaves': 6, 'config/min_child_weight': 2.978767810916454, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532448312341252, 'config/reg_alpha': 0.0015779591896894877, 'config/reg_lambda': 1.3500086298278042, 'experiment_tag': 'exp', 'time_total_s': 9.27203631401062}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.886949931065886
CO(0)的mse=0.0014169640877656918
CO(0)的mae=0.025748026041580097
CO(0)的mar=0.07687852894975102
总共花费的时间为：69.13
凉山彝族自治州
2571A
2572A
2573A
2574A
2575A
[flaml.automl: 09-18 19:43:59] {2390} INFO - task = regression
[flaml.automl: 09-18 19:43:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:43:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:43:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:43:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:43:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:44:00] {3025} INFO - Estimated sufficient time budget=62246s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 19:44:00] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1333,	best estimator xgboost's best error=0.1333
[flaml.automl: 09-18 19:44:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:44:03] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-18 19:44:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:44:04] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-18 19:44:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:44:09] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0852,	best estimator xgboost's best error=0.0852
[flaml.automl: 09-18 19:44:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:44:10] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0727,	best estimator xgboost's best error=0.0727
[flaml.automl: 09-18 19:44:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:44:11] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0661,	best estimator xgboost's best error=0.0661
[flaml.automl: 09-18 19:44:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:44:13] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0661,	best estimator xgboost's best error=0.0661
[flaml.automl: 09-18 19:44:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:44:15] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0661,	best estimator xgboost's best error=0.0661
[flaml.automl: 09-18 19:44:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:44:17] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0661,	best estimator xgboost's best error=0.0661
[flaml.automl: 09-18 19:44:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:44:19] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0661,	best estimator xgboost's best error=0.0661
[flaml.automl: 09-18 19:44:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:44:21] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0659,	best estimator xgboost's best error=0.0659
[flaml.automl: 09-18 19:44:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:44:22] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0659,	best estimator xgboost's best error=0.0659
[flaml.automl: 09-18 19:44:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:44:28] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0634,	best estimator xgboost's best error=0.0634
[flaml.automl: 09-18 19:44:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:44:41] {3072} INFO -  at 41.6s,	estimator xgboost's best error=0.0613,	best estimator xgboost's best error=0.0613
[flaml.automl: 09-18 19:44:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 19:44:47] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.0613,	best estimator xgboost's best error=0.0613
[flaml.automl: 09-18 19:44:59] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 19:44:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:44:59] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:44:59] {2637} INFO - Time taken to find the best model: 41.590665102005005
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52241}
CO(0)最佳损失：0.9387223749962067
CO(0)最好结果：{'pred_time': 6.938945826120566e-06, 'wall_clock_time': 41.590665102005005, 'metric_for_logging': {'pred_time': 6.938945826120566e-06}, 'val_loss': 0.061277625003793336, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52241}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52241, 'experiment_tag': 'exp', 'time_total_s': 12.103142261505127}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.6830031330009747
CO(0)的mse=0.009933769010839104
CO(0)的mae=0.06332447437766911
CO(0)的mar=0.09657808412509432
总共花费的时间为：60.98
六盘水市
2576A
2577A
2578A
2579A
2580A
[flaml.automl: 09-18 20:01:33] {2390} INFO - task = regression
[flaml.automl: 09-18 20:01:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:01:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:01:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:01:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:01:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:01:34] {3025} INFO - Estimated sufficient time budget=63216s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 20:01:34] {3072} INFO -  at 1.5s,	estimator xgboost's best error=0.1491,	best estimator xgboost's best error=0.1491
[flaml.automl: 09-18 20:01:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:01:36] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.0909,	best estimator xgboost's best error=0.0909
[flaml.automl: 09-18 20:01:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:01:37] {3072} INFO -  at 4.8s,	estimator xgboost's best error=0.0909,	best estimator xgboost's best error=0.0909
[flaml.automl: 09-18 20:01:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:01:42] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.0909,	best estimator xgboost's best error=0.0909
[flaml.automl: 09-18 20:01:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:01:43] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0744,	best estimator xgboost's best error=0.0744
[flaml.automl: 09-18 20:01:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:01:45] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-18 20:01:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:01:46] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-18 20:01:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:01:49] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-18 20:01:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:01:50] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-18 20:01:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:01:53] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-18 20:01:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:01:54] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-18 20:01:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:01:55] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0657,	best estimator xgboost's best error=0.0657
[flaml.automl: 09-18 20:01:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:02:02] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.0625,	best estimator xgboost's best error=0.0625
[flaml.automl: 09-18 20:02:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:02:14] {3072} INFO -  at 41.6s,	estimator xgboost's best error=0.0594,	best estimator xgboost's best error=0.0594
[flaml.automl: 09-18 20:02:14] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 20:02:21] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.0594,	best estimator xgboost's best error=0.0594
[flaml.automl: 09-18 20:02:33] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 20:02:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:02:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:02:33] {2637} INFO - Time taken to find the best model: 41.551520109176636
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52739}
CO(0)最佳损失：0.940607623804409
CO(0)最好结果：{'pred_time': 7.110366235414056e-06, 'wall_clock_time': 41.551520109176636, 'metric_for_logging': {'pred_time': 7.110366235414056e-06}, 'val_loss': 0.05939237619559106, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52739}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52739, 'experiment_tag': 'exp', 'time_total_s': 12.128448963165283}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.7512264258933492
CO(0)的mse=0.01575193922031167
CO(0)的mae=0.0628870207909486
CO(0)的mar=0.10933163084430787
总共花费的时间为：61.13
安顺市
3122A
3123A
3124A
3125A
[flaml.automl: 09-18 20:15:53] {2390} INFO - task = regression
[flaml.automl: 09-18 20:15:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:15:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:15:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:15:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:15:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:15:54] {3025} INFO - Estimated sufficient time budget=48332s. Estimated necessary time budget=48s.
[flaml.automl: 09-18 20:15:54] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1182,	best estimator xgboost's best error=0.1182
[flaml.automl: 09-18 20:15:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:15:56] {3072} INFO -  at 3.3s,	estimator xgboost's best error=0.0639,	best estimator xgboost's best error=0.0639
[flaml.automl: 09-18 20:15:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:15:57] {3072} INFO -  at 4.4s,	estimator xgboost's best error=0.0639,	best estimator xgboost's best error=0.0639
[flaml.automl: 09-18 20:15:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:16:03] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.0639,	best estimator xgboost's best error=0.0639
[flaml.automl: 09-18 20:16:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:16:04] {3072} INFO -  at 11.7s,	estimator xgboost's best error=0.0450,	best estimator xgboost's best error=0.0450
[flaml.automl: 09-18 20:16:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:16:06] {3072} INFO -  at 13.3s,	estimator xgboost's best error=0.0362,	best estimator xgboost's best error=0.0362
[flaml.automl: 09-18 20:16:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:16:07] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0362,	best estimator xgboost's best error=0.0362
[flaml.automl: 09-18 20:16:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:16:10] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.0362,	best estimator xgboost's best error=0.0362
[flaml.automl: 09-18 20:16:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:16:11] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.0362,	best estimator xgboost's best error=0.0362
[flaml.automl: 09-18 20:16:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:16:13] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0362,	best estimator xgboost's best error=0.0362
[flaml.automl: 09-18 20:16:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:16:15] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.0361,	best estimator xgboost's best error=0.0361
[flaml.automl: 09-18 20:16:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:16:15] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.0361,	best estimator xgboost's best error=0.0361
[flaml.automl: 09-18 20:16:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:16:22] {3072} INFO -  at 29.3s,	estimator xgboost's best error=0.0298,	best estimator xgboost's best error=0.0298
[flaml.automl: 09-18 20:16:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:16:34] {3072} INFO -  at 41.3s,	estimator xgboost's best error=0.0288,	best estimator xgboost's best error=0.0288
[flaml.automl: 09-18 20:16:34] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 20:16:40] {3072} INFO -  at 47.7s,	estimator xgboost's best error=0.0288,	best estimator xgboost's best error=0.0288
[flaml.automl: 09-18 20:16:40] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 20:16:51] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0281,	best estimator xgboost's best error=0.0281
[flaml.automl: 09-18 20:17:13] {3335} INFO - retrain xgboost for 21.5s
[flaml.automl: 09-18 20:17:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:17:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:17:13] {2637} INFO - Time taken to find the best model: 59.104369163513184
[flaml.automl: 09-18 20:17:13] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 42577}
CO(0)最佳损失：0.9719306114183448
CO(0)最好结果：{'pred_time': 8.80989658482305e-06, 'wall_clock_time': 59.104369163513184, 'metric_for_logging': {'pred_time': 8.80989658482305e-06}, 'val_loss': 0.028069388581655214, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 20, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966, 'FLAML_sample_size': 42577}, 'config/n_estimators': 19, 'config/max_leaves': 20, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'config/FLAML_sample_size': 42577, 'experiment_tag': 'exp', 'time_total_s': 11.36716628074646}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=20,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9517127883346367
CO(0)的mse=0.001947806610201633
CO(0)的mae=0.028106889134388087
CO(0)的mar=0.07431269143797528
总共花费的时间为：81.38
铜仁地区
2585A
2586A
[flaml.automl: 09-18 20:23:41] {2390} INFO - task = regression
[flaml.automl: 09-18 20:23:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:23:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:23:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:23:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:23:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:23:44] {3025} INFO - Estimated sufficient time budget=28559s. Estimated necessary time budget=29s.
[flaml.automl: 09-18 20:23:44] {3072} INFO -  at 3.0s,	estimator xgboost's best error=0.1341,	best estimator xgboost's best error=0.1341
[flaml.automl: 09-18 20:23:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:23:47] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.0784,	best estimator xgboost's best error=0.0784
[flaml.automl: 09-18 20:23:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:23:49] {3072} INFO -  at 8.3s,	estimator xgboost's best error=0.0784,	best estimator xgboost's best error=0.0784
[flaml.automl: 09-18 20:23:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:24:00] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0784,	best estimator xgboost's best error=0.0784
[flaml.automl: 09-18 20:24:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:24:01] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.0613,	best estimator xgboost's best error=0.0613
[flaml.automl: 09-18 20:24:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:24:01] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 20:24:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:24:03] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 20:24:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:24:05] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 20:24:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:24:06] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 20:24:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:24:09] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 20:24:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:24:10] {3072} INFO -  at 28.9s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 20:24:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:24:11] {3072} INFO -  at 30.1s,	estimator xgboost's best error=0.0539,	best estimator xgboost's best error=0.0539
[flaml.automl: 09-18 20:24:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:24:17] {3072} INFO -  at 36.0s,	estimator xgboost's best error=0.0474,	best estimator xgboost's best error=0.0474
[flaml.automl: 09-18 20:24:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:24:27] {3072} INFO -  at 46.5s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-18 20:24:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 20:24:33] {3072} INFO -  at 52.6s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-18 20:24:44] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 20:24:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:24:44] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:24:44] {2637} INFO - Time taken to find the best model: 46.47700643539429
[flaml.automl: 09-18 20:24:44] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.95478224710755
CO(0)最好结果：{'pred_time': 1.9435160207432626e-05, 'wall_clock_time': 46.47700643539429, 'metric_for_logging': {'pred_time': 1.9435160207432626e-05}, 'val_loss': 0.045217752892449996, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.454122066497803}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.914186778720092
CO(0)的mse=0.004840155439091241
CO(0)的mae=0.04406489237565575
CO(0)的mar=0.11094784809224918
总共花费的时间为：63.44
毕节市
2587A
2588A
3537A
[flaml.automl: 09-18 20:34:51] {2390} INFO - task = regression
[flaml.automl: 09-18 20:34:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:34:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:34:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:34:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:34:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:34:54] {3025} INFO - Estimated sufficient time budget=31570s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 20:34:54] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0915,	best estimator xgboost's best error=0.0915
[flaml.automl: 09-18 20:34:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:34:59] {3072} INFO -  at 8.1s,	estimator xgboost's best error=0.0561,	best estimator xgboost's best error=0.0561
[flaml.automl: 09-18 20:34:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:35:02] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0561,	best estimator xgboost's best error=0.0561
[flaml.automl: 09-18 20:35:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:35:18] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.0561,	best estimator xgboost's best error=0.0561
[flaml.automl: 09-18 20:35:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:35:20] {3072} INFO -  at 28.9s,	estimator xgboost's best error=0.0448,	best estimator xgboost's best error=0.0448
[flaml.automl: 09-18 20:35:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:35:22] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-18 20:35:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:35:26] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-18 20:35:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:35:30] {3072} INFO -  at 38.8s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-18 20:35:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:35:31] {3072} INFO -  at 40.6s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-18 20:35:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:35:34] {3072} INFO -  at 43.2s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-18 20:35:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:35:35] {3072} INFO -  at 44.4s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-18 20:35:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:35:36] {3072} INFO -  at 45.6s,	estimator xgboost's best error=0.0399,	best estimator xgboost's best error=0.0399
[flaml.automl: 09-18 20:35:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:35:43] {3072} INFO -  at 52.0s,	estimator xgboost's best error=0.0356,	best estimator xgboost's best error=0.0356
[flaml.automl: 09-18 20:35:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:35:51] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.0339,	best estimator xgboost's best error=0.0339
[flaml.automl: 09-18 20:36:03] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 20:36:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:36:03] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:36:03] {2637} INFO - Time taken to find the best model: 59.74668335914612
[flaml.automl: 09-18 20:36:03] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9660922763941595
CO(0)最好结果：{'pred_time': 1.2254050384415151e-05, 'wall_clock_time': 59.74668335914612, 'metric_for_logging': {'pred_time': 1.2254050384415151e-05}, 'val_loss': 0.03390772360584046, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 7.711803674697876}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.882763104865756
CO(0)的mse=0.002746374301952017
CO(0)的mae=0.034543903468975255
CO(0)的mar=0.06659912154978997
总共花费的时间为：72.61
黔西南布依族苗族自治州
2589A
2590A
[flaml.automl: 09-18 20:42:39] {2390} INFO - task = regression
[flaml.automl: 09-18 20:42:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:42:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:42:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:42:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:42:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:42:42] {3025} INFO - Estimated sufficient time budget=27509s. Estimated necessary time budget=28s.
[flaml.automl: 09-18 20:42:42] {3072} INFO -  at 3.0s,	estimator xgboost's best error=0.1231,	best estimator xgboost's best error=0.1231
[flaml.automl: 09-18 20:42:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:42:46] {3072} INFO -  at 7.5s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 20:42:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:42:48] {3072} INFO -  at 9.4s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 20:42:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:42:59] {3072} INFO -  at 19.8s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 20:42:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:43:00] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0531,	best estimator xgboost's best error=0.0531
[flaml.automl: 09-18 20:43:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:43:01] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 20:43:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:43:03] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 20:43:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:43:05] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 20:43:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:43:06] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 20:43:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:43:09] {3072} INFO -  at 30.0s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 20:43:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:43:10] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 20:43:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:43:11] {3072} INFO -  at 32.3s,	estimator xgboost's best error=0.0461,	best estimator xgboost's best error=0.0461
[flaml.automl: 09-18 20:43:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:43:17] {3072} INFO -  at 38.2s,	estimator xgboost's best error=0.0403,	best estimator xgboost's best error=0.0403
[flaml.automl: 09-18 20:43:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:43:27] {3072} INFO -  at 48.6s,	estimator xgboost's best error=0.0384,	best estimator xgboost's best error=0.0384
[flaml.automl: 09-18 20:43:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 20:43:33] {3072} INFO -  at 54.7s,	estimator xgboost's best error=0.0384,	best estimator xgboost's best error=0.0384
[flaml.automl: 09-18 20:43:44] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 20:43:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:43:44] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:43:44] {2637} INFO - Time taken to find the best model: 48.57998204231262
[flaml.automl: 09-18 20:43:44] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9615852421898488
CO(0)最好结果：{'pred_time': 1.63889068128047e-05, 'wall_clock_time': 48.57998204231262, 'metric_for_logging': {'pred_time': 1.63889068128047e-05}, 'val_loss': 0.0384147578101512, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.353612899780273}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9262266046999459
CO(0)的mse=0.003231534418433921
CO(0)的mae=0.03672227723363141
CO(0)的mar=0.08031933044754795
总共花费的时间为：65.65
黔东南苗族侗族自治州
2591A
[flaml.automl: 09-18 20:47:05] {2390} INFO - task = regression
[flaml.automl: 09-18 20:47:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:47:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:47:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:47:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:47:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:47:06] {3025} INFO - Estimated sufficient time budget=12047s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:47:06] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1208,	best estimator xgboost's best error=0.1208
[flaml.automl: 09-18 20:47:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:47:08] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.0809,	best estimator xgboost's best error=0.0809
[flaml.automl: 09-18 20:47:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:47:09] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0809,	best estimator xgboost's best error=0.0809
[flaml.automl: 09-18 20:47:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:47:16] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.0809,	best estimator xgboost's best error=0.0809
[flaml.automl: 09-18 20:47:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:47:17] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.0595,	best estimator xgboost's best error=0.0595
[flaml.automl: 09-18 20:47:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:47:19] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0582,	best estimator xgboost's best error=0.0582
[flaml.automl: 09-18 20:47:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:47:21] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.0519,	best estimator xgboost's best error=0.0519
[flaml.automl: 09-18 20:47:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:47:23] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.0519,	best estimator xgboost's best error=0.0519
[flaml.automl: 09-18 20:47:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:47:25] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0512,	best estimator xgboost's best error=0.0512
[flaml.automl: 09-18 20:47:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:47:27] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.0512,	best estimator xgboost's best error=0.0512
[flaml.automl: 09-18 20:47:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:47:29] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-18 20:47:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:47:30] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-18 20:47:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:47:36] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0425,	best estimator xgboost's best error=0.0425
[flaml.automl: 09-18 20:47:36] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:47:45] {3072} INFO -  at 40.4s,	estimator xgboost's best error=0.0410,	best estimator xgboost's best error=0.0410
[flaml.automl: 09-18 20:47:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 20:47:50] {3072} INFO -  at 45.6s,	estimator xgboost's best error=0.0410,	best estimator xgboost's best error=0.0410
[flaml.automl: 09-18 20:47:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 20:48:04] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.0410,	best estimator xgboost's best error=0.0410
[flaml.automl: 09-18 20:48:14] {3335} INFO - retrain xgboost for 9.3s
[flaml.automl: 09-18 20:48:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 20:48:14] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:48:14] {2637} INFO - Time taken to find the best model: 40.393386125564575
CO(0)最佳参数：{'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}
CO(0)最佳损失：0.9589909323254737
CO(0)最好结果：{'pred_time': 3.190632284122662e-05, 'wall_clock_time': 40.393386125564575, 'metric_for_logging': {'pred_time': 3.190632284122662e-05}, 'val_loss': 0.04100906767452624, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}, 'config/n_estimators': 26, 'config/max_leaves': 6, 'config/min_child_weight': 2.978767810916454, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532448312341252, 'config/reg_alpha': 0.0015779591896894877, 'config/reg_lambda': 1.3500086298278042, 'experiment_tag': 'exp', 'time_total_s': 9.327362060546875}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9112926572506308
CO(0)的mse=0.003937820024448641
CO(0)的mae=0.03871823276960067
CO(0)的mar=0.06707348872649226
总共花费的时间为：68.99
黔南布依族苗族自治州
2593A
3538A
[flaml.automl: 09-18 20:55:07] {2390} INFO - task = regression
[flaml.automl: 09-18 20:55:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:55:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:55:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:55:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:55:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:55:09] {3025} INFO - Estimated sufficient time budget=11754s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:55:09] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1486,	best estimator xgboost's best error=0.1486
[flaml.automl: 09-18 20:55:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:55:11] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0775,	best estimator xgboost's best error=0.0775
[flaml.automl: 09-18 20:55:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:55:12] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.0775,	best estimator xgboost's best error=0.0775
[flaml.automl: 09-18 20:55:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:55:21] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0775,	best estimator xgboost's best error=0.0775
[flaml.automl: 09-18 20:55:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:55:22] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.0462,	best estimator xgboost's best error=0.0462
[flaml.automl: 09-18 20:55:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:55:24] {3072} INFO -  at 16.7s,	estimator xgboost's best error=0.0355,	best estimator xgboost's best error=0.0355
[flaml.automl: 09-18 20:55:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:55:26] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.0355,	best estimator xgboost's best error=0.0355
[flaml.automl: 09-18 20:55:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:55:28] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0355,	best estimator xgboost's best error=0.0355
[flaml.automl: 09-18 20:55:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:55:29] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0355,	best estimator xgboost's best error=0.0355
[flaml.automl: 09-18 20:55:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:55:32] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0355,	best estimator xgboost's best error=0.0355
[flaml.automl: 09-18 20:55:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:55:33] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0355,	best estimator xgboost's best error=0.0355
[flaml.automl: 09-18 20:55:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:55:34] {3072} INFO -  at 26.8s,	estimator xgboost's best error=0.0355,	best estimator xgboost's best error=0.0355
[flaml.automl: 09-18 20:55:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:55:40] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.0311,	best estimator xgboost's best error=0.0311
[flaml.automl: 09-18 20:55:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:55:51] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-18 20:55:51] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 20:55:57] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-18 20:55:57] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 20:56:06] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-18 20:56:17] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 20:56:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:56:17] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:56:17] {2637} INFO - Time taken to find the best model: 43.37686491012573
[flaml.automl: 09-18 20:56:17] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9727192367310981
CO(0)最好结果：{'pred_time': 1.6131105913609936e-05, 'wall_clock_time': 43.37686491012573, 'metric_for_logging': {'pred_time': 1.6131105913609936e-05}, 'val_loss': 0.027280763268901987, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.483627319335938}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.96543442455464
CO(0)的mse=0.001890494544907436
CO(0)的mae=0.027176171870257497
CO(0)的mar=0.04344023628557885
总共花费的时间为：70.03
保山市
2594A
2595A
[flaml.automl: 09-18 21:03:15] {2390} INFO - task = regression
[flaml.automl: 09-18 21:03:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:03:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:03:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:03:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:03:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:03:16] {3025} INFO - Estimated sufficient time budget=11663s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:03:16] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.0969,	best estimator xgboost's best error=0.0969
[flaml.automl: 09-18 21:03:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:03:18] {3072} INFO -  at 3.3s,	estimator xgboost's best error=0.0601,	best estimator xgboost's best error=0.0601
[flaml.automl: 09-18 21:03:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:03:19] {3072} INFO -  at 4.5s,	estimator xgboost's best error=0.0601,	best estimator xgboost's best error=0.0601
[flaml.automl: 09-18 21:03:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:03:28] {3072} INFO -  at 13.5s,	estimator xgboost's best error=0.0601,	best estimator xgboost's best error=0.0601
[flaml.automl: 09-18 21:03:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:03:29] {3072} INFO -  at 14.6s,	estimator xgboost's best error=0.0462,	best estimator xgboost's best error=0.0462
[flaml.automl: 09-18 21:03:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:03:31] {3072} INFO -  at 16.1s,	estimator xgboost's best error=0.0406,	best estimator xgboost's best error=0.0406
[flaml.automl: 09-18 21:03:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:03:32] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0406,	best estimator xgboost's best error=0.0406
[flaml.automl: 09-18 21:03:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:03:35] {3072} INFO -  at 20.0s,	estimator xgboost's best error=0.0406,	best estimator xgboost's best error=0.0406
[flaml.automl: 09-18 21:03:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:03:36] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0406,	best estimator xgboost's best error=0.0406
[flaml.automl: 09-18 21:03:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:03:38] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.0406,	best estimator xgboost's best error=0.0406
[flaml.automl: 09-18 21:03:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:03:39] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0406,	best estimator xgboost's best error=0.0406
[flaml.automl: 09-18 21:03:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:03:40] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0406,	best estimator xgboost's best error=0.0406
[flaml.automl: 09-18 21:03:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:03:46] {3072} INFO -  at 31.4s,	estimator xgboost's best error=0.0368,	best estimator xgboost's best error=0.0368
[flaml.automl: 09-18 21:03:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 21:03:56] {3072} INFO -  at 41.8s,	estimator xgboost's best error=0.0325,	best estimator xgboost's best error=0.0325
[flaml.automl: 09-18 21:03:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 21:04:02] {3072} INFO -  at 47.8s,	estimator xgboost's best error=0.0325,	best estimator xgboost's best error=0.0325
[flaml.automl: 09-18 21:04:02] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 21:04:15] {3072} INFO -  at 60.1s,	estimator xgboost's best error=0.0317,	best estimator xgboost's best error=0.0317
[flaml.automl: 09-18 21:04:47] {3335} INFO - retrain xgboost for 31.9s
[flaml.automl: 09-18 21:04:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:04:47] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:04:47] {2637} INFO - Time taken to find the best model: 60.050957679748535
[flaml.automl: 09-18 21:04:47] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.9682593812136833
CO(0)最好结果：{'pred_time': 3.326848690386258e-05, 'wall_clock_time': 60.050957679748535, 'metric_for_logging': {'pred_time': 3.326848690386258e-05}, 'val_loss': 0.0317406187863167, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 12.220265865325928}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9127947682285923
CO(0)的mse=0.0022449029009648596
CO(0)的mae=0.030976965517692166
CO(0)的mar=0.06859552844757957
总共花费的时间为：92.50
昭通市
2596A
2597A
[flaml.automl: 09-18 21:11:39] {2390} INFO - task = regression
[flaml.automl: 09-18 21:11:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:11:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:11:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:11:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:11:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:11:40] {3025} INFO - Estimated sufficient time budget=12414s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:11:40] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1164,	best estimator xgboost's best error=0.1164
[flaml.automl: 09-18 21:11:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:11:42] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 21:11:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:11:43] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 21:11:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:11:52] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0720,	best estimator xgboost's best error=0.0720
[flaml.automl: 09-18 21:11:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:11:54] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.0585,	best estimator xgboost's best error=0.0585
[flaml.automl: 09-18 21:11:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:11:55] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 21:11:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:11:57] {3072} INFO -  at 18.2s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 21:11:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:11:59] {3072} INFO -  at 20.7s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 21:11:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:12:00] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 21:12:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:12:03] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 21:12:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:12:04] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 21:12:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:12:05] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 21:12:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:12:11] {3072} INFO -  at 32.3s,	estimator xgboost's best error=0.0472,	best estimator xgboost's best error=0.0472
[flaml.automl: 09-18 21:12:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 21:12:21] {3072} INFO -  at 42.4s,	estimator xgboost's best error=0.0441,	best estimator xgboost's best error=0.0441
[flaml.automl: 09-18 21:12:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 21:12:27] {3072} INFO -  at 48.4s,	estimator xgboost's best error=0.0441,	best estimator xgboost's best error=0.0441
[flaml.automl: 09-18 21:12:27] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 21:12:38] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.0439,	best estimator xgboost's best error=0.0439
[flaml.automl: 09-18 21:12:56] {3335} INFO - retrain xgboost for 17.1s
[flaml.automl: 09-18 21:12:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:12:56] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:12:56] {2637} INFO - Time taken to find the best model: 59.88225817680359
[flaml.automl: 09-18 21:12:56] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.9561309788750315
CO(0)最好结果：{'pred_time': 1.6062967963032908e-05, 'wall_clock_time': 59.88225817680359, 'metric_for_logging': {'pred_time': 1.6062967963032908e-05}, 'val_loss': 0.043869021124968474, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 11.502157926559448}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8851055858738576
CO(0)的mse=0.004288425849062088
CO(0)的mae=0.04353199347848216
CO(0)的mar=0.09342670374663595
总共花费的时间为：77.41
丽江市
2598A
2599A
2600A
[flaml.automl: 09-18 21:23:39] {2390} INFO - task = regression
[flaml.automl: 09-18 21:23:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:23:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:23:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:23:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:23:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:23:42] {3025} INFO - Estimated sufficient time budget=27168s. Estimated necessary time budget=27s.
[flaml.automl: 09-18 21:23:42] {3072} INFO -  at 3.0s,	estimator xgboost's best error=0.0941,	best estimator xgboost's best error=0.0941
[flaml.automl: 09-18 21:23:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:23:48] {3072} INFO -  at 8.9s,	estimator xgboost's best error=0.0545,	best estimator xgboost's best error=0.0545
[flaml.automl: 09-18 21:23:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:23:51] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.0545,	best estimator xgboost's best error=0.0545
[flaml.automl: 09-18 21:23:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:24:17] {3072} INFO -  at 38.4s,	estimator xgboost's best error=0.0545,	best estimator xgboost's best error=0.0545
[flaml.automl: 09-18 21:24:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:24:21] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.0409,	best estimator xgboost's best error=0.0409
[flaml.automl: 09-18 21:24:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:24:25] {3072} INFO -  at 46.0s,	estimator xgboost's best error=0.0352,	best estimator xgboost's best error=0.0352
[flaml.automl: 09-18 21:24:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:24:29] {3072} INFO -  at 50.3s,	estimator xgboost's best error=0.0352,	best estimator xgboost's best error=0.0352
[flaml.automl: 09-18 21:24:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:24:35] {3072} INFO -  at 56.0s,	estimator xgboost's best error=0.0352,	best estimator xgboost's best error=0.0352
[flaml.automl: 09-18 21:24:39] {3335} INFO - retrain xgboost for 3.7s
[flaml.automl: 09-18 21:24:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 21:24:39] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:24:39] {2637} INFO - Time taken to find the best model: 45.96596932411194
[flaml.automl: 09-18 21:24:39] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
CO(0)最佳损失：0.9647976965110207
CO(0)最好结果：{'pred_time': 2.9408456014355583e-05, 'wall_clock_time': 45.96596932411194, 'metric_for_logging': {'pred_time': 2.9408456014355583e-05}, 'val_loss': 0.03520230348897931, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 4.265743970870972}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8542791186202638
CO(0)的mse=0.0025918682998591733
CO(0)的mae=0.03444384356113067
CO(0)的mar=0.059607258991784456
总共花费的时间为：60.38
普洱市
2601A
2602A
[flaml.automl: 09-18 21:30:59] {2390} INFO - task = regression
[flaml.automl: 09-18 21:30:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:30:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:30:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:30:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:31:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:31:01] {3025} INFO - Estimated sufficient time budget=18857s. Estimated necessary time budget=19s.
[flaml.automl: 09-18 21:31:01] {3072} INFO -  at 2.1s,	estimator xgboost's best error=0.1283,	best estimator xgboost's best error=0.1283
[flaml.automl: 09-18 21:31:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:31:05] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.0737,	best estimator xgboost's best error=0.0737
[flaml.automl: 09-18 21:31:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:31:07] {3072} INFO -  at 7.8s,	estimator xgboost's best error=0.0737,	best estimator xgboost's best error=0.0737
[flaml.automl: 09-18 21:31:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:31:22] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0737,	best estimator xgboost's best error=0.0737
[flaml.automl: 09-18 21:31:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:31:24] {3072} INFO -  at 24.7s,	estimator xgboost's best error=0.0553,	best estimator xgboost's best error=0.0553
[flaml.automl: 09-18 21:31:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:31:27] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 21:31:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:31:30] {3072} INFO -  at 30.3s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 21:31:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:31:35] {3072} INFO -  at 35.5s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 21:31:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:31:38] {3072} INFO -  at 38.2s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 21:31:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:31:44] {3072} INFO -  at 44.7s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 21:31:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:31:47] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 21:31:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:31:50] {3072} INFO -  at 50.9s,	estimator xgboost's best error=0.0483,	best estimator xgboost's best error=0.0483
[flaml.automl: 09-18 21:31:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:31:58] {3072} INFO -  at 58.7s,	estimator xgboost's best error=0.0421,	best estimator xgboost's best error=0.0421
[flaml.automl: 09-18 21:32:15] {3335} INFO - retrain xgboost for 16.6s
[flaml.automl: 09-18 21:32:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 21:32:15] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:32:15] {2637} INFO - Time taken to find the best model: 58.70826172828674
[flaml.automl: 09-18 21:32:15] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9579108067760476
CO(0)最好结果：{'pred_time': 4.900119699509497e-05, 'wall_clock_time': 58.70826172828674, 'metric_for_logging': {'pred_time': 4.900119699509497e-05}, 'val_loss': 0.042089193223952444, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 7.776119232177734}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9244310924481101
CO(0)的mse=0.003610032995308843
CO(0)的mae=0.040161058879866034
CO(0)的mar=0.07363699405202334
总共花费的时间为：75.84
临沧市
2603A
2604A
[flaml.automl: 09-18 21:38:50] {2390} INFO - task = regression
[flaml.automl: 09-18 21:38:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:38:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:38:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:38:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:38:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:38:51] {3025} INFO - Estimated sufficient time budget=12104s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:38:51] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1566,	best estimator xgboost's best error=0.1566
[flaml.automl: 09-18 21:38:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:38:53] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0836,	best estimator xgboost's best error=0.0836
[flaml.automl: 09-18 21:38:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:38:54] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.0836,	best estimator xgboost's best error=0.0836
[flaml.automl: 09-18 21:38:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:39:04] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0836,	best estimator xgboost's best error=0.0836
[flaml.automl: 09-18 21:39:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:39:05] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.0599,	best estimator xgboost's best error=0.0599
[flaml.automl: 09-18 21:39:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:39:06] {3072} INFO -  at 16.7s,	estimator xgboost's best error=0.0534,	best estimator xgboost's best error=0.0534
[flaml.automl: 09-18 21:39:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:39:08] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.0534,	best estimator xgboost's best error=0.0534
[flaml.automl: 09-18 21:39:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:39:10] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.0534,	best estimator xgboost's best error=0.0534
[flaml.automl: 09-18 21:39:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:39:12] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.0534,	best estimator xgboost's best error=0.0534
[flaml.automl: 09-18 21:39:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:39:14] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0534,	best estimator xgboost's best error=0.0534
[flaml.automl: 09-18 21:39:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:39:15] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0534,	best estimator xgboost's best error=0.0534
[flaml.automl: 09-18 21:39:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:39:16] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.0534,	best estimator xgboost's best error=0.0534
[flaml.automl: 09-18 21:39:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:39:22] {3072} INFO -  at 32.7s,	estimator xgboost's best error=0.0474,	best estimator xgboost's best error=0.0474
[flaml.automl: 09-18 21:39:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 21:39:33] {3072} INFO -  at 43.1s,	estimator xgboost's best error=0.0450,	best estimator xgboost's best error=0.0450
[flaml.automl: 09-18 21:39:33] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 21:39:39] {3072} INFO -  at 49.1s,	estimator xgboost's best error=0.0450,	best estimator xgboost's best error=0.0450
[flaml.automl: 09-18 21:39:39] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 21:39:49] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.0450,	best estimator xgboost's best error=0.0450
[flaml.automl: 09-18 21:40:00] {3335} INFO - retrain xgboost for 10.5s
[flaml.automl: 09-18 21:40:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:40:00] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:40:00] {2637} INFO - Time taken to find the best model: 43.06498742103577
[flaml.automl: 09-18 21:40:00] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9550078983414073
CO(0)最好结果：{'pred_time': 1.9679450187362543e-05, 'wall_clock_time': 43.06498742103577, 'metric_for_logging': {'pred_time': 1.9679450187362543e-05}, 'val_loss': 0.04499210165859271, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.37954592704773}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8367930792038865
CO(0)的mse=0.0038045702111174575
CO(0)的mae=0.043829904101800266
CO(0)的mar=0.06125935395240518
总共花费的时间为：70.73
楚雄州
2605A
2606A
[flaml.automl: 09-18 21:46:25] {2390} INFO - task = regression
[flaml.automl: 09-18 21:46:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:46:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:46:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:46:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:46:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:46:27] {3025} INFO - Estimated sufficient time budget=11706s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:46:27] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1105,	best estimator xgboost's best error=0.1105
[flaml.automl: 09-18 21:46:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:46:29] {3072} INFO -  at 3.3s,	estimator xgboost's best error=0.0622,	best estimator xgboost's best error=0.0622
[flaml.automl: 09-18 21:46:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:46:30] {3072} INFO -  at 4.5s,	estimator xgboost's best error=0.0622,	best estimator xgboost's best error=0.0622
[flaml.automl: 09-18 21:46:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:46:39] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0622,	best estimator xgboost's best error=0.0622
[flaml.automl: 09-18 21:46:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:46:40] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.0455,	best estimator xgboost's best error=0.0455
[flaml.automl: 09-18 21:46:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:46:42] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.0407,	best estimator xgboost's best error=0.0407
[flaml.automl: 09-18 21:46:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:46:43] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.0407,	best estimator xgboost's best error=0.0407
[flaml.automl: 09-18 21:46:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:46:46] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.0407,	best estimator xgboost's best error=0.0407
[flaml.automl: 09-18 21:46:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:46:47] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0407,	best estimator xgboost's best error=0.0407
[flaml.automl: 09-18 21:46:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:46:49] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.0407,	best estimator xgboost's best error=0.0407
[flaml.automl: 09-18 21:46:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:46:51] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.0407,	best estimator xgboost's best error=0.0407
[flaml.automl: 09-18 21:46:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:46:52] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0407,	best estimator xgboost's best error=0.0407
[flaml.automl: 09-18 21:46:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:46:58] {3072} INFO -  at 32.6s,	estimator xgboost's best error=0.0353,	best estimator xgboost's best error=0.0353
[flaml.automl: 09-18 21:46:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 21:47:08] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.0335,	best estimator xgboost's best error=0.0335
[flaml.automl: 09-18 21:47:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 21:47:14] {3072} INFO -  at 49.1s,	estimator xgboost's best error=0.0335,	best estimator xgboost's best error=0.0335
[flaml.automl: 09-18 21:47:14] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 21:47:25] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.0333,	best estimator xgboost's best error=0.0333
[flaml.automl: 09-18 21:47:42] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-18 21:47:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:47:42] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:47:42] {2637} INFO - Time taken to find the best model: 59.85034894943237
[flaml.automl: 09-18 21:47:42] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.966746886233944
CO(0)最好结果：{'pred_time': 1.6734174737758604e-05, 'wall_clock_time': 59.85034894943237, 'metric_for_logging': {'pred_time': 1.6734174737758604e-05}, 'val_loss': 0.03325311376605604, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 10.721583843231201}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.887287754846171
CO(0)的mse=0.002428422652301605
CO(0)的mae=0.03313681674249484
CO(0)的mar=0.05158557274385546
总共花费的时间为：77.50
红河州
2609A
3038A
[flaml.automl: 09-18 21:54:14] {2390} INFO - task = regression
[flaml.automl: 09-18 21:54:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:54:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:54:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:54:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:54:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:54:16] {3025} INFO - Estimated sufficient time budget=11753s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:54:16] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1065,	best estimator xgboost's best error=0.1065
[flaml.automl: 09-18 21:54:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:54:18] {3072} INFO -  at 3.3s,	estimator xgboost's best error=0.0630,	best estimator xgboost's best error=0.0630
[flaml.automl: 09-18 21:54:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:54:19] {3072} INFO -  at 4.5s,	estimator xgboost's best error=0.0630,	best estimator xgboost's best error=0.0630
[flaml.automl: 09-18 21:54:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:54:28] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0630,	best estimator xgboost's best error=0.0630
[flaml.automl: 09-18 21:54:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:54:29] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.0497,	best estimator xgboost's best error=0.0497
[flaml.automl: 09-18 21:54:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:54:31] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 21:54:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:54:32] {3072} INFO -  at 18.0s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 21:54:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:54:35] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 21:54:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:54:36] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 21:54:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:54:38] {3072} INFO -  at 23.9s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 21:54:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:54:39] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 21:54:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:54:41] {3072} INFO -  at 26.2s,	estimator xgboost's best error=0.0426,	best estimator xgboost's best error=0.0426
[flaml.automl: 09-18 21:54:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 21:54:47] {3072} INFO -  at 32.2s,	estimator xgboost's best error=0.0384,	best estimator xgboost's best error=0.0384
[flaml.automl: 09-18 21:54:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 21:54:57] {3072} INFO -  at 42.6s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-18 21:54:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 21:55:03] {3072} INFO -  at 48.7s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-18 21:55:03] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 21:55:14] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0365,	best estimator xgboost's best error=0.0365
[flaml.automl: 09-18 21:55:24] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 21:55:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:55:24] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:55:24] {2637} INFO - Time taken to find the best model: 42.590598821640015
[flaml.automl: 09-18 21:55:24] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9635359563828492
CO(0)最好结果：{'pred_time': 1.7430558464672782e-05, 'wall_clock_time': 42.590598821640015, 'metric_for_logging': {'pred_time': 1.7430558464672782e-05}, 'val_loss': 0.03646404361715078, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.389825105667114}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8903767466934778
CO(0)的mse=0.003401828956491446
CO(0)的mae=0.03732230163258616
CO(0)的mar=0.07347782542292797
总共花费的时间为：70.12
文山州
2610A
2611A
[flaml.automl: 09-18 22:02:19] {2390} INFO - task = regression
[flaml.automl: 09-18 22:02:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:02:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:02:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:02:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:02:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:02:23] {3025} INFO - Estimated sufficient time budget=33970s. Estimated necessary time budget=34s.
[flaml.automl: 09-18 22:02:23] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1156,	best estimator xgboost's best error=0.1156
[flaml.automl: 09-18 22:02:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:02:28] {3072} INFO -  at 9.1s,	estimator xgboost's best error=0.0661,	best estimator xgboost's best error=0.0661
[flaml.automl: 09-18 22:02:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:02:31] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.0661,	best estimator xgboost's best error=0.0661
[flaml.automl: 09-18 22:02:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:02:53] {3072} INFO -  at 33.9s,	estimator xgboost's best error=0.0661,	best estimator xgboost's best error=0.0661
[flaml.automl: 09-18 22:02:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:02:55] {3072} INFO -  at 36.0s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-18 22:02:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:02:58] {3072} INFO -  at 39.0s,	estimator xgboost's best error=0.0394,	best estimator xgboost's best error=0.0394
[flaml.automl: 09-18 22:02:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:03:01] {3072} INFO -  at 42.1s,	estimator xgboost's best error=0.0394,	best estimator xgboost's best error=0.0394
[flaml.automl: 09-18 22:03:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:03:05] {3072} INFO -  at 46.5s,	estimator xgboost's best error=0.0394,	best estimator xgboost's best error=0.0394
[flaml.automl: 09-18 22:03:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:03:08] {3072} INFO -  at 48.5s,	estimator xgboost's best error=0.0394,	best estimator xgboost's best error=0.0394
[flaml.automl: 09-18 22:03:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:03:12] {3072} INFO -  at 53.1s,	estimator xgboost's best error=0.0394,	best estimator xgboost's best error=0.0394
[flaml.automl: 09-18 22:03:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:03:14] {3072} INFO -  at 55.3s,	estimator xgboost's best error=0.0394,	best estimator xgboost's best error=0.0394
[flaml.automl: 09-18 22:03:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:03:17] {3072} INFO -  at 57.5s,	estimator xgboost's best error=0.0394,	best estimator xgboost's best error=0.0394
[flaml.automl: 09-18 22:03:19] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-18 22:03:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:03:19] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:03:19] {2637} INFO - Time taken to find the best model: 38.9738609790802
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
CO(0)最佳损失：0.9605990244057943
CO(0)最好结果：{'pred_time': 3.537710691183962e-05, 'wall_clock_time': 38.9738609790802, 'metric_for_logging': {'pred_time': 3.537710691183962e-05}, 'val_loss': 0.039400975594205766, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 2.9893996715545654}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9154405972670803
CO(0)的mse=0.003008653888683177
CO(0)的mae=0.039213223556344434
CO(0)的mar=0.10615697419676048
总共花费的时间为：61.15
西双版纳州
2612A
2613A
[flaml.automl: 09-18 22:09:59] {2390} INFO - task = regression
[flaml.automl: 09-18 22:09:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:09:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:09:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:09:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:09:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:10:01] {3025} INFO - Estimated sufficient time budget=12082s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 22:10:01] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1305,	best estimator xgboost's best error=0.1305
[flaml.automl: 09-18 22:10:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:10:03] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-18 22:10:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:10:04] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-18 22:10:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:10:13] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-18 22:10:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:10:15] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0592,	best estimator xgboost's best error=0.0592
[flaml.automl: 09-18 22:10:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:10:16] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.0518,	best estimator xgboost's best error=0.0518
[flaml.automl: 09-18 22:10:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:10:18] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0518,	best estimator xgboost's best error=0.0518
[flaml.automl: 09-18 22:10:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:10:20] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0518,	best estimator xgboost's best error=0.0518
[flaml.automl: 09-18 22:10:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:10:22] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0518,	best estimator xgboost's best error=0.0518
[flaml.automl: 09-18 22:10:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:10:27] {3072} INFO -  at 27.3s,	estimator xgboost's best error=0.0518,	best estimator xgboost's best error=0.0518
[flaml.automl: 09-18 22:10:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:10:29] {3072} INFO -  at 29.3s,	estimator xgboost's best error=0.0518,	best estimator xgboost's best error=0.0518
[flaml.automl: 09-18 22:10:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:10:31] {3072} INFO -  at 31.5s,	estimator xgboost's best error=0.0518,	best estimator xgboost's best error=0.0518
[flaml.automl: 09-18 22:10:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:10:42] {3072} INFO -  at 42.5s,	estimator xgboost's best error=0.0465,	best estimator xgboost's best error=0.0465
[flaml.automl: 09-18 22:10:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:11:00] {3072} INFO -  at 60.5s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 22:11:32] {3335} INFO - retrain xgboost for 31.9s
[flaml.automl: 09-18 22:11:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:11:32] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:11:32] {2637} INFO - Time taken to find the best model: 60.53528094291687
[flaml.automl: 09-18 22:11:32] {2648} WARNING - Time taken to find the best model is 101% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9563678707854265
CO(0)最好结果：{'pred_time': 5.8154970114431914e-05, 'wall_clock_time': 60.53528094291687, 'metric_for_logging': {'pred_time': 5.8154970114431914e-05}, 'val_loss': 0.04363212921457344, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 18.0095534324646}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9164297656482144
CO(0)的mse=0.004800666438728348
CO(0)的mae=0.043732535199122395
CO(0)的mar=0.09451745253588922
总共花费的时间为：92.94
大理州
2614A
2615A
[flaml.automl: 09-18 22:17:52] {2390} INFO - task = regression
[flaml.automl: 09-18 22:17:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:17:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:17:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:17:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:17:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:17:54] {3025} INFO - Estimated sufficient time budget=22086s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 22:17:54] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.0983,	best estimator xgboost's best error=0.0983
[flaml.automl: 09-18 22:17:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:17:58] {3072} INFO -  at 6.2s,	estimator xgboost's best error=0.0519,	best estimator xgboost's best error=0.0519
[flaml.automl: 09-18 22:17:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:18:00] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.0519,	best estimator xgboost's best error=0.0519
[flaml.automl: 09-18 22:18:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:18:17] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.0519,	best estimator xgboost's best error=0.0519
[flaml.automl: 09-18 22:18:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:18:19] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.0360,	best estimator xgboost's best error=0.0360
[flaml.automl: 09-18 22:18:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:18:22] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0281,	best estimator xgboost's best error=0.0281
[flaml.automl: 09-18 22:18:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:18:25] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.0281,	best estimator xgboost's best error=0.0281
[flaml.automl: 09-18 22:18:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:18:30] {3072} INFO -  at 38.9s,	estimator xgboost's best error=0.0281,	best estimator xgboost's best error=0.0281
[flaml.automl: 09-18 22:18:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:18:34] {3072} INFO -  at 42.2s,	estimator xgboost's best error=0.0281,	best estimator xgboost's best error=0.0281
[flaml.automl: 09-18 22:18:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:18:42] {3072} INFO -  at 50.3s,	estimator xgboost's best error=0.0281,	best estimator xgboost's best error=0.0281
[flaml.automl: 09-18 22:18:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:18:46] {3072} INFO -  at 54.4s,	estimator xgboost's best error=0.0281,	best estimator xgboost's best error=0.0281
[flaml.automl: 09-18 22:18:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:18:49] {3072} INFO -  at 57.9s,	estimator xgboost's best error=0.0281,	best estimator xgboost's best error=0.0281
[flaml.automl: 09-18 22:18:53] {3335} INFO - retrain xgboost for 3.2s
[flaml.automl: 09-18 22:18:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:18:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:18:53] {2637} INFO - Time taken to find the best model: 30.992629051208496
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
CO(0)最佳损失：0.9718698335719899
CO(0)最好结果：{'pred_time': 2.9235163489913323e-05, 'wall_clock_time': 30.992629051208496, 'metric_for_logging': {'pred_time': 2.9235163489913323e-05}, 'val_loss': 0.028130166428010184, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 2.930098056793213}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9180617842072465
CO(0)的mse=0.0016877691070322387
CO(0)的mae=0.028750377032939672
CO(0)的mar=0.05319434108038723
总共花费的时间为：61.66
德宏州
2616A
[flaml.automl: 09-18 22:22:31] {2390} INFO - task = regression
[flaml.automl: 09-18 22:22:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:22:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:22:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:22:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:22:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:22:33] {3025} INFO - Estimated sufficient time budget=22462s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 22:22:33] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.1406,	best estimator xgboost's best error=0.1406
[flaml.automl: 09-18 22:22:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:22:37] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.0875,	best estimator xgboost's best error=0.0875
[flaml.automl: 09-18 22:22:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:22:39] {3072} INFO -  at 8.1s,	estimator xgboost's best error=0.0875,	best estimator xgboost's best error=0.0875
[flaml.automl: 09-18 22:22:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:22:51] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.0875,	best estimator xgboost's best error=0.0875
[flaml.automl: 09-18 22:22:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:22:53] {3072} INFO -  at 22.4s,	estimator xgboost's best error=0.0558,	best estimator xgboost's best error=0.0558
[flaml.automl: 09-18 22:22:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:22:56] {3072} INFO -  at 25.0s,	estimator xgboost's best error=0.0486,	best estimator xgboost's best error=0.0486
[flaml.automl: 09-18 22:22:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:22:59] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.0486,	best estimator xgboost's best error=0.0486
[flaml.automl: 09-18 22:22:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:23:02] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.0486,	best estimator xgboost's best error=0.0486
[flaml.automl: 09-18 22:23:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:23:04] {3072} INFO -  at 33.3s,	estimator xgboost's best error=0.0486,	best estimator xgboost's best error=0.0486
[flaml.automl: 09-18 22:23:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:23:08] {3072} INFO -  at 37.2s,	estimator xgboost's best error=0.0486,	best estimator xgboost's best error=0.0486
[flaml.automl: 09-18 22:23:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:23:10] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.0486,	best estimator xgboost's best error=0.0486
[flaml.automl: 09-18 22:23:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:23:12] {3072} INFO -  at 41.2s,	estimator xgboost's best error=0.0486,	best estimator xgboost's best error=0.0486
[flaml.automl: 09-18 22:23:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:23:19] {3072} INFO -  at 48.5s,	estimator xgboost's best error=0.0486,	best estimator xgboost's best error=0.0486
[flaml.automl: 09-18 22:23:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:23:22] {3072} INFO -  at 50.9s,	estimator xgboost's best error=0.0462,	best estimator xgboost's best error=0.0462
[flaml.automl: 09-18 22:23:22] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 22:23:23] {3072} INFO -  at 52.5s,	estimator xgboost's best error=0.0462,	best estimator xgboost's best error=0.0462
[flaml.automl: 09-18 22:23:23] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 22:23:27] {3072} INFO -  at 56.2s,	estimator xgboost's best error=0.0462,	best estimator xgboost's best error=0.0462
[flaml.automl: 09-18 22:23:27] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 22:23:29] {3072} INFO -  at 58.3s,	estimator xgboost's best error=0.0462,	best estimator xgboost's best error=0.0462
[flaml.automl: 09-18 22:23:32] {3335} INFO - retrain xgboost for 2.4s
[flaml.automl: 09-18 22:23:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:23:32] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:23:32] {2637} INFO - Time taken to find the best model: 50.947628021240234
[flaml.automl: 09-18 22:23:32] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}
CO(0)最佳损失：0.9537742285643015
CO(0)最好结果：{'pred_time': 3.413088085708358e-05, 'wall_clock_time': 50.947628021240234, 'metric_for_logging': {'pred_time': 3.413088085708358e-05}, 'val_loss': 0.04622577143569855, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}, 'config/n_estimators': 8, 'config/max_leaves': 5, 'config/min_child_weight': 0.758814211387459, 'config/learning_rate': 0.29313806474778853, 'config/subsample': 0.7520389032316431, 'config/colsample_bylevel': 0.8985205690300798, 'config/colsample_bytree': 0.7992659131995336, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.2266745236797676, 'experiment_tag': 'exp', 'time_total_s': 2.47708797454834}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.816052233527417
CO(0)的mse=0.005224716824162378
CO(0)的mae=0.050896692253311865
CO(0)的mar=0.0774281529258837
总共花费的时间为：61.00
怒江州
2618A
2619A
[flaml.automl: 09-18 22:30:11] {2390} INFO - task = regression
[flaml.automl: 09-18 22:30:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:30:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:30:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:30:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:30:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:30:13] {3025} INFO - Estimated sufficient time budget=22616s. Estimated necessary time budget=23s.
[flaml.automl: 09-18 22:30:13] {3072} INFO -  at 2.4s,	estimator xgboost's best error=0.2337,	best estimator xgboost's best error=0.2337
[flaml.automl: 09-18 22:30:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:30:17] {3072} INFO -  at 6.5s,	estimator xgboost's best error=0.1127,	best estimator xgboost's best error=0.1127
[flaml.automl: 09-18 22:30:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:30:20] {3072} INFO -  at 8.8s,	estimator xgboost's best error=0.1127,	best estimator xgboost's best error=0.1127
[flaml.automl: 09-18 22:30:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:30:37] {3072} INFO -  at 26.2s,	estimator xgboost's best error=0.1127,	best estimator xgboost's best error=0.1127
[flaml.automl: 09-18 22:30:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:30:39] {3072} INFO -  at 28.3s,	estimator xgboost's best error=0.0627,	best estimator xgboost's best error=0.0627
[flaml.automl: 09-18 22:30:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:30:42] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0500,	best estimator xgboost's best error=0.0500
[flaml.automl: 09-18 22:30:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:30:45] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.0500,	best estimator xgboost's best error=0.0500
[flaml.automl: 09-18 22:30:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:30:49] {3072} INFO -  at 38.5s,	estimator xgboost's best error=0.0500,	best estimator xgboost's best error=0.0500
[flaml.automl: 09-18 22:30:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:30:52] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.0500,	best estimator xgboost's best error=0.0500
[flaml.automl: 09-18 22:30:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:30:56] {3072} INFO -  at 45.4s,	estimator xgboost's best error=0.0500,	best estimator xgboost's best error=0.0500
[flaml.automl: 09-18 22:30:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:30:58] {3072} INFO -  at 47.4s,	estimator xgboost's best error=0.0500,	best estimator xgboost's best error=0.0500
[flaml.automl: 09-18 22:30:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:31:00] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0500,	best estimator xgboost's best error=0.0500
[flaml.automl: 09-18 22:31:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:31:10] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0376,	best estimator xgboost's best error=0.0376
[flaml.automl: 09-18 22:31:21] {3335} INFO - retrain xgboost for 11.0s
[flaml.automl: 09-18 22:31:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:31:21] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:31:21] {2637} INFO - Time taken to find the best model: 59.37637138366699
[flaml.automl: 09-18 22:31:21] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9623949210922158
CO(0)最好结果：{'pred_time': 3.2323538451581386e-05, 'wall_clock_time': 59.37637138366699, 'metric_for_logging': {'pred_time': 3.2323538451581386e-05}, 'val_loss': 0.037605078907784174, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.891680002212524}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9570802084073393
CO(0)的mse=0.0030520401122923584
CO(0)的mae=0.03933150230700958
CO(0)的mar=0.05070911922119032
总共花费的时间为：70.88
迪庆州
迪庆州没有数据
昌都市
2622A
[flaml.automl: 09-18 22:34:57] {2390} INFO - task = regression
[flaml.automl: 09-18 22:34:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:34:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:34:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:34:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:34:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:34:59] {3025} INFO - Estimated sufficient time budget=12033s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 22:34:59] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1040,	best estimator xgboost's best error=0.1040
[flaml.automl: 09-18 22:34:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:35:01] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.0727,	best estimator xgboost's best error=0.0727
[flaml.automl: 09-18 22:35:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:35:02] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0727,	best estimator xgboost's best error=0.0727
[flaml.automl: 09-18 22:35:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:35:10] {3072} INFO -  at 13.0s,	estimator xgboost's best error=0.0727,	best estimator xgboost's best error=0.0727
[flaml.automl: 09-18 22:35:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:35:13] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.0596,	best estimator xgboost's best error=0.0596
[flaml.automl: 09-18 22:35:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:35:15] {3072} INFO -  at 18.0s,	estimator xgboost's best error=0.0594,	best estimator xgboost's best error=0.0594
[flaml.automl: 09-18 22:35:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:35:18] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0563,	best estimator xgboost's best error=0.0563
[flaml.automl: 09-18 22:35:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:35:23] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0563,	best estimator xgboost's best error=0.0563
[flaml.automl: 09-18 22:35:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:35:26] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.0563,	best estimator xgboost's best error=0.0563
[flaml.automl: 09-18 22:35:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:35:31] {3072} INFO -  at 33.3s,	estimator xgboost's best error=0.0545,	best estimator xgboost's best error=0.0545
[flaml.automl: 09-18 22:35:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:35:34] {3072} INFO -  at 36.3s,	estimator xgboost's best error=0.0545,	best estimator xgboost's best error=0.0545
[flaml.automl: 09-18 22:35:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:35:36] {3072} INFO -  at 38.3s,	estimator xgboost's best error=0.0545,	best estimator xgboost's best error=0.0545
[flaml.automl: 09-18 22:35:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:35:54] {3072} INFO -  at 56.3s,	estimator xgboost's best error=0.0503,	best estimator xgboost's best error=0.0503
[flaml.automl: 09-18 22:36:11] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-18 22:36:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:36:11] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:36:11] {2637} INFO - Time taken to find the best model: 56.284762382507324
[flaml.automl: 09-18 22:36:11] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
CO(0)最佳损失：0.9496912998407335
CO(0)最好结果：{'pred_time': 6.687601473486166e-05, 'wall_clock_time': 56.284762382507324, 'metric_for_logging': {'pred_time': 6.687601473486166e-05}, 'val_loss': 0.050308700159266466, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 12, 'config/max_leaves': 14, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 17.945546865463257}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7005178343286441
CO(0)的mse=0.007146748427743725
CO(0)的mae=0.05097734248736377
CO(0)的mar=0.07595905445418238
总共花费的时间为：73.92
山南市
2624A
2625A
[flaml.automl: 09-18 22:42:39] {2390} INFO - task = regression
[flaml.automl: 09-18 22:42:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:42:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:42:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:42:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:42:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:42:40] {3025} INFO - Estimated sufficient time budget=12038s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 22:42:40] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.0890,	best estimator xgboost's best error=0.0890
[flaml.automl: 09-18 22:42:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:42:43] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 22:42:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:42:44] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 22:42:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:42:53] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0502,	best estimator xgboost's best error=0.0502
[flaml.automl: 09-18 22:42:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:42:54] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0335,	best estimator xgboost's best error=0.0335
[flaml.automl: 09-18 22:42:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:42:56] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0299,	best estimator xgboost's best error=0.0299
[flaml.automl: 09-18 22:42:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:42:58] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0282,	best estimator xgboost's best error=0.0282
[flaml.automl: 09-18 22:42:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:43:00] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0282,	best estimator xgboost's best error=0.0282
[flaml.automl: 09-18 22:43:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:43:02] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.0282,	best estimator xgboost's best error=0.0282
[flaml.automl: 09-18 22:43:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:43:05] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.0251,	best estimator xgboost's best error=0.0251
[flaml.automl: 09-18 22:43:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:43:07] {3072} INFO -  at 27.5s,	estimator xgboost's best error=0.0251,	best estimator xgboost's best error=0.0251
[flaml.automl: 09-18 22:43:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:43:08] {3072} INFO -  at 28.7s,	estimator xgboost's best error=0.0251,	best estimator xgboost's best error=0.0251
[flaml.automl: 09-18 22:43:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:43:20] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.0211,	best estimator xgboost's best error=0.0211
[flaml.automl: 09-18 22:43:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:43:39] {3072} INFO -  at 60.1s,	estimator xgboost's best error=0.0201,	best estimator xgboost's best error=0.0201
[flaml.automl: 09-18 22:44:19] {3335} INFO - retrain xgboost for 40.3s
[flaml.automl: 09-18 22:44:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:44:19] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:44:19] {2637} INFO - Time taken to find the best model: 60.10757040977478
[flaml.automl: 09-18 22:44:19] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
CO(0)最佳损失：0.9798644763995
CO(0)最好结果：{'pred_time': 3.433138243714522e-05, 'wall_clock_time': 60.10757040977478, 'metric_for_logging': {'pred_time': 3.433138243714522e-05}, 'val_loss': 0.020135523600499923, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 19.427895307540894}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9320255924565783
CO(0)的mse=0.0010616400897592788
CO(0)的mae=0.020305375842543574
CO(0)的mar=0.05036706173770084
总共花费的时间为：100.89
日喀则市
2626A
[flaml.automl: 09-18 22:47:28] {2390} INFO - task = regression
[flaml.automl: 09-18 22:47:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:47:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:47:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:47:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:47:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:47:31] {3025} INFO - Estimated sufficient time budget=21197s. Estimated necessary time budget=21s.
[flaml.automl: 09-18 22:47:31] {3072} INFO -  at 2.2s,	estimator xgboost's best error=0.0775,	best estimator xgboost's best error=0.0775
[flaml.automl: 09-18 22:47:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:47:34] {3072} INFO -  at 5.2s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-18 22:47:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:47:36] {3072} INFO -  at 7.6s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-18 22:47:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:47:47] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.0603,	best estimator xgboost's best error=0.0603
[flaml.automl: 09-18 22:47:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:47:49] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.0513,	best estimator xgboost's best error=0.0513
[flaml.automl: 09-18 22:47:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:47:52] {3072} INFO -  at 23.1s,	estimator xgboost's best error=0.0513,	best estimator xgboost's best error=0.0513
[flaml.automl: 09-18 22:47:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:47:54] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.0492,	best estimator xgboost's best error=0.0492
[flaml.automl: 09-18 22:47:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:47:58] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0492,	best estimator xgboost's best error=0.0492
[flaml.automl: 09-18 22:47:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:48:00] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.0492,	best estimator xgboost's best error=0.0492
[flaml.automl: 09-18 22:48:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:48:04] {3072} INFO -  at 35.8s,	estimator xgboost's best error=0.0477,	best estimator xgboost's best error=0.0477
[flaml.automl: 09-18 22:48:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:48:07] {3072} INFO -  at 38.4s,	estimator xgboost's best error=0.0474,	best estimator xgboost's best error=0.0474
[flaml.automl: 09-18 22:48:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:48:09] {3072} INFO -  at 40.3s,	estimator xgboost's best error=0.0474,	best estimator xgboost's best error=0.0474
[flaml.automl: 09-18 22:48:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:48:19] {3072} INFO -  at 50.7s,	estimator xgboost's best error=0.0436,	best estimator xgboost's best error=0.0436
[flaml.automl: 09-18 22:48:33] {3335} INFO - retrain xgboost for 13.5s
[flaml.automl: 09-18 22:48:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.8668816728674211, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158146,
             max_delta_step=0, max_depth=0, max_leaves=8,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:48:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:48:33] {2637} INFO - Time taken to find the best model: 50.73409175872803
[flaml.automl: 09-18 22:48:33] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 12, 'max_leaves': 8, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158146, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
CO(0)最佳损失：0.9563941583354337
CO(0)最好结果：{'pred_time': 9.987047284741864e-05, 'wall_clock_time': 50.73409175872803, 'metric_for_logging': {'pred_time': 9.987047284741864e-05}, 'val_loss': 0.04360584166456625, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 8, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158146, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 12, 'config/max_leaves': 8, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158146, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 10.384086847305298}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.8668816728674211, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158146,
             max_delta_step=0, max_depth=0, max_leaves=8,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.6704242553615618
CO(0)的mse=0.00706903752962345
CO(0)的mae=0.04555017775476697
CO(0)的mar=0.07944394314527621
总共花费的时间为：64.59
那曲地区
2628A
[flaml.automl: 09-18 22:51:46] {2390} INFO - task = regression
[flaml.automl: 09-18 22:51:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:51:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:51:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:51:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:51:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:51:49] {3025} INFO - Estimated sufficient time budget=21966s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 22:51:49] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.2481,	best estimator xgboost's best error=0.2481
[flaml.automl: 09-18 22:51:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:51:52] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.1841,	best estimator xgboost's best error=0.1841
[flaml.automl: 09-18 22:51:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:51:54] {3072} INFO -  at 7.9s,	estimator xgboost's best error=0.1841,	best estimator xgboost's best error=0.1841
[flaml.automl: 09-18 22:51:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:52:07] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.1841,	best estimator xgboost's best error=0.1841
[flaml.automl: 09-18 22:52:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:52:10] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.1639,	best estimator xgboost's best error=0.1639
[flaml.automl: 09-18 22:52:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:52:13] {3072} INFO -  at 26.2s,	estimator xgboost's best error=0.1639,	best estimator xgboost's best error=0.1639
[flaml.automl: 09-18 22:52:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:52:16] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.1526,	best estimator xgboost's best error=0.1526
[flaml.automl: 09-18 22:52:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:52:20] {3072} INFO -  at 33.7s,	estimator xgboost's best error=0.1526,	best estimator xgboost's best error=0.1526
[flaml.automl: 09-18 22:52:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:52:23] {3072} INFO -  at 36.7s,	estimator xgboost's best error=0.1504,	best estimator xgboost's best error=0.1504
[flaml.automl: 09-18 22:52:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:52:28] {3072} INFO -  at 41.5s,	estimator xgboost's best error=0.1447,	best estimator xgboost's best error=0.1447
[flaml.automl: 09-18 22:52:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:52:30] {3072} INFO -  at 43.6s,	estimator xgboost's best error=0.1447,	best estimator xgboost's best error=0.1447
[flaml.automl: 09-18 22:52:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:52:46] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.1447,	best estimator xgboost's best error=0.1447
[flaml.automl: 09-18 22:52:49] {3335} INFO - retrain xgboost for 3.5s
[flaml.automl: 09-18 22:52:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7131638226939452, colsample_bynode=1,
             colsample_bytree=0.9042542631072509, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.516196926773037,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.11142535609078662, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=1.7275428408777198,
             scale_pos_weight=1, subsample=0.8604509987112716,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:52:49] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:52:49] {2637} INFO - Time taken to find the best model: 41.53582191467285
CO(0)最佳参数：{'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.11142535609078662, 'learning_rate': 0.516196926773037, 'subsample': 0.8604509987112716, 'colsample_bylevel': 0.7131638226939452, 'colsample_bytree': 0.9042542631072509, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 1.7275428408777198}
CO(0)最佳损失：0.8553167894279134
CO(0)最好结果：{'pred_time': 5.719456179388638e-05, 'wall_clock_time': 41.53582191467285, 'metric_for_logging': {'pred_time': 5.719456179388638e-05}, 'val_loss': 0.14468321057208652, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.11142535609078662, 'learning_rate': 0.516196926773037, 'subsample': 0.8604509987112716, 'colsample_bylevel': 0.7131638226939452, 'colsample_bytree': 0.9042542631072509, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 1.7275428408777198}, 'config/n_estimators': 6, 'config/max_leaves': 7, 'config/min_child_weight': 0.11142535609078662, 'config/learning_rate': 0.516196926773037, 'config/subsample': 0.8604509987112716, 'config/colsample_bylevel': 0.7131638226939452, 'config/colsample_bytree': 0.9042542631072509, 'config/reg_alpha': 0.007347191482940116, 'config/reg_lambda': 1.7275428408777198, 'experiment_tag': 'exp', 'time_total_s': 4.798426389694214}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7131638226939452, colsample_bynode=1,
             colsample_bytree=0.9042542631072509, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.516196926773037,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.11142535609078662, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=1.7275428408777198,
             scale_pos_weight=1, subsample=0.8604509987112716,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.66615931386428
CO(0)的mse=0.05215829276530751
CO(0)的mae=0.14731532806810063
CO(0)的mar=0.2685703034502221
总共花费的时间为：63.33
阿里地区
2630A
[flaml.automl: 09-18 22:56:32] {2390} INFO - task = regression
[flaml.automl: 09-18 22:56:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:56:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:56:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:56:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:56:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:56:33] {3025} INFO - Estimated sufficient time budget=11924s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 22:56:33] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.0971,	best estimator xgboost's best error=0.0971
[flaml.automl: 09-18 22:56:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:56:35] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.0632,	best estimator xgboost's best error=0.0632
[flaml.automl: 09-18 22:56:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:56:36] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0632,	best estimator xgboost's best error=0.0632
[flaml.automl: 09-18 22:56:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:56:43] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.0632,	best estimator xgboost's best error=0.0632
[flaml.automl: 09-18 22:56:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:56:45] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0418,	best estimator xgboost's best error=0.0418
[flaml.automl: 09-18 22:56:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:56:46] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.0418,	best estimator xgboost's best error=0.0418
[flaml.automl: 09-18 22:56:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:56:48] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.0391,	best estimator xgboost's best error=0.0391
[flaml.automl: 09-18 22:56:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:56:50] {3072} INFO -  at 18.0s,	estimator xgboost's best error=0.0391,	best estimator xgboost's best error=0.0391
[flaml.automl: 09-18 22:56:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:56:52] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0391,	best estimator xgboost's best error=0.0391
[flaml.automl: 09-18 22:56:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:56:54] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.0391,	best estimator xgboost's best error=0.0391
[flaml.automl: 09-18 22:56:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:56:56] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0375,	best estimator xgboost's best error=0.0375
[flaml.automl: 09-18 22:56:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:56:57] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.0375,	best estimator xgboost's best error=0.0375
[flaml.automl: 09-18 22:56:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:57:03] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.0336,	best estimator xgboost's best error=0.0336
[flaml.automl: 09-18 22:57:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:57:12] {3072} INFO -  at 40.0s,	estimator xgboost's best error=0.0320,	best estimator xgboost's best error=0.0320
[flaml.automl: 09-18 22:57:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 22:57:17] {3072} INFO -  at 45.1s,	estimator xgboost's best error=0.0320,	best estimator xgboost's best error=0.0320
[flaml.automl: 09-18 22:57:17] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 22:57:32] {3072} INFO -  at 59.6s,	estimator xgboost's best error=0.0320,	best estimator xgboost's best error=0.0320
[flaml.automl: 09-18 22:57:41] {3335} INFO - retrain xgboost for 8.9s
[flaml.automl: 09-18 22:57:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:57:41] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:57:41] {2637} INFO - Time taken to find the best model: 39.95012283325195
CO(0)最佳参数：{'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
CO(0)最佳损失：0.96802841040203
CO(0)最好结果：{'pred_time': 3.776083821835725e-05, 'wall_clock_time': 39.95012283325195, 'metric_for_logging': {'pred_time': 3.776083821835725e-05}, 'val_loss': 0.031971589597969946, 'training_iteration': 1, 'config': {'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 25, 'config/max_leaves': 6, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 8.946839332580566}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8308396932633038
CO(0)的mse=0.003022868165392734
CO(0)的mae=0.033715464763797436
CO(0)的mar=0.07861729070177685
总共花费的时间为：68.75
林芝市
2632A
2633A
[flaml.automl: 09-18 23:04:36] {2390} INFO - task = regression
[flaml.automl: 09-18 23:04:36] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:04:36] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:04:36] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:04:36] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:04:36] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:04:38] {3025} INFO - Estimated sufficient time budget=21589s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 23:04:38] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.0863,	best estimator xgboost's best error=0.0863
[flaml.automl: 09-18 23:04:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:04:42] {3072} INFO -  at 6.4s,	estimator xgboost's best error=0.0478,	best estimator xgboost's best error=0.0478
[flaml.automl: 09-18 23:04:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:04:44] {3072} INFO -  at 8.7s,	estimator xgboost's best error=0.0478,	best estimator xgboost's best error=0.0478
[flaml.automl: 09-18 23:04:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:04:55] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0478,	best estimator xgboost's best error=0.0478
[flaml.automl: 09-18 23:04:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:04:56] {3072} INFO -  at 20.0s,	estimator xgboost's best error=0.0348,	best estimator xgboost's best error=0.0348
[flaml.automl: 09-18 23:04:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:04:57] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-18 23:04:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:04:59] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-18 23:04:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:05:01] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-18 23:05:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:05:02] {3072} INFO -  at 26.8s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-18 23:05:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:05:05] {3072} INFO -  at 29.3s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-18 23:05:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:05:06] {3072} INFO -  at 30.4s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-18 23:05:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:05:07] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.0273,	best estimator xgboost's best error=0.0273
[flaml.automl: 09-18 23:05:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:05:13] {3072} INFO -  at 37.7s,	estimator xgboost's best error=0.0215,	best estimator xgboost's best error=0.0215
[flaml.automl: 09-18 23:05:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 23:05:24] {3072} INFO -  at 48.0s,	estimator xgboost's best error=0.0196,	best estimator xgboost's best error=0.0196
[flaml.automl: 09-18 23:05:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 23:05:30] {3072} INFO -  at 54.1s,	estimator xgboost's best error=0.0196,	best estimator xgboost's best error=0.0196
[flaml.automl: 09-18 23:05:40] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-18 23:05:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:05:40] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:05:40] {2637} INFO - Time taken to find the best model: 48.018091440200806
[flaml.automl: 09-18 23:05:40] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.98040060933505
CO(0)最好结果：{'pred_time': 1.68205355440238e-05, 'wall_clock_time': 48.018091440200806, 'metric_for_logging': {'pred_time': 1.68205355440238e-05}, 'val_loss': 0.019599390664949933, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.360982656478882}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9552176681605284
CO(0)的mse=0.0011042100085475117
CO(0)的mae=0.020711053121606284
CO(0)的mar=0.04497453868538543
总共花费的时间为：65.04
汉中市
2634A
2635A
2636A
2637A
[flaml.automl: 09-18 23:18:38] {2390} INFO - task = regression
[flaml.automl: 09-18 23:18:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:18:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:18:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:18:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:18:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:18:39] {3025} INFO - Estimated sufficient time budget=49226s. Estimated necessary time budget=49s.
[flaml.automl: 09-18 23:18:39] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.2033,	best estimator xgboost's best error=0.2033
[flaml.automl: 09-18 23:18:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:18:42] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.1125,	best estimator xgboost's best error=0.1125
[flaml.automl: 09-18 23:18:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:18:43] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1125,	best estimator xgboost's best error=0.1125
[flaml.automl: 09-18 23:18:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:18:49] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.1125,	best estimator xgboost's best error=0.1125
[flaml.automl: 09-18 23:18:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:18:50] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0848,	best estimator xgboost's best error=0.0848
[flaml.automl: 09-18 23:18:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:18:52] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0737,	best estimator xgboost's best error=0.0737
[flaml.automl: 09-18 23:18:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:18:53] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0737,	best estimator xgboost's best error=0.0737
[flaml.automl: 09-18 23:18:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:18:56] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0737,	best estimator xgboost's best error=0.0737
[flaml.automl: 09-18 23:18:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:18:57] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0737,	best estimator xgboost's best error=0.0737
[flaml.automl: 09-18 23:18:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:19:00] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0737,	best estimator xgboost's best error=0.0737
[flaml.automl: 09-18 23:19:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:19:01] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0732,	best estimator xgboost's best error=0.0732
[flaml.automl: 09-18 23:19:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:19:02] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0732,	best estimator xgboost's best error=0.0732
[flaml.automl: 09-18 23:19:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:19:09] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-18 23:19:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 23:19:21] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.0650,	best estimator xgboost's best error=0.0650
[flaml.automl: 09-18 23:19:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 23:19:27] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0650,	best estimator xgboost's best error=0.0650
[flaml.automl: 09-18 23:19:40] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 23:19:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:19:40] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:19:40] {2637} INFO - Time taken to find the best model: 42.951770305633545
[flaml.automl: 09-18 23:19:40] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41388}
CO(0)最佳损失：0.9350041028140395
CO(0)最好结果：{'pred_time': 8.726099258558884e-06, 'wall_clock_time': 42.951770305633545, 'metric_for_logging': {'pred_time': 8.726099258558884e-06}, 'val_loss': 0.06499589718596058, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41388}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41388, 'experiment_tag': 'exp', 'time_total_s': 12.005198001861572}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8899071512542773
CO(0)的mse=0.011411745910942834
CO(0)的mae=0.06769570037950041
CO(0)的mar=0.09791355641315824
总共花费的时间为：62.22
榆林市
2638A
2639A
2640A
2641A
[flaml.automl: 09-18 23:32:32] {2390} INFO - task = regression
[flaml.automl: 09-18 23:32:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:32:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:32:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:32:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:32:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:32:33] {3025} INFO - Estimated sufficient time budget=49681s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 23:32:33] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1511,	best estimator xgboost's best error=0.1511
[flaml.automl: 09-18 23:32:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:32:35] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0986,	best estimator xgboost's best error=0.0986
[flaml.automl: 09-18 23:32:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:32:36] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0986,	best estimator xgboost's best error=0.0986
[flaml.automl: 09-18 23:32:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:32:43] {3072} INFO -  at 11.0s,	estimator xgboost's best error=0.0986,	best estimator xgboost's best error=0.0986
[flaml.automl: 09-18 23:32:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:32:44] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0837,	best estimator xgboost's best error=0.0837
[flaml.automl: 09-18 23:32:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:32:45] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.0760,	best estimator xgboost's best error=0.0760
[flaml.automl: 09-18 23:32:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:32:47] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0760,	best estimator xgboost's best error=0.0760
[flaml.automl: 09-18 23:32:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:32:49] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.0760,	best estimator xgboost's best error=0.0760
[flaml.automl: 09-18 23:32:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:32:51] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0760,	best estimator xgboost's best error=0.0760
[flaml.automl: 09-18 23:32:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:32:53] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0760,	best estimator xgboost's best error=0.0760
[flaml.automl: 09-18 23:32:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:32:55] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.0755,	best estimator xgboost's best error=0.0755
[flaml.automl: 09-18 23:32:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:32:56] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0755,	best estimator xgboost's best error=0.0755
[flaml.automl: 09-18 23:32:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:33:03] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.0713,	best estimator xgboost's best error=0.0713
[flaml.automl: 09-18 23:33:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 23:33:15] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.0672,	best estimator xgboost's best error=0.0672
[flaml.automl: 09-18 23:33:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 23:33:21] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0672,	best estimator xgboost's best error=0.0672
[flaml.automl: 09-18 23:33:33] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 23:33:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:33:33] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:33:33] {2637} INFO - Time taken to find the best model: 43.00346398353577
[flaml.automl: 09-18 23:33:33] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41442}
CO(0)最佳损失：0.9328056364880064
CO(0)最好结果：{'pred_time': 8.981623944708113e-06, 'wall_clock_time': 43.00346398353577, 'metric_for_logging': {'pred_time': 8.981623944708113e-06}, 'val_loss': 0.06719436351199362, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41442}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41442, 'experiment_tag': 'exp', 'time_total_s': 12.101190567016602}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.794144126274672
CO(0)的mse=0.013775218718382908
CO(0)的mae=0.06953010433489441
CO(0)的mar=0.108807352410558
总共花费的时间为：62.26
安康市
2642A
2643A
2644A
[flaml.automl: 09-18 23:44:05] {2390} INFO - task = regression
[flaml.automl: 09-18 23:44:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:44:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:44:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:44:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:44:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:44:06] {3025} INFO - Estimated sufficient time budget=11812s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 23:44:06] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1084,	best estimator xgboost's best error=0.1084
[flaml.automl: 09-18 23:44:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:44:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0693,	best estimator xgboost's best error=0.0693
[flaml.automl: 09-18 23:44:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:44:09] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0693,	best estimator xgboost's best error=0.0693
[flaml.automl: 09-18 23:44:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:44:23] {3072} INFO -  at 18.2s,	estimator xgboost's best error=0.0693,	best estimator xgboost's best error=0.0693
[flaml.automl: 09-18 23:44:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:44:25] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0572,	best estimator xgboost's best error=0.0572
[flaml.automl: 09-18 23:44:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:44:28] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.0527,	best estimator xgboost's best error=0.0527
[flaml.automl: 09-18 23:44:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:44:31] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.0521,	best estimator xgboost's best error=0.0521
[flaml.automl: 09-18 23:44:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:44:36] {3072} INFO -  at 31.4s,	estimator xgboost's best error=0.0521,	best estimator xgboost's best error=0.0521
[flaml.automl: 09-18 23:44:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:44:39] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.0521,	best estimator xgboost's best error=0.0521
[flaml.automl: 09-18 23:44:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:44:45] {3072} INFO -  at 40.1s,	estimator xgboost's best error=0.0504,	best estimator xgboost's best error=0.0504
[flaml.automl: 09-18 23:44:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:44:48] {3072} INFO -  at 43.2s,	estimator xgboost's best error=0.0504,	best estimator xgboost's best error=0.0504
[flaml.automl: 09-18 23:44:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:44:50] {3072} INFO -  at 45.3s,	estimator xgboost's best error=0.0504,	best estimator xgboost's best error=0.0504
[flaml.automl: 09-18 23:44:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:45:04] {3072} INFO -  at 58.9s,	estimator xgboost's best error=0.0469,	best estimator xgboost's best error=0.0469
[flaml.automl: 09-18 23:45:27] {3335} INFO - retrain xgboost for 23.8s
[flaml.automl: 09-18 23:45:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:45:27] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:45:27] {2637} INFO - Time taken to find the best model: 58.94373059272766
[flaml.automl: 09-18 23:45:27] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
CO(0)最佳损失：0.9531145555602195
CO(0)最好结果：{'pred_time': 2.293540894585867e-05, 'wall_clock_time': 58.94373059272766, 'metric_for_logging': {'pred_time': 2.293540894585867e-05}, 'val_loss': 0.046885444439780495, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 13.599281787872314}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8882383968332903
CO(0)的mse=0.004332584295102602
CO(0)的mae=0.04290450971254671
CO(0)的mar=0.08002402299838308
总共花费的时间为：83.31
商洛市
2645A
[flaml.automl: 09-18 23:48:43] {2390} INFO - task = regression
[flaml.automl: 09-18 23:48:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:48:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:48:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:48:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:48:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:48:44] {3025} INFO - Estimated sufficient time budget=11997s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 23:48:44] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1118,	best estimator xgboost's best error=0.1118
[flaml.automl: 09-18 23:48:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:48:46] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.0777,	best estimator xgboost's best error=0.0777
[flaml.automl: 09-18 23:48:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:48:47] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0777,	best estimator xgboost's best error=0.0777
[flaml.automl: 09-18 23:48:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:48:54] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.0777,	best estimator xgboost's best error=0.0777
[flaml.automl: 09-18 23:48:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:48:55] {3072} INFO -  at 12.6s,	estimator xgboost's best error=0.0620,	best estimator xgboost's best error=0.0620
[flaml.automl: 09-18 23:48:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:48:57] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0609,	best estimator xgboost's best error=0.0609
[flaml.automl: 09-18 23:48:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:48:59] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.0555,	best estimator xgboost's best error=0.0555
[flaml.automl: 09-18 23:48:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:49:01] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.0555,	best estimator xgboost's best error=0.0555
[flaml.automl: 09-18 23:49:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:49:02] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0555,	best estimator xgboost's best error=0.0555
[flaml.automl: 09-18 23:49:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:49:05] {3072} INFO -  at 22.4s,	estimator xgboost's best error=0.0506,	best estimator xgboost's best error=0.0506
[flaml.automl: 09-18 23:49:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:49:07] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0506,	best estimator xgboost's best error=0.0506
[flaml.automl: 09-18 23:49:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:49:08] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.0506,	best estimator xgboost's best error=0.0506
[flaml.automl: 09-18 23:49:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:49:25] {3072} INFO -  at 42.8s,	estimator xgboost's best error=0.0468,	best estimator xgboost's best error=0.0468
[flaml.automl: 09-18 23:49:43] {3335} INFO - retrain xgboost for 17.6s
[flaml.automl: 09-18 23:49:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:49:43] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:49:43] {2637} INFO - Time taken to find the best model: 42.75070929527283
[flaml.automl: 09-18 23:49:43] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
CO(0)最佳损失：0.9531991270567095
CO(0)最好结果：{'pred_time': 6.678425952007896e-05, 'wall_clock_time': 42.75070929527283, 'metric_for_logging': {'pred_time': 6.678425952007896e-05}, 'val_loss': 0.04680087294329055, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 12, 'config/max_leaves': 14, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 17.620433807373047}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8665080690507994
CO(0)的mse=0.004829562364000161
CO(0)的mae=0.04608196871294788
CO(0)的mar=0.11130159301030948
总共花费的时间为：60.73
白银市
2647A
2648A
[flaml.automl: 09-18 23:56:38] {2390} INFO - task = regression
[flaml.automl: 09-18 23:56:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:56:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:56:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:56:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:56:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:56:43] {3025} INFO - Estimated sufficient time budget=48952s. Estimated necessary time budget=49s.
[flaml.automl: 09-18 23:56:43] {3072} INFO -  at 5.1s,	estimator xgboost's best error=0.1858,	best estimator xgboost's best error=0.1858
[flaml.automl: 09-18 23:56:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:56:55] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.1245,	best estimator xgboost's best error=0.1245
[flaml.automl: 09-18 23:56:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:56:59] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.1245,	best estimator xgboost's best error=0.1245
[flaml.automl: 09-18 23:56:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:57:15] {3072} INFO -  at 37.0s,	estimator xgboost's best error=0.1245,	best estimator xgboost's best error=0.1245
[flaml.automl: 09-18 23:57:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:57:17] {3072} INFO -  at 38.8s,	estimator xgboost's best error=0.1074,	best estimator xgboost's best error=0.1074
[flaml.automl: 09-18 23:57:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:57:19] {3072} INFO -  at 41.2s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-18 23:57:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:57:21] {3072} INFO -  at 43.6s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-18 23:57:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:57:25] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-18 23:57:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:57:26] {3072} INFO -  at 48.3s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-18 23:57:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:57:30] {3072} INFO -  at 52.0s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-18 23:57:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:57:32] {3072} INFO -  at 53.8s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-18 23:57:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:57:33] {3072} INFO -  at 55.7s,	estimator xgboost's best error=0.0977,	best estimator xgboost's best error=0.0977
[flaml.automl: 09-18 23:57:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:57:37] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.0950,	best estimator xgboost's best error=0.0950
[flaml.automl: 09-18 23:57:48] {3335} INFO - retrain xgboost for 10.3s
[flaml.automl: 09-18 23:57:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:57:48] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:57:48] {2637} INFO - Time taken to find the best model: 59.548285722732544
[flaml.automl: 09-18 23:57:48] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.9049739950092873
CO(0)最好结果：{'pred_time': 2.2254795069739782e-05, 'wall_clock_time': 59.548285722732544, 'metric_for_logging': {'pred_time': 2.2254795069739782e-05}, 'val_loss': 0.09502600499071269, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 3.8103408813476562}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7738211419487601
CO(0)的mse=0.020749437813333434
CO(0)的mae=0.09306152573511292
CO(0)的mar=0.14794226105775427
总共花费的时间为：70.73
天水市
2649A
2650A
2651A
[flaml.automl: 09-19 00:08:10] {2390} INFO - task = regression
[flaml.automl: 09-19 00:08:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:08:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:08:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:08:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:08:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:08:11] {3025} INFO - Estimated sufficient time budget=12203s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:08:11] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1644,	best estimator xgboost's best error=0.1644
[flaml.automl: 09-19 00:08:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:08:13] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0975,	best estimator xgboost's best error=0.0975
[flaml.automl: 09-19 00:08:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:08:14] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0975,	best estimator xgboost's best error=0.0975
[flaml.automl: 09-19 00:08:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:08:24] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0975,	best estimator xgboost's best error=0.0975
[flaml.automl: 09-19 00:08:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:08:25] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.0758,	best estimator xgboost's best error=0.0758
[flaml.automl: 09-19 00:08:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:08:27] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-19 00:08:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:08:29] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-19 00:08:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:08:31] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-19 00:08:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:08:32] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-19 00:08:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:08:35] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-19 00:08:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:08:36] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-19 00:08:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:08:37] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.0668,	best estimator xgboost's best error=0.0668
[flaml.automl: 09-19 00:08:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:08:44] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.0598,	best estimator xgboost's best error=0.0598
[flaml.automl: 09-19 00:08:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:08:56] {3072} INFO -  at 46.2s,	estimator xgboost's best error=0.0560,	best estimator xgboost's best error=0.0560
[flaml.automl: 09-19 00:08:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 00:09:02] {3072} INFO -  at 52.7s,	estimator xgboost's best error=0.0560,	best estimator xgboost's best error=0.0560
[flaml.automl: 09-19 00:09:21] {3335} INFO - retrain xgboost for 18.6s
[flaml.automl: 09-19 00:09:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:09:21] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:09:21] {2637} INFO - Time taken to find the best model: 46.18572926521301
[flaml.automl: 09-19 00:09:21] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9440227254440751
CO(0)最好结果：{'pred_time': 1.2231647240323683e-05, 'wall_clock_time': 46.18572926521301, 'metric_for_logging': {'pred_time': 1.2231647240323683e-05}, 'val_loss': 0.05597727455592489, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 12.043369054794312}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8920613101046033
CO(0)的mse=0.008934128937921039
CO(0)的mae=0.05558128520376561
CO(0)的mar=0.11740893504676926
总共花费的时间为：72.09
武威市
2652A
[flaml.automl: 09-19 00:12:56] {2390} INFO - task = regression
[flaml.automl: 09-19 00:12:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:12:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:12:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:12:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:12:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:13:00] {3025} INFO - Estimated sufficient time budget=42102s. Estimated necessary time budget=42s.
[flaml.automl: 09-19 00:13:00] {3072} INFO -  at 4.4s,	estimator xgboost's best error=0.1627,	best estimator xgboost's best error=0.1627
[flaml.automl: 09-19 00:13:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:13:06] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.1186,	best estimator xgboost's best error=0.1186
[flaml.automl: 09-19 00:13:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:13:09] {3072} INFO -  at 13.0s,	estimator xgboost's best error=0.1186,	best estimator xgboost's best error=0.1186
[flaml.automl: 09-19 00:13:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:13:24] {3072} INFO -  at 28.3s,	estimator xgboost's best error=0.1186,	best estimator xgboost's best error=0.1186
[flaml.automl: 09-19 00:13:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:13:26] {3072} INFO -  at 30.4s,	estimator xgboost's best error=0.0888,	best estimator xgboost's best error=0.0888
[flaml.automl: 09-19 00:13:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:13:29] {3072} INFO -  at 33.4s,	estimator xgboost's best error=0.0817,	best estimator xgboost's best error=0.0817
[flaml.automl: 09-19 00:13:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:13:32] {3072} INFO -  at 36.4s,	estimator xgboost's best error=0.0810,	best estimator xgboost's best error=0.0810
[flaml.automl: 09-19 00:13:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:13:39] {3072} INFO -  at 43.3s,	estimator xgboost's best error=0.0810,	best estimator xgboost's best error=0.0810
[flaml.automl: 09-19 00:13:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:13:42] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0810,	best estimator xgboost's best error=0.0810
[flaml.automl: 09-19 00:13:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:13:47] {3072} INFO -  at 50.6s,	estimator xgboost's best error=0.0810,	best estimator xgboost's best error=0.0810
[flaml.automl: 09-19 00:13:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:13:49] {3072} INFO -  at 53.4s,	estimator xgboost's best error=0.0810,	best estimator xgboost's best error=0.0810
[flaml.automl: 09-19 00:13:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:13:51] {3072} INFO -  at 55.4s,	estimator xgboost's best error=0.0810,	best estimator xgboost's best error=0.0810
[flaml.automl: 09-19 00:13:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:13:56] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-19 00:14:12] {3335} INFO - retrain xgboost for 16.3s
[flaml.automl: 09-19 00:14:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:14:12] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:14:12] {2637} INFO - Time taken to find the best model: 59.84514331817627
[flaml.automl: 09-19 00:14:12] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
CO(0)最佳损失：0.9259018800564024
CO(0)最好结果：{'pred_time': 4.50459893492144e-05, 'wall_clock_time': 59.84514331817627, 'metric_for_logging': {'pred_time': 4.50459893492144e-05}, 'val_loss': 0.07409811994359757, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 12, 'config/max_leaves': 14, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 4.468016147613525}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7576178997905848
CO(0)的mse=0.01608350850406165
CO(0)的mae=0.07759567709712296
CO(0)的mar=0.16682980899238198
总共花费的时间为：76.50
张掖市
2654A
2655A
[flaml.automl: 09-19 00:20:59] {2390} INFO - task = regression
[flaml.automl: 09-19 00:20:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:20:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:20:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:20:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:20:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:21:02] {3025} INFO - Estimated sufficient time budget=31446s. Estimated necessary time budget=31s.
[flaml.automl: 09-19 00:21:02] {3072} INFO -  at 3.3s,	estimator xgboost's best error=0.1416,	best estimator xgboost's best error=0.1416
[flaml.automl: 09-19 00:21:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:21:08] {3072} INFO -  at 8.8s,	estimator xgboost's best error=0.0887,	best estimator xgboost's best error=0.0887
[flaml.automl: 09-19 00:21:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:21:11] {3072} INFO -  at 12.6s,	estimator xgboost's best error=0.0887,	best estimator xgboost's best error=0.0887
[flaml.automl: 09-19 00:21:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:21:37] {3072} INFO -  at 38.7s,	estimator xgboost's best error=0.0887,	best estimator xgboost's best error=0.0887
[flaml.automl: 09-19 00:21:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:21:41] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.0686,	best estimator xgboost's best error=0.0686
[flaml.automl: 09-19 00:21:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:21:44] {3072} INFO -  at 44.9s,	estimator xgboost's best error=0.0614,	best estimator xgboost's best error=0.0614
[flaml.automl: 09-19 00:21:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:21:47] {3072} INFO -  at 47.7s,	estimator xgboost's best error=0.0614,	best estimator xgboost's best error=0.0614
[flaml.automl: 09-19 00:21:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:21:51] {3072} INFO -  at 52.0s,	estimator xgboost's best error=0.0614,	best estimator xgboost's best error=0.0614
[flaml.automl: 09-19 00:21:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:21:53] {3072} INFO -  at 54.0s,	estimator xgboost's best error=0.0614,	best estimator xgboost's best error=0.0614
[flaml.automl: 09-19 00:21:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:21:57] {3072} INFO -  at 58.5s,	estimator xgboost's best error=0.0614,	best estimator xgboost's best error=0.0614
[flaml.automl: 09-19 00:22:00] {3335} INFO - retrain xgboost for 2.8s
[flaml.automl: 09-19 00:22:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:22:00] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:22:00] {2637} INFO - Time taken to find the best model: 44.9000198841095
[flaml.automl: 09-19 00:22:00] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
CO(0)最佳损失：0.9385777810681977
CO(0)最好结果：{'pred_time': 2.989418278465797e-05, 'wall_clock_time': 44.9000198841095, 'metric_for_logging': {'pred_time': 2.989418278465797e-05}, 'val_loss': 0.06142221893180228, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 2.8906965255737305}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.6587021268528521
CO(0)的mse=0.009299200722888094
CO(0)的mae=0.06244950607690352
CO(0)的mar=0.22979995259316185
总共花费的时间为：61.95
平凉市
2656A
2657A
[flaml.automl: 09-19 00:29:03] {2390} INFO - task = regression
[flaml.automl: 09-19 00:29:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:29:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:29:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:29:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:29:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:29:05] {3025} INFO - Estimated sufficient time budget=12044s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:29:05] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.1698,	best estimator xgboost's best error=0.1698
[flaml.automl: 09-19 00:29:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:29:08] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.1109,	best estimator xgboost's best error=0.1109
[flaml.automl: 09-19 00:29:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:29:10] {3072} INFO -  at 6.6s,	estimator xgboost's best error=0.1109,	best estimator xgboost's best error=0.1109
[flaml.automl: 09-19 00:29:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:29:32] {3072} INFO -  at 28.3s,	estimator xgboost's best error=0.1109,	best estimator xgboost's best error=0.1109
[flaml.automl: 09-19 00:29:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:29:34] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0926,	best estimator xgboost's best error=0.0926
[flaml.automl: 09-19 00:29:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:29:39] {3072} INFO -  at 35.8s,	estimator xgboost's best error=0.0813,	best estimator xgboost's best error=0.0813
[flaml.automl: 09-19 00:29:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:29:44] {3072} INFO -  at 40.9s,	estimator xgboost's best error=0.0813,	best estimator xgboost's best error=0.0813
[flaml.automl: 09-19 00:29:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:29:54] {3072} INFO -  at 51.2s,	estimator xgboost's best error=0.0813,	best estimator xgboost's best error=0.0813
[flaml.automl: 09-19 00:29:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:29:59] {3072} INFO -  at 55.5s,	estimator xgboost's best error=0.0813,	best estimator xgboost's best error=0.0813
[flaml.automl: 09-19 00:30:07] {3335} INFO - retrain xgboost for 8.3s
[flaml.automl: 09-19 00:30:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:30:07] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:30:07] {2637} INFO - Time taken to find the best model: 35.83086943626404
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
CO(0)最佳损失：0.9186698605957604
CO(0)最好结果：{'pred_time': 6.30877026919277e-05, 'wall_clock_time': 35.83086943626404, 'metric_for_logging': {'pred_time': 6.30877026919277e-05}, 'val_loss': 0.08133013940423962, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 4.710535287857056}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.6925813445325062
CO(0)的mse=0.017580352354723706
CO(0)的mae=0.08249760461417194
CO(0)的mar=0.3430657565801446
总共花费的时间为：64.48
酒泉市
2658A
2659A
[flaml.automl: 09-19 00:37:03] {2390} INFO - task = regression
[flaml.automl: 09-19 00:37:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:37:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:37:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:37:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:37:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:37:05] {3025} INFO - Estimated sufficient time budget=12258s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:37:05] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1252,	best estimator xgboost's best error=0.1252
[flaml.automl: 09-19 00:37:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:37:07] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.0930,	best estimator xgboost's best error=0.0930
[flaml.automl: 09-19 00:37:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:37:08] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.0930,	best estimator xgboost's best error=0.0930
[flaml.automl: 09-19 00:37:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:37:17] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0930,	best estimator xgboost's best error=0.0930
[flaml.automl: 09-19 00:37:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:37:19] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0820,	best estimator xgboost's best error=0.0820
[flaml.automl: 09-19 00:37:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:37:20] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.0769,	best estimator xgboost's best error=0.0769
[flaml.automl: 09-19 00:37:20] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:37:22] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0766,	best estimator xgboost's best error=0.0766
[flaml.automl: 09-19 00:37:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:37:25] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0766,	best estimator xgboost's best error=0.0766
[flaml.automl: 09-19 00:37:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:37:26] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0765,	best estimator xgboost's best error=0.0765
[flaml.automl: 09-19 00:37:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:37:29] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-19 00:37:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:37:30] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.0741,	best estimator xgboost's best error=0.0741
[flaml.automl: 09-19 00:37:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:37:48] {3072} INFO -  at 44.9s,	estimator xgboost's best error=0.0712,	best estimator xgboost's best error=0.0712
[flaml.automl: 09-19 00:38:11] {3335} INFO - retrain xgboost for 22.3s
[flaml.automl: 09-19 00:38:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=15, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:38:11] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:38:11] {2637} INFO - Time taken to find the best model: 44.90469002723694
[flaml.automl: 09-19 00:38:11] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
CO(0)最佳损失：0.9287762763708349
CO(0)最好结果：{'pred_time': 3.373292442388065e-05, 'wall_clock_time': 44.90469002723694, 'metric_for_logging': {'pred_time': 3.373292442388065e-05}, 'val_loss': 0.07122372362916511, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 17.900607585906982}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=15, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.5456636834966291
CO(0)的mse=0.012870338294742713
CO(0)的mae=0.07238267786233885
CO(0)的mar=0.14630856089565958
总共花费的时间为：67.70
庆阳市
2662A
[flaml.automl: 09-19 00:41:50] {2390} INFO - task = regression
[flaml.automl: 09-19 00:41:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:41:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:41:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:41:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:41:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:41:51] {3025} INFO - Estimated sufficient time budget=11940s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:41:51] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1174,	best estimator xgboost's best error=0.1174
[flaml.automl: 09-19 00:41:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:41:53] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.0799,	best estimator xgboost's best error=0.0799
[flaml.automl: 09-19 00:41:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:41:54] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0799,	best estimator xgboost's best error=0.0799
[flaml.automl: 09-19 00:41:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:42:01] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.0799,	best estimator xgboost's best error=0.0799
[flaml.automl: 09-19 00:42:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:42:02] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.0533,	best estimator xgboost's best error=0.0533
[flaml.automl: 09-19 00:42:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:42:04] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0533,	best estimator xgboost's best error=0.0533
[flaml.automl: 09-19 00:42:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:42:06] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-19 00:42:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:42:08] {3072} INFO -  at 18.2s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-19 00:42:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:42:10] {3072} INFO -  at 19.8s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-19 00:42:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:42:12] {3072} INFO -  at 22.4s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-19 00:42:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:42:14] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.0429,	best estimator xgboost's best error=0.0429
[flaml.automl: 09-19 00:42:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:42:15] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.0429,	best estimator xgboost's best error=0.0429
[flaml.automl: 09-19 00:42:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:42:21] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0371,	best estimator xgboost's best error=0.0371
[flaml.automl: 09-19 00:42:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:42:30] {3072} INFO -  at 40.3s,	estimator xgboost's best error=0.0353,	best estimator xgboost's best error=0.0353
[flaml.automl: 09-19 00:42:30] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 00:42:35] {3072} INFO -  at 45.5s,	estimator xgboost's best error=0.0353,	best estimator xgboost's best error=0.0353
[flaml.automl: 09-19 00:42:35] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 00:42:49] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0353,	best estimator xgboost's best error=0.0353
[flaml.automl: 09-19 00:42:58] {3335} INFO - retrain xgboost for 9.2s
[flaml.automl: 09-19 00:42:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:42:58] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:42:58] {2637} INFO - Time taken to find the best model: 40.33340263366699
CO(0)最佳参数：{'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
CO(0)最佳损失：0.9647240401707233
CO(0)最好结果：{'pred_time': 3.384208364973744e-05, 'wall_clock_time': 40.33340263366699, 'metric_for_logging': {'pred_time': 3.384208364973744e-05}, 'val_loss': 0.035275959829276636, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 26, 'config/max_leaves': 6, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 9.28141450881958}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9166863212706265
CO(0)的mse=0.0031898629366930876
CO(0)的mae=0.0363603819908258
CO(0)的mar=0.06233097977647419
总共花费的时间为：68.81
定西市
2663A
2664A
[flaml.automl: 09-19 00:50:29] {2390} INFO - task = regression
[flaml.automl: 09-19 00:50:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:50:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:50:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:50:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:50:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:50:30] {3025} INFO - Estimated sufficient time budget=12156s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:50:30] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1944,	best estimator xgboost's best error=0.1944
[flaml.automl: 09-19 00:50:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:50:32] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1120,	best estimator xgboost's best error=0.1120
[flaml.automl: 09-19 00:50:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:50:34] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.1120,	best estimator xgboost's best error=0.1120
[flaml.automl: 09-19 00:50:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:50:43] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.1120,	best estimator xgboost's best error=0.1120
[flaml.automl: 09-19 00:50:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:50:44] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0835,	best estimator xgboost's best error=0.0835
[flaml.automl: 09-19 00:50:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:50:46] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 00:50:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:50:48] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 00:50:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:50:50] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 00:50:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:50:51] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 00:50:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:50:54] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 00:50:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:50:55] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 00:50:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:50:56] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 00:50:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:51:02] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.0667,	best estimator xgboost's best error=0.0667
[flaml.automl: 09-19 00:51:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:51:12] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0629,	best estimator xgboost's best error=0.0629
[flaml.automl: 09-19 00:51:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 00:51:23] {3072} INFO -  at 53.9s,	estimator xgboost's best error=0.0629,	best estimator xgboost's best error=0.0629
[flaml.automl: 09-19 00:51:42] {3335} INFO - retrain xgboost for 19.2s
[flaml.automl: 09-19 00:51:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:51:42] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:51:42] {2637} INFO - Time taken to find the best model: 43.38194179534912
[flaml.automl: 09-19 00:51:42] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.937140462218025
CO(0)最好结果：{'pred_time': 1.6896642974711453e-05, 'wall_clock_time': 43.38194179534912, 'metric_for_logging': {'pred_time': 1.6896642974711453e-05}, 'val_loss': 0.06285953778197491, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.438238143920898}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8841536852436096
CO(0)的mse=0.010748032054580324
CO(0)的mae=0.06267662101682142
CO(0)的mar=0.4295071173457934
总共花费的时间为：73.57
陇南市
2665A
3247A
[flaml.automl: 09-19 00:58:59] {2390} INFO - task = regression
[flaml.automl: 09-19 00:58:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:58:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:58:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:58:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:58:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:59:00] {3025} INFO - Estimated sufficient time budget=11973s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:59:00] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1328,	best estimator xgboost's best error=0.1328
[flaml.automl: 09-19 00:59:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:59:02] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.0841,	best estimator xgboost's best error=0.0841
[flaml.automl: 09-19 00:59:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:59:03] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0841,	best estimator xgboost's best error=0.0841
[flaml.automl: 09-19 00:59:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:59:12] {3072} INFO -  at 12.7s,	estimator xgboost's best error=0.0841,	best estimator xgboost's best error=0.0841
[flaml.automl: 09-19 00:59:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:59:13] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0548,	best estimator xgboost's best error=0.0548
[flaml.automl: 09-19 00:59:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:59:14] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-19 00:59:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:59:16] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-19 00:59:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:59:18] {3072} INFO -  at 19.5s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-19 00:59:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:59:20] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-19 00:59:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:59:24] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-19 00:59:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:59:26] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-19 00:59:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:59:28] {3072} INFO -  at 28.9s,	estimator xgboost's best error=0.0470,	best estimator xgboost's best error=0.0470
[flaml.automl: 09-19 00:59:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:59:38] {3072} INFO -  at 38.6s,	estimator xgboost's best error=0.0409,	best estimator xgboost's best error=0.0409
[flaml.automl: 09-19 00:59:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:59:57] {3072} INFO -  at 57.8s,	estimator xgboost's best error=0.0404,	best estimator xgboost's best error=0.0404
[flaml.automl: 09-19 01:00:20] {3335} INFO - retrain xgboost for 23.4s
[flaml.automl: 09-19 01:00:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:00:20] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:00:20] {2637} INFO - Time taken to find the best model: 57.84410572052002
[flaml.automl: 09-19 01:00:20] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9596017121727046
CO(0)最好结果：{'pred_time': 5.0427941715016084e-05, 'wall_clock_time': 57.84410572052002, 'metric_for_logging': {'pred_time': 5.0427941715016084e-05}, 'val_loss': 0.04039828782729542, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 19.228165864944458}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.838867048146998
CO(0)的mse=0.004111707366509878
CO(0)的mae=0.038151725612091646
CO(0)的mar=0.1944292899095985
总共花费的时间为：81.72
临夏回族自治州
2667A
2668A
[flaml.automl: 09-19 01:06:58] {2390} INFO - task = regression
[flaml.automl: 09-19 01:06:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:06:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:06:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:06:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:06:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:07:03] {3025} INFO - Estimated sufficient time budget=48422s. Estimated necessary time budget=48s.
[flaml.automl: 09-19 01:07:03] {3072} INFO -  at 5.1s,	estimator xgboost's best error=0.2449,	best estimator xgboost's best error=0.2449
[flaml.automl: 09-19 01:07:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:07:11] {3072} INFO -  at 13.1s,	estimator xgboost's best error=0.1526,	best estimator xgboost's best error=0.1526
[flaml.automl: 09-19 01:07:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:07:15] {3072} INFO -  at 16.7s,	estimator xgboost's best error=0.1526,	best estimator xgboost's best error=0.1526
[flaml.automl: 09-19 01:07:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:07:25] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.1526,	best estimator xgboost's best error=0.1526
[flaml.automl: 09-19 01:07:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:07:26] {3072} INFO -  at 28.3s,	estimator xgboost's best error=0.1219,	best estimator xgboost's best error=0.1219
[flaml.automl: 09-19 01:07:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:07:28] {3072} INFO -  at 29.8s,	estimator xgboost's best error=0.1074,	best estimator xgboost's best error=0.1074
[flaml.automl: 09-19 01:07:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:07:29] {3072} INFO -  at 31.4s,	estimator xgboost's best error=0.1074,	best estimator xgboost's best error=0.1074
[flaml.automl: 09-19 01:07:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:07:32] {3072} INFO -  at 33.8s,	estimator xgboost's best error=0.1074,	best estimator xgboost's best error=0.1074
[flaml.automl: 09-19 01:07:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:07:33] {3072} INFO -  at 34.9s,	estimator xgboost's best error=0.1074,	best estimator xgboost's best error=0.1074
[flaml.automl: 09-19 01:07:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:07:35] {3072} INFO -  at 37.3s,	estimator xgboost's best error=0.1074,	best estimator xgboost's best error=0.1074
[flaml.automl: 09-19 01:07:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:07:36] {3072} INFO -  at 38.4s,	estimator xgboost's best error=0.1074,	best estimator xgboost's best error=0.1074
[flaml.automl: 09-19 01:07:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:07:37] {3072} INFO -  at 39.6s,	estimator xgboost's best error=0.1074,	best estimator xgboost's best error=0.1074
[flaml.automl: 09-19 01:07:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:07:43] {3072} INFO -  at 45.5s,	estimator xgboost's best error=0.0988,	best estimator xgboost's best error=0.0988
[flaml.automl: 09-19 01:07:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:07:54] {3072} INFO -  at 55.7s,	estimator xgboost's best error=0.0932,	best estimator xgboost's best error=0.0932
[flaml.automl: 09-19 01:08:04] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-19 01:08:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:08:04] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:08:04] {2637} INFO - Time taken to find the best model: 55.72112679481506
[flaml.automl: 09-19 01:08:04] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.906846196047912
CO(0)最好结果：{'pred_time': 1.6587035711792402e-05, 'wall_clock_time': 55.72112679481506, 'metric_for_logging': {'pred_time': 1.6587035711792402e-05}, 'val_loss': 0.09315380395208811, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.264629125595093}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8994813608417891
CO(0)的mse=0.024117181023028816
CO(0)的mae=0.09199167461033193
CO(0)的mar=0.3762675735239171
总共花费的时间为：66.81
甘南州
2669A
[flaml.automl: 09-19 01:11:23] {2390} INFO - task = regression
[flaml.automl: 09-19 01:11:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:11:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:11:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:11:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:11:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:11:24] {3025} INFO - Estimated sufficient time budget=11583s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:11:24] {3072} INFO -  at 1.2s,	estimator xgboost's best error=0.1360,	best estimator xgboost's best error=0.1360
[flaml.automl: 09-19 01:11:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:11:26] {3072} INFO -  at 3.0s,	estimator xgboost's best error=0.0973,	best estimator xgboost's best error=0.0973
[flaml.automl: 09-19 01:11:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:11:27] {3072} INFO -  at 4.2s,	estimator xgboost's best error=0.0973,	best estimator xgboost's best error=0.0973
[flaml.automl: 09-19 01:11:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:11:34] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0973,	best estimator xgboost's best error=0.0973
[flaml.automl: 09-19 01:11:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:11:35] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0750,	best estimator xgboost's best error=0.0750
[flaml.automl: 09-19 01:11:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:11:37] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-19 01:11:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:11:38] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-19 01:11:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:11:41] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-19 01:11:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:11:42] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-19 01:11:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:11:45] {3072} INFO -  at 22.2s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-19 01:11:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:11:47] {3072} INFO -  at 23.9s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-19 01:11:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:11:49] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-19 01:11:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:11:57] {3072} INFO -  at 33.7s,	estimator xgboost's best error=0.0685,	best estimator xgboost's best error=0.0685
[flaml.automl: 09-19 01:11:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:12:01] {3072} INFO -  at 37.7s,	estimator xgboost's best error=0.0671,	best estimator xgboost's best error=0.0671
[flaml.automl: 09-19 01:12:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:12:03] {3072} INFO -  at 39.9s,	estimator xgboost's best error=0.0671,	best estimator xgboost's best error=0.0671
[flaml.automl: 09-19 01:12:03] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 01:12:10] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.0671,	best estimator xgboost's best error=0.0671
[flaml.automl: 09-19 01:12:10] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 01:12:13] {3072} INFO -  at 49.5s,	estimator xgboost's best error=0.0671,	best estimator xgboost's best error=0.0671
[flaml.automl: 09-19 01:12:13] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-19 01:12:15] {3072} INFO -  at 52.3s,	estimator xgboost's best error=0.0671,	best estimator xgboost's best error=0.0671
[flaml.automl: 09-19 01:12:15] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-19 01:12:22] {3072} INFO -  at 58.9s,	estimator xgboost's best error=0.0650,	best estimator xgboost's best error=0.0650
[flaml.automl: 09-19 01:12:31] {3335} INFO - retrain xgboost for 8.8s
[flaml.automl: 09-19 01:12:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7178642069579442, colsample_bynode=1,
             colsample_bytree=0.7808640703646168, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9481937935550555,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.2628045260160708, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9333220038816206, scale_pos_weight=1,
             subsample=0.8151775216506888, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:12:31] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:12:31] {2637} INFO - Time taken to find the best model: 58.90012812614441
[flaml.automl: 09-19 01:12:31] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 9, 'max_leaves': 10, 'min_child_weight': 0.2628045260160708, 'learning_rate': 0.9481937935550555, 'subsample': 0.8151775216506888, 'colsample_bylevel': 0.7178642069579442, 'colsample_bytree': 0.7808640703646168, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9333220038816206}
CO(0)最佳损失：0.9349881388819479
CO(0)最好结果：{'pred_time': 5.6298092158153805e-05, 'wall_clock_time': 58.90012812614441, 'metric_for_logging': {'pred_time': 5.6298092158153805e-05}, 'val_loss': 0.06501186111805216, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 10, 'min_child_weight': 0.2628045260160708, 'learning_rate': 0.9481937935550555, 'subsample': 0.8151775216506888, 'colsample_bylevel': 0.7178642069579442, 'colsample_bytree': 0.7808640703646168, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9333220038816206}, 'config/n_estimators': 9, 'config/max_leaves': 10, 'config/min_child_weight': 0.2628045260160708, 'config/learning_rate': 0.9481937935550555, 'config/subsample': 0.8151775216506888, 'config/colsample_bylevel': 0.7178642069579442, 'config/colsample_bytree': 0.7808640703646168, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.9333220038816206, 'experiment_tag': 'exp', 'time_total_s': 6.594963788986206}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7178642069579442, colsample_bynode=1,
             colsample_bytree=0.7808640703646168, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9481937935550555,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.2628045260160708, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9333220038816206, scale_pos_weight=1,
             subsample=0.8151775216506888, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8726278130042531
CO(0)的mse=0.0072145607977797625
CO(0)的mae=0.05537801123487227
CO(0)的mar=0.12031252883957205
总共花费的时间为：67.88
海东地区
3867A
[flaml.automl: 09-19 01:16:01] {2390} INFO - task = regression
[flaml.automl: 09-19 01:16:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:16:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:16:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:16:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:16:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:16:03] {3025} INFO - Estimated sufficient time budget=11598s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:16:03] {3072} INFO -  at 1.2s,	estimator xgboost's best error=0.1575,	best estimator xgboost's best error=0.1575
[flaml.automl: 09-19 01:16:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:16:04] {3072} INFO -  at 3.0s,	estimator xgboost's best error=0.1251,	best estimator xgboost's best error=0.1251
[flaml.automl: 09-19 01:16:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:16:06] {3072} INFO -  at 4.2s,	estimator xgboost's best error=0.1251,	best estimator xgboost's best error=0.1251
[flaml.automl: 09-19 01:16:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:16:12] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.1251,	best estimator xgboost's best error=0.1251
[flaml.automl: 09-19 01:16:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:16:13] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.1080,	best estimator xgboost's best error=0.1080
[flaml.automl: 09-19 01:16:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:16:15] {3072} INFO -  at 13.3s,	estimator xgboost's best error=0.1005,	best estimator xgboost's best error=0.1005
[flaml.automl: 09-19 01:16:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:16:16] {3072} INFO -  at 14.6s,	estimator xgboost's best error=0.1005,	best estimator xgboost's best error=0.1005
[flaml.automl: 09-19 01:16:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:16:18] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.1005,	best estimator xgboost's best error=0.1005
[flaml.automl: 09-19 01:16:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:16:19] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.1005,	best estimator xgboost's best error=0.1005
[flaml.automl: 09-19 01:16:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:16:22] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.1005,	best estimator xgboost's best error=0.1005
[flaml.automl: 09-19 01:16:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:16:23] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.1003,	best estimator xgboost's best error=0.1003
[flaml.automl: 09-19 01:16:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:16:24] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.1003,	best estimator xgboost's best error=0.1003
[flaml.automl: 09-19 01:16:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:16:27] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0933,	best estimator xgboost's best error=0.0933
[flaml.automl: 09-19 01:16:27] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:16:33] {3072} INFO -  at 31.4s,	estimator xgboost's best error=0.0913,	best estimator xgboost's best error=0.0913
[flaml.automl: 09-19 01:16:33] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:16:37] {3072} INFO -  at 35.7s,	estimator xgboost's best error=0.0913,	best estimator xgboost's best error=0.0913
[flaml.automl: 09-19 01:16:37] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 01:16:52] {3072} INFO -  at 50.5s,	estimator xgboost's best error=0.0901,	best estimator xgboost's best error=0.0901
[flaml.automl: 09-19 01:17:06] {3335} INFO - retrain xgboost for 14.4s
[flaml.automl: 09-19 01:17:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.5862152095094147, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=13.316419974805074, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.0813980854508363, scale_pos_weight=1,
             subsample=0.9326362799728674, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:17:06] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:17:06] {2637} INFO - Time taken to find the best model: 50.54642581939697
[flaml.automl: 09-19 01:17:06] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 14, 'max_leaves': 11, 'min_child_weight': 13.316419974805074, 'learning_rate': 0.5310029457853219, 'subsample': 0.9326362799728674, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.5862152095094147, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.0813980854508363}
CO(0)最佳损失：0.909933055175622
CO(0)最好结果：{'pred_time': 0.00010825162922112754, 'wall_clock_time': 50.54642581939697, 'metric_for_logging': {'pred_time': 0.00010825162922112754}, 'val_loss': 0.090066944824378, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 11, 'min_child_weight': 13.316419974805074, 'learning_rate': 0.5310029457853219, 'subsample': 0.9326362799728674, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.5862152095094147, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.0813980854508363}, 'config/n_estimators': 14, 'config/max_leaves': 11, 'config/min_child_weight': 13.316419974805074, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.9326362799728674, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.5862152095094147, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.0813980854508363, 'experiment_tag': 'exp', 'time_total_s': 14.84138536453247}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.5862152095094147, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=13.316419974805074, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.0813980854508363, scale_pos_weight=1,
             subsample=0.9326362799728674, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.506473595809799
CO(0)的mse=0.02363664619811921
CO(0)的mae=0.08479575578674026
CO(0)的mar=0.17137447644201706
总共花费的时间为：65.12
海北藏族自治州
2671A
[flaml.automl: 09-19 01:20:15] {2390} INFO - task = regression
[flaml.automl: 09-19 01:20:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:20:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:20:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:20:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:20:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:20:17] {3025} INFO - Estimated sufficient time budget=11555s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:20:17] {3072} INFO -  at 1.2s,	estimator xgboost's best error=0.1211,	best estimator xgboost's best error=0.1211
[flaml.automl: 09-19 01:20:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:20:18] {3072} INFO -  at 3.0s,	estimator xgboost's best error=0.0796,	best estimator xgboost's best error=0.0796
[flaml.automl: 09-19 01:20:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:20:20] {3072} INFO -  at 4.2s,	estimator xgboost's best error=0.0796,	best estimator xgboost's best error=0.0796
[flaml.automl: 09-19 01:20:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:20:26] {3072} INFO -  at 11.2s,	estimator xgboost's best error=0.0796,	best estimator xgboost's best error=0.0796
[flaml.automl: 09-19 01:20:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:20:28] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.0577,	best estimator xgboost's best error=0.0577
[flaml.automl: 09-19 01:20:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:20:29] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-19 01:20:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:20:31] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-19 01:20:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:20:33] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-19 01:20:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:20:34] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-19 01:20:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:20:37] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-19 01:20:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:20:38] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-19 01:20:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:20:39] {3072} INFO -  at 23.5s,	estimator xgboost's best error=0.0501,	best estimator xgboost's best error=0.0501
[flaml.automl: 09-19 01:20:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:20:44] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.0465,	best estimator xgboost's best error=0.0465
[flaml.automl: 09-19 01:20:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:20:56] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.0370,	best estimator xgboost's best error=0.0370
[flaml.automl: 09-19 01:20:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:21:05] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.0370,	best estimator xgboost's best error=0.0370
[flaml.automl: 09-19 01:21:24] {3335} INFO - retrain xgboost for 18.9s
[flaml.automl: 09-19 01:21:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:21:24] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:21:24] {2637} INFO - Time taken to find the best model: 40.68629765510559
CO(0)最佳参数：{'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9630413424131632
CO(0)最好结果：{'pred_time': 6.988649642608694e-05, 'wall_clock_time': 40.68629765510559, 'metric_for_logging': {'pred_time': 6.988649642608694e-05}, 'val_loss': 0.03695865758683677, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 17, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.609116792678833}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9019718893666882
CO(0)的mse=0.004129843979587742
CO(0)的mae=0.04016756915978049
CO(0)的mar=0.10956141623766315
总共花费的时间为：68.71
黄南藏族自治州
2672A
[flaml.automl: 09-19 01:24:20] {2390} INFO - task = regression
[flaml.automl: 09-19 01:24:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:24:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:24:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:24:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:24:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:24:21] {3025} INFO - Estimated sufficient time budget=12007s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:24:21] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1555,	best estimator xgboost's best error=0.1555
[flaml.automl: 09-19 01:24:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:24:23] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.1045,	best estimator xgboost's best error=0.1045
[flaml.automl: 09-19 01:24:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:24:24] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.1045,	best estimator xgboost's best error=0.1045
[flaml.automl: 09-19 01:24:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:24:31] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.1045,	best estimator xgboost's best error=0.1045
[flaml.automl: 09-19 01:24:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:24:32] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.0691,	best estimator xgboost's best error=0.0691
[flaml.automl: 09-19 01:24:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:24:34] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0630,	best estimator xgboost's best error=0.0630
[flaml.automl: 09-19 01:24:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:24:35] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.0604,	best estimator xgboost's best error=0.0604
[flaml.automl: 09-19 01:24:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:24:38] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.0604,	best estimator xgboost's best error=0.0604
[flaml.automl: 09-19 01:24:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:24:39] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0604,	best estimator xgboost's best error=0.0604
[flaml.automl: 09-19 01:24:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:24:42] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.0568,	best estimator xgboost's best error=0.0568
[flaml.automl: 09-19 01:24:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:24:43] {3072} INFO -  at 23.9s,	estimator xgboost's best error=0.0568,	best estimator xgboost's best error=0.0568
[flaml.automl: 09-19 01:24:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:24:45] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.0568,	best estimator xgboost's best error=0.0568
[flaml.automl: 09-19 01:24:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:24:53] {3072} INFO -  at 33.9s,	estimator xgboost's best error=0.0512,	best estimator xgboost's best error=0.0512
[flaml.automl: 09-19 01:24:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:25:08] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.0493,	best estimator xgboost's best error=0.0493
[flaml.automl: 09-19 01:25:26] {3335} INFO - retrain xgboost for 18.4s
[flaml.automl: 09-19 01:25:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:25:26] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:25:26] {2637} INFO - Time taken to find the best model: 48.23155236244202
[flaml.automl: 09-19 01:25:26] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 23, 'max_leaves': 11, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
CO(0)最佳损失：0.9506717429967884
CO(0)最好结果：{'pred_time': 3.4529473033808915e-05, 'wall_clock_time': 48.23155236244202, 'metric_for_logging': {'pred_time': 3.4529473033808915e-05}, 'val_loss': 0.04932825700321165, 'training_iteration': 1, 'config': {'n_estimators': 23, 'max_leaves': 11, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 23, 'config/max_leaves': 11, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 14.359153032302856}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8867517718230371
CO(0)的mse=0.006531588936512431
CO(0)的mae=0.0517837371240844
CO(0)的mar=0.10590264464019157
总共花费的时间为：66.80
海南藏族自治州
2673A
[flaml.automl: 09-19 01:28:38] {2390} INFO - task = regression
[flaml.automl: 09-19 01:28:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:28:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:28:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:28:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:28:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:28:39] {3025} INFO - Estimated sufficient time budget=12017s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:28:39] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1348,	best estimator xgboost's best error=0.1348
[flaml.automl: 09-19 01:28:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:28:41] {3072} INFO -  at 3.1s,	estimator xgboost's best error=0.0963,	best estimator xgboost's best error=0.0963
[flaml.automl: 09-19 01:28:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:28:42] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0963,	best estimator xgboost's best error=0.0963
[flaml.automl: 09-19 01:28:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:28:49] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.0963,	best estimator xgboost's best error=0.0963
[flaml.automl: 09-19 01:28:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:28:50] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.0765,	best estimator xgboost's best error=0.0765
[flaml.automl: 09-19 01:28:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:28:52] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 01:28:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:28:54] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 01:28:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:28:56] {3072} INFO -  at 18.0s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 01:28:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:28:57] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 01:28:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:28:59] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 01:28:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:29:01] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 01:29:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:29:02] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 01:29:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:29:07] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 01:29:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:29:10] {3072} INFO -  at 32.0s,	estimator xgboost's best error=0.0698,	best estimator xgboost's best error=0.0698
[flaml.automl: 09-19 01:29:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:29:11] {3072} INFO -  at 33.6s,	estimator xgboost's best error=0.0698,	best estimator xgboost's best error=0.0698
[flaml.automl: 09-19 01:29:12] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 01:29:15] {3072} INFO -  at 37.5s,	estimator xgboost's best error=0.0698,	best estimator xgboost's best error=0.0698
[flaml.automl: 09-19 01:29:15] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 01:29:17] {3072} INFO -  at 39.6s,	estimator xgboost's best error=0.0698,	best estimator xgboost's best error=0.0698
[flaml.automl: 09-19 01:29:17] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-19 01:29:19] {3072} INFO -  at 41.4s,	estimator xgboost's best error=0.0698,	best estimator xgboost's best error=0.0698
[flaml.automl: 09-19 01:29:19] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-19 01:29:25] {3072} INFO -  at 46.7s,	estimator xgboost's best error=0.0693,	best estimator xgboost's best error=0.0693
[flaml.automl: 09-19 01:29:25] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-19 01:29:27] {3072} INFO -  at 49.0s,	estimator xgboost's best error=0.0693,	best estimator xgboost's best error=0.0693
[flaml.automl: 09-19 01:29:27] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-19 01:29:37] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0664,	best estimator xgboost's best error=0.0664
[flaml.automl: 09-19 01:29:58] {3335} INFO - retrain xgboost for 21.3s
[flaml.automl: 09-19 01:29:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7984762384270153, colsample_bynode=1,
             colsample_bytree=0.7850109323524345, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.05372235539459749,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.15476030896055476, scale_pos_weight=1,
             subsample=0.9068360058362719, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:29:58] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:29:58] {2637} INFO - Time taken to find the best model: 59.24466347694397
[flaml.automl: 09-19 01:29:58] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 14, 'max_leaves': 18, 'min_child_weight': 0.05372235539459749, 'learning_rate': 1.0, 'subsample': 0.9068360058362719, 'colsample_bylevel': 0.7984762384270153, 'colsample_bytree': 0.7850109323524345, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.15476030896055476}
CO(0)最佳损失：0.9336341599781178
CO(0)最好结果：{'pred_time': 3.55056297021566e-05, 'wall_clock_time': 59.24466347694397, 'metric_for_logging': {'pred_time': 3.55056297021566e-05}, 'val_loss': 0.06636584002188217, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 18, 'min_child_weight': 0.05372235539459749, 'learning_rate': 1.0, 'subsample': 0.9068360058362719, 'colsample_bylevel': 0.7984762384270153, 'colsample_bytree': 0.7850109323524345, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.15476030896055476}, 'config/n_estimators': 14, 'config/max_leaves': 18, 'config/min_child_weight': 0.05372235539459749, 'config/learning_rate': 1.0, 'config/subsample': 0.9068360058362719, 'config/colsample_bylevel': 0.7984762384270153, 'config/colsample_bytree': 0.7850109323524345, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.15476030896055476, 'experiment_tag': 'exp', 'time_total_s': 10.232935190200806}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7984762384270153, colsample_bynode=1,
             colsample_bytree=0.7850109323524345, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.05372235539459749,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.15476030896055476, scale_pos_weight=1,
             subsample=0.9068360058362719, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.6743743086520391
CO(0)的mse=0.008879011982977917
CO(0)的mae=0.06466299423416187
CO(0)的mar=0.11140306135623623
总共花费的时间为：80.84
果洛藏族自治州
2674A
[flaml.automl: 09-19 01:33:32] {2390} INFO - task = regression
[flaml.automl: 09-19 01:33:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:33:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:33:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:33:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:33:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:33:34] {3025} INFO - Estimated sufficient time budget=20481s. Estimated necessary time budget=20s.
[flaml.automl: 09-19 01:33:34] {3072} INFO -  at 2.2s,	estimator xgboost's best error=0.1108,	best estimator xgboost's best error=0.1108
[flaml.automl: 09-19 01:33:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:33:37] {3072} INFO -  at 5.1s,	estimator xgboost's best error=0.0867,	best estimator xgboost's best error=0.0867
[flaml.automl: 09-19 01:33:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:33:39] {3072} INFO -  at 7.1s,	estimator xgboost's best error=0.0867,	best estimator xgboost's best error=0.0867
[flaml.automl: 09-19 01:33:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:33:50] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.0867,	best estimator xgboost's best error=0.0867
[flaml.automl: 09-19 01:33:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:33:52] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.0714,	best estimator xgboost's best error=0.0714
[flaml.automl: 09-19 01:33:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:33:54] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.0698,	best estimator xgboost's best error=0.0698
[flaml.automl: 09-19 01:33:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:33:57] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0679,	best estimator xgboost's best error=0.0679
[flaml.automl: 09-19 01:33:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:34:01] {3072} INFO -  at 29.5s,	estimator xgboost's best error=0.0679,	best estimator xgboost's best error=0.0679
[flaml.automl: 09-19 01:34:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:34:04] {3072} INFO -  at 32.2s,	estimator xgboost's best error=0.0679,	best estimator xgboost's best error=0.0679
[flaml.automl: 09-19 01:34:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:34:08] {3072} INFO -  at 36.6s,	estimator xgboost's best error=0.0675,	best estimator xgboost's best error=0.0675
[flaml.automl: 09-19 01:34:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:34:11] {3072} INFO -  at 39.3s,	estimator xgboost's best error=0.0675,	best estimator xgboost's best error=0.0675
[flaml.automl: 09-19 01:34:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:34:13] {3072} INFO -  at 41.3s,	estimator xgboost's best error=0.0675,	best estimator xgboost's best error=0.0675
[flaml.automl: 09-19 01:34:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:34:29] {3072} INFO -  at 57.7s,	estimator xgboost's best error=0.0666,	best estimator xgboost's best error=0.0666
[flaml.automl: 09-19 01:34:46] {3335} INFO - retrain xgboost for 16.2s
[flaml.automl: 09-19 01:34:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 01:34:46] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:34:46] {2637} INFO - Time taken to find the best model: 57.732165575027466
[flaml.automl: 09-19 01:34:46] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
CO(0)最佳损失：0.9333501740346858
CO(0)最好结果：{'pred_time': 6.53078841192425e-05, 'wall_clock_time': 57.732165575027466, 'metric_for_logging': {'pred_time': 6.53078841192425e-05}, 'val_loss': 0.06664982596531421, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 12, 'config/max_leaves': 14, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 16.456681966781616}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=14,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.49360253356087735
CO(0)的mse=0.008392010899469513
CO(0)的mae=0.05992671269778987
CO(0)的mar=0.19695828292124343
总共花费的时间为：74.22
玉树藏族自治州
2675A
[flaml.automl: 09-19 01:38:34] {2390} INFO - task = regression
[flaml.automl: 09-19 01:38:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:38:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:38:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:38:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:38:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:38:36] {3025} INFO - Estimated sufficient time budget=20367s. Estimated necessary time budget=20s.
[flaml.automl: 09-19 01:38:36] {3072} INFO -  at 2.1s,	estimator xgboost's best error=0.1092,	best estimator xgboost's best error=0.1092
[flaml.automl: 09-19 01:38:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:38:41] {3072} INFO -  at 7.3s,	estimator xgboost's best error=0.0734,	best estimator xgboost's best error=0.0734
[flaml.automl: 09-19 01:38:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:38:45] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.0734,	best estimator xgboost's best error=0.0734
[flaml.automl: 09-19 01:38:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:39:05] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.0734,	best estimator xgboost's best error=0.0734
[flaml.automl: 09-19 01:39:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:39:08] {3072} INFO -  at 34.3s,	estimator xgboost's best error=0.0519,	best estimator xgboost's best error=0.0519
[flaml.automl: 09-19 01:39:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:39:13] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.0519,	best estimator xgboost's best error=0.0519
[flaml.automl: 09-19 01:39:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:39:20] {3072} INFO -  at 45.9s,	estimator xgboost's best error=0.0488,	best estimator xgboost's best error=0.0488
[flaml.automl: 09-19 01:39:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:39:29] {3072} INFO -  at 54.9s,	estimator xgboost's best error=0.0488,	best estimator xgboost's best error=0.0488
[flaml.automl: 09-19 01:39:33] {3335} INFO - retrain xgboost for 4.5s
[flaml.automl: 09-19 01:39:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 01:39:33] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:39:33] {2637} INFO - Time taken to find the best model: 45.91859269142151
[flaml.automl: 09-19 01:39:33] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
CO(0)最佳损失：0.9511671498130573
CO(0)最好结果：{'pred_time': 0.00013266025424902916, 'wall_clock_time': 45.91859269142151, 'metric_for_logging': {'pred_time': 0.00013266025424902916}, 'val_loss': 0.04883285018694271, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 6.727463006973267}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.7647453051157147
CO(0)的mse=0.005984373255930367
CO(0)的mae=0.05071680062168049
CO(0)的mar=0.11168364322492075
总共花费的时间为：59.70
海西蒙古族藏族自治州
2676A
[flaml.automl: 09-19 01:42:50] {2390} INFO - task = regression
[flaml.automl: 09-19 01:42:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:42:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:42:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:42:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:42:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:42:51] {3025} INFO - Estimated sufficient time budget=12127s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:42:51] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1238,	best estimator xgboost's best error=0.1238
[flaml.automl: 09-19 01:42:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:42:53] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.0912,	best estimator xgboost's best error=0.0912
[flaml.automl: 09-19 01:42:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:42:54] {3072} INFO -  at 4.3s,	estimator xgboost's best error=0.0912,	best estimator xgboost's best error=0.0912
[flaml.automl: 09-19 01:42:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:43:01] {3072} INFO -  at 11.4s,	estimator xgboost's best error=0.0912,	best estimator xgboost's best error=0.0912
[flaml.automl: 09-19 01:43:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:43:02] {3072} INFO -  at 12.5s,	estimator xgboost's best error=0.0702,	best estimator xgboost's best error=0.0702
[flaml.automl: 09-19 01:43:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:43:04] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0679,	best estimator xgboost's best error=0.0679
[flaml.automl: 09-19 01:43:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:43:05] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.0647,	best estimator xgboost's best error=0.0647
[flaml.automl: 09-19 01:43:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:43:08] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.0647,	best estimator xgboost's best error=0.0647
[flaml.automl: 09-19 01:43:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:43:09] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.0647,	best estimator xgboost's best error=0.0647
[flaml.automl: 09-19 01:43:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:43:12] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.0647,	best estimator xgboost's best error=0.0647
[flaml.automl: 09-19 01:43:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:43:13] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-19 01:43:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:43:15] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.0644,	best estimator xgboost's best error=0.0644
[flaml.automl: 09-19 01:43:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:43:20] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.0642,	best estimator xgboost's best error=0.0642
[flaml.automl: 09-19 01:43:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:43:28] {3072} INFO -  at 38.7s,	estimator xgboost's best error=0.0611,	best estimator xgboost's best error=0.0611
[flaml.automl: 09-19 01:43:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:43:33] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.0611,	best estimator xgboost's best error=0.0611
[flaml.automl: 09-19 01:43:33] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 01:43:46] {3072} INFO -  at 56.6s,	estimator xgboost's best error=0.0611,	best estimator xgboost's best error=0.0611
[flaml.automl: 09-19 01:43:54] {3335} INFO - retrain xgboost for 8.2s
[flaml.automl: 09-19 01:43:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:43:54] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:43:54] {2637} INFO - Time taken to find the best model: 38.65241885185242
CO(0)最佳参数：{'n_estimators': 23, 'max_leaves': 6, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463}
CO(0)最佳损失：0.9389300517298607
CO(0)最好结果：{'pred_time': 3.357949269134025e-05, 'wall_clock_time': 38.65241885185242, 'metric_for_logging': {'pred_time': 3.357949269134025e-05}, 'val_loss': 0.06106994827013938, 'training_iteration': 1, 'config': {'n_estimators': 23, 'max_leaves': 6, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463}, 'config/n_estimators': 23, 'config/max_leaves': 6, 'config/min_child_weight': 6.532042959250319, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8835265175628242, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.10065328594609463, 'experiment_tag': 'exp', 'time_total_s': 8.161882877349854}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=23, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.4776299754955935
CO(0)的mse=0.0076982983049668965
CO(0)的mae=0.05734354830724262
CO(0)的mar=0.20475899900838743
总共花费的时间为：64.98
吴忠市
2677A
3648A
[flaml.automl: 09-19 01:51:08] {2390} INFO - task = regression
[flaml.automl: 09-19 01:51:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:51:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:51:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:51:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:51:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:51:09] {3025} INFO - Estimated sufficient time budget=11222s. Estimated necessary time budget=11s.
[flaml.automl: 09-19 01:51:09] {3072} INFO -  at 1.2s,	estimator xgboost's best error=0.1379,	best estimator xgboost's best error=0.1379
[flaml.automl: 09-19 01:51:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:51:11] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-19 01:51:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:51:12] {3072} INFO -  at 4.4s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-19 01:51:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:51:21] {3072} INFO -  at 13.4s,	estimator xgboost's best error=0.0949,	best estimator xgboost's best error=0.0949
[flaml.automl: 09-19 01:51:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:51:23] {3072} INFO -  at 14.5s,	estimator xgboost's best error=0.0839,	best estimator xgboost's best error=0.0839
[flaml.automl: 09-19 01:51:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:51:24] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.0762,	best estimator xgboost's best error=0.0762
[flaml.automl: 09-19 01:51:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:51:26] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.0762,	best estimator xgboost's best error=0.0762
[flaml.automl: 09-19 01:51:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:51:28] {3072} INFO -  at 19.8s,	estimator xgboost's best error=0.0762,	best estimator xgboost's best error=0.0762
[flaml.automl: 09-19 01:51:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:51:29] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.0762,	best estimator xgboost's best error=0.0762
[flaml.automl: 09-19 01:51:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:51:31] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.0762,	best estimator xgboost's best error=0.0762
[flaml.automl: 09-19 01:51:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:51:33] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0762,	best estimator xgboost's best error=0.0762
[flaml.automl: 09-19 01:51:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:51:34] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0762,	best estimator xgboost's best error=0.0762
[flaml.automl: 09-19 01:51:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:51:40] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.0707,	best estimator xgboost's best error=0.0707
[flaml.automl: 09-19 01:51:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:51:50] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.0701,	best estimator xgboost's best error=0.0701
[flaml.automl: 09-19 01:51:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 01:51:56] {3072} INFO -  at 48.0s,	estimator xgboost's best error=0.0701,	best estimator xgboost's best error=0.0701
[flaml.automl: 09-19 01:51:56] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 01:52:08] {3072} INFO -  at 59.6s,	estimator xgboost's best error=0.0693,	best estimator xgboost's best error=0.0693
[flaml.automl: 09-19 01:52:25] {3335} INFO - retrain xgboost for 17.4s
[flaml.automl: 09-19 01:52:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:52:25] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:52:25] {2637} INFO - Time taken to find the best model: 59.64518332481384
[flaml.automl: 09-19 01:52:25] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.9307007963517575
CO(0)最好结果：{'pred_time': 1.695117012399142e-05, 'wall_clock_time': 59.64518332481384, 'metric_for_logging': {'pred_time': 1.695117012399142e-05}, 'val_loss': 0.06929920364824249, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 11.661737203598022}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8262782108500513
CO(0)的mse=0.010014421123212791
CO(0)的mae=0.06723926986553305
CO(0)的mar=0.15891804742450932
总共花费的时间为：77.38
中卫市
2680A
3650A
[flaml.automl: 09-19 01:59:12] {2390} INFO - task = regression
[flaml.automl: 09-19 01:59:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:59:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:59:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:59:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:59:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:59:13] {3025} INFO - Estimated sufficient time budget=12032s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:59:13] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1322,	best estimator xgboost's best error=0.1322
[flaml.automl: 09-19 01:59:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:59:15] {3072} INFO -  at 3.2s,	estimator xgboost's best error=0.0843,	best estimator xgboost's best error=0.0843
[flaml.automl: 09-19 01:59:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:59:17] {3072} INFO -  at 4.4s,	estimator xgboost's best error=0.0843,	best estimator xgboost's best error=0.0843
[flaml.automl: 09-19 01:59:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:59:25] {3072} INFO -  at 12.8s,	estimator xgboost's best error=0.0843,	best estimator xgboost's best error=0.0843
[flaml.automl: 09-19 01:59:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:59:26] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-19 01:59:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:59:28] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.0477,	best estimator xgboost's best error=0.0477
[flaml.automl: 09-19 01:59:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:59:29] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.0477,	best estimator xgboost's best error=0.0477
[flaml.automl: 09-19 01:59:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:59:32] {3072} INFO -  at 19.6s,	estimator xgboost's best error=0.0477,	best estimator xgboost's best error=0.0477
[flaml.automl: 09-19 01:59:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:59:33] {3072} INFO -  at 20.7s,	estimator xgboost's best error=0.0477,	best estimator xgboost's best error=0.0477
[flaml.automl: 09-19 01:59:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:59:35] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.0477,	best estimator xgboost's best error=0.0477
[flaml.automl: 09-19 01:59:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:59:37] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.0477,	best estimator xgboost's best error=0.0477
[flaml.automl: 09-19 01:59:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:59:38] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.0477,	best estimator xgboost's best error=0.0477
[flaml.automl: 09-19 01:59:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:59:44] {3072} INFO -  at 31.5s,	estimator xgboost's best error=0.0424,	best estimator xgboost's best error=0.0424
[flaml.automl: 09-19 01:59:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:59:54] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-19 01:59:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 02:00:00] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.0415,	best estimator xgboost's best error=0.0415
[flaml.automl: 09-19 02:00:00] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 02:00:12] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.0408,	best estimator xgboost's best error=0.0408
[flaml.automl: 09-19 02:00:29] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-19 02:00:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:00:29] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:00:29] {2637} INFO - Time taken to find the best model: 59.720219373703
[flaml.automl: 09-19 02:00:29] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.9591597015889183
CO(0)最好结果：{'pred_time': 1.8239952196043238e-05, 'wall_clock_time': 59.720219373703, 'metric_for_logging': {'pred_time': 1.8239952196043238e-05}, 'val_loss': 0.040840298411081735, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 11.647896528244019}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8643195311024241
CO(0)的mse=0.004056803357020875
CO(0)的mae=0.04071815206878636
CO(0)的mar=0.12403305839918859
总共花费的时间为：77.47
固原市
2683A
2684A
2685A
3522A
[flaml.automl: 09-19 02:13:48] {2390} INFO - task = regression
[flaml.automl: 09-19 02:13:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:13:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:13:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:13:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:13:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:13:50] {3025} INFO - Estimated sufficient time budget=85721s. Estimated necessary time budget=86s.
[flaml.automl: 09-19 02:13:50] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.1331,	best estimator xgboost's best error=0.1331
[flaml.automl: 09-19 02:13:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:13:54] {3072} INFO -  at 6.4s,	estimator xgboost's best error=0.0879,	best estimator xgboost's best error=0.0879
[flaml.automl: 09-19 02:13:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:13:56] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.0879,	best estimator xgboost's best error=0.0879
[flaml.automl: 09-19 02:13:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:14:02] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.0879,	best estimator xgboost's best error=0.0879
[flaml.automl: 09-19 02:14:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:14:04] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.0740,	best estimator xgboost's best error=0.0740
[flaml.automl: 09-19 02:14:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:14:07] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0664,	best estimator xgboost's best error=0.0664
[flaml.automl: 09-19 02:14:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:14:09] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.0664,	best estimator xgboost's best error=0.0664
[flaml.automl: 09-19 02:14:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:14:13] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.0664,	best estimator xgboost's best error=0.0664
[flaml.automl: 09-19 02:14:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:14:15] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.0664,	best estimator xgboost's best error=0.0664
[flaml.automl: 09-19 02:14:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:14:19] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.0664,	best estimator xgboost's best error=0.0664
[flaml.automl: 09-19 02:14:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:14:21] {3072} INFO -  at 33.1s,	estimator xgboost's best error=0.0656,	best estimator xgboost's best error=0.0656
[flaml.automl: 09-19 02:14:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:14:23] {3072} INFO -  at 35.0s,	estimator xgboost's best error=0.0656,	best estimator xgboost's best error=0.0656
[flaml.automl: 09-19 02:14:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:14:33] {3072} INFO -  at 45.0s,	estimator xgboost's best error=0.0584,	best estimator xgboost's best error=0.0584
[flaml.automl: 09-19 02:14:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 02:14:47] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-19 02:15:07] {3335} INFO - retrain xgboost for 19.6s
[flaml.automl: 09-19 02:15:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:15:07] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:15:07] {2637} INFO - Time taken to find the best model: 59.242926359176636
[flaml.automl: 09-19 02:15:07] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41271}
CO(0)最佳损失：0.9432617620855921
CO(0)最好结果：{'pred_time': 1.8139931503675207e-05, 'wall_clock_time': 59.242926359176636, 'metric_for_logging': {'pred_time': 1.8139931503675207e-05}, 'val_loss': 0.05673823791440792, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41271}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41271, 'experiment_tag': 'exp', 'time_total_s': 14.228858947753906}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8391779977868543
CO(0)的mse=0.008932634935075389
CO(0)的mae=0.05510125883423037
CO(0)的mar=0.12710446175876888
总共花费的时间为：79.80
吐鲁番地区
2686A
2687A
[flaml.automl: 09-19 02:22:21] {2390} INFO - task = regression
[flaml.automl: 09-19 02:22:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:22:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:22:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:22:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:22:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:22:22] {3025} INFO - Estimated sufficient time budget=12101s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 02:22:22] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.3163,	best estimator xgboost's best error=0.3163
[flaml.automl: 09-19 02:22:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:22:24] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1826,	best estimator xgboost's best error=0.1826
[flaml.automl: 09-19 02:22:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:22:26] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.1826,	best estimator xgboost's best error=0.1826
[flaml.automl: 09-19 02:22:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:22:35] {3072} INFO -  at 14.0s,	estimator xgboost's best error=0.1826,	best estimator xgboost's best error=0.1826
[flaml.automl: 09-19 02:22:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:22:36] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.1477,	best estimator xgboost's best error=0.1477
[flaml.automl: 09-19 02:22:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:22:38] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.1348,	best estimator xgboost's best error=0.1348
[flaml.automl: 09-19 02:22:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:22:39] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.1348,	best estimator xgboost's best error=0.1348
[flaml.automl: 09-19 02:22:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:22:42] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.1348,	best estimator xgboost's best error=0.1348
[flaml.automl: 09-19 02:22:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:22:43] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.1348,	best estimator xgboost's best error=0.1348
[flaml.automl: 09-19 02:22:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:22:45] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.1348,	best estimator xgboost's best error=0.1348
[flaml.automl: 09-19 02:22:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:22:47] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.1348,	best estimator xgboost's best error=0.1348
[flaml.automl: 09-19 02:22:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:22:48] {3072} INFO -  at 26.8s,	estimator xgboost's best error=0.1348,	best estimator xgboost's best error=0.1348
[flaml.automl: 09-19 02:22:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:22:54] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.1308,	best estimator xgboost's best error=0.1308
[flaml.automl: 09-19 02:22:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 02:23:04] {3072} INFO -  at 43.3s,	estimator xgboost's best error=0.1207,	best estimator xgboost's best error=0.1207
[flaml.automl: 09-19 02:23:04] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 02:23:10] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.1207,	best estimator xgboost's best error=0.1207
[flaml.automl: 09-19 02:23:10] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 02:23:20] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.1195,	best estimator xgboost's best error=0.1195
[flaml.automl: 09-19 02:23:40] {3335} INFO - retrain xgboost for 19.7s
[flaml.automl: 09-19 02:23:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:23:40] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:23:40] {2637} INFO - Time taken to find the best model: 59.082462549209595
[flaml.automl: 09-19 02:23:40] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.8804831301508158
CO(0)最好结果：{'pred_time': 1.6513513108036172e-05, 'wall_clock_time': 59.082462549209595, 'metric_for_logging': {'pred_time': 1.6513513108036172e-05}, 'val_loss': 0.11951686984918422, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 9.712132453918457}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8863356864588117
CO(0)的mse=0.046017499751621255
CO(0)的mae=0.12014206462351218
CO(0)的mar=0.11918270817418529
总共花费的时间为：79.22
哈密地区
2688A
2689A
[flaml.automl: 09-19 02:30:30] {2390} INFO - task = regression
[flaml.automl: 09-19 02:30:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:30:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:30:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:30:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:30:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:30:32] {3025} INFO - Estimated sufficient time budget=21856s. Estimated necessary time budget=22s.
[flaml.automl: 09-19 02:30:32] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.1779,	best estimator xgboost's best error=0.1779
[flaml.automl: 09-19 02:30:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:30:35] {3072} INFO -  at 5.6s,	estimator xgboost's best error=0.1374,	best estimator xgboost's best error=0.1374
[flaml.automl: 09-19 02:30:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:30:37] {3072} INFO -  at 7.5s,	estimator xgboost's best error=0.1374,	best estimator xgboost's best error=0.1374
[flaml.automl: 09-19 02:30:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:30:51] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.1374,	best estimator xgboost's best error=0.1374
[flaml.automl: 09-19 02:30:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:30:53] {3072} INFO -  at 23.7s,	estimator xgboost's best error=0.1168,	best estimator xgboost's best error=0.1168
[flaml.automl: 09-19 02:30:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:30:56] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.1091,	best estimator xgboost's best error=0.1091
[flaml.automl: 09-19 02:30:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:30:59] {3072} INFO -  at 29.3s,	estimator xgboost's best error=0.1091,	best estimator xgboost's best error=0.1091
[flaml.automl: 09-19 02:30:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:31:03] {3072} INFO -  at 33.3s,	estimator xgboost's best error=0.1091,	best estimator xgboost's best error=0.1091
[flaml.automl: 09-19 02:31:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:31:05] {3072} INFO -  at 35.0s,	estimator xgboost's best error=0.1091,	best estimator xgboost's best error=0.1091
[flaml.automl: 09-19 02:31:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:31:09] {3072} INFO -  at 39.1s,	estimator xgboost's best error=0.1091,	best estimator xgboost's best error=0.1091
[flaml.automl: 09-19 02:31:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:31:11] {3072} INFO -  at 41.1s,	estimator xgboost's best error=0.1091,	best estimator xgboost's best error=0.1091
[flaml.automl: 09-19 02:31:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:31:13] {3072} INFO -  at 43.1s,	estimator xgboost's best error=0.1091,	best estimator xgboost's best error=0.1091
[flaml.automl: 09-19 02:31:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:31:23] {3072} INFO -  at 53.1s,	estimator xgboost's best error=0.1060,	best estimator xgboost's best error=0.1060
[flaml.automl: 09-19 02:31:35] {3335} INFO - retrain xgboost for 11.6s
[flaml.automl: 09-19 02:31:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:31:35] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:31:35] {2637} INFO - Time taken to find the best model: 53.08465886116028
[flaml.automl: 09-19 02:31:35] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
CO(0)最佳损失：0.8940202213643347
CO(0)最好结果：{'pred_time': 3.12428514496619e-05, 'wall_clock_time': 53.08465886116028, 'metric_for_logging': {'pred_time': 3.12428514496619e-05}, 'val_loss': 0.10597977863566531, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 13, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 13, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.979846000671387}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=13, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.5763005897257908
CO(0)的mse=0.03038507210920907
CO(0)的mae=0.10838040581441657
CO(0)的mar=0.31924778840107654
总共花费的时间为：65.22
昌吉州
2690A
3613A
[flaml.automl: 09-19 02:38:07] {2390} INFO - task = regression
[flaml.automl: 09-19 02:38:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:38:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:38:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:38:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:38:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:38:11] {3025} INFO - Estimated sufficient time budget=34872s. Estimated necessary time budget=35s.
[flaml.automl: 09-19 02:38:11] {3072} INFO -  at 3.6s,	estimator xgboost's best error=0.2039,	best estimator xgboost's best error=0.2039
[flaml.automl: 09-19 02:38:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:38:16] {3072} INFO -  at 8.9s,	estimator xgboost's best error=0.1333,	best estimator xgboost's best error=0.1333
[flaml.automl: 09-19 02:38:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:38:19] {3072} INFO -  at 11.2s,	estimator xgboost's best error=0.1333,	best estimator xgboost's best error=0.1333
[flaml.automl: 09-19 02:38:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:38:29] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.1333,	best estimator xgboost's best error=0.1333
[flaml.automl: 09-19 02:38:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:38:30] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.0914,	best estimator xgboost's best error=0.0914
[flaml.automl: 09-19 02:38:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:38:32] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-19 02:38:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:38:33] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-19 02:38:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:38:36] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-19 02:38:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:38:37] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-19 02:38:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:38:39] {3072} INFO -  at 32.1s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-19 02:38:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:38:41] {3072} INFO -  at 33.2s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-19 02:38:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:38:42] {3072} INFO -  at 34.4s,	estimator xgboost's best error=0.0753,	best estimator xgboost's best error=0.0753
[flaml.automl: 09-19 02:38:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:38:49] {3072} INFO -  at 42.1s,	estimator xgboost's best error=0.0704,	best estimator xgboost's best error=0.0704
[flaml.automl: 09-19 02:38:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 02:39:07] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0678,	best estimator xgboost's best error=0.0678
[flaml.automl: 09-19 02:39:30] {3335} INFO - retrain xgboost for 23.1s
[flaml.automl: 09-19 02:39:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:39:30] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:39:30] {2637} INFO - Time taken to find the best model: 59.18163204193115
[flaml.automl: 09-19 02:39:30] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9321564826031874
CO(0)最好结果：{'pred_time': 4.6816102592146604e-05, 'wall_clock_time': 59.18163204193115, 'metric_for_logging': {'pred_time': 4.6816102592146604e-05}, 'val_loss': 0.06784351739681257, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 17.119429349899292}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9391492858295618
CO(0)的mse=0.012318373855456807
CO(0)的mae=0.059888292661881136
CO(0)的mar=0.08735103952913406
总共花费的时间为：82.77
博尔塔拉蒙古自治州
2693A
2694A
[flaml.automl: 09-19 02:46:07] {2390} INFO - task = regression
[flaml.automl: 09-19 02:46:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:46:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:46:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:46:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:46:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:46:08] {3025} INFO - Estimated sufficient time budget=12068s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 02:46:08] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1871,	best estimator xgboost's best error=0.1871
[flaml.automl: 09-19 02:46:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:46:10] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1077,	best estimator xgboost's best error=0.1077
[flaml.automl: 09-19 02:46:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:46:11] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.1077,	best estimator xgboost's best error=0.1077
[flaml.automl: 09-19 02:46:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:46:27] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.1077,	best estimator xgboost's best error=0.1077
[flaml.automl: 09-19 02:46:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:46:30] {3072} INFO -  at 23.5s,	estimator xgboost's best error=0.0791,	best estimator xgboost's best error=0.0791
[flaml.automl: 09-19 02:46:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:46:34] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.0695,	best estimator xgboost's best error=0.0695
[flaml.automl: 09-19 02:46:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:46:39] {3072} INFO -  at 32.6s,	estimator xgboost's best error=0.0695,	best estimator xgboost's best error=0.0695
[flaml.automl: 09-19 02:46:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:46:46] {3072} INFO -  at 39.5s,	estimator xgboost's best error=0.0695,	best estimator xgboost's best error=0.0695
[flaml.automl: 09-19 02:46:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:46:49] {3072} INFO -  at 42.9s,	estimator xgboost's best error=0.0695,	best estimator xgboost's best error=0.0695
[flaml.automl: 09-19 02:46:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:46:59] {3072} INFO -  at 52.5s,	estimator xgboost's best error=0.0695,	best estimator xgboost's best error=0.0695
[flaml.automl: 09-19 02:46:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:47:02] {3072} INFO -  at 55.8s,	estimator xgboost's best error=0.0695,	best estimator xgboost's best error=0.0695
[flaml.automl: 09-19 02:47:06] {3335} INFO - retrain xgboost for 4.1s
[flaml.automl: 09-19 02:47:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:47:06] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:47:06] {2637} INFO - Time taken to find the best model: 27.891106128692627
CO(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
CO(0)最佳损失：0.9305100605962326
CO(0)最好结果：{'pred_time': 5.155951482481325e-05, 'wall_clock_time': 27.891106128692627, 'metric_for_logging': {'pred_time': 5.155951482481325e-05}, 'val_loss': 0.06948993940376738, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 4.435428142547607}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8452794598513004
CO(0)的mse=0.010909592529035938
CO(0)的mae=0.07007051475614423
CO(0)的mar=0.355544127495048
总共花费的时间为：60.43
阿克苏地区
2695A
2696A
[flaml.automl: 09-19 02:54:06] {2390} INFO - task = regression
[flaml.automl: 09-19 02:54:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:54:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:54:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:54:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:54:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:54:07] {3025} INFO - Estimated sufficient time budget=12025s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 02:54:07] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.2629,	best estimator xgboost's best error=0.2629
[flaml.automl: 09-19 02:54:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:54:09] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1706,	best estimator xgboost's best error=0.1706
[flaml.automl: 09-19 02:54:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:54:10] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.1706,	best estimator xgboost's best error=0.1706
[flaml.automl: 09-19 02:54:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:54:20] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.1706,	best estimator xgboost's best error=0.1706
[flaml.automl: 09-19 02:54:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:54:21] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.1433,	best estimator xgboost's best error=0.1433
[flaml.automl: 09-19 02:54:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:54:22] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-19 02:54:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:54:24] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-19 02:54:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:54:26] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-19 02:54:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:54:28] {3072} INFO -  at 22.4s,	estimator xgboost's best error=0.1350,	best estimator xgboost's best error=0.1350
[flaml.automl: 09-19 02:54:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:54:31] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.1296,	best estimator xgboost's best error=0.1296
[flaml.automl: 09-19 02:54:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:54:33] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.1296,	best estimator xgboost's best error=0.1296
[flaml.automl: 09-19 02:54:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:54:34] {3072} INFO -  at 28.2s,	estimator xgboost's best error=0.1296,	best estimator xgboost's best error=0.1296
[flaml.automl: 09-19 02:54:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:54:46] {3072} INFO -  at 40.3s,	estimator xgboost's best error=0.1225,	best estimator xgboost's best error=0.1225
[flaml.automl: 09-19 02:54:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 02:55:05] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.1189,	best estimator xgboost's best error=0.1189
[flaml.automl: 09-19 02:55:27] {3335} INFO - retrain xgboost for 21.9s
[flaml.automl: 09-19 02:55:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:55:27] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:55:27] {2637} INFO - Time taken to find the best model: 59.50806713104248
[flaml.automl: 09-19 02:55:27] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
CO(0)最佳损失：0.8811353178728057
CO(0)最好结果：{'pred_time': 1.678614526115063e-05, 'wall_clock_time': 59.50806713104248, 'metric_for_logging': {'pred_time': 1.678614526115063e-05}, 'val_loss': 0.11886468212719421, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 19.235560178756714}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.812051097858775
CO(0)的mse=0.04054191003394193
CO(0)的mae=0.11266468374319792
CO(0)的mar=0.1896814849384316
总共花费的时间为：81.87
克孜勒苏柯尔克孜自治州
2697A
3665A
[flaml.automl: 09-19 03:02:07] {2390} INFO - task = regression
[flaml.automl: 09-19 03:02:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:02:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:02:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:02:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:02:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:02:09] {3025} INFO - Estimated sufficient time budget=21771s. Estimated necessary time budget=22s.
[flaml.automl: 09-19 03:02:09] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.2040,	best estimator xgboost's best error=0.2040
[flaml.automl: 09-19 03:02:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:02:13] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.1132,	best estimator xgboost's best error=0.1132
[flaml.automl: 09-19 03:02:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:02:15] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.1132,	best estimator xgboost's best error=0.1132
[flaml.automl: 09-19 03:02:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:02:33] {3072} INFO -  at 26.1s,	estimator xgboost's best error=0.1132,	best estimator xgboost's best error=0.1132
[flaml.automl: 09-19 03:02:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:02:35] {3072} INFO -  at 28.3s,	estimator xgboost's best error=0.0775,	best estimator xgboost's best error=0.0775
[flaml.automl: 09-19 03:02:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:02:38] {3072} INFO -  at 31.2s,	estimator xgboost's best error=0.0675,	best estimator xgboost's best error=0.0675
[flaml.automl: 09-19 03:02:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:02:41] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.0661,	best estimator xgboost's best error=0.0661
[flaml.automl: 09-19 03:02:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:02:46] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.0661,	best estimator xgboost's best error=0.0661
[flaml.automl: 09-19 03:02:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:02:49] {3072} INFO -  at 42.3s,	estimator xgboost's best error=0.0661,	best estimator xgboost's best error=0.0661
[flaml.automl: 09-19 03:02:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:02:55] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.0656,	best estimator xgboost's best error=0.0656
[flaml.automl: 09-19 03:02:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:02:58] {3072} INFO -  at 51.1s,	estimator xgboost's best error=0.0656,	best estimator xgboost's best error=0.0656
[flaml.automl: 09-19 03:02:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:03:00] {3072} INFO -  at 53.2s,	estimator xgboost's best error=0.0656,	best estimator xgboost's best error=0.0656
[flaml.automl: 09-19 03:03:00] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:03:05] {3072} INFO -  at 58.3s,	estimator xgboost's best error=0.0624,	best estimator xgboost's best error=0.0624
[flaml.automl: 09-19 03:03:31] {3335} INFO - retrain xgboost for 26.0s
[flaml.automl: 09-19 03:03:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 03:03:31] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:03:31] {2637} INFO - Time taken to find the best model: 58.29257822036743
[flaml.automl: 09-19 03:03:31] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
CO(0)最佳损失：0.9375691382686298
CO(0)最好结果：{'pred_time': 3.690804401836285e-05, 'wall_clock_time': 58.29257822036743, 'metric_for_logging': {'pred_time': 3.690804401836285e-05}, 'val_loss': 0.06243086173137029, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 5.059702157974243}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8939145266847808
CO(0)的mse=0.016721822115366887
CO(0)的mae=0.06458610993435458
CO(0)的mar=0.14536357156019905
总共花费的时间为：84.84
喀什地区
2698A
2699A
2700A
[flaml.automl: 09-19 03:13:59] {2390} INFO - task = regression
[flaml.automl: 09-19 03:13:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:13:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:13:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:13:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:13:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:14:00] {3025} INFO - Estimated sufficient time budget=12263s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 03:14:00] {3072} INFO -  at 1.4s,	estimator xgboost's best error=0.3960,	best estimator xgboost's best error=0.3960
[flaml.automl: 09-19 03:14:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:14:02] {3072} INFO -  at 3.5s,	estimator xgboost's best error=0.2488,	best estimator xgboost's best error=0.2488
[flaml.automl: 09-19 03:14:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:14:03] {3072} INFO -  at 4.7s,	estimator xgboost's best error=0.2488,	best estimator xgboost's best error=0.2488
[flaml.automl: 09-19 03:14:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:14:14] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.2488,	best estimator xgboost's best error=0.2488
[flaml.automl: 09-19 03:14:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:14:15] {3072} INFO -  at 16.1s,	estimator xgboost's best error=0.2144,	best estimator xgboost's best error=0.2144
[flaml.automl: 09-19 03:14:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:14:16] {3072} INFO -  at 17.7s,	estimator xgboost's best error=0.1916,	best estimator xgboost's best error=0.1916
[flaml.automl: 09-19 03:14:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:14:18] {3072} INFO -  at 19.3s,	estimator xgboost's best error=0.1916,	best estimator xgboost's best error=0.1916
[flaml.automl: 09-19 03:14:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:14:20] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.1916,	best estimator xgboost's best error=0.1916
[flaml.automl: 09-19 03:14:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:14:22] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.1916,	best estimator xgboost's best error=0.1916
[flaml.automl: 09-19 03:14:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:14:24] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.1851,	best estimator xgboost's best error=0.1851
[flaml.automl: 09-19 03:14:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:14:26] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.1851,	best estimator xgboost's best error=0.1851
[flaml.automl: 09-19 03:14:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:14:27] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.1851,	best estimator xgboost's best error=0.1851
[flaml.automl: 09-19 03:14:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:14:39] {3072} INFO -  at 40.3s,	estimator xgboost's best error=0.1735,	best estimator xgboost's best error=0.1735
[flaml.automl: 09-19 03:14:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:14:58] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.1595,	best estimator xgboost's best error=0.1595
[flaml.automl: 09-19 03:15:19] {3335} INFO - retrain xgboost for 20.4s
[flaml.automl: 09-19 03:15:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9652964050842439, colsample_bynode=1,
             colsample_bytree=0.7352194849388544, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.539600825670331,
             max_delta_step=0, max_depth=0, max_leaves=19,
             min_child_weight=0.007803063106529637, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0014064040975307255, reg_lambda=0.22263881644247355,
             scale_pos_weight=1, subsample=0.7005478327435369,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 03:15:19] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:15:19] {2637} INFO - Time taken to find the best model: 59.852662801742554
[flaml.automl: 09-19 03:15:19] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 19, 'min_child_weight': 0.007803063106529637, 'learning_rate': 0.539600825670331, 'subsample': 0.7005478327435369, 'colsample_bylevel': 0.9652964050842439, 'colsample_bytree': 0.7352194849388544, 'reg_alpha': 0.0014064040975307255, 'reg_lambda': 0.22263881644247355}
CO(0)最佳损失：0.8405127367181511
CO(0)最好结果：{'pred_time': 1.1612080979621273e-05, 'wall_clock_time': 59.852662801742554, 'metric_for_logging': {'pred_time': 1.1612080979621273e-05}, 'val_loss': 0.15948726328184892, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 19, 'min_child_weight': 0.007803063106529637, 'learning_rate': 0.539600825670331, 'subsample': 0.7005478327435369, 'colsample_bylevel': 0.9652964050842439, 'colsample_bytree': 0.7352194849388544, 'reg_alpha': 0.0014064040975307255, 'reg_lambda': 0.22263881644247355}, 'config/n_estimators': 19, 'config/max_leaves': 19, 'config/min_child_weight': 0.007803063106529637, 'config/learning_rate': 0.539600825670331, 'config/subsample': 0.7005478327435369, 'config/colsample_bylevel': 0.9652964050842439, 'config/colsample_bytree': 0.7352194849388544, 'config/reg_alpha': 0.0014064040975307255, 'config/reg_lambda': 0.22263881644247355, 'experiment_tag': 'exp', 'time_total_s': 19.547116994857788}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9652964050842439, colsample_bynode=1,
             colsample_bytree=0.7352194849388544, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.539600825670331,
             max_delta_step=0, max_depth=0, max_leaves=19,
             min_child_weight=0.007803063106529637, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0014064040975307255, reg_lambda=0.22263881644247355,
             scale_pos_weight=1, subsample=0.7005478327435369,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.8738504352847866
CO(0)的mse=0.07966170810341328
CO(0)的mae=0.15324963994952479
CO(0)的mar=0.17268037388717195
总共花费的时间为：80.89
和田地区
3614A
3615A
[flaml.automl: 09-19 03:37:41] {2390} INFO - task = regression
[flaml.automl: 09-19 03:37:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:37:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:37:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:37:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:37:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:37:44] {3025} INFO - Estimated sufficient time budget=21833s. Estimated necessary time budget=22s.
[flaml.automl: 09-19 03:37:44] {3072} INFO -  at 2.3s,	estimator xgboost's best error=0.4563,	best estimator xgboost's best error=0.4563
[flaml.automl: 09-19 03:37:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:37:47] {3072} INFO -  at 5.7s,	estimator xgboost's best error=0.2756,	best estimator xgboost's best error=0.2756
[flaml.automl: 09-19 03:37:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:37:49] {3072} INFO -  at 8.0s,	estimator xgboost's best error=0.2756,	best estimator xgboost's best error=0.2756
[flaml.automl: 09-19 03:37:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:38:04] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.2756,	best estimator xgboost's best error=0.2756
[flaml.automl: 09-19 03:38:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:38:05] {3072} INFO -  at 23.6s,	estimator xgboost's best error=0.1854,	best estimator xgboost's best error=0.1854
[flaml.automl: 09-19 03:38:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:38:07] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-19 03:38:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:38:08] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-19 03:38:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:38:11] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-19 03:38:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:38:12] {3072} INFO -  at 30.6s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-19 03:38:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:38:14] {3072} INFO -  at 33.1s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-19 03:38:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:38:16] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-19 03:38:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:38:17] {3072} INFO -  at 35.4s,	estimator xgboost's best error=0.1640,	best estimator xgboost's best error=0.1640
[flaml.automl: 09-19 03:38:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:38:23] {3072} INFO -  at 41.5s,	estimator xgboost's best error=0.1525,	best estimator xgboost's best error=0.1525
[flaml.automl: 09-19 03:38:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:38:33] {3072} INFO -  at 52.1s,	estimator xgboost's best error=0.1444,	best estimator xgboost's best error=0.1444
[flaml.automl: 09-19 03:38:44] {3335} INFO - retrain xgboost for 10.6s
[flaml.automl: 09-19 03:38:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:38:44] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:38:44] {2637} INFO - Time taken to find the best model: 52.09429693222046
[flaml.automl: 09-19 03:38:44] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.855635530234698
CO(0)最好结果：{'pred_time': 1.690090733828429e-05, 'wall_clock_time': 52.09429693222046, 'metric_for_logging': {'pred_time': 1.690090733828429e-05}, 'val_loss': 0.144364469765302, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.587780952453613}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8945378990360373
CO(0)的mse=0.07128243097652452
CO(0)的mae=0.14646564431935724
CO(0)的mar=0.2549472016074886
总共花费的时间为：63.17
伊犁哈萨克州
2703A
2704A
2705A
[flaml.automl: 09-19 03:47:47] {2390} INFO - task = regression
[flaml.automl: 09-19 03:47:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:47:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:47:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:47:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:47:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:47:49] {3025} INFO - Estimated sufficient time budget=23772s. Estimated necessary time budget=24s.
[flaml.automl: 09-19 03:47:49] {3072} INFO -  at 2.6s,	estimator xgboost's best error=0.3437,	best estimator xgboost's best error=0.3437
[flaml.automl: 09-19 03:47:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:47:53] {3072} INFO -  at 6.7s,	estimator xgboost's best error=0.2165,	best estimator xgboost's best error=0.2165
[flaml.automl: 09-19 03:47:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:47:55] {3072} INFO -  at 9.1s,	estimator xgboost's best error=0.2165,	best estimator xgboost's best error=0.2165
[flaml.automl: 09-19 03:47:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:48:12] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.2165,	best estimator xgboost's best error=0.2165
[flaml.automl: 09-19 03:48:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:48:14] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.1864,	best estimator xgboost's best error=0.1864
[flaml.automl: 09-19 03:48:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:48:17] {3072} INFO -  at 30.8s,	estimator xgboost's best error=0.1723,	best estimator xgboost's best error=0.1723
[flaml.automl: 09-19 03:48:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:48:20] {3072} INFO -  at 33.6s,	estimator xgboost's best error=0.1723,	best estimator xgboost's best error=0.1723
[flaml.automl: 09-19 03:48:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:48:24] {3072} INFO -  at 37.6s,	estimator xgboost's best error=0.1723,	best estimator xgboost's best error=0.1723
[flaml.automl: 09-19 03:48:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:48:26] {3072} INFO -  at 39.6s,	estimator xgboost's best error=0.1723,	best estimator xgboost's best error=0.1723
[flaml.automl: 09-19 03:48:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:48:30] {3072} INFO -  at 44.0s,	estimator xgboost's best error=0.1672,	best estimator xgboost's best error=0.1672
[flaml.automl: 09-19 03:48:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:48:33] {3072} INFO -  at 46.9s,	estimator xgboost's best error=0.1672,	best estimator xgboost's best error=0.1672
[flaml.automl: 09-19 03:48:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:48:35] {3072} INFO -  at 48.7s,	estimator xgboost's best error=0.1672,	best estimator xgboost's best error=0.1672
[flaml.automl: 09-19 03:48:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:48:45] {3072} INFO -  at 58.7s,	estimator xgboost's best error=0.1665,	best estimator xgboost's best error=0.1665
[flaml.automl: 09-19 03:48:56] {3335} INFO - retrain xgboost for 11.3s
[flaml.automl: 09-19 03:48:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8068591840934569, colsample_bynode=1,
             colsample_bytree=0.653516717901374, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9977579754449118,
             max_delta_step=0, max_depth=0, max_leaves=25,
             min_child_weight=0.013548610495542106, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00546465520560431,
             reg_lambda=1.128463058774389, scale_pos_weight=1,
             subsample=0.8083911828355876, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:48:56] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:48:56] {2637} INFO - Time taken to find the best model: 58.67171263694763
[flaml.automl: 09-19 03:48:56] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 25, 'min_child_weight': 0.013548610495542106, 'learning_rate': 0.9977579754449118, 'subsample': 0.8083911828355876, 'colsample_bylevel': 0.8068591840934569, 'colsample_bytree': 0.653516717901374, 'reg_alpha': 0.00546465520560431, 'reg_lambda': 1.128463058774389}
CO(0)最佳损失：0.833523161221374
CO(0)最好结果：{'pred_time': 1.1026909497975299e-05, 'wall_clock_time': 58.67171263694763, 'metric_for_logging': {'pred_time': 1.1026909497975299e-05}, 'val_loss': 0.16647683877862596, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 25, 'min_child_weight': 0.013548610495542106, 'learning_rate': 0.9977579754449118, 'subsample': 0.8083911828355876, 'colsample_bylevel': 0.8068591840934569, 'colsample_bytree': 0.653516717901374, 'reg_alpha': 0.00546465520560431, 'reg_lambda': 1.128463058774389}, 'config/n_estimators': 8, 'config/max_leaves': 25, 'config/min_child_weight': 0.013548610495542106, 'config/learning_rate': 0.9977579754449118, 'config/subsample': 0.8083911828355876, 'config/colsample_bylevel': 0.8068591840934569, 'config/colsample_bytree': 0.653516717901374, 'config/reg_alpha': 0.00546465520560431, 'config/reg_lambda': 1.128463058774389, 'experiment_tag': 'exp', 'time_total_s': 9.93352484703064}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8068591840934569, colsample_bynode=1,
             colsample_bytree=0.653516717901374, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.9977579754449118,
             max_delta_step=0, max_depth=0, max_leaves=25,
             min_child_weight=0.013548610495542106, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00546465520560431,
             reg_lambda=1.128463058774389, scale_pos_weight=1,
             subsample=0.8083911828355876, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.8158302497260008
CO(0)的mse=0.10406415939578186
CO(0)的mae=0.16799576608602043
CO(0)的mar=0.1569527775862511
总共花费的时间为：70.58
塔城地区
2706A
[flaml.automl: 09-19 03:51:56] {2390} INFO - task = regression
[flaml.automl: 09-19 03:51:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:51:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:51:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:51:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:51:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:51:59] {3025} INFO - Estimated sufficient time budget=21294s. Estimated necessary time budget=21s.
[flaml.automl: 09-19 03:51:59] {3072} INFO -  at 2.2s,	estimator xgboost's best error=0.1659,	best estimator xgboost's best error=0.1659
[flaml.automl: 09-19 03:51:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:52:02] {3072} INFO -  at 5.4s,	estimator xgboost's best error=0.1203,	best estimator xgboost's best error=0.1203
[flaml.automl: 09-19 03:52:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:52:04] {3072} INFO -  at 7.5s,	estimator xgboost's best error=0.1203,	best estimator xgboost's best error=0.1203
[flaml.automl: 09-19 03:52:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:52:17] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.1203,	best estimator xgboost's best error=0.1203
[flaml.automl: 09-19 03:52:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:52:19] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.0955,	best estimator xgboost's best error=0.0955
[flaml.automl: 09-19 03:52:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:52:21] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.0871,	best estimator xgboost's best error=0.0871
[flaml.automl: 09-19 03:52:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:52:24] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.0871,	best estimator xgboost's best error=0.0871
[flaml.automl: 09-19 03:52:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:52:28] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.0871,	best estimator xgboost's best error=0.0871
[flaml.automl: 09-19 03:52:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:52:30] {3072} INFO -  at 33.7s,	estimator xgboost's best error=0.0871,	best estimator xgboost's best error=0.0871
[flaml.automl: 09-19 03:52:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:52:34] {3072} INFO -  at 37.6s,	estimator xgboost's best error=0.0871,	best estimator xgboost's best error=0.0871
[flaml.automl: 09-19 03:52:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:52:36] {3072} INFO -  at 39.7s,	estimator xgboost's best error=0.0871,	best estimator xgboost's best error=0.0871
[flaml.automl: 09-19 03:52:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:52:38] {3072} INFO -  at 41.4s,	estimator xgboost's best error=0.0871,	best estimator xgboost's best error=0.0871
[flaml.automl: 09-19 03:52:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:52:44] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.0871,	best estimator xgboost's best error=0.0871
[flaml.automl: 09-19 03:52:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:52:46] {3072} INFO -  at 49.7s,	estimator xgboost's best error=0.0862,	best estimator xgboost's best error=0.0862
[flaml.automl: 09-19 03:52:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 03:52:48] {3072} INFO -  at 51.3s,	estimator xgboost's best error=0.0862,	best estimator xgboost's best error=0.0862
[flaml.automl: 09-19 03:52:48] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 03:52:51] {3072} INFO -  at 55.1s,	estimator xgboost's best error=0.0862,	best estimator xgboost's best error=0.0862
[flaml.automl: 09-19 03:52:51] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 03:52:54] {3072} INFO -  at 57.3s,	estimator xgboost's best error=0.0862,	best estimator xgboost's best error=0.0862
[flaml.automl: 09-19 03:52:54] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-19 03:52:55] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.0862,	best estimator xgboost's best error=0.0862
[flaml.automl: 09-19 03:52:58] {3335} INFO - retrain xgboost for 2.5s
[flaml.automl: 09-19 03:52:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:52:58] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:52:58] {2637} INFO - Time taken to find the best model: 49.70239472389221
[flaml.automl: 09-19 03:52:58] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 8, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}
CO(0)最佳损失：0.9137612169719849
CO(0)最好结果：{'pred_time': 3.3337635682620545e-05, 'wall_clock_time': 49.70239472389221, 'metric_for_logging': {'pred_time': 3.3337635682620545e-05}, 'val_loss': 0.08623878302801515, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}, 'config/n_estimators': 8, 'config/max_leaves': 5, 'config/min_child_weight': 0.758814211387459, 'config/learning_rate': 0.29313806474778853, 'config/subsample': 0.7520389032316431, 'config/colsample_bylevel': 0.8985205690300798, 'config/colsample_bytree': 0.7992659131995336, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.2266745236797676, 'experiment_tag': 'exp', 'time_total_s': 2.533813714981079}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.23480070291208555
CO(0)的mse=0.02034093787466318
CO(0)的mae=0.093269315682294
CO(0)的mar=1.7014076328851713
总共花费的时间为：61.95
阿勒泰地区
阿勒泰地区没有数据
石河子市
2709A
2710A
3442A
[flaml.automl: 09-19 04:02:56] {2390} INFO - task = regression
[flaml.automl: 09-19 04:02:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 04:02:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 04:02:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 04:02:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 04:02:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 04:02:57] {3025} INFO - Estimated sufficient time budget=11865s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 04:02:57] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.2344,	best estimator xgboost's best error=0.2344
[flaml.automl: 09-19 04:02:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 04:02:59] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.1327,	best estimator xgboost's best error=0.1327
[flaml.automl: 09-19 04:02:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 04:03:00] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.1327,	best estimator xgboost's best error=0.1327
[flaml.automl: 09-19 04:03:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 04:03:10] {3072} INFO -  at 14.5s,	estimator xgboost's best error=0.1327,	best estimator xgboost's best error=0.1327
[flaml.automl: 09-19 04:03:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 04:03:11] {3072} INFO -  at 15.6s,	estimator xgboost's best error=0.0948,	best estimator xgboost's best error=0.0948
[flaml.automl: 09-19 04:03:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:03:13] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-19 04:03:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:03:14] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-19 04:03:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:03:17] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-19 04:03:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:03:18] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-19 04:03:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:03:21] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-19 04:03:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 04:03:22] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-19 04:03:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 04:03:24] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.0822,	best estimator xgboost's best error=0.0822
[flaml.automl: 09-19 04:03:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 04:03:35] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.0730,	best estimator xgboost's best error=0.0730
[flaml.automl: 09-19 04:03:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 04:03:55] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.0711,	best estimator xgboost's best error=0.0711
[flaml.automl: 09-19 04:04:17] {3335} INFO - retrain xgboost for 21.8s
[flaml.automl: 09-19 04:04:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 04:04:17] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:04:17] {2637} INFO - Time taken to find the best model: 59.46467876434326
[flaml.automl: 09-19 04:04:17] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9289126230328313
CO(0)最好结果：{'pred_time': 2.1888378864257262e-05, 'wall_clock_time': 59.46467876434326, 'metric_for_logging': {'pred_time': 2.1888378864257262e-05}, 'val_loss': 0.07108737696716873, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 20.21749711036682}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9405562822147722
CO(0)的mse=0.012661705895158368
CO(0)的mae=0.07023507653154183
CO(0)的mar=0.09500112383197962
总共花费的时间为：81.99
五家渠市
2711A
3441A
[flaml.automl: 09-19 04:10:24] {2390} INFO - task = regression
[flaml.automl: 09-19 04:10:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 04:10:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 04:10:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 04:10:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 04:10:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 04:10:25] {3025} INFO - Estimated sufficient time budget=11702s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 04:10:25] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.3446,	best estimator xgboost's best error=0.3446
[flaml.automl: 09-19 04:10:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 04:10:27] {3072} INFO -  at 3.3s,	estimator xgboost's best error=0.1882,	best estimator xgboost's best error=0.1882
[flaml.automl: 09-19 04:10:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 04:10:28] {3072} INFO -  at 4.5s,	estimator xgboost's best error=0.1882,	best estimator xgboost's best error=0.1882
[flaml.automl: 09-19 04:10:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 04:10:37] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.1882,	best estimator xgboost's best error=0.1882
[flaml.automl: 09-19 04:10:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 04:10:38] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.1268,	best estimator xgboost's best error=0.1268
[flaml.automl: 09-19 04:10:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:10:40] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.1040,	best estimator xgboost's best error=0.1040
[flaml.automl: 09-19 04:10:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:10:41] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.1040,	best estimator xgboost's best error=0.1040
[flaml.automl: 09-19 04:10:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:10:44] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.1040,	best estimator xgboost's best error=0.1040
[flaml.automl: 09-19 04:10:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:10:45] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.1040,	best estimator xgboost's best error=0.1040
[flaml.automl: 09-19 04:10:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:10:47] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.1040,	best estimator xgboost's best error=0.1040
[flaml.automl: 09-19 04:10:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 04:10:48] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.1040,	best estimator xgboost's best error=0.1040
[flaml.automl: 09-19 04:10:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 04:10:50] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.1040,	best estimator xgboost's best error=0.1040
[flaml.automl: 09-19 04:10:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 04:10:55] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.0953,	best estimator xgboost's best error=0.0953
[flaml.automl: 09-19 04:10:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 04:11:06] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.0896,	best estimator xgboost's best error=0.0896
[flaml.automl: 09-19 04:11:06] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 04:11:11] {3072} INFO -  at 47.9s,	estimator xgboost's best error=0.0896,	best estimator xgboost's best error=0.0896
[flaml.automl: 09-19 04:11:11] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 04:11:23] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.0884,	best estimator xgboost's best error=0.0884
[flaml.automl: 09-19 04:11:40] {3335} INFO - retrain xgboost for 16.9s
[flaml.automl: 09-19 04:11:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 04:11:40] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:11:40] {2637} INFO - Time taken to find the best model: 59.30399990081787
[flaml.automl: 09-19 04:11:40] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.9115606659476253
CO(0)最好结果：{'pred_time': 1.666133296668232e-05, 'wall_clock_time': 59.30399990081787, 'metric_for_logging': {'pred_time': 1.666133296668232e-05}, 'val_loss': 0.08843933405237468, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 11.372545003890991}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9545204729419725
CO(0)的mse=0.020450268599480622
CO(0)的mae=0.08345798888377419
CO(0)的mar=0.19026098765315666
总共花费的时间为：76.55
三沙市
三沙市没有数据
兰州新区
3245A
3246A
[flaml.automl: 09-19 04:16:52] {2390} INFO - task = regression
[flaml.automl: 09-19 04:16:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 04:16:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 04:16:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 04:16:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 04:16:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 04:16:54] {3025} INFO - Estimated sufficient time budget=11729s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 04:16:54] {3072} INFO -  at 1.3s,	estimator xgboost's best error=0.1192,	best estimator xgboost's best error=0.1192
[flaml.automl: 09-19 04:16:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 04:16:56] {3072} INFO -  at 3.3s,	estimator xgboost's best error=0.0876,	best estimator xgboost's best error=0.0876
[flaml.automl: 09-19 04:16:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 04:16:57] {3072} INFO -  at 4.5s,	estimator xgboost's best error=0.0876,	best estimator xgboost's best error=0.0876
[flaml.automl: 09-19 04:16:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 04:17:06] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.0876,	best estimator xgboost's best error=0.0876
[flaml.automl: 09-19 04:17:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 04:17:07] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.0778,	best estimator xgboost's best error=0.0778
[flaml.automl: 09-19 04:17:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:17:09] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.0712,	best estimator xgboost's best error=0.0712
[flaml.automl: 09-19 04:17:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:17:10] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0712,	best estimator xgboost's best error=0.0712
[flaml.automl: 09-19 04:17:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:17:13] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.0712,	best estimator xgboost's best error=0.0712
[flaml.automl: 09-19 04:17:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:17:14] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.0712,	best estimator xgboost's best error=0.0712
[flaml.automl: 09-19 04:17:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:17:16] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.0712,	best estimator xgboost's best error=0.0712
[flaml.automl: 09-19 04:17:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 04:17:17] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.0712,	best estimator xgboost's best error=0.0712
[flaml.automl: 09-19 04:17:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 04:17:18] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.0712,	best estimator xgboost's best error=0.0712
[flaml.automl: 09-19 04:17:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 04:17:24] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.0675,	best estimator xgboost's best error=0.0675
[flaml.automl: 09-19 04:17:24] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 04:17:34] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.0662,	best estimator xgboost's best error=0.0662
[flaml.automl: 09-19 04:17:34] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 04:17:40] {3072} INFO -  at 47.9s,	estimator xgboost's best error=0.0662,	best estimator xgboost's best error=0.0662
[flaml.automl: 09-19 04:17:40] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 04:17:51] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.0655,	best estimator xgboost's best error=0.0655
[flaml.automl: 09-19 04:18:08] {3335} INFO - retrain xgboost for 16.8s
[flaml.automl: 09-19 04:18:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 04:18:08] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:18:08] {2637} INFO - Time taken to find the best model: 59.17052364349365
[flaml.automl: 09-19 04:18:08] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}
CO(0)最佳损失：0.9344858235303058
CO(0)最好结果：{'pred_time': 1.670538155564115e-05, 'wall_clock_time': 59.17052364349365, 'metric_for_logging': {'pred_time': 1.670538155564115e-05}, 'val_loss': 0.06551417646969418, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 17, 'min_child_weight': 0.6440508854244227, 'learning_rate': 0.5310029457853219, 'subsample': 0.8431663364020375, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.6207511589453646, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.6841568087527966}, 'config/n_estimators': 18, 'config/max_leaves': 17, 'config/min_child_weight': 0.6440508854244227, 'config/learning_rate': 0.5310029457853219, 'config/subsample': 0.8431663364020375, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.6207511589453646, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.6841568087527966, 'experiment_tag': 'exp', 'time_total_s': 11.26102590560913}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.6207511589453646, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5310029457853219,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.6440508854244227, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.6841568087527966, scale_pos_weight=1,
             subsample=0.8431663364020375, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.6977486511743903
CO(0)的mse=0.01026075615917361
CO(0)的mae=0.06609609230204315
CO(0)的mar=0.1444416944689166
总共花费的时间为：76.34
赣江新区
3414A
[flaml.automl: 09-19 04:20:59] {2390} INFO - task = regression
[flaml.automl: 09-19 04:20:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 04:20:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 04:20:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 04:20:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 04:20:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 04:21:00] {3025} INFO - Estimated sufficient time budget=11618s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 04:21:00] {3072} INFO -  at 1.2s,	estimator xgboost's best error=0.1352,	best estimator xgboost's best error=0.1352
[flaml.automl: 09-19 04:21:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 04:21:02] {3072} INFO -  at 3.0s,	estimator xgboost's best error=0.0869,	best estimator xgboost's best error=0.0869
[flaml.automl: 09-19 04:21:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 04:21:03] {3072} INFO -  at 4.2s,	estimator xgboost's best error=0.0869,	best estimator xgboost's best error=0.0869
[flaml.automl: 09-19 04:21:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 04:21:10] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.0869,	best estimator xgboost's best error=0.0869
[flaml.automl: 09-19 04:21:10] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 04:21:11] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.0605,	best estimator xgboost's best error=0.0605
[flaml.automl: 09-19 04:21:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:21:13] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.0513,	best estimator xgboost's best error=0.0513
[flaml.automl: 09-19 04:21:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:21:15] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.0507,	best estimator xgboost's best error=0.0507
[flaml.automl: 09-19 04:21:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:21:17] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.0507,	best estimator xgboost's best error=0.0507
[flaml.automl: 09-19 04:21:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:21:18] {3072} INFO -  at 19.2s,	estimator xgboost's best error=0.0495,	best estimator xgboost's best error=0.0495
[flaml.automl: 09-19 04:21:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:21:21] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0495,	best estimator xgboost's best error=0.0495
[flaml.automl: 09-19 04:21:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 04:21:23] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.0495,	best estimator xgboost's best error=0.0495
[flaml.automl: 09-19 04:21:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 04:21:24] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.0495,	best estimator xgboost's best error=0.0495
[flaml.automl: 09-19 04:21:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 04:21:29] {3072} INFO -  at 30.0s,	estimator xgboost's best error=0.0397,	best estimator xgboost's best error=0.0397
[flaml.automl: 09-19 04:21:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 04:21:38] {3072} INFO -  at 38.7s,	estimator xgboost's best error=0.0388,	best estimator xgboost's best error=0.0388
[flaml.automl: 09-19 04:21:38] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 04:21:43] {3072} INFO -  at 43.6s,	estimator xgboost's best error=0.0388,	best estimator xgboost's best error=0.0388
[flaml.automl: 09-19 04:21:43] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 04:21:57] {3072} INFO -  at 57.6s,	estimator xgboost's best error=0.0388,	best estimator xgboost's best error=0.0388
[flaml.automl: 09-19 04:22:05] {3335} INFO - retrain xgboost for 8.6s
[flaml.automl: 09-19 04:22:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 04:22:05] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:22:05] {2637} INFO - Time taken to find the best model: 38.68951964378357
CO(0)最佳参数：{'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}
CO(0)最佳损失：0.9611608972781289
CO(0)最好结果：{'pred_time': 3.2000828266939855e-05, 'wall_clock_time': 38.68951964378357, 'metric_for_logging': {'pred_time': 3.2000828266939855e-05}, 'val_loss': 0.038839102721871036, 'training_iteration': 1, 'config': {'n_estimators': 25, 'max_leaves': 6, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}, 'config/n_estimators': 25, 'config/max_leaves': 6, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'experiment_tag': 'exp', 'time_total_s': 8.681793451309204}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=25, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
CO(0)的R2=0.9050868553344575
CO(0)的mse=0.0038481023842454345
CO(0)的mae=0.04215695366281289
CO(0)的mar=0.07062365317583681
总共花费的时间为：66.38
儋州市
3541A
3542A
[flaml.automl: 09-19 04:27:06] {2390} INFO - task = regression
[flaml.automl: 09-19 04:27:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 04:27:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 04:27:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 04:27:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 04:27:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 04:27:08] {3025} INFO - Estimated sufficient time budget=17960s. Estimated necessary time budget=18s.
[flaml.automl: 09-19 04:27:08] {3072} INFO -  at 1.9s,	estimator xgboost's best error=0.0686,	best estimator xgboost's best error=0.0686
[flaml.automl: 09-19 04:27:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 04:27:11] {3072} INFO -  at 5.0s,	estimator xgboost's best error=0.0419,	best estimator xgboost's best error=0.0419
[flaml.automl: 09-19 04:27:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 04:27:13] {3072} INFO -  at 6.7s,	estimator xgboost's best error=0.0419,	best estimator xgboost's best error=0.0419
[flaml.automl: 09-19 04:27:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 04:27:25] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.0419,	best estimator xgboost's best error=0.0419
[flaml.automl: 09-19 04:27:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 04:27:26] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.0337,	best estimator xgboost's best error=0.0337
[flaml.automl: 09-19 04:27:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:27:28] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.0279,	best estimator xgboost's best error=0.0279
[flaml.automl: 09-19 04:27:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:27:29] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.0279,	best estimator xgboost's best error=0.0279
[flaml.automl: 09-19 04:27:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:27:32] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.0279,	best estimator xgboost's best error=0.0279
[flaml.automl: 09-19 04:27:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:27:33] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.0279,	best estimator xgboost's best error=0.0279
[flaml.automl: 09-19 04:27:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:27:35] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.0279,	best estimator xgboost's best error=0.0279
[flaml.automl: 09-19 04:27:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 04:27:36] {3072} INFO -  at 30.1s,	estimator xgboost's best error=0.0279,	best estimator xgboost's best error=0.0279
[flaml.automl: 09-19 04:27:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 04:27:37] {3072} INFO -  at 31.3s,	estimator xgboost's best error=0.0279,	best estimator xgboost's best error=0.0279
[flaml.automl: 09-19 04:27:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 04:27:43] {3072} INFO -  at 37.1s,	estimator xgboost's best error=0.0247,	best estimator xgboost's best error=0.0247
[flaml.automl: 09-19 04:27:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 04:27:54] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.0241,	best estimator xgboost's best error=0.0241
[flaml.automl: 09-19 04:27:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 04:27:59] {3072} INFO -  at 53.2s,	estimator xgboost's best error=0.0241,	best estimator xgboost's best error=0.0241
[flaml.automl: 09-19 04:28:10] {3335} INFO - retrain xgboost for 10.1s
[flaml.automl: 09-19 04:28:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 04:28:10] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:28:10] {2637} INFO - Time taken to find the best model: 47.298540353775024
[flaml.automl: 09-19 04:28:10] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
CO(0)最佳损失：0.9758964536516015
CO(0)最好结果：{'pred_time': 1.6106744749384563e-05, 'wall_clock_time': 47.298540353775024, 'metric_for_logging': {'pred_time': 1.6106744749384563e-05}, 'val_loss': 0.024103546348398456, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.152217626571655}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.913910737707777
CO(0)的mse=0.001220048296157131
CO(0)的mae=0.023491341817428648
CO(0)的mar=0.04472921604939212
总共花费的时间为：63.70
雄安新区
3584A
3585A
3586A
3587A
3588A
3589A
[flaml.automl: 09-19 04:43:32] {2390} INFO - task = regression
[flaml.automl: 09-19 04:43:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 04:43:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 04:43:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 04:43:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 04:43:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 04:43:32] {3025} INFO - Estimated sufficient time budget=22506s. Estimated necessary time budget=23s.
[flaml.automl: 09-19 04:43:32] {3072} INFO -  at 0.6s,	estimator xgboost's best error=0.1633,	best estimator xgboost's best error=0.1633
[flaml.automl: 09-19 04:43:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 04:43:34] {3072} INFO -  at 2.6s,	estimator xgboost's best error=0.0992,	best estimator xgboost's best error=0.0992
[flaml.automl: 09-19 04:43:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 04:43:35] {3072} INFO -  at 3.8s,	estimator xgboost's best error=0.0992,	best estimator xgboost's best error=0.0992
[flaml.automl: 09-19 04:43:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 04:43:40] {3072} INFO -  at 8.3s,	estimator xgboost's best error=0.0992,	best estimator xgboost's best error=0.0992
[flaml.automl: 09-19 04:43:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 04:43:41] {3072} INFO -  at 9.3s,	estimator xgboost's best error=0.0793,	best estimator xgboost's best error=0.0793
[flaml.automl: 09-19 04:43:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:43:42] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.0659,	best estimator xgboost's best error=0.0659
[flaml.automl: 09-19 04:43:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:43:44] {3072} INFO -  at 12.4s,	estimator xgboost's best error=0.0659,	best estimator xgboost's best error=0.0659
[flaml.automl: 09-19 04:43:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:43:46] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.0659,	best estimator xgboost's best error=0.0659
[flaml.automl: 09-19 04:43:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:43:47] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.0659,	best estimator xgboost's best error=0.0659
[flaml.automl: 09-19 04:43:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:43:50] {3072} INFO -  at 18.3s,	estimator xgboost's best error=0.0659,	best estimator xgboost's best error=0.0659
[flaml.automl: 09-19 04:43:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 04:43:51] {3072} INFO -  at 19.8s,	estimator xgboost's best error=0.0656,	best estimator xgboost's best error=0.0656
[flaml.automl: 09-19 04:43:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 04:43:52] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.0656,	best estimator xgboost's best error=0.0656
[flaml.automl: 09-19 04:43:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 04:43:57] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.0588,	best estimator xgboost's best error=0.0588
[flaml.automl: 09-19 04:43:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 04:44:09] {3072} INFO -  at 37.2s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-19 04:44:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 04:44:15] {3072} INFO -  at 43.3s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-19 04:44:15] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 04:44:31] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.0567,	best estimator xgboost's best error=0.0567
[flaml.automl: 09-19 04:44:42] {3335} INFO - retrain xgboost for 11.5s
[flaml.automl: 09-19 04:44:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 04:44:42] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:44:42] {2637} INFO - Time taken to find the best model: 37.15911841392517
CO(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 60886}
CO(0)最佳损失：0.9433244604524396
CO(0)最好结果：{'pred_time': 5.777792010193142e-06, 'wall_clock_time': 37.15911841392517, 'metric_for_logging': {'pred_time': 5.777792010193142e-06}, 'val_loss': 0.056675539547560425, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 60886}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 60886, 'experiment_tag': 'exp', 'time_total_s': 11.394206047058105}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.9087391902950339
CO(0)的mse=0.00796965930994991
CO(0)的mae=0.05726918962614895
CO(0)的mar=0.12201447964593963
总共花费的时间为：71.90
西咸新区
3606A
3607A
3608A
[flaml.automl: 09-19 04:52:15] {2390} INFO - task = regression
[flaml.automl: 09-19 04:52:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 04:52:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 04:52:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 04:52:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 04:52:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 04:52:16] {3025} INFO - Estimated sufficient time budget=15537s. Estimated necessary time budget=16s.
[flaml.automl: 09-19 04:52:16] {3072} INFO -  at 1.8s,	estimator xgboost's best error=0.1553,	best estimator xgboost's best error=0.1553
[flaml.automl: 09-19 04:52:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 04:52:19] {3072} INFO -  at 4.1s,	estimator xgboost's best error=0.0890,	best estimator xgboost's best error=0.0890
[flaml.automl: 09-19 04:52:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 04:52:20] {3072} INFO -  at 5.7s,	estimator xgboost's best error=0.0890,	best estimator xgboost's best error=0.0890
[flaml.automl: 09-19 04:52:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 04:52:32] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.0890,	best estimator xgboost's best error=0.0890
[flaml.automl: 09-19 04:52:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 04:52:34] {3072} INFO -  at 19.5s,	estimator xgboost's best error=0.0678,	best estimator xgboost's best error=0.0678
[flaml.automl: 09-19 04:52:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:52:36] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-19 04:52:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:52:38] {3072} INFO -  at 23.5s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-19 04:52:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:52:41] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-19 04:52:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:52:42] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-19 04:52:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:52:46] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-19 04:52:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 04:52:47] {3072} INFO -  at 33.1s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-19 04:52:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 04:52:49] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.0537,	best estimator xgboost's best error=0.0537
[flaml.automl: 09-19 04:52:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 04:52:55] {3072} INFO -  at 40.9s,	estimator xgboost's best error=0.0466,	best estimator xgboost's best error=0.0466
[flaml.automl: 09-19 04:52:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 04:52:55] {3072} INFO -  at 41.0s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-19 04:52:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 04:52:55] {3072} INFO -  at 41.1s,	estimator xgboost's best error=0.0452,	best estimator xgboost's best error=0.0452
[flaml.automl: 09-19 04:52:55] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 04:52:57] {3072} INFO -  at 42.2s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:52:57] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 04:52:58] {3072} INFO -  at 43.2s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:52:58] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-19 04:52:58] {3072} INFO -  at 43.3s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:52:58] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-19 04:52:58] {3072} INFO -  at 44.0s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:52:58] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-19 04:52:58] {3072} INFO -  at 44.0s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:52:58] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-19 04:52:59] {3072} INFO -  at 44.5s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:52:59] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-19 04:52:59] {3072} INFO -  at 45.0s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:52:59] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-19 04:53:00] {3072} INFO -  at 45.2s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:53:00] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-19 04:53:00] {3072} INFO -  at 45.6s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:53:00] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-19 04:53:00] {3072} INFO -  at 45.7s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:53:00] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-19 04:53:00] {3072} INFO -  at 45.8s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:53:00] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-19 04:53:01] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:53:01] {2897} INFO - iteration 27, current learner xgboost
[flaml.automl: 09-19 04:53:01] {3072} INFO -  at 47.0s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:53:01] {2897} INFO - iteration 28, current learner xgboost
[flaml.automl: 09-19 04:53:01] {3072} INFO -  at 47.1s,	estimator xgboost's best error=0.0446,	best estimator xgboost's best error=0.0446
[flaml.automl: 09-19 04:53:01] {2897} INFO - iteration 29, current learner xgboost
[flaml.automl: 09-19 04:53:14] {3072} INFO -  at 60.0s,	estimator xgboost's best error=0.0438,	best estimator xgboost's best error=0.0438
[flaml.automl: 09-19 04:54:14] {3335} INFO - retrain xgboost for 59.6s
[flaml.automl: 09-19 04:54:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5867633682398518, colsample_bynode=1,
             colsample_bytree=0.6396133838733672, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.20099043767384006,
             max_delta_step=0, max_depth=0, max_leaves=26,
             min_child_weight=0.616832156956789, missing=nan,
             monotone_constraints='()', n_estimators=44, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9063536627122738, scale_pos_weight=1,
             subsample=0.8168593363754089, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 04:54:14] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:54:14] {2637} INFO - Time taken to find the best model: 60.00173044204712
[flaml.automl: 09-19 04:54:14] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
CO(0)最佳参数：{'n_estimators': 105, 'max_leaves': 26, 'min_child_weight': 0.616832156956789, 'learning_rate': 0.20099043767384006, 'subsample': 0.8168593363754089, 'colsample_bylevel': 0.5867633682398518, 'colsample_bytree': 0.6396133838733672, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9063536627122738}
CO(0)最佳损失：0.9562051127082034
CO(0)最好结果：{'pred_time': 1.1117170347538825e-05, 'wall_clock_time': 60.00173044204712, 'metric_for_logging': {'pred_time': 1.1117170347538825e-05}, 'val_loss': 0.04379488729179661, 'training_iteration': 1, 'config': {'n_estimators': 105, 'max_leaves': 26, 'min_child_weight': 0.616832156956789, 'learning_rate': 0.20099043767384006, 'subsample': 0.8168593363754089, 'colsample_bylevel': 0.5867633682398518, 'colsample_bytree': 0.6396133838733672, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.9063536627122738}, 'config/n_estimators': 105, 'config/max_leaves': 26, 'config/min_child_weight': 0.616832156956789, 'config/learning_rate': 0.20099043767384006, 'config/subsample': 0.8168593363754089, 'config/colsample_bylevel': 0.5867633682398518, 'config/colsample_bytree': 0.6396133838733672, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.9063536627122738, 'experiment_tag': 'exp', 'time_total_s': 12.895966053009033}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5867633682398518, colsample_bynode=1,
             colsample_bytree=0.6396133838733672, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.20099043767384006,
             max_delta_step=0, max_depth=0, max_leaves=26,
             min_child_weight=0.616832156956789, missing=nan,
             monotone_constraints='()', n_estimators=44, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.9063536627122738, scale_pos_weight=1,
             subsample=0.8168593363754089, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
CO(0)的R2=0.947288990166847
CO(0)的mse=0.0039642800996764205
CO(0)的mae=0.04276453484859698
CO(0)的mar=0.09171314176750449
总共花费的时间为：120.20
