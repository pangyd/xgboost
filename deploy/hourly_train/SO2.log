nohup: ignoring input
北京市
1005A
1008A
1009A
1010A
1012A
3417A
3418A
3671A
3672A
3673A
3674A
3675A
3695A
3696A
3697A
[flaml.automl: 09-16 04:28:50] {2390} INFO - task = regression
[flaml.automl: 09-16 04:28:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 04:28:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 04:28:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 04:28:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 04:28:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 04:28:52] {3025} INFO - Estimated sufficient time budget=266289s. Estimated necessary time budget=266s.
[flaml.automl: 09-16 04:28:52] {3072} INFO -  at 2.3s,	estimator xgboost's best error=1.9655,	best estimator xgboost's best error=1.9655
[flaml.automl: 09-16 04:28:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 04:28:53] {3072} INFO -  at 4.0s,	estimator xgboost's best error=0.9981,	best estimator xgboost's best error=0.9981
[flaml.automl: 09-16 04:28:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 04:28:54] {3072} INFO -  at 5.2s,	estimator xgboost's best error=0.9981,	best estimator xgboost's best error=0.9981
[flaml.automl: 09-16 04:28:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 04:28:56] {3072} INFO -  at 6.7s,	estimator xgboost's best error=0.9981,	best estimator xgboost's best error=0.9981
[flaml.automl: 09-16 04:28:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 04:28:57] {3072} INFO -  at 7.9s,	estimator xgboost's best error=0.6376,	best estimator xgboost's best error=0.6376
[flaml.automl: 09-16 04:28:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 04:28:59] {3072} INFO -  at 9.4s,	estimator xgboost's best error=0.6220,	best estimator xgboost's best error=0.6220
[flaml.automl: 09-16 04:28:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 04:29:00] {3072} INFO -  at 10.8s,	estimator xgboost's best error=0.5627,	best estimator xgboost's best error=0.5627
[flaml.automl: 09-16 04:29:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 04:29:01] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.5627,	best estimator xgboost's best error=0.5627
[flaml.automl: 09-16 04:29:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 04:29:02] {3072} INFO -  at 13.1s,	estimator xgboost's best error=0.5323,	best estimator xgboost's best error=0.5323
[flaml.automl: 09-16 04:29:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 04:29:03] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.5323,	best estimator xgboost's best error=0.5323
[flaml.automl: 09-16 04:29:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 04:29:05] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.4743,	best estimator xgboost's best error=0.4743
[flaml.automl: 09-16 04:29:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 04:29:06] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.4743,	best estimator xgboost's best error=0.4743
[flaml.automl: 09-16 04:29:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 04:29:07] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.4568,	best estimator xgboost's best error=0.4568
[flaml.automl: 09-16 04:29:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 04:29:08] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.4568,	best estimator xgboost's best error=0.4568
[flaml.automl: 09-16 04:29:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 04:29:09] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.4568,	best estimator xgboost's best error=0.4568
[flaml.automl: 09-16 04:29:09] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 04:29:10] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.4568,	best estimator xgboost's best error=0.4568
[flaml.automl: 09-16 04:29:10] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 04:29:11] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:11] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 04:29:12] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:12] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 04:29:13] {3072} INFO -  at 23.6s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:13] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 04:29:14] {3072} INFO -  at 24.5s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:14] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 04:29:15] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:15] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 04:29:18] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:18] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 04:29:21] {3072} INFO -  at 32.1s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:21] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-16 04:29:23] {3072} INFO -  at 33.9s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:23] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-16 04:29:26] {3072} INFO -  at 36.9s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:26] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-16 04:29:29] {3072} INFO -  at 39.4s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:29] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-16 04:29:31] {3072} INFO -  at 41.5s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:31] {2897} INFO - iteration 27, current learner xgboost
[flaml.automl: 09-16 04:29:32] {3072} INFO -  at 42.9s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:32] {2897} INFO - iteration 28, current learner xgboost
[flaml.automl: 09-16 04:29:34] {3072} INFO -  at 44.5s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:34] {2897} INFO - iteration 29, current learner xgboost
[flaml.automl: 09-16 04:29:35] {3072} INFO -  at 45.8s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:35] {2897} INFO - iteration 30, current learner xgboost
[flaml.automl: 09-16 04:29:36] {3072} INFO -  at 46.9s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:36] {2897} INFO - iteration 31, current learner xgboost
[flaml.automl: 09-16 04:29:37] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:37] {2897} INFO - iteration 32, current learner xgboost
[flaml.automl: 09-16 04:29:38] {3072} INFO -  at 49.0s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:38] {2897} INFO - iteration 33, current learner xgboost
[flaml.automl: 09-16 04:29:39] {3072} INFO -  at 50.1s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:39] {2897} INFO - iteration 34, current learner xgboost
[flaml.automl: 09-16 04:29:46] {3072} INFO -  at 56.6s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:29:46] {2897} INFO - iteration 35, current learner xgboost
[flaml.automl: 09-16 04:29:49] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.4564,	best estimator xgboost's best error=0.4564
[flaml.automl: 09-16 04:30:01] {3335} INFO - retrain xgboost for 12.3s
[flaml.automl: 09-16 04:30:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7514744407541849, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=5, min_child_weight=1.2956226448388868,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.006319582753542223, reg_lambda=3.519639482084842,
             scale_pos_weight=1, subsample=0.9468514399123292,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 04:30:01] {2636} INFO - fit succeeded
[flaml.automl: 09-16 04:30:01] {2637} INFO - Time taken to find the best model: 21.86548686027527
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 5, 'min_child_weight': 1.2956226448388868, 'learning_rate': 1.0, 'subsample': 0.9468514399123292, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7514744407541849, 'reg_alpha': 0.006319582753542223, 'reg_lambda': 3.519639482084842, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：0.5436079769799211
SO2(0)最好结果：{'pred_time': 2.397609412621452e-06, 'wall_clock_time': 21.86548686027527, 'metric_for_logging': {'pred_time': 2.397609412621452e-06}, 'val_loss': 0.45639202302007886, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 5, 'min_child_weight': 1.2956226448388868, 'learning_rate': 1.0, 'subsample': 0.9468514399123292, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7514744407541849, 'reg_alpha': 0.006319582753542223, 'reg_lambda': 3.519639482084842, 'FLAML_sample_size': 10000}, 'config/n_estimators': 11, 'config/max_leaves': 5, 'config/min_child_weight': 1.2956226448388868, 'config/learning_rate': 1.0, 'config/subsample': 0.9468514399123292, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7514744407541849, 'config/reg_alpha': 0.006319582753542223, 'config/reg_lambda': 3.519639482084842, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 1.0618505477905273}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7514744407541849, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=5, min_child_weight=1.2956226448388868,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.006319582753542223, reg_lambda=3.519639482084842,
             scale_pos_weight=1, subsample=0.9468514399123292,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.48663667551193945
SO2(0)的mse=0.5137867446109003
SO2(0)的mae=0.45846671259482097
SO2(0)的mar=0.2521859634100809
总共花费的时间为：75.14
天津市
1015A
1019A
2859A
2860A
2922A
3325A
3326A
3327A
3460A
3461A
3462A
[flaml.automl: 09-16 05:01:43] {2390} INFO - task = regression
[flaml.automl: 09-16 05:01:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:01:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:01:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:01:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:01:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:01:44] {3025} INFO - Estimated sufficient time budget=146603s. Estimated necessary time budget=147s.
[flaml.automl: 09-16 05:01:44] {3072} INFO -  at 1.7s,	estimator xgboost's best error=5.0226,	best estimator xgboost's best error=5.0226
[flaml.automl: 09-16 05:01:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:01:47] {3072} INFO -  at 3.8s,	estimator xgboost's best error=2.3384,	best estimator xgboost's best error=2.3384
[flaml.automl: 09-16 05:01:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:01:48] {3072} INFO -  at 5.1s,	estimator xgboost's best error=2.3384,	best estimator xgboost's best error=2.3384
[flaml.automl: 09-16 05:01:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:01:50] {3072} INFO -  at 6.8s,	estimator xgboost's best error=2.3384,	best estimator xgboost's best error=2.3384
[flaml.automl: 09-16 05:01:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:01:51] {3072} INFO -  at 7.9s,	estimator xgboost's best error=1.4681,	best estimator xgboost's best error=1.4681
[flaml.automl: 09-16 05:01:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:01:52] {3072} INFO -  at 9.5s,	estimator xgboost's best error=1.2589,	best estimator xgboost's best error=1.2589
[flaml.automl: 09-16 05:01:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:01:54] {3072} INFO -  at 11.1s,	estimator xgboost's best error=1.1922,	best estimator xgboost's best error=1.1922
[flaml.automl: 09-16 05:01:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:01:56] {3072} INFO -  at 13.0s,	estimator xgboost's best error=1.1922,	best estimator xgboost's best error=1.1922
[flaml.automl: 09-16 05:01:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:01:57] {3072} INFO -  at 14.6s,	estimator xgboost's best error=1.1922,	best estimator xgboost's best error=1.1922
[flaml.automl: 09-16 05:01:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:01:59] {3072} INFO -  at 16.0s,	estimator xgboost's best error=1.1922,	best estimator xgboost's best error=1.1922
[flaml.automl: 09-16 05:01:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:02:00] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.1922,	best estimator xgboost's best error=1.1922
[flaml.automl: 09-16 05:02:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:02:02] {3072} INFO -  at 19.2s,	estimator xgboost's best error=1.1682,	best estimator xgboost's best error=1.1682
[flaml.automl: 09-16 05:02:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:02:03] {3072} INFO -  at 20.4s,	estimator xgboost's best error=1.1682,	best estimator xgboost's best error=1.1682
[flaml.automl: 09-16 05:02:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:02:10] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.9456,	best estimator xgboost's best error=0.9456
[flaml.automl: 09-16 05:02:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 05:02:23] {3072} INFO -  at 40.3s,	estimator xgboost's best error=0.9366,	best estimator xgboost's best error=0.9366
[flaml.automl: 09-16 05:02:23] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 05:02:30] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.9366,	best estimator xgboost's best error=0.9366
[flaml.automl: 09-16 05:02:43] {3335} INFO - retrain xgboost for 12.9s
[flaml.automl: 09-16 05:02:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 05:02:43] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:02:43] {2637} INFO - Time taken to find the best model: 40.316139459609985
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 122355}
SO2(0)最佳损失：0.06337387219321278
SO2(0)最好结果：{'pred_time': 3.2591062210769854e-06, 'wall_clock_time': 40.316139459609985, 'metric_for_logging': {'pred_time': 3.2591062210769854e-06}, 'val_loss': 0.9366261278067872, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 122355}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 122355, 'experiment_tag': 'exp', 'time_total_s': 12.939880609512329}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8365448904729389
SO2(0)的mse=2.364711154084259
SO2(0)的mae=0.9257475050035793
SO2(0)的mar=0.12032258713470224
总共花费的时间为：61.80
石家庄市
1030A
1032A
1033A
1035A
2862A
3456A
3572A
3573A
3574A
[flaml.automl: 09-16 05:29:19] {2390} INFO - task = regression
[flaml.automl: 09-16 05:29:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:29:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:29:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:29:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:29:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:29:20] {3025} INFO - Estimated sufficient time budget=116655s. Estimated necessary time budget=117s.
[flaml.automl: 09-16 05:29:20] {3072} INFO -  at 1.6s,	estimator xgboost's best error=5.0493,	best estimator xgboost's best error=5.0493
[flaml.automl: 09-16 05:29:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:29:22] {3072} INFO -  at 3.7s,	estimator xgboost's best error=2.4424,	best estimator xgboost's best error=2.4424
[flaml.automl: 09-16 05:29:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:29:24] {3072} INFO -  at 4.9s,	estimator xgboost's best error=2.4424,	best estimator xgboost's best error=2.4424
[flaml.automl: 09-16 05:29:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:29:26] {3072} INFO -  at 7.6s,	estimator xgboost's best error=2.4424,	best estimator xgboost's best error=2.4424
[flaml.automl: 09-16 05:29:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:29:28] {3072} INFO -  at 8.8s,	estimator xgboost's best error=1.8069,	best estimator xgboost's best error=1.8069
[flaml.automl: 09-16 05:29:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:29:29] {3072} INFO -  at 10.4s,	estimator xgboost's best error=1.8069,	best estimator xgboost's best error=1.8069
[flaml.automl: 09-16 05:29:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:29:31] {3072} INFO -  at 12.1s,	estimator xgboost's best error=1.2795,	best estimator xgboost's best error=1.2795
[flaml.automl: 09-16 05:29:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:29:33] {3072} INFO -  at 14.3s,	estimator xgboost's best error=1.2795,	best estimator xgboost's best error=1.2795
[flaml.automl: 09-16 05:29:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:29:35] {3072} INFO -  at 16.0s,	estimator xgboost's best error=1.2795,	best estimator xgboost's best error=1.2795
[flaml.automl: 09-16 05:29:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:29:36] {3072} INFO -  at 17.7s,	estimator xgboost's best error=1.2795,	best estimator xgboost's best error=1.2795
[flaml.automl: 09-16 05:29:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:29:38] {3072} INFO -  at 19.2s,	estimator xgboost's best error=1.2795,	best estimator xgboost's best error=1.2795
[flaml.automl: 09-16 05:29:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:29:40] {3072} INFO -  at 21.0s,	estimator xgboost's best error=1.2636,	best estimator xgboost's best error=1.2636
[flaml.automl: 09-16 05:29:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:29:41] {3072} INFO -  at 22.1s,	estimator xgboost's best error=1.2636,	best estimator xgboost's best error=1.2636
[flaml.automl: 09-16 05:29:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:29:48] {3072} INFO -  at 29.2s,	estimator xgboost's best error=1.1725,	best estimator xgboost's best error=1.1725
[flaml.automl: 09-16 05:29:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 05:30:01] {3072} INFO -  at 42.1s,	estimator xgboost's best error=1.1473,	best estimator xgboost's best error=1.1473
[flaml.automl: 09-16 05:30:01] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 05:30:08] {3072} INFO -  at 49.2s,	estimator xgboost's best error=1.1473,	best estimator xgboost's best error=1.1473
[flaml.automl: 09-16 05:30:27] {3335} INFO - retrain xgboost for 19.5s
[flaml.automl: 09-16 05:30:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 05:30:27] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:30:27] {2637} INFO - Time taken to find the best model: 42.08398675918579
[flaml.automl: 09-16 05:30:27] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 96510}
SO2(0)最佳损失：-0.14728348187793783
SO2(0)最好结果：{'pred_time': 3.962986329650309e-06, 'wall_clock_time': 42.08398675918579, 'metric_for_logging': {'pred_time': 3.962986329650309e-06}, 'val_loss': 1.1472834818779378, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 96510}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 96510, 'experiment_tag': 'exp', 'time_total_s': 12.866198778152466}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8516615900087501
SO2(0)的mse=3.9915024825450813
SO2(0)的mae=1.1614695804838469
SO2(0)的mar=0.16711375975521142
总共花费的时间为：70.30
唐山市
1041A
3575A
3576A
3577A
3578A
3692A
[flaml.automl: 09-16 05:49:01] {2390} INFO - task = regression
[flaml.automl: 09-16 05:49:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 05:49:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 05:49:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 05:49:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 05:49:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 05:49:02] {3025} INFO - Estimated sufficient time budget=71562s. Estimated necessary time budget=72s.
[flaml.automl: 09-16 05:49:02] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.7860,	best estimator xgboost's best error=4.7860
[flaml.automl: 09-16 05:49:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 05:49:05] {3072} INFO -  at 4.0s,	estimator xgboost's best error=2.5554,	best estimator xgboost's best error=2.5554
[flaml.automl: 09-16 05:49:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 05:49:07] {3072} INFO -  at 6.1s,	estimator xgboost's best error=2.5554,	best estimator xgboost's best error=2.5554
[flaml.automl: 09-16 05:49:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 05:49:11] {3072} INFO -  at 10.1s,	estimator xgboost's best error=2.5554,	best estimator xgboost's best error=2.5554
[flaml.automl: 09-16 05:49:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 05:49:13] {3072} INFO -  at 12.2s,	estimator xgboost's best error=2.1064,	best estimator xgboost's best error=2.1064
[flaml.automl: 09-16 05:49:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 05:49:16] {3072} INFO -  at 15.1s,	estimator xgboost's best error=2.1064,	best estimator xgboost's best error=2.1064
[flaml.automl: 09-16 05:49:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 05:49:19] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.8330,	best estimator xgboost's best error=1.8330
[flaml.automl: 09-16 05:49:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 05:49:22] {3072} INFO -  at 20.9s,	estimator xgboost's best error=1.8330,	best estimator xgboost's best error=1.8330
[flaml.automl: 09-16 05:49:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 05:49:25] {3072} INFO -  at 23.9s,	estimator xgboost's best error=1.8330,	best estimator xgboost's best error=1.8330
[flaml.automl: 09-16 05:49:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 05:49:27] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.8330,	best estimator xgboost's best error=1.8330
[flaml.automl: 09-16 05:49:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 05:49:30] {3072} INFO -  at 28.7s,	estimator xgboost's best error=1.8330,	best estimator xgboost's best error=1.8330
[flaml.automl: 09-16 05:49:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 05:49:33] {3072} INFO -  at 31.8s,	estimator xgboost's best error=1.8266,	best estimator xgboost's best error=1.8266
[flaml.automl: 09-16 05:49:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 05:49:35] {3072} INFO -  at 34.0s,	estimator xgboost's best error=1.8266,	best estimator xgboost's best error=1.8266
[flaml.automl: 09-16 05:49:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 05:49:48] {3072} INFO -  at 47.0s,	estimator xgboost's best error=1.7863,	best estimator xgboost's best error=1.7863
[flaml.automl: 09-16 05:50:01] {3335} INFO - retrain xgboost for 12.9s
[flaml.automl: 09-16 05:50:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 05:50:01] {2636} INFO - fit succeeded
[flaml.automl: 09-16 05:50:01] {2637} INFO - Time taken to find the best model: 47.0212197303772
[flaml.automl: 09-16 05:50:01] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 59640}
SO2(0)最佳损失：-0.7862924567099268
SO2(0)最好结果：{'pred_time': 1.0651917556157132e-05, 'wall_clock_time': 47.0212197303772, 'metric_for_logging': {'pred_time': 1.0651917556157132e-05}, 'val_loss': 1.7862924567099268, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 59640}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 59640, 'experiment_tag': 'exp', 'time_total_s': 12.980720520019531}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7009334833365573
SO2(0)的mse=9.25809141333483
SO2(0)的mae=1.8379568068631136
SO2(0)的mar=0.30230794785544585
总共花费的时间为：60.92
秦皇岛市
1043A
1044A
3132A
[flaml.automl: 09-16 06:00:22] {2390} INFO - task = regression
[flaml.automl: 09-16 06:00:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:00:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:00:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:00:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:00:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:00:25] {3025} INFO - Estimated sufficient time budget=23238s. Estimated necessary time budget=23s.
[flaml.automl: 09-16 06:00:25] {3072} INFO -  at 2.5s,	estimator xgboost's best error=6.1604,	best estimator xgboost's best error=6.1604
[flaml.automl: 09-16 06:00:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:00:27] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.9351,	best estimator xgboost's best error=2.9351
[flaml.automl: 09-16 06:00:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:00:28] {3072} INFO -  at 5.9s,	estimator xgboost's best error=2.9351,	best estimator xgboost's best error=2.9351
[flaml.automl: 09-16 06:00:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:00:39] {3072} INFO -  at 16.3s,	estimator xgboost's best error=2.9351,	best estimator xgboost's best error=2.9351
[flaml.automl: 09-16 06:00:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:00:41] {3072} INFO -  at 18.3s,	estimator xgboost's best error=2.0362,	best estimator xgboost's best error=2.0362
[flaml.automl: 09-16 06:00:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:00:43] {3072} INFO -  at 21.1s,	estimator xgboost's best error=2.0362,	best estimator xgboost's best error=2.0362
[flaml.automl: 09-16 06:00:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:00:46] {3072} INFO -  at 24.0s,	estimator xgboost's best error=1.5844,	best estimator xgboost's best error=1.5844
[flaml.automl: 09-16 06:00:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:00:51] {3072} INFO -  at 28.8s,	estimator xgboost's best error=1.5844,	best estimator xgboost's best error=1.5844
[flaml.automl: 09-16 06:00:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:00:54] {3072} INFO -  at 31.9s,	estimator xgboost's best error=1.5844,	best estimator xgboost's best error=1.5844
[flaml.automl: 09-16 06:00:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:01:00] {3072} INFO -  at 37.6s,	estimator xgboost's best error=1.5844,	best estimator xgboost's best error=1.5844
[flaml.automl: 09-16 06:01:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:01:02] {3072} INFO -  at 40.3s,	estimator xgboost's best error=1.5572,	best estimator xgboost's best error=1.5572
[flaml.automl: 09-16 06:01:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:01:05] {3072} INFO -  at 42.4s,	estimator xgboost's best error=1.5572,	best estimator xgboost's best error=1.5572
[flaml.automl: 09-16 06:01:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:01:16] {3072} INFO -  at 53.3s,	estimator xgboost's best error=1.4457,	best estimator xgboost's best error=1.4457
[flaml.automl: 09-16 06:01:27] {3335} INFO - retrain xgboost for 11.1s
[flaml.automl: 09-16 06:01:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:01:27] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:01:27] {2637} INFO - Time taken to find the best model: 53.33471155166626
[flaml.automl: 09-16 06:01:27] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
SO2(0)最佳损失：-0.4457089412887769
SO2(0)最好结果：{'pred_time': 1.8334874486643744e-05, 'wall_clock_time': 53.33471155166626, 'metric_for_logging': {'pred_time': 1.8334874486643744e-05}, 'val_loss': 1.445708941288777, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 10.928548574447632}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7909398075633838
SO2(0)的mse=5.326626843536717
SO2(0)的mae=1.384313419068228
SO2(0)的mar=0.15317773821088926
总共花费的时间为：65.07
邯郸市
1050A
3579A
3580A
[flaml.automl: 09-16 06:10:47] {2390} INFO - task = regression
[flaml.automl: 09-16 06:10:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:10:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:10:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:10:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:10:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:10:50] {3025} INFO - Estimated sufficient time budget=23329s. Estimated necessary time budget=23s.
[flaml.automl: 09-16 06:10:50] {3072} INFO -  at 2.5s,	estimator xgboost's best error=5.8718,	best estimator xgboost's best error=5.8718
[flaml.automl: 09-16 06:10:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:10:54] {3072} INFO -  at 6.5s,	estimator xgboost's best error=2.9712,	best estimator xgboost's best error=2.9712
[flaml.automl: 09-16 06:10:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:10:56] {3072} INFO -  at 8.5s,	estimator xgboost's best error=2.9712,	best estimator xgboost's best error=2.9712
[flaml.automl: 09-16 06:10:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:11:12] {3072} INFO -  at 24.8s,	estimator xgboost's best error=2.9712,	best estimator xgboost's best error=2.9712
[flaml.automl: 09-16 06:11:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:11:14] {3072} INFO -  at 27.0s,	estimator xgboost's best error=2.4761,	best estimator xgboost's best error=2.4761
[flaml.automl: 09-16 06:11:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:11:16] {3072} INFO -  at 29.4s,	estimator xgboost's best error=2.4761,	best estimator xgboost's best error=2.4761
[flaml.automl: 09-16 06:11:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:11:19] {3072} INFO -  at 32.0s,	estimator xgboost's best error=2.0480,	best estimator xgboost's best error=2.0480
[flaml.automl: 09-16 06:11:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:11:27] {3072} INFO -  at 39.4s,	estimator xgboost's best error=2.0480,	best estimator xgboost's best error=2.0480
[flaml.automl: 09-16 06:11:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:11:31] {3072} INFO -  at 44.1s,	estimator xgboost's best error=2.0480,	best estimator xgboost's best error=2.0480
[flaml.automl: 09-16 06:11:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:11:40] {3072} INFO -  at 52.8s,	estimator xgboost's best error=2.0402,	best estimator xgboost's best error=2.0402
[flaml.automl: 09-16 06:11:48] {3335} INFO - retrain xgboost for 8.6s
[flaml.automl: 09-16 06:11:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 06:11:49] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:11:49] {2637} INFO - Time taken to find the best model: 52.83003354072571
[flaml.automl: 09-16 06:11:49] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}
SO2(0)最佳损失：-1.040216235447597
SO2(0)最好结果：{'pred_time': 3.1345674207994154e-05, 'wall_clock_time': 52.83003354072571, 'metric_for_logging': {'pred_time': 3.1345674207994154e-05}, 'val_loss': 2.040216235447597, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'experiment_tag': 'exp', 'time_total_s': 8.72694706916809}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.571806730753091
SO2(0)的mse=12.669427657599197
SO2(0)的mae=2.044534900877531
SO2(0)的mar=0.21829870262630743
总共花费的时间为：62.18
保定市
1051A
1053A
1054A
1055A
1056A
3581A
3582A
3583A
[flaml.automl: 09-16 06:34:57] {2390} INFO - task = regression
[flaml.automl: 09-16 06:34:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:34:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:34:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:34:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:34:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:34:58] {3025} INFO - Estimated sufficient time budget=104061s. Estimated necessary time budget=104s.
[flaml.automl: 09-16 06:34:58] {3072} INFO -  at 1.6s,	estimator xgboost's best error=4.7033,	best estimator xgboost's best error=4.7033
[flaml.automl: 09-16 06:34:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:35:00] {3072} INFO -  at 3.7s,	estimator xgboost's best error=2.2127,	best estimator xgboost's best error=2.2127
[flaml.automl: 09-16 06:35:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:35:01] {3072} INFO -  at 4.9s,	estimator xgboost's best error=2.2127,	best estimator xgboost's best error=2.2127
[flaml.automl: 09-16 06:35:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:35:04] {3072} INFO -  at 7.6s,	estimator xgboost's best error=2.2127,	best estimator xgboost's best error=2.2127
[flaml.automl: 09-16 06:35:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:35:05] {3072} INFO -  at 8.7s,	estimator xgboost's best error=1.4114,	best estimator xgboost's best error=1.4114
[flaml.automl: 09-16 06:35:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:35:07] {3072} INFO -  at 10.3s,	estimator xgboost's best error=1.4114,	best estimator xgboost's best error=1.4114
[flaml.automl: 09-16 06:35:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:35:09] {3072} INFO -  at 12.0s,	estimator xgboost's best error=1.0182,	best estimator xgboost's best error=1.0182
[flaml.automl: 09-16 06:35:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:35:11] {3072} INFO -  at 14.3s,	estimator xgboost's best error=1.0182,	best estimator xgboost's best error=1.0182
[flaml.automl: 09-16 06:35:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:35:12] {3072} INFO -  at 15.9s,	estimator xgboost's best error=1.0182,	best estimator xgboost's best error=1.0182
[flaml.automl: 09-16 06:35:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:35:15] {3072} INFO -  at 18.1s,	estimator xgboost's best error=1.0182,	best estimator xgboost's best error=1.0182
[flaml.automl: 09-16 06:35:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:35:16] {3072} INFO -  at 19.5s,	estimator xgboost's best error=1.0103,	best estimator xgboost's best error=1.0103
[flaml.automl: 09-16 06:35:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:35:17] {3072} INFO -  at 20.7s,	estimator xgboost's best error=1.0103,	best estimator xgboost's best error=1.0103
[flaml.automl: 09-16 06:35:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:35:19] {3072} INFO -  at 22.4s,	estimator xgboost's best error=0.9084,	best estimator xgboost's best error=0.9084
[flaml.automl: 09-16 06:35:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 06:35:21] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.9084,	best estimator xgboost's best error=0.9084
[flaml.automl: 09-16 06:35:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 06:35:22] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.9084,	best estimator xgboost's best error=0.9084
[flaml.automl: 09-16 06:35:22] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 06:35:24] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.9084,	best estimator xgboost's best error=0.9084
[flaml.automl: 09-16 06:35:24] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 06:35:25] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.9084,	best estimator xgboost's best error=0.9084
[flaml.automl: 09-16 06:35:25] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 06:35:31] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.8884,	best estimator xgboost's best error=0.8884
[flaml.automl: 09-16 06:35:31] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 06:35:34] {3072} INFO -  at 37.0s,	estimator xgboost's best error=0.8884,	best estimator xgboost's best error=0.8884
[flaml.automl: 09-16 06:35:34] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 06:35:56] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.8884,	best estimator xgboost's best error=0.8884
[flaml.automl: 09-16 06:36:07] {3335} INFO - retrain xgboost for 10.9s
[flaml.automl: 09-16 06:36:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 06:36:07] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:36:07] {2637} INFO - Time taken to find the best model: 34.62428259849548
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 87444}
SO2(0)最佳损失：0.11160894192036774
SO2(0)最好结果：{'pred_time': 4.406339459499238e-06, 'wall_clock_time': 34.62428259849548, 'metric_for_logging': {'pred_time': 4.406339459499238e-06}, 'val_loss': 0.8883910580796323, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 87444}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'config/FLAML_sample_size': 87444, 'experiment_tag': 'exp', 'time_total_s': 6.042673110961914}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8417581977737968
SO2(0)的mse=1.9667716075375754
SO2(0)的mae=0.8870343096545997
SO2(0)的mar=0.14176871154704082
总共花费的时间为：72.13
张家口市
1060A
1061A
3131A
3323A
[flaml.automl: 09-16 06:48:12] {2390} INFO - task = regression
[flaml.automl: 09-16 06:48:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:48:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:48:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:48:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:48:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:48:13] {3025} INFO - Estimated sufficient time budget=49093s. Estimated necessary time budget=49s.
[flaml.automl: 09-16 06:48:13] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.0382,	best estimator xgboost's best error=4.0382
[flaml.automl: 09-16 06:48:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:48:15] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.9254,	best estimator xgboost's best error=1.9254
[flaml.automl: 09-16 06:48:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:48:16] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.9254,	best estimator xgboost's best error=1.9254
[flaml.automl: 09-16 06:48:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:48:23] {3072} INFO -  at 11.4s,	estimator xgboost's best error=1.9254,	best estimator xgboost's best error=1.9254
[flaml.automl: 09-16 06:48:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:48:25] {3072} INFO -  at 13.6s,	estimator xgboost's best error=1.3074,	best estimator xgboost's best error=1.3074
[flaml.automl: 09-16 06:48:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:48:28] {3072} INFO -  at 16.2s,	estimator xgboost's best error=1.1944,	best estimator xgboost's best error=1.1944
[flaml.automl: 09-16 06:48:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:48:31] {3072} INFO -  at 19.2s,	estimator xgboost's best error=1.1531,	best estimator xgboost's best error=1.1531
[flaml.automl: 09-16 06:48:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:48:36] {3072} INFO -  at 24.1s,	estimator xgboost's best error=1.1531,	best estimator xgboost's best error=1.1531
[flaml.automl: 09-16 06:48:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:48:38] {3072} INFO -  at 27.0s,	estimator xgboost's best error=1.1531,	best estimator xgboost's best error=1.1531
[flaml.automl: 09-16 06:48:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:48:42] {3072} INFO -  at 30.9s,	estimator xgboost's best error=1.1531,	best estimator xgboost's best error=1.1531
[flaml.automl: 09-16 06:48:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:48:45] {3072} INFO -  at 33.3s,	estimator xgboost's best error=1.1531,	best estimator xgboost's best error=1.1531
[flaml.automl: 09-16 06:48:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:48:48] {3072} INFO -  at 36.3s,	estimator xgboost's best error=1.1443,	best estimator xgboost's best error=1.1443
[flaml.automl: 09-16 06:48:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:48:50] {3072} INFO -  at 38.3s,	estimator xgboost's best error=1.1443,	best estimator xgboost's best error=1.1443
[flaml.automl: 09-16 06:48:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 06:49:08] {3072} INFO -  at 56.4s,	estimator xgboost's best error=0.9895,	best estimator xgboost's best error=0.9895
[flaml.automl: 09-16 06:49:27] {3335} INFO - retrain xgboost for 19.6s
[flaml.automl: 09-16 06:49:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 06:49:27] {2636} INFO - fit succeeded
[flaml.automl: 09-16 06:49:27] {2637} INFO - Time taken to find the best model: 56.38642501831055
[flaml.automl: 09-16 06:49:27] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 41203}
SO2(0)最佳损失：0.010457603190501819
SO2(0)最好结果：{'pred_time': 2.675297010879533e-05, 'wall_clock_time': 56.38642501831055, 'metric_for_logging': {'pred_time': 2.675297010879533e-05}, 'val_loss': 0.9895423968094982, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 41203}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 41203, 'experiment_tag': 'exp', 'time_total_s': 18.077521324157715}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7386158716373403
SO2(0)的mse=3.002903243059149
SO2(0)的mae=0.944191431539898
SO2(0)的mar=0.15466875590688647
总共花费的时间为：77.01
承德市
1063A
1064A
1065A
[flaml.automl: 09-16 06:58:52] {2390} INFO - task = regression
[flaml.automl: 09-16 06:58:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 06:58:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 06:58:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 06:58:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 06:58:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 06:58:54] {3025} INFO - Estimated sufficient time budget=12152s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 06:58:54] {3072} INFO -  at 1.3s,	estimator xgboost's best error=5.2882,	best estimator xgboost's best error=5.2882
[flaml.automl: 09-16 06:58:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 06:58:56] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.5770,	best estimator xgboost's best error=2.5770
[flaml.automl: 09-16 06:58:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 06:58:57] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.5770,	best estimator xgboost's best error=2.5770
[flaml.automl: 09-16 06:58:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 06:59:07] {3072} INFO -  at 14.7s,	estimator xgboost's best error=2.5770,	best estimator xgboost's best error=2.5770
[flaml.automl: 09-16 06:59:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 06:59:08] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.6931,	best estimator xgboost's best error=1.6931
[flaml.automl: 09-16 06:59:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 06:59:10] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.6423,	best estimator xgboost's best error=1.6423
[flaml.automl: 09-16 06:59:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 06:59:11] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.5104,	best estimator xgboost's best error=1.5104
[flaml.automl: 09-16 06:59:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 06:59:14] {3072} INFO -  at 21.8s,	estimator xgboost's best error=1.5104,	best estimator xgboost's best error=1.5104
[flaml.automl: 09-16 06:59:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 06:59:16] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.5104,	best estimator xgboost's best error=1.5104
[flaml.automl: 09-16 06:59:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 06:59:19] {3072} INFO -  at 26.4s,	estimator xgboost's best error=1.5104,	best estimator xgboost's best error=1.5104
[flaml.automl: 09-16 06:59:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 06:59:20] {3072} INFO -  at 27.9s,	estimator xgboost's best error=1.5104,	best estimator xgboost's best error=1.5104
[flaml.automl: 09-16 06:59:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 06:59:22] {3072} INFO -  at 29.2s,	estimator xgboost's best error=1.5104,	best estimator xgboost's best error=1.5104
[flaml.automl: 09-16 06:59:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 06:59:34] {3072} INFO -  at 41.8s,	estimator xgboost's best error=1.4727,	best estimator xgboost's best error=1.4727
[flaml.automl: 09-16 06:59:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 06:59:52] {3072} INFO -  at 59.4s,	estimator xgboost's best error=1.4460,	best estimator xgboost's best error=1.4460
[flaml.automl: 09-16 07:00:25] {3335} INFO - retrain xgboost for 32.8s
[flaml.automl: 09-16 07:00:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:00:25] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:00:25] {2637} INFO - Time taken to find the best model: 59.41445589065552
[flaml.automl: 09-16 07:00:25] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767}
SO2(0)最佳损失：-0.4459850761427808
SO2(0)最好结果：{'pred_time': 1.8289762679353612e-05, 'wall_clock_time': 59.41445589065552, 'metric_for_logging': {'pred_time': 1.8289762679353612e-05}, 'val_loss': 1.4459850761427808, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'experiment_tag': 'exp', 'time_total_s': 17.614229440689087}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.44516718151447143
SO2(0)的mse=12.504771366744672
SO2(0)的mae=1.397666555316887
SO2(0)的mar=0.15682325650546092
总共花费的时间为：92.83
廊坊市
1070A
2919A
[flaml.automl: 09-16 07:08:02] {2390} INFO - task = regression
[flaml.automl: 09-16 07:08:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:08:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:08:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:08:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:08:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:08:04] {3025} INFO - Estimated sufficient time budget=22666s. Estimated necessary time budget=23s.
[flaml.automl: 09-16 07:08:04] {3072} INFO -  at 2.4s,	estimator xgboost's best error=4.4858,	best estimator xgboost's best error=4.4858
[flaml.automl: 09-16 07:08:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:08:09] {3072} INFO -  at 6.9s,	estimator xgboost's best error=2.0631,	best estimator xgboost's best error=2.0631
[flaml.automl: 09-16 07:08:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:08:12] {3072} INFO -  at 10.2s,	estimator xgboost's best error=2.0631,	best estimator xgboost's best error=2.0631
[flaml.automl: 09-16 07:08:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:08:38] {3072} INFO -  at 35.6s,	estimator xgboost's best error=2.0631,	best estimator xgboost's best error=2.0631
[flaml.automl: 09-16 07:08:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:08:41] {3072} INFO -  at 38.7s,	estimator xgboost's best error=1.1423,	best estimator xgboost's best error=1.1423
[flaml.automl: 09-16 07:08:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:08:45] {3072} INFO -  at 42.9s,	estimator xgboost's best error=1.1423,	best estimator xgboost's best error=1.1423
[flaml.automl: 09-16 07:08:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:08:50] {3072} INFO -  at 47.5s,	estimator xgboost's best error=0.8022,	best estimator xgboost's best error=0.8022
[flaml.automl: 09-16 07:08:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:08:56] {3072} INFO -  at 54.5s,	estimator xgboost's best error=0.8022,	best estimator xgboost's best error=0.8022
[flaml.automl: 09-16 07:08:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:09:00] {3072} INFO -  at 57.5s,	estimator xgboost's best error=0.8022,	best estimator xgboost's best error=0.8022
[flaml.automl: 09-16 07:09:03] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-16 07:09:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 07:09:03] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:09:03] {2637} INFO - Time taken to find the best model: 47.49977469444275
[flaml.automl: 09-16 07:09:03] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：0.19780913662987865
SO2(0)最好结果：{'pred_time': 5.944382834744144e-05, 'wall_clock_time': 47.49977469444275, 'metric_for_logging': {'pred_time': 5.944382834744144e-05}, 'val_loss': 0.8021908633701214, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.551462888717651}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8689450942090453
SO2(0)的mse=1.260006942453919
SO2(0)的mae=0.7771625334194231
SO2(0)的mar=0.13012995066392177
总共花费的时间为：61.01
沧州市
1071A
1073A
3324A
[flaml.automl: 09-16 07:19:07] {2390} INFO - task = regression
[flaml.automl: 09-16 07:19:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:19:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:19:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:19:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:19:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:19:10] {3025} INFO - Estimated sufficient time budget=32787s. Estimated necessary time budget=33s.
[flaml.automl: 09-16 07:19:10] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.9462,	best estimator xgboost's best error=4.9462
[flaml.automl: 09-16 07:19:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:19:16] {3072} INFO -  at 9.1s,	estimator xgboost's best error=2.3295,	best estimator xgboost's best error=2.3295
[flaml.automl: 09-16 07:19:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:19:19] {3072} INFO -  at 12.4s,	estimator xgboost's best error=2.3295,	best estimator xgboost's best error=2.3295
[flaml.automl: 09-16 07:19:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:19:46] {3072} INFO -  at 39.6s,	estimator xgboost's best error=2.3295,	best estimator xgboost's best error=2.3295
[flaml.automl: 09-16 07:19:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:19:50] {3072} INFO -  at 42.7s,	estimator xgboost's best error=1.5475,	best estimator xgboost's best error=1.5475
[flaml.automl: 09-16 07:19:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:19:54] {3072} INFO -  at 47.0s,	estimator xgboost's best error=1.5475,	best estimator xgboost's best error=1.5475
[flaml.automl: 09-16 07:19:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:19:58] {3072} INFO -  at 51.5s,	estimator xgboost's best error=1.1062,	best estimator xgboost's best error=1.1062
[flaml.automl: 09-16 07:19:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:20:03] {3072} INFO -  at 56.3s,	estimator xgboost's best error=1.1062,	best estimator xgboost's best error=1.1062
[flaml.automl: 09-16 07:20:05] {3335} INFO - retrain xgboost for 1.6s
[flaml.automl: 09-16 07:20:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 07:20:05] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:20:05] {2637} INFO - Time taken to find the best model: 51.54850459098816
[flaml.automl: 09-16 07:20:05] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-0.10620542862790328
SO2(0)最好结果：{'pred_time': 3.219110685274414e-05, 'wall_clock_time': 51.54850459098816, 'metric_for_logging': {'pred_time': 3.219110685274414e-05}, 'val_loss': 1.1062054286279033, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.580885171890259}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7696874045282827
SO2(0)的mse=2.9771590390180913
SO2(0)的mae=1.1096494704412074
SO2(0)的mar=0.15335357736571634
总共花费的时间为：58.44
衡水市
1074A
1075A
3188A
3459A
[flaml.automl: 09-16 07:34:03] {2390} INFO - task = regression
[flaml.automl: 09-16 07:34:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:34:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:34:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:34:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:34:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:34:05] {3025} INFO - Estimated sufficient time budget=97555s. Estimated necessary time budget=98s.
[flaml.automl: 09-16 07:34:05] {3072} INFO -  at 2.4s,	estimator xgboost's best error=6.9269,	best estimator xgboost's best error=6.9269
[flaml.automl: 09-16 07:34:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:34:09] {3072} INFO -  at 6.0s,	estimator xgboost's best error=3.1936,	best estimator xgboost's best error=3.1936
[flaml.automl: 09-16 07:34:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:34:11] {3072} INFO -  at 8.1s,	estimator xgboost's best error=3.1936,	best estimator xgboost's best error=3.1936
[flaml.automl: 09-16 07:34:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:34:16] {3072} INFO -  at 13.5s,	estimator xgboost's best error=3.1936,	best estimator xgboost's best error=3.1936
[flaml.automl: 09-16 07:34:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:34:18] {3072} INFO -  at 15.4s,	estimator xgboost's best error=1.8842,	best estimator xgboost's best error=1.8842
[flaml.automl: 09-16 07:34:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:34:21] {3072} INFO -  at 17.8s,	estimator xgboost's best error=1.3468,	best estimator xgboost's best error=1.3468
[flaml.automl: 09-16 07:34:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:34:23] {3072} INFO -  at 20.4s,	estimator xgboost's best error=1.3468,	best estimator xgboost's best error=1.3468
[flaml.automl: 09-16 07:34:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:34:27] {3072} INFO -  at 24.3s,	estimator xgboost's best error=1.3468,	best estimator xgboost's best error=1.3468
[flaml.automl: 09-16 07:34:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 07:34:29] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.3468,	best estimator xgboost's best error=1.3468
[flaml.automl: 09-16 07:34:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 07:34:32] {3072} INFO -  at 28.6s,	estimator xgboost's best error=1.3468,	best estimator xgboost's best error=1.3468
[flaml.automl: 09-16 07:34:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 07:34:36] {3072} INFO -  at 33.2s,	estimator xgboost's best error=1.3468,	best estimator xgboost's best error=1.3468
[flaml.automl: 09-16 07:34:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 07:34:39] {3072} INFO -  at 36.4s,	estimator xgboost's best error=1.3468,	best estimator xgboost's best error=1.3468
[flaml.automl: 09-16 07:34:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 07:34:57] {3072} INFO -  at 54.1s,	estimator xgboost's best error=1.2093,	best estimator xgboost's best error=1.2093
[flaml.automl: 09-16 07:35:07] {3335} INFO - retrain xgboost for 9.7s
[flaml.automl: 09-16 07:35:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 07:35:07] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:35:07] {2637} INFO - Time taken to find the best model: 54.097076177597046
[flaml.automl: 09-16 07:35:07] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 44272}
SO2(0)最佳损失：-0.20931378174118875
SO2(0)最好结果：{'pred_time': 1.839738551194106e-05, 'wall_clock_time': 54.097076177597046, 'metric_for_logging': {'pred_time': 1.839738551194106e-05}, 'val_loss': 1.2093137817411888, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 44272}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 44272, 'experiment_tag': 'exp', 'time_total_s': 17.647101879119873}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8592497767741585
SO2(0)的mse=3.335546064720657
SO2(0)的mae=1.2431450871770477
SO2(0)的mar=0.13266883599999296
总共花费的时间为：64.48
邢台市
1078A
1079A
1080A
[flaml.automl: 09-16 07:44:30] {2390} INFO - task = regression
[flaml.automl: 09-16 07:44:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 07:44:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 07:44:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 07:44:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 07:44:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 07:44:33] {3025} INFO - Estimated sufficient time budget=31587s. Estimated necessary time budget=32s.
[flaml.automl: 09-16 07:44:33] {3072} INFO -  at 3.3s,	estimator xgboost's best error=4.8655,	best estimator xgboost's best error=4.8655
[flaml.automl: 09-16 07:44:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 07:44:39] {3072} INFO -  at 9.1s,	estimator xgboost's best error=2.3911,	best estimator xgboost's best error=2.3911
[flaml.automl: 09-16 07:44:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 07:44:42] {3072} INFO -  at 12.3s,	estimator xgboost's best error=2.3911,	best estimator xgboost's best error=2.3911
[flaml.automl: 09-16 07:44:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 07:45:12] {3072} INFO -  at 42.1s,	estimator xgboost's best error=2.3911,	best estimator xgboost's best error=2.3911
[flaml.automl: 09-16 07:45:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 07:45:15] {3072} INFO -  at 45.1s,	estimator xgboost's best error=1.8797,	best estimator xgboost's best error=1.8797
[flaml.automl: 09-16 07:45:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 07:45:19] {3072} INFO -  at 49.6s,	estimator xgboost's best error=1.7635,	best estimator xgboost's best error=1.7635
[flaml.automl: 09-16 07:45:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 07:45:24] {3072} INFO -  at 54.0s,	estimator xgboost's best error=1.6092,	best estimator xgboost's best error=1.6092
[flaml.automl: 09-16 07:45:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 07:45:29] {3072} INFO -  at 59.2s,	estimator xgboost's best error=1.6092,	best estimator xgboost's best error=1.6092
[flaml.automl: 09-16 07:45:32] {3335} INFO - retrain xgboost for 2.7s
[flaml.automl: 09-16 07:45:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 07:45:32] {2636} INFO - fit succeeded
[flaml.automl: 09-16 07:45:32] {2637} INFO - Time taken to find the best model: 54.021528244018555
[flaml.automl: 09-16 07:45:32] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：-0.6092101561259351
SO2(0)最好结果：{'pred_time': 3.530136234973061e-05, 'wall_clock_time': 54.021528244018555, 'metric_for_logging': {'pred_time': 3.530136234973061e-05}, 'val_loss': 1.609210156125935, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.376377105712891}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6236077639595299
SO2(0)的mse=6.29454498056326
SO2(0)的mae=1.5736724255474686
SO2(0)的mar=0.24727795320963356
总共花费的时间为：62.42
太原市
1081A
1084A
1085A
1086A
1087A
3185A
[flaml.automl: 09-16 08:05:20] {2390} INFO - task = regression
[flaml.automl: 09-16 08:05:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:05:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:05:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:05:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:05:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:05:22] {3025} INFO - Estimated sufficient time budget=144143s. Estimated necessary time budget=144s.
[flaml.automl: 09-16 08:05:22] {3072} INFO -  at 2.5s,	estimator xgboost's best error=7.4679,	best estimator xgboost's best error=7.4679
[flaml.automl: 09-16 08:05:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:05:26] {3072} INFO -  at 6.4s,	estimator xgboost's best error=3.7154,	best estimator xgboost's best error=3.7154
[flaml.automl: 09-16 08:05:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:05:28] {3072} INFO -  at 8.6s,	estimator xgboost's best error=3.7154,	best estimator xgboost's best error=3.7154
[flaml.automl: 09-16 08:05:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:05:31] {3072} INFO -  at 11.8s,	estimator xgboost's best error=3.7154,	best estimator xgboost's best error=3.7154
[flaml.automl: 09-16 08:05:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:05:34] {3072} INFO -  at 13.9s,	estimator xgboost's best error=3.0969,	best estimator xgboost's best error=3.0969
[flaml.automl: 09-16 08:05:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:05:36] {3072} INFO -  at 16.9s,	estimator xgboost's best error=3.0969,	best estimator xgboost's best error=3.0969
[flaml.automl: 09-16 08:05:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:05:39] {3072} INFO -  at 19.5s,	estimator xgboost's best error=2.5719,	best estimator xgboost's best error=2.5719
[flaml.automl: 09-16 08:05:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:05:42] {3072} INFO -  at 22.2s,	estimator xgboost's best error=2.5719,	best estimator xgboost's best error=2.5719
[flaml.automl: 09-16 08:05:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:05:44] {3072} INFO -  at 24.8s,	estimator xgboost's best error=2.5719,	best estimator xgboost's best error=2.5719
[flaml.automl: 09-16 08:05:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:05:46] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.5719,	best estimator xgboost's best error=2.5719
[flaml.automl: 09-16 08:05:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:05:48] {3072} INFO -  at 28.8s,	estimator xgboost's best error=2.5719,	best estimator xgboost's best error=2.5719
[flaml.automl: 09-16 08:05:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:05:52] {3072} INFO -  at 32.0s,	estimator xgboost's best error=2.5117,	best estimator xgboost's best error=2.5117
[flaml.automl: 09-16 08:05:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:05:54] {3072} INFO -  at 34.2s,	estimator xgboost's best error=2.5117,	best estimator xgboost's best error=2.5117
[flaml.automl: 09-16 08:05:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:06:06] {3072} INFO -  at 46.9s,	estimator xgboost's best error=2.4491,	best estimator xgboost's best error=2.4491
[flaml.automl: 09-16 08:06:06] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:06:19] {3072} INFO -  at 59.6s,	estimator xgboost's best error=2.3767,	best estimator xgboost's best error=2.3767
[flaml.automl: 09-16 08:06:32] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-16 08:06:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:06:32] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:06:32] {2637} INFO - Time taken to find the best model: 59.63704323768616
[flaml.automl: 09-16 08:06:32] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 65704}
SO2(0)最佳损失：-1.3766903981266405
SO2(0)最好结果：{'pred_time': 5.381480583898369e-06, 'wall_clock_time': 59.63704323768616, 'metric_for_logging': {'pred_time': 5.381480583898369e-06}, 'val_loss': 2.3766903981266405, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 65704}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 65704, 'experiment_tag': 'exp', 'time_total_s': 12.78300428390503}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7510901880906232
SO2(0)的mse=17.51172578809858
SO2(0)的mae=2.4470331001975363
SO2(0)的mar=0.21980602144793598
总共花费的时间为：73.45
呼和浩特市
1095A
1097A
3698A
3699A
[flaml.automl: 09-16 08:18:40] {2390} INFO - task = regression
[flaml.automl: 09-16 08:18:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:18:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:18:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:18:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:18:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:18:42] {3025} INFO - Estimated sufficient time budget=97713s. Estimated necessary time budget=98s.
[flaml.automl: 09-16 08:18:42] {3072} INFO -  at 2.4s,	estimator xgboost's best error=6.2310,	best estimator xgboost's best error=6.2310
[flaml.automl: 09-16 08:18:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:18:46] {3072} INFO -  at 6.1s,	estimator xgboost's best error=2.9106,	best estimator xgboost's best error=2.9106
[flaml.automl: 09-16 08:18:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:18:48] {3072} INFO -  at 8.5s,	estimator xgboost's best error=2.9106,	best estimator xgboost's best error=2.9106
[flaml.automl: 09-16 08:18:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:18:53] {3072} INFO -  at 13.5s,	estimator xgboost's best error=2.9106,	best estimator xgboost's best error=2.9106
[flaml.automl: 09-16 08:18:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:18:55] {3072} INFO -  at 15.6s,	estimator xgboost's best error=1.7381,	best estimator xgboost's best error=1.7381
[flaml.automl: 09-16 08:18:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:18:58] {3072} INFO -  at 18.4s,	estimator xgboost's best error=1.6505,	best estimator xgboost's best error=1.6505
[flaml.automl: 09-16 08:18:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:19:01] {3072} INFO -  at 21.5s,	estimator xgboost's best error=1.4892,	best estimator xgboost's best error=1.4892
[flaml.automl: 09-16 08:19:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:19:05] {3072} INFO -  at 25.0s,	estimator xgboost's best error=1.4892,	best estimator xgboost's best error=1.4892
[flaml.automl: 09-16 08:19:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:19:07] {3072} INFO -  at 28.0s,	estimator xgboost's best error=1.4892,	best estimator xgboost's best error=1.4892
[flaml.automl: 09-16 08:19:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:19:11] {3072} INFO -  at 31.3s,	estimator xgboost's best error=1.4892,	best estimator xgboost's best error=1.4892
[flaml.automl: 09-16 08:19:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:19:13] {3072} INFO -  at 33.9s,	estimator xgboost's best error=1.4892,	best estimator xgboost's best error=1.4892
[flaml.automl: 09-16 08:19:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:19:17] {3072} INFO -  at 37.0s,	estimator xgboost's best error=1.4851,	best estimator xgboost's best error=1.4851
[flaml.automl: 09-16 08:19:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:19:19] {3072} INFO -  at 39.1s,	estimator xgboost's best error=1.4851,	best estimator xgboost's best error=1.4851
[flaml.automl: 09-16 08:19:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:19:27] {3072} INFO -  at 47.8s,	estimator xgboost's best error=1.3252,	best estimator xgboost's best error=1.3252
[flaml.automl: 09-16 08:19:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:19:39] {3072} INFO -  at 59.8s,	estimator xgboost's best error=1.3060,	best estimator xgboost's best error=1.3060
[flaml.automl: 09-16 08:19:52] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-16 08:19:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:19:52] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:19:52] {2637} INFO - Time taken to find the best model: 59.753639221191406
[flaml.automl: 09-16 08:19:52] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 44206}
SO2(0)最佳损失：-0.306030248586052
SO2(0)最好结果：{'pred_time': 9.561157770187924e-06, 'wall_clock_time': 59.753639221191406, 'metric_for_logging': {'pred_time': 9.561157770187924e-06}, 'val_loss': 1.306030248586052, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 44206}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 44206, 'experiment_tag': 'exp', 'time_total_s': 11.93635082244873}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8364850263430683
SO2(0)的mse=4.180307573065048
SO2(0)的mae=1.2648478463352186
SO2(0)的mar=0.1282084678800953
总共花费的时间为：73.14
沈阳市
1098A
1099A
1100A
1104A
1105A
1106A
2900A
[flaml.automl: 09-16 08:42:40] {2390} INFO - task = regression
[flaml.automl: 09-16 08:42:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:42:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:42:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:42:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:42:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:42:42] {3025} INFO - Estimated sufficient time budget=168104s. Estimated necessary time budget=168s.
[flaml.automl: 09-16 08:42:42] {3072} INFO -  at 2.6s,	estimator xgboost's best error=8.4067,	best estimator xgboost's best error=8.4067
[flaml.automl: 09-16 08:42:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:42:45] {3072} INFO -  at 5.9s,	estimator xgboost's best error=4.5812,	best estimator xgboost's best error=4.5812
[flaml.automl: 09-16 08:42:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:42:47] {3072} INFO -  at 8.2s,	estimator xgboost's best error=4.5812,	best estimator xgboost's best error=4.5812
[flaml.automl: 09-16 08:42:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:42:51] {3072} INFO -  at 11.5s,	estimator xgboost's best error=4.5812,	best estimator xgboost's best error=4.5812
[flaml.automl: 09-16 08:42:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:42:53] {3072} INFO -  at 13.6s,	estimator xgboost's best error=3.1728,	best estimator xgboost's best error=3.1728
[flaml.automl: 09-16 08:42:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:42:55] {3072} INFO -  at 16.0s,	estimator xgboost's best error=3.1728,	best estimator xgboost's best error=3.1728
[flaml.automl: 09-16 08:42:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:42:58] {3072} INFO -  at 18.6s,	estimator xgboost's best error=2.7346,	best estimator xgboost's best error=2.7346
[flaml.automl: 09-16 08:42:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:43:00] {3072} INFO -  at 20.6s,	estimator xgboost's best error=2.7346,	best estimator xgboost's best error=2.7346
[flaml.automl: 09-16 08:43:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:43:02] {3072} INFO -  at 22.7s,	estimator xgboost's best error=2.7346,	best estimator xgboost's best error=2.7346
[flaml.automl: 09-16 08:43:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:43:04] {3072} INFO -  at 24.7s,	estimator xgboost's best error=2.7346,	best estimator xgboost's best error=2.7346
[flaml.automl: 09-16 08:43:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 08:43:06] {3072} INFO -  at 26.5s,	estimator xgboost's best error=2.7346,	best estimator xgboost's best error=2.7346
[flaml.automl: 09-16 08:43:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 08:43:09] {3072} INFO -  at 29.8s,	estimator xgboost's best error=2.6336,	best estimator xgboost's best error=2.6336
[flaml.automl: 09-16 08:43:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 08:43:10] {3072} INFO -  at 31.0s,	estimator xgboost's best error=2.6336,	best estimator xgboost's best error=2.6336
[flaml.automl: 09-16 08:43:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 08:43:17] {3072} INFO -  at 38.1s,	estimator xgboost's best error=2.5759,	best estimator xgboost's best error=2.5759
[flaml.automl: 09-16 08:43:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 08:43:38] {3072} INFO -  at 59.0s,	estimator xgboost's best error=2.5399,	best estimator xgboost's best error=2.5399
[flaml.automl: 09-16 08:44:01] {3335} INFO - retrain xgboost for 22.6s
[flaml.automl: 09-16 08:44:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 08:44:01] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:44:01] {2637} INFO - Time taken to find the best model: 58.99832201004028
[flaml.automl: 09-16 08:44:01] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 75051}
SO2(0)最佳损失：-1.5399114444673203
SO2(0)最好结果：{'pred_time': 9.623991785575445e-06, 'wall_clock_time': 58.99832201004028, 'metric_for_logging': {'pred_time': 9.623991785575445e-06}, 'val_loss': 2.5399114444673203, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 75051}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 75051, 'experiment_tag': 'exp', 'time_total_s': 20.916897535324097}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7150669038734745
SO2(0)的mse=17.564759825795058
SO2(0)的mae=2.4526066386928678
SO2(0)的mar=0.18018990294479367
总共花费的时间为：83.60
大连市
1110A
1117A
[flaml.automl: 09-16 08:50:55] {2390} INFO - task = regression
[flaml.automl: 09-16 08:50:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 08:50:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 08:50:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 08:50:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 08:50:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 08:50:57] {3025} INFO - Estimated sufficient time budget=22411s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 08:50:57] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.0956,	best estimator xgboost's best error=5.0956
[flaml.automl: 09-16 08:50:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 08:51:02] {3072} INFO -  at 6.9s,	estimator xgboost's best error=2.3869,	best estimator xgboost's best error=2.3869
[flaml.automl: 09-16 08:51:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 08:51:05] {3072} INFO -  at 10.4s,	estimator xgboost's best error=2.3869,	best estimator xgboost's best error=2.3869
[flaml.automl: 09-16 08:51:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 08:51:29] {3072} INFO -  at 34.6s,	estimator xgboost's best error=2.3869,	best estimator xgboost's best error=2.3869
[flaml.automl: 09-16 08:51:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 08:51:31] {3072} INFO -  at 36.8s,	estimator xgboost's best error=1.5925,	best estimator xgboost's best error=1.5925
[flaml.automl: 09-16 08:51:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 08:51:34] {3072} INFO -  at 39.6s,	estimator xgboost's best error=1.4157,	best estimator xgboost's best error=1.4157
[flaml.automl: 09-16 08:51:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 08:51:37] {3072} INFO -  at 42.0s,	estimator xgboost's best error=1.3427,	best estimator xgboost's best error=1.3427
[flaml.automl: 09-16 08:51:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 08:51:41] {3072} INFO -  at 46.3s,	estimator xgboost's best error=1.3427,	best estimator xgboost's best error=1.3427
[flaml.automl: 09-16 08:51:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 08:51:44] {3072} INFO -  at 49.1s,	estimator xgboost's best error=1.3427,	best estimator xgboost's best error=1.3427
[flaml.automl: 09-16 08:51:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 08:51:49] {3072} INFO -  at 54.6s,	estimator xgboost's best error=1.2788,	best estimator xgboost's best error=1.2788
[flaml.automl: 09-16 08:51:55] {3335} INFO - retrain xgboost for 5.4s
[flaml.automl: 09-16 08:51:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 08:51:55] {2636} INFO - fit succeeded
[flaml.automl: 09-16 08:51:55] {2637} INFO - Time taken to find the best model: 54.63137912750244
[flaml.automl: 09-16 08:51:55] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-0.2787546178738829
SO2(0)最好结果：{'pred_time': 3.6504218833263855e-05, 'wall_clock_time': 54.63137912750244, 'metric_for_logging': {'pred_time': 3.6504218833263855e-05}, 'val_loss': 1.278754617873883, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 5.484431982040405}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6698802933932405
SO2(0)的mse=3.658524905585752
SO2(0)的mae=1.250480144424776
SO2(0)的mar=0.158310843008081
总共花费的时间为：60.45
长春市
1119A
1120A
1121A
1122A
1124A
1125A
1126A
1128A
[flaml.automl: 09-16 09:15:44] {2390} INFO - task = regression
[flaml.automl: 09-16 09:15:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 09:15:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 09:15:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 09:15:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 09:15:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 09:15:46] {3025} INFO - Estimated sufficient time budget=174250s. Estimated necessary time budget=174s.
[flaml.automl: 09-16 09:15:46] {3072} INFO -  at 2.5s,	estimator xgboost's best error=4.5507,	best estimator xgboost's best error=4.5507
[flaml.automl: 09-16 09:15:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 09:15:49] {3072} INFO -  at 5.4s,	estimator xgboost's best error=2.8197,	best estimator xgboost's best error=2.8197
[flaml.automl: 09-16 09:15:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 09:15:51] {3072} INFO -  at 7.4s,	estimator xgboost's best error=2.8197,	best estimator xgboost's best error=2.8197
[flaml.automl: 09-16 09:15:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 09:15:53] {3072} INFO -  at 9.6s,	estimator xgboost's best error=2.8197,	best estimator xgboost's best error=2.8197
[flaml.automl: 09-16 09:15:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 09:15:55] {3072} INFO -  at 11.5s,	estimator xgboost's best error=1.5069,	best estimator xgboost's best error=1.5069
[flaml.automl: 09-16 09:15:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 09:15:57] {3072} INFO -  at 13.1s,	estimator xgboost's best error=1.5069,	best estimator xgboost's best error=1.5069
[flaml.automl: 09-16 09:15:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 09:15:59] {3072} INFO -  at 14.8s,	estimator xgboost's best error=1.1600,	best estimator xgboost's best error=1.1600
[flaml.automl: 09-16 09:15:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 09:16:01] {3072} INFO -  at 17.0s,	estimator xgboost's best error=1.1600,	best estimator xgboost's best error=1.1600
[flaml.automl: 09-16 09:16:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 09:16:02] {3072} INFO -  at 18.6s,	estimator xgboost's best error=1.1600,	best estimator xgboost's best error=1.1600
[flaml.automl: 09-16 09:16:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 09:16:05] {3072} INFO -  at 20.8s,	estimator xgboost's best error=1.1600,	best estimator xgboost's best error=1.1600
[flaml.automl: 09-16 09:16:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 09:16:06] {3072} INFO -  at 22.2s,	estimator xgboost's best error=1.1600,	best estimator xgboost's best error=1.1600
[flaml.automl: 09-16 09:16:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 09:16:08] {3072} INFO -  at 23.9s,	estimator xgboost's best error=1.1600,	best estimator xgboost's best error=1.1600
[flaml.automl: 09-16 09:16:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 09:16:09] {3072} INFO -  at 25.1s,	estimator xgboost's best error=1.1600,	best estimator xgboost's best error=1.1600
[flaml.automl: 09-16 09:16:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 09:16:16] {3072} INFO -  at 32.2s,	estimator xgboost's best error=1.1075,	best estimator xgboost's best error=1.1075
[flaml.automl: 09-16 09:16:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 09:16:29] {3072} INFO -  at 45.1s,	estimator xgboost's best error=1.0962,	best estimator xgboost's best error=1.0962
[flaml.automl: 09-16 09:16:29] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 09:16:36] {3072} INFO -  at 52.2s,	estimator xgboost's best error=1.0962,	best estimator xgboost's best error=1.0962
[flaml.automl: 09-16 09:16:49] {3335} INFO - retrain xgboost for 12.9s
[flaml.automl: 09-16 09:16:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 09:16:49] {2636} INFO - fit succeeded
[flaml.automl: 09-16 09:16:49] {2637} INFO - Time taken to find the best model: 45.07201409339905
[flaml.automl: 09-16 09:16:49] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 85324}
SO2(0)最佳损失：-0.09624920977958706
SO2(0)最好结果：{'pred_time': 4.406078947381296e-06, 'wall_clock_time': 45.07201409339905, 'metric_for_logging': {'pred_time': 4.406078947381296e-06}, 'val_loss': 1.096249209779587, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 85324}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 85324, 'experiment_tag': 'exp', 'time_total_s': 12.875765085220337}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.775348055002745
SO2(0)的mse=4.967491398779151
SO2(0)的mae=1.1172279643451146
SO2(0)的mar=0.13595365083811262
总共花费的时间为：66.92
哈尔滨市
1129A
1130A
1139A
1140A
[flaml.automl: 09-16 09:30:05] {2390} INFO - task = regression
[flaml.automl: 09-16 09:30:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 09:30:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 09:30:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 09:30:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 09:30:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 09:30:08] {3025} INFO - Estimated sufficient time budget=133553s. Estimated necessary time budget=134s.
[flaml.automl: 09-16 09:30:08] {3072} INFO -  at 3.4s,	estimator xgboost's best error=8.1369,	best estimator xgboost's best error=8.1369
[flaml.automl: 09-16 09:30:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 09:30:14] {3072} INFO -  at 9.0s,	estimator xgboost's best error=3.8460,	best estimator xgboost's best error=3.8460
[flaml.automl: 09-16 09:30:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 09:30:17] {3072} INFO -  at 12.2s,	estimator xgboost's best error=3.8460,	best estimator xgboost's best error=3.8460
[flaml.automl: 09-16 09:30:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 09:30:23] {3072} INFO -  at 18.1s,	estimator xgboost's best error=3.8460,	best estimator xgboost's best error=3.8460
[flaml.automl: 09-16 09:30:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 09:30:27] {3072} INFO -  at 22.6s,	estimator xgboost's best error=2.5842,	best estimator xgboost's best error=2.5842
[flaml.automl: 09-16 09:30:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 09:30:31] {3072} INFO -  at 26.2s,	estimator xgboost's best error=2.5842,	best estimator xgboost's best error=2.5842
[flaml.automl: 09-16 09:30:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 09:30:34] {3072} INFO -  at 29.7s,	estimator xgboost's best error=2.5842,	best estimator xgboost's best error=2.5842
[flaml.automl: 09-16 09:30:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 09:30:37] {3072} INFO -  at 32.0s,	estimator xgboost's best error=2.5842,	best estimator xgboost's best error=2.5842
[flaml.automl: 09-16 09:30:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 09:30:39] {3072} INFO -  at 34.5s,	estimator xgboost's best error=2.5842,	best estimator xgboost's best error=2.5842
[flaml.automl: 09-16 09:30:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 09:30:44] {3072} INFO -  at 39.1s,	estimator xgboost's best error=2.5753,	best estimator xgboost's best error=2.5753
[flaml.automl: 09-16 09:30:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 09:30:51] {3072} INFO -  at 46.2s,	estimator xgboost's best error=2.5753,	best estimator xgboost's best error=2.5753
[flaml.automl: 09-16 09:30:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 09:30:56] {3072} INFO -  at 51.0s,	estimator xgboost's best error=2.3268,	best estimator xgboost's best error=2.3268
[flaml.automl: 09-16 09:30:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 09:31:00] {3072} INFO -  at 55.3s,	estimator xgboost's best error=2.3268,	best estimator xgboost's best error=2.3268
[flaml.automl: 09-16 09:31:04] {3335} INFO - retrain xgboost for 4.4s
[flaml.automl: 09-16 09:31:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8660549326361876, colsample_bynode=1,
             colsample_bytree=0.900406128221973, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.590319080116618,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=53.34352431682733, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.7783323860653508, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 09:31:04] {2636} INFO - fit succeeded
[flaml.automl: 09-16 09:31:04] {2637} INFO - Time taken to find the best model: 51.038166761398315
[flaml.automl: 09-16 09:31:04] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 53.34352431682733, 'learning_rate': 0.590319080116618, 'subsample': 1.0, 'colsample_bylevel': 0.8660549326361876, 'colsample_bytree': 0.900406128221973, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.7783323860653508, 'FLAML_sample_size': 41642}
SO2(0)最佳损失：-1.32679300605027
SO2(0)最好结果：{'pred_time': 3.303460227714029e-05, 'wall_clock_time': 51.038166761398315, 'metric_for_logging': {'pred_time': 3.303460227714029e-05}, 'val_loss': 2.32679300605027, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 53.34352431682733, 'learning_rate': 0.590319080116618, 'subsample': 1.0, 'colsample_bylevel': 0.8660549326361876, 'colsample_bytree': 0.900406128221973, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.7783323860653508, 'FLAML_sample_size': 41642}, 'config/n_estimators': 4, 'config/max_leaves': 4, 'config/min_child_weight': 53.34352431682733, 'config/learning_rate': 0.590319080116618, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8660549326361876, 'config/colsample_bytree': 0.900406128221973, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.7783323860653508, 'config/FLAML_sample_size': 41642, 'experiment_tag': 'exp', 'time_total_s': 4.847949743270874}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8660549326361876, colsample_bynode=1,
             colsample_bytree=0.900406128221973, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.590319080116618,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=53.34352431682733, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.7783323860653508, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6835785429176757
SO2(0)的mse=14.412728302400096
SO2(0)的mae=2.312878069815481
SO2(0)的mar=0.20119114402326033
总共花费的时间为：60.70
上海市
1143A
1144A
1145A
1148A
1150A
3265A
3266A
3269A
3270A
3271A
3272A
3273A
3274A
3544A
[flaml.automl: 09-16 10:13:09] {2390} INFO - task = regression
[flaml.automl: 09-16 10:13:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 10:13:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 10:13:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 10:13:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 10:13:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 10:13:11] {3025} INFO - Estimated sufficient time budget=205504s. Estimated necessary time budget=206s.
[flaml.automl: 09-16 10:13:11] {3072} INFO -  at 2.1s,	estimator xgboost's best error=3.9156,	best estimator xgboost's best error=3.9156
[flaml.automl: 09-16 10:13:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 10:13:12] {3072} INFO -  at 3.2s,	estimator xgboost's best error=3.9156,	best estimator xgboost's best error=3.9156
[flaml.automl: 09-16 10:13:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 10:13:13] {3072} INFO -  at 4.4s,	estimator xgboost's best error=3.9156,	best estimator xgboost's best error=3.9156
[flaml.automl: 09-16 10:13:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 10:13:14] {3072} INFO -  at 5.7s,	estimator xgboost's best error=3.9156,	best estimator xgboost's best error=3.9156
[flaml.automl: 09-16 10:13:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 10:13:16] {3072} INFO -  at 6.9s,	estimator xgboost's best error=2.3336,	best estimator xgboost's best error=2.3336
[flaml.automl: 09-16 10:13:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 10:13:17] {3072} INFO -  at 7.9s,	estimator xgboost's best error=2.3336,	best estimator xgboost's best error=2.3336
[flaml.automl: 09-16 10:13:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 10:13:18] {3072} INFO -  at 9.2s,	estimator xgboost's best error=2.3336,	best estimator xgboost's best error=2.3336
[flaml.automl: 09-16 10:13:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 10:13:19] {3072} INFO -  at 10.3s,	estimator xgboost's best error=2.3336,	best estimator xgboost's best error=2.3336
[flaml.automl: 09-16 10:13:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 10:13:20] {3072} INFO -  at 11.5s,	estimator xgboost's best error=1.0615,	best estimator xgboost's best error=1.0615
[flaml.automl: 09-16 10:13:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 10:13:21] {3072} INFO -  at 12.6s,	estimator xgboost's best error=1.0615,	best estimator xgboost's best error=1.0615
[flaml.automl: 09-16 10:13:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 10:13:22] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.7723,	best estimator xgboost's best error=0.7723
[flaml.automl: 09-16 10:13:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 10:13:24] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.7723,	best estimator xgboost's best error=0.7723
[flaml.automl: 09-16 10:13:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 10:13:25] {3072} INFO -  at 16.4s,	estimator xgboost's best error=0.4624,	best estimator xgboost's best error=0.4624
[flaml.automl: 09-16 10:13:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 10:13:26] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.4624,	best estimator xgboost's best error=0.4624
[flaml.automl: 09-16 10:13:26] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 10:13:28] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.4624,	best estimator xgboost's best error=0.4624
[flaml.automl: 09-16 10:13:28] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 10:13:30] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.4624,	best estimator xgboost's best error=0.4624
[flaml.automl: 09-16 10:13:30] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 10:13:31] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.4624,	best estimator xgboost's best error=0.4624
[flaml.automl: 09-16 10:13:31] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 10:13:35] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.4624,	best estimator xgboost's best error=0.4624
[flaml.automl: 09-16 10:13:35] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 10:13:37] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.4624,	best estimator xgboost's best error=0.4624
[flaml.automl: 09-16 10:13:37] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 10:13:47] {3072} INFO -  at 38.8s,	estimator xgboost's best error=0.4624,	best estimator xgboost's best error=0.4624
[flaml.automl: 09-16 10:13:47] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 10:13:49] {3072} INFO -  at 40.6s,	estimator xgboost's best error=0.4624,	best estimator xgboost's best error=0.4624
[flaml.automl: 09-16 10:13:49] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 10:14:03] {3072} INFO -  at 54.1s,	estimator xgboost's best error=0.4624,	best estimator xgboost's best error=0.4624
[flaml.automl: 09-16 10:14:03] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 10:14:08] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.4624,	best estimator xgboost's best error=0.4624
[flaml.automl: 09-16 10:14:12] {3335} INFO - retrain xgboost for 4.5s
[flaml.automl: 09-16 10:14:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.6715420641966449, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=2.2396378354111817,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.006131241309171052, reg_lambda=1.660563253113505,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 10:14:12] {2636} INFO - fit succeeded
[flaml.automl: 09-16 10:14:12] {2637} INFO - Time taken to find the best model: 16.38588047027588
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 2.2396378354111817, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.6715420641966449, 'reg_alpha': 0.006131241309171052, 'reg_lambda': 1.660563253113505, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：0.5375719440958322
SO2(0)最好结果：{'pred_time': 4.2322368238744634e-06, 'wall_clock_time': 16.38588047027588, 'metric_for_logging': {'pred_time': 4.2322368238744634e-06}, 'val_loss': 0.46242805590416786, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 2.2396378354111817, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.6715420641966449, 'reg_alpha': 0.006131241309171052, 'reg_lambda': 1.660563253113505, 'FLAML_sample_size': 10000}, 'config/n_estimators': 8, 'config/max_leaves': 9, 'config/min_child_weight': 2.2396378354111817, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.6715420641966449, 'config/reg_alpha': 0.006131241309171052, 'config/reg_lambda': 1.660563253113505, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 1.2998085021972656}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.6715420641966449, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=2.2396378354111817,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.006131241309171052, reg_lambda=1.660563253113505,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8192873303598984
SO2(0)的mse=0.7100608183477262
SO2(0)的mae=0.47801602486645983
SO2(0)的mar=0.10018999901139375
总共花费的时间为：65.98
南京市
1151A
1152A
1153A
1154A
3423A
3424A
3427A
[flaml.automl: 09-16 10:35:16] {2390} INFO - task = regression
[flaml.automl: 09-16 10:35:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 10:35:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 10:35:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 10:35:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 10:35:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 10:35:19] {3025} INFO - Estimated sufficient time budget=173346s. Estimated necessary time budget=173s.
[flaml.automl: 09-16 10:35:19] {3072} INFO -  at 2.6s,	estimator xgboost's best error=3.1828,	best estimator xgboost's best error=3.1828
[flaml.automl: 09-16 10:35:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 10:35:22] {3072} INFO -  at 6.2s,	estimator xgboost's best error=1.7591,	best estimator xgboost's best error=1.7591
[flaml.automl: 09-16 10:35:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 10:35:24] {3072} INFO -  at 8.3s,	estimator xgboost's best error=1.7591,	best estimator xgboost's best error=1.7591
[flaml.automl: 09-16 10:35:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 10:35:27] {3072} INFO -  at 11.5s,	estimator xgboost's best error=1.7591,	best estimator xgboost's best error=1.7591
[flaml.automl: 09-16 10:35:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 10:35:30] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.9983,	best estimator xgboost's best error=0.9983
[flaml.automl: 09-16 10:35:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 10:35:32] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.9983,	best estimator xgboost's best error=0.9983
[flaml.automl: 09-16 10:35:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 10:35:35] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.8949,	best estimator xgboost's best error=0.8949
[flaml.automl: 09-16 10:35:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 10:35:36] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.8949,	best estimator xgboost's best error=0.8949
[flaml.automl: 09-16 10:35:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 10:35:39] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.8949,	best estimator xgboost's best error=0.8949
[flaml.automl: 09-16 10:35:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 10:35:40] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.8949,	best estimator xgboost's best error=0.8949
[flaml.automl: 09-16 10:35:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 10:35:43] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.8409,	best estimator xgboost's best error=0.8409
[flaml.automl: 09-16 10:35:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 10:35:45] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.8409,	best estimator xgboost's best error=0.8409
[flaml.automl: 09-16 10:35:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 10:35:46] {3072} INFO -  at 29.9s,	estimator xgboost's best error=0.7553,	best estimator xgboost's best error=0.7553
[flaml.automl: 09-16 10:35:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 10:35:48] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.7553,	best estimator xgboost's best error=0.7553
[flaml.automl: 09-16 10:35:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 10:35:49] {3072} INFO -  at 33.4s,	estimator xgboost's best error=0.7553,	best estimator xgboost's best error=0.7553
[flaml.automl: 09-16 10:35:49] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 10:35:51] {3072} INFO -  at 35.4s,	estimator xgboost's best error=0.7553,	best estimator xgboost's best error=0.7553
[flaml.automl: 09-16 10:35:51] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 10:35:53] {3072} INFO -  at 36.7s,	estimator xgboost's best error=0.7553,	best estimator xgboost's best error=0.7553
[flaml.automl: 09-16 10:35:53] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 10:36:04] {3072} INFO -  at 47.7s,	estimator xgboost's best error=0.7342,	best estimator xgboost's best error=0.7342
[flaml.automl: 09-16 10:36:04] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 10:36:06] {3072} INFO -  at 50.1s,	estimator xgboost's best error=0.7342,	best estimator xgboost's best error=0.7342
[flaml.automl: 09-16 10:36:12] {3335} INFO - retrain xgboost for 6.0s
[flaml.automl: 09-16 10:36:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 10:36:12] {2636} INFO - fit succeeded
[flaml.automl: 09-16 10:36:12] {2637} INFO - Time taken to find the best model: 47.723185300827026
[flaml.automl: 09-16 10:36:12] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 76457}
SO2(0)最佳损失：0.26578297263990014
SO2(0)最好结果：{'pred_time': 5.5093886488575044e-06, 'wall_clock_time': 47.723185300827026, 'metric_for_logging': {'pred_time': 5.5093886488575044e-06}, 'val_loss': 0.7342170273600999, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 76457}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'config/FLAML_sample_size': 76457, 'experiment_tag': 'exp', 'time_total_s': 10.98020625114441}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7996301410798363
SO2(0)的mse=1.2915303619803749
SO2(0)的mae=0.719059595213098
SO2(0)的mar=0.17583482015471266
总共花费的时间为：57.64
苏州市
1160A
1164A
1165A
1166A
1167A
3289A
3290A
3425A
3431A
[flaml.automl: 09-16 11:03:10] {2390} INFO - task = regression
[flaml.automl: 09-16 11:03:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:03:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:03:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:03:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:03:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:03:12] {3025} INFO - Estimated sufficient time budget=222141s. Estimated necessary time budget=222s.
[flaml.automl: 09-16 11:03:12] {3072} INFO -  at 2.7s,	estimator xgboost's best error=3.5385,	best estimator xgboost's best error=3.5385
[flaml.automl: 09-16 11:03:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:03:15] {3072} INFO -  at 5.3s,	estimator xgboost's best error=2.5085,	best estimator xgboost's best error=2.5085
[flaml.automl: 09-16 11:03:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:03:17] {3072} INFO -  at 7.5s,	estimator xgboost's best error=2.5085,	best estimator xgboost's best error=2.5085
[flaml.automl: 09-16 11:03:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:03:19] {3072} INFO -  at 9.7s,	estimator xgboost's best error=2.5085,	best estimator xgboost's best error=2.5085
[flaml.automl: 09-16 11:03:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:03:22] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.8791,	best estimator xgboost's best error=0.8791
[flaml.automl: 09-16 11:03:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:03:24] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.8791,	best estimator xgboost's best error=0.8791
[flaml.automl: 09-16 11:03:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:03:26] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.6595,	best estimator xgboost's best error=0.6595
[flaml.automl: 09-16 11:03:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:03:28] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.6595,	best estimator xgboost's best error=0.6595
[flaml.automl: 09-16 11:03:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:03:29] {3072} INFO -  at 19.5s,	estimator xgboost's best error=0.6595,	best estimator xgboost's best error=0.6595
[flaml.automl: 09-16 11:03:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:03:31] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.6595,	best estimator xgboost's best error=0.6595
[flaml.automl: 09-16 11:03:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:03:32] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.5828,	best estimator xgboost's best error=0.5828
[flaml.automl: 09-16 11:03:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:03:34] {3072} INFO -  at 23.9s,	estimator xgboost's best error=0.5828,	best estimator xgboost's best error=0.5828
[flaml.automl: 09-16 11:03:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:03:35] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.5285,	best estimator xgboost's best error=0.5285
[flaml.automl: 09-16 11:03:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:03:36] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.5285,	best estimator xgboost's best error=0.5285
[flaml.automl: 09-16 11:03:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:03:38] {3072} INFO -  at 28.0s,	estimator xgboost's best error=0.5285,	best estimator xgboost's best error=0.5285
[flaml.automl: 09-16 11:03:38] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 11:03:39] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.5285,	best estimator xgboost's best error=0.5285
[flaml.automl: 09-16 11:03:39] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 11:03:40] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.5285,	best estimator xgboost's best error=0.5285
[flaml.automl: 09-16 11:03:40] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 11:03:46] {3072} INFO -  at 36.5s,	estimator xgboost's best error=0.5233,	best estimator xgboost's best error=0.5233
[flaml.automl: 09-16 11:03:46] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 11:03:49] {3072} INFO -  at 39.0s,	estimator xgboost's best error=0.5233,	best estimator xgboost's best error=0.5233
[flaml.automl: 09-16 11:03:49] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 11:04:08] {3072} INFO -  at 58.0s,	estimator xgboost's best error=0.5233,	best estimator xgboost's best error=0.5233
[flaml.automl: 09-16 11:04:19] {3335} INFO - retrain xgboost for 10.9s
[flaml.automl: 09-16 11:04:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:04:19] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:04:19] {2637} INFO - Time taken to find the best model: 36.54233384132385
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 97839}
SO2(0)最佳损失：0.47669860385903196
SO2(0)最好结果：{'pred_time': 3.7727859506895676e-06, 'wall_clock_time': 36.54233384132385, 'metric_for_logging': {'pred_time': 3.7727859506895676e-06}, 'val_loss': 0.523301396140968, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403, 'FLAML_sample_size': 97839}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'config/FLAML_sample_size': 97839, 'experiment_tag': 'exp', 'time_total_s': 6.041238784790039}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8582441519559223
SO2(0)的mse=0.6863878947603597
SO2(0)的mae=0.5211716989618588
SO2(0)的mar=0.10348282711098886
总共花费的时间为：70.93
南通市
1168A
1169A
1171A
1172A
3291A
3432A
[flaml.automl: 09-16 11:22:32] {2390} INFO - task = regression
[flaml.automl: 09-16 11:22:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:22:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:22:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:22:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:22:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:22:33] {3025} INFO - Estimated sufficient time budget=79181s. Estimated necessary time budget=79s.
[flaml.automl: 09-16 11:22:33] {3072} INFO -  at 1.5s,	estimator xgboost's best error=3.7015,	best estimator xgboost's best error=3.7015
[flaml.automl: 09-16 11:22:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:22:35] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.7319,	best estimator xgboost's best error=1.7319
[flaml.automl: 09-16 11:22:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:22:36] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.7319,	best estimator xgboost's best error=1.7319
[flaml.automl: 09-16 11:22:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:22:40] {3072} INFO -  at 8.5s,	estimator xgboost's best error=1.7319,	best estimator xgboost's best error=1.7319
[flaml.automl: 09-16 11:22:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:22:41] {3072} INFO -  at 9.6s,	estimator xgboost's best error=1.0737,	best estimator xgboost's best error=1.0737
[flaml.automl: 09-16 11:22:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:22:43] {3072} INFO -  at 11.2s,	estimator xgboost's best error=0.9212,	best estimator xgboost's best error=0.9212
[flaml.automl: 09-16 11:22:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:22:44] {3072} INFO -  at 12.8s,	estimator xgboost's best error=0.8363,	best estimator xgboost's best error=0.8363
[flaml.automl: 09-16 11:22:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:22:47] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.8363,	best estimator xgboost's best error=0.8363
[flaml.automl: 09-16 11:22:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:22:48] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.8363,	best estimator xgboost's best error=0.8363
[flaml.automl: 09-16 11:22:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:22:52] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.8363,	best estimator xgboost's best error=0.8363
[flaml.automl: 09-16 11:22:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:22:54] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.8363,	best estimator xgboost's best error=0.8363
[flaml.automl: 09-16 11:22:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:22:58] {3072} INFO -  at 26.3s,	estimator xgboost's best error=0.8347,	best estimator xgboost's best error=0.8347
[flaml.automl: 09-16 11:22:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:23:00] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.8347,	best estimator xgboost's best error=0.8347
[flaml.automl: 09-16 11:23:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:23:13] {3072} INFO -  at 41.3s,	estimator xgboost's best error=0.6491,	best estimator xgboost's best error=0.6491
[flaml.automl: 09-16 11:23:13] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 11:23:31] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.6434,	best estimator xgboost's best error=0.6434
[flaml.automl: 09-16 11:23:52] {3335} INFO - retrain xgboost for 21.4s
[flaml.automl: 09-16 11:23:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:23:52] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:23:52] {2637} INFO - Time taken to find the best model: 59.71153116226196
[flaml.automl: 09-16 11:23:52] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 65919}
SO2(0)最佳损失：0.35657680277124604
SO2(0)最好结果：{'pred_time': 1.1996539379549515e-05, 'wall_clock_time': 59.71153116226196, 'metric_for_logging': {'pred_time': 1.1996539379549515e-05}, 'val_loss': 0.643423197228754, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 65919}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 65919, 'experiment_tag': 'exp', 'time_total_s': 18.452842712402344}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.879461558488988
SO2(0)的mse=1.1591710771147015
SO2(0)的mae=0.6495026619959586
SO2(0)的mar=0.1257599674334863
总共花费的时间为：82.06
连云港市
1173A
3000A
3008A
3009A
3434A
3664A
[flaml.automl: 09-16 11:42:46] {2390} INFO - task = regression
[flaml.automl: 09-16 11:42:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:42:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:42:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:42:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:42:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:42:48] {3025} INFO - Estimated sufficient time budget=143014s. Estimated necessary time budget=143s.
[flaml.automl: 09-16 11:42:48] {3072} INFO -  at 2.6s,	estimator xgboost's best error=4.6298,	best estimator xgboost's best error=4.6298
[flaml.automl: 09-16 11:42:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:42:52] {3072} INFO -  at 6.5s,	estimator xgboost's best error=2.2004,	best estimator xgboost's best error=2.2004
[flaml.automl: 09-16 11:42:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:42:54] {3072} INFO -  at 8.7s,	estimator xgboost's best error=2.2004,	best estimator xgboost's best error=2.2004
[flaml.automl: 09-16 11:42:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:42:57] {3072} INFO -  at 11.8s,	estimator xgboost's best error=2.2004,	best estimator xgboost's best error=2.2004
[flaml.automl: 09-16 11:42:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:42:59] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.4380,	best estimator xgboost's best error=1.4380
[flaml.automl: 09-16 11:42:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:43:02] {3072} INFO -  at 16.6s,	estimator xgboost's best error=1.4380,	best estimator xgboost's best error=1.4380
[flaml.automl: 09-16 11:43:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:43:05] {3072} INFO -  at 19.7s,	estimator xgboost's best error=1.1277,	best estimator xgboost's best error=1.1277
[flaml.automl: 09-16 11:43:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:43:08] {3072} INFO -  at 22.4s,	estimator xgboost's best error=1.1277,	best estimator xgboost's best error=1.1277
[flaml.automl: 09-16 11:43:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:43:11] {3072} INFO -  at 24.9s,	estimator xgboost's best error=1.1277,	best estimator xgboost's best error=1.1277
[flaml.automl: 09-16 11:43:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:43:13] {3072} INFO -  at 27.5s,	estimator xgboost's best error=1.1277,	best estimator xgboost's best error=1.1277
[flaml.automl: 09-16 11:43:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:43:15] {3072} INFO -  at 29.7s,	estimator xgboost's best error=1.1277,	best estimator xgboost's best error=1.1277
[flaml.automl: 09-16 11:43:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:43:18] {3072} INFO -  at 32.8s,	estimator xgboost's best error=1.1254,	best estimator xgboost's best error=1.1254
[flaml.automl: 09-16 11:43:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:43:21] {3072} INFO -  at 35.0s,	estimator xgboost's best error=1.1254,	best estimator xgboost's best error=1.1254
[flaml.automl: 09-16 11:43:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 11:43:34] {3072} INFO -  at 48.1s,	estimator xgboost's best error=1.0234,	best estimator xgboost's best error=1.0234
[flaml.automl: 09-16 11:43:46] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-16 11:43:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:43:46] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:43:46] {2637} INFO - Time taken to find the best model: 48.0675847530365
[flaml.automl: 09-16 11:43:46] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 66165}
SO2(0)最佳损失：-0.023402307413607604
SO2(0)最好结果：{'pred_time': 9.508659582791831e-06, 'wall_clock_time': 48.0675847530365, 'metric_for_logging': {'pred_time': 9.508659582791831e-06}, 'val_loss': 1.0234023074136076, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 66165}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 66165, 'experiment_tag': 'exp', 'time_total_s': 13.077393531799316}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8166361211454569
SO2(0)的mse=3.2739637689714685
SO2(0)的mae=1.0098610931371501
SO2(0)的mar=0.15283609970101567
总共花费的时间为：61.07
徐州市
1177A
3006A
3288A
[flaml.automl: 09-16 11:53:59] {2390} INFO - task = regression
[flaml.automl: 09-16 11:53:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 11:53:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 11:53:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 11:53:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 11:53:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 11:54:02] {3025} INFO - Estimated sufficient time budget=22522s. Estimated necessary time budget=23s.
[flaml.automl: 09-16 11:54:02] {3072} INFO -  at 2.4s,	estimator xgboost's best error=5.5226,	best estimator xgboost's best error=5.5226
[flaml.automl: 09-16 11:54:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 11:54:06] {3072} INFO -  at 6.4s,	estimator xgboost's best error=2.5067,	best estimator xgboost's best error=2.5067
[flaml.automl: 09-16 11:54:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 11:54:08] {3072} INFO -  at 8.7s,	estimator xgboost's best error=2.5067,	best estimator xgboost's best error=2.5067
[flaml.automl: 09-16 11:54:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 11:54:27] {3072} INFO -  at 27.6s,	estimator xgboost's best error=2.5067,	best estimator xgboost's best error=2.5067
[flaml.automl: 09-16 11:54:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 11:54:29] {3072} INFO -  at 29.8s,	estimator xgboost's best error=1.4272,	best estimator xgboost's best error=1.4272
[flaml.automl: 09-16 11:54:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 11:54:32] {3072} INFO -  at 32.7s,	estimator xgboost's best error=1.4272,	best estimator xgboost's best error=1.4272
[flaml.automl: 09-16 11:54:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 11:54:35] {3072} INFO -  at 35.9s,	estimator xgboost's best error=1.0931,	best estimator xgboost's best error=1.0931
[flaml.automl: 09-16 11:54:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 11:54:40] {3072} INFO -  at 40.8s,	estimator xgboost's best error=1.0931,	best estimator xgboost's best error=1.0931
[flaml.automl: 09-16 11:54:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 11:54:43] {3072} INFO -  at 43.9s,	estimator xgboost's best error=1.0931,	best estimator xgboost's best error=1.0931
[flaml.automl: 09-16 11:54:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 11:54:48] {3072} INFO -  at 48.3s,	estimator xgboost's best error=1.0931,	best estimator xgboost's best error=1.0931
[flaml.automl: 09-16 11:54:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 11:54:49] {3072} INFO -  at 49.7s,	estimator xgboost's best error=1.0844,	best estimator xgboost's best error=1.0844
[flaml.automl: 09-16 11:54:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 11:54:50] {3072} INFO -  at 50.9s,	estimator xgboost's best error=1.0844,	best estimator xgboost's best error=1.0844
[flaml.automl: 09-16 11:54:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 11:54:56] {3072} INFO -  at 56.8s,	estimator xgboost's best error=1.0167,	best estimator xgboost's best error=1.0167
[flaml.automl: 09-16 11:55:02] {3335} INFO - retrain xgboost for 5.9s
[flaml.automl: 09-16 11:55:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 11:55:02] {2636} INFO - fit succeeded
[flaml.automl: 09-16 11:55:02] {2637} INFO - Time taken to find the best model: 56.84394955635071
[flaml.automl: 09-16 11:55:02] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
SO2(0)最佳损失：-0.01673845923392281
SO2(0)最好结果：{'pred_time': 1.1351215690939117e-05, 'wall_clock_time': 56.84394955635071, 'metric_for_logging': {'pred_time': 1.1351215690939117e-05}, 'val_loss': 1.0167384592339228, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 5.962742805480957}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7711937535731874
SO2(0)的mse=2.3362857103828074
SO2(0)的mae=0.9529255009011218
SO2(0)的mar=0.10897936303450473
总共花费的时间为：63.25
扬州市
1186A
3164A
3195A
3294A
[flaml.automl: 09-16 12:07:53] {2390} INFO - task = regression
[flaml.automl: 09-16 12:07:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:07:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:07:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:07:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:07:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:07:55] {3025} INFO - Estimated sufficient time budget=99709s. Estimated necessary time budget=100s.
[flaml.automl: 09-16 12:07:55] {3072} INFO -  at 2.4s,	estimator xgboost's best error=5.1974,	best estimator xgboost's best error=5.1974
[flaml.automl: 09-16 12:07:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:07:59] {3072} INFO -  at 6.3s,	estimator xgboost's best error=2.3689,	best estimator xgboost's best error=2.3689
[flaml.automl: 09-16 12:07:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:08:01] {3072} INFO -  at 8.6s,	estimator xgboost's best error=2.3689,	best estimator xgboost's best error=2.3689
[flaml.automl: 09-16 12:08:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:08:06] {3072} INFO -  at 13.7s,	estimator xgboost's best error=2.3689,	best estimator xgboost's best error=2.3689
[flaml.automl: 09-16 12:08:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:08:08] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.2663,	best estimator xgboost's best error=1.2663
[flaml.automl: 09-16 12:08:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:08:11] {3072} INFO -  at 18.7s,	estimator xgboost's best error=1.0413,	best estimator xgboost's best error=1.0413
[flaml.automl: 09-16 12:08:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:08:14] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.9306,	best estimator xgboost's best error=0.9306
[flaml.automl: 09-16 12:08:14] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:08:18] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.9306,	best estimator xgboost's best error=0.9306
[flaml.automl: 09-16 12:08:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:08:21] {3072} INFO -  at 28.2s,	estimator xgboost's best error=0.9306,	best estimator xgboost's best error=0.9306
[flaml.automl: 09-16 12:08:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:08:24] {3072} INFO -  at 31.5s,	estimator xgboost's best error=0.9306,	best estimator xgboost's best error=0.9306
[flaml.automl: 09-16 12:08:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:08:27] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.9306,	best estimator xgboost's best error=0.9306
[flaml.automl: 09-16 12:08:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:08:30] {3072} INFO -  at 37.3s,	estimator xgboost's best error=0.9306,	best estimator xgboost's best error=0.9306
[flaml.automl: 09-16 12:08:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:08:32] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.9306,	best estimator xgboost's best error=0.9306
[flaml.automl: 09-16 12:08:32] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:08:39] {3072} INFO -  at 46.0s,	estimator xgboost's best error=0.7717,	best estimator xgboost's best error=0.7717
[flaml.automl: 09-16 12:08:39] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 12:08:51] {3072} INFO -  at 58.6s,	estimator xgboost's best error=0.7609,	best estimator xgboost's best error=0.7609
[flaml.automl: 09-16 12:09:04] {3335} INFO - retrain xgboost for 12.6s
[flaml.automl: 09-16 12:09:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 12:09:04] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:09:04] {2637} INFO - Time taken to find the best model: 58.645891427993774
[flaml.automl: 09-16 12:09:04] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 44573}
SO2(0)最佳损失：0.23909272669590576
SO2(0)最好结果：{'pred_time': 8.334397258793203e-06, 'wall_clock_time': 58.645891427993774, 'metric_for_logging': {'pred_time': 8.334397258793203e-06}, 'val_loss': 0.7609072733040942, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 44573}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 44573, 'experiment_tag': 'exp', 'time_total_s': 12.601604223251343}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8550872067040791
SO2(0)的mse=1.4137888611844627
SO2(0)的mae=0.7753334719550278
SO2(0)的mar=0.09918095476458359
总共花费的时间为：72.00
无锡市
1189A
1190A
1191A
1192A
1193A
1194A
1195A
3428A
[flaml.automl: 09-16 12:34:05] {2390} INFO - task = regression
[flaml.automl: 09-16 12:34:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:34:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:34:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:34:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:34:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:34:07] {3025} INFO - Estimated sufficient time budget=197084s. Estimated necessary time budget=197s.
[flaml.automl: 09-16 12:34:07] {3072} INFO -  at 2.9s,	estimator xgboost's best error=4.1027,	best estimator xgboost's best error=4.1027
[flaml.automl: 09-16 12:34:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:34:10] {3072} INFO -  at 5.8s,	estimator xgboost's best error=2.5049,	best estimator xgboost's best error=2.5049
[flaml.automl: 09-16 12:34:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:34:13] {3072} INFO -  at 8.0s,	estimator xgboost's best error=2.5049,	best estimator xgboost's best error=2.5049
[flaml.automl: 09-16 12:34:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:34:15] {3072} INFO -  at 10.1s,	estimator xgboost's best error=2.5049,	best estimator xgboost's best error=2.5049
[flaml.automl: 09-16 12:34:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:34:17] {3072} INFO -  at 12.3s,	estimator xgboost's best error=1.0460,	best estimator xgboost's best error=1.0460
[flaml.automl: 09-16 12:34:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:34:19] {3072} INFO -  at 14.6s,	estimator xgboost's best error=1.0460,	best estimator xgboost's best error=1.0460
[flaml.automl: 09-16 12:34:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:34:21] {3072} INFO -  at 16.8s,	estimator xgboost's best error=1.0460,	best estimator xgboost's best error=1.0460
[flaml.automl: 09-16 12:34:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:34:23] {3072} INFO -  at 18.7s,	estimator xgboost's best error=1.0460,	best estimator xgboost's best error=1.0460
[flaml.automl: 09-16 12:34:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:34:25] {3072} INFO -  at 20.7s,	estimator xgboost's best error=0.9096,	best estimator xgboost's best error=0.9096
[flaml.automl: 09-16 12:34:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:34:28] {3072} INFO -  at 23.0s,	estimator xgboost's best error=0.9096,	best estimator xgboost's best error=0.9096
[flaml.automl: 09-16 12:34:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:34:29] {3072} INFO -  at 24.8s,	estimator xgboost's best error=0.9096,	best estimator xgboost's best error=0.9096
[flaml.automl: 09-16 12:34:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:34:31] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.9096,	best estimator xgboost's best error=0.9096
[flaml.automl: 09-16 12:34:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:34:32] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.7218,	best estimator xgboost's best error=0.7218
[flaml.automl: 09-16 12:34:32] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:34:33] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.7218,	best estimator xgboost's best error=0.7218
[flaml.automl: 09-16 12:34:33] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 12:34:35] {3072} INFO -  at 30.4s,	estimator xgboost's best error=0.7218,	best estimator xgboost's best error=0.7218
[flaml.automl: 09-16 12:34:35] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 12:34:36] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.7218,	best estimator xgboost's best error=0.7218
[flaml.automl: 09-16 12:34:36] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 12:34:37] {3072} INFO -  at 32.8s,	estimator xgboost's best error=0.7218,	best estimator xgboost's best error=0.7218
[flaml.automl: 09-16 12:34:37] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 12:34:45] {3072} INFO -  at 40.9s,	estimator xgboost's best error=0.6788,	best estimator xgboost's best error=0.6788
[flaml.automl: 09-16 12:34:45] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 12:34:49] {3072} INFO -  at 44.4s,	estimator xgboost's best error=0.6788,	best estimator xgboost's best error=0.6788
[flaml.automl: 09-16 12:34:49] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 12:35:05] {3072} INFO -  at 60.8s,	estimator xgboost's best error=0.6788,	best estimator xgboost's best error=0.6788
[flaml.automl: 09-16 12:35:18] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 12:35:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 12:35:18] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:35:18] {2637} INFO - Time taken to find the best model: 40.86422610282898
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 87956}
SO2(0)最佳损失：0.321153959049169
SO2(0)最好结果：{'pred_time': 9.342651460866753e-06, 'wall_clock_time': 40.86422610282898, 'metric_for_logging': {'pred_time': 9.342651460866753e-06}, 'val_loss': 0.678846040950831, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 87956}, 'config/n_estimators': 8, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609401, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466859, 'config/FLAML_sample_size': 87956, 'experiment_tag': 'exp', 'time_total_s': 8.061668395996094}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8500264833332586
SO2(0)的mse=1.0542438200408049
SO2(0)的mae=0.667913791300985
SO2(0)的mar=0.10946414481608219
总共花费的时间为：75.00
常州市
1196A
3003A
3010A
3429A
3430A
[flaml.automl: 09-16 12:51:50] {2390} INFO - task = regression
[flaml.automl: 09-16 12:51:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:51:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:51:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:51:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:51:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:51:51] {3025} INFO - Estimated sufficient time budget=67907s. Estimated necessary time budget=68s.
[flaml.automl: 09-16 12:51:51] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.7017,	best estimator xgboost's best error=4.7017
[flaml.automl: 09-16 12:51:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:51:54] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.1342,	best estimator xgboost's best error=2.1342
[flaml.automl: 09-16 12:51:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:51:55] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.1342,	best estimator xgboost's best error=2.1342
[flaml.automl: 09-16 12:51:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:51:59] {3072} INFO -  at 9.2s,	estimator xgboost's best error=2.1342,	best estimator xgboost's best error=2.1342
[flaml.automl: 09-16 12:51:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:52:00] {3072} INFO -  at 10.4s,	estimator xgboost's best error=1.2098,	best estimator xgboost's best error=1.2098
[flaml.automl: 09-16 12:52:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:52:02] {3072} INFO -  at 11.9s,	estimator xgboost's best error=1.2098,	best estimator xgboost's best error=1.2098
[flaml.automl: 09-16 12:52:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:52:04] {3072} INFO -  at 13.6s,	estimator xgboost's best error=1.0391,	best estimator xgboost's best error=1.0391
[flaml.automl: 09-16 12:52:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:52:06] {3072} INFO -  at 16.2s,	estimator xgboost's best error=1.0391,	best estimator xgboost's best error=1.0391
[flaml.automl: 09-16 12:52:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:52:08] {3072} INFO -  at 17.8s,	estimator xgboost's best error=1.0391,	best estimator xgboost's best error=1.0391
[flaml.automl: 09-16 12:52:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:52:11] {3072} INFO -  at 20.8s,	estimator xgboost's best error=1.0391,	best estimator xgboost's best error=1.0391
[flaml.automl: 09-16 12:52:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:52:13] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.0325,	best estimator xgboost's best error=1.0325
[flaml.automl: 09-16 12:52:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:52:15] {3072} INFO -  at 24.7s,	estimator xgboost's best error=1.0325,	best estimator xgboost's best error=1.0325
[flaml.automl: 09-16 12:52:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:52:18] {3072} INFO -  at 27.5s,	estimator xgboost's best error=1.0325,	best estimator xgboost's best error=1.0325
[flaml.automl: 09-16 12:52:18] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:52:20] {3072} INFO -  at 30.0s,	estimator xgboost's best error=1.0325,	best estimator xgboost's best error=1.0325
[flaml.automl: 09-16 12:52:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 12:52:22] {3072} INFO -  at 32.0s,	estimator xgboost's best error=1.0325,	best estimator xgboost's best error=1.0325
[flaml.automl: 09-16 12:52:22] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 12:52:25] {3072} INFO -  at 34.5s,	estimator xgboost's best error=1.0293,	best estimator xgboost's best error=1.0293
[flaml.automl: 09-16 12:52:25] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 12:52:28] {3072} INFO -  at 38.3s,	estimator xgboost's best error=1.0141,	best estimator xgboost's best error=1.0141
[flaml.automl: 09-16 12:52:28] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 12:52:31] {3072} INFO -  at 40.5s,	estimator xgboost's best error=1.0141,	best estimator xgboost's best error=1.0141
[flaml.automl: 09-16 12:52:31] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 12:52:33] {3072} INFO -  at 42.5s,	estimator xgboost's best error=1.0141,	best estimator xgboost's best error=1.0141
[flaml.automl: 09-16 12:52:33] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 12:52:41] {3072} INFO -  at 51.4s,	estimator xgboost's best error=1.0141,	best estimator xgboost's best error=1.0141
[flaml.automl: 09-16 12:52:41] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 12:52:43] {3072} INFO -  at 53.2s,	estimator xgboost's best error=1.0141,	best estimator xgboost's best error=1.0141
[flaml.automl: 09-16 12:52:43] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 12:52:49] {3072} INFO -  at 58.8s,	estimator xgboost's best error=0.9717,	best estimator xgboost's best error=0.9717
[flaml.automl: 09-16 12:53:00] {3335} INFO - retrain xgboost for 11.0s
[flaml.automl: 09-16 12:53:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7627053873155315, colsample_bynode=1,
             colsample_bytree=0.8896785359943279, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=26.165688980896686,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.1037368446408032, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 12:53:00] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:53:00] {2637} INFO - Time taken to find the best model: 58.751429319381714
[flaml.automl: 09-16 12:53:00] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 26.165688980896686, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7627053873155315, 'colsample_bytree': 0.8896785359943279, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.1037368446408032, 'FLAML_sample_size': 55244}
SO2(0)最佳损失：0.028256456633308558
SO2(0)最好结果：{'pred_time': 7.34398352981371e-06, 'wall_clock_time': 58.751429319381714, 'metric_for_logging': {'pred_time': 7.34398352981371e-06}, 'val_loss': 0.9717435433666914, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 26.165688980896686, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7627053873155315, 'colsample_bytree': 0.8896785359943279, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.1037368446408032, 'FLAML_sample_size': 55244}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 26.165688980896686, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7627053873155315, 'config/colsample_bytree': 0.8896785359943279, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.1037368446408032, 'config/FLAML_sample_size': 55244, 'experiment_tag': 'exp', 'time_total_s': 5.571207284927368}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7627053873155315, colsample_bynode=1,
             colsample_bytree=0.8896785359943279, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=26.165688980896686,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.1037368446408032, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.669010964448638
SO2(0)的mse=2.5488895925106045
SO2(0)的mae=0.9818184942710506
SO2(0)的mar=0.12719392746415836
总共花费的时间为：70.53
镇江市
3287A
[flaml.automl: 09-16 12:56:11] {2390} INFO - task = regression
[flaml.automl: 09-16 12:56:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 12:56:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 12:56:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 12:56:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 12:56:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 12:56:12] {3025} INFO - Estimated sufficient time budget=11787s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 12:56:12] {3072} INFO -  at 1.2s,	estimator xgboost's best error=4.6265,	best estimator xgboost's best error=4.6265
[flaml.automl: 09-16 12:56:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 12:56:14] {3072} INFO -  at 3.1s,	estimator xgboost's best error=2.4595,	best estimator xgboost's best error=2.4595
[flaml.automl: 09-16 12:56:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 12:56:15] {3072} INFO -  at 4.2s,	estimator xgboost's best error=2.4595,	best estimator xgboost's best error=2.4595
[flaml.automl: 09-16 12:56:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 12:56:22] {3072} INFO -  at 11.0s,	estimator xgboost's best error=2.4595,	best estimator xgboost's best error=2.4595
[flaml.automl: 09-16 12:56:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 12:56:23] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.1212,	best estimator xgboost's best error=1.1212
[flaml.automl: 09-16 12:56:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 12:56:24] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.9338,	best estimator xgboost's best error=0.9338
[flaml.automl: 09-16 12:56:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 12:56:26] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.9338,	best estimator xgboost's best error=0.9338
[flaml.automl: 09-16 12:56:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 12:56:28] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.9338,	best estimator xgboost's best error=0.9338
[flaml.automl: 09-16 12:56:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 12:56:29] {3072} INFO -  at 18.7s,	estimator xgboost's best error=0.9338,	best estimator xgboost's best error=0.9338
[flaml.automl: 09-16 12:56:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 12:56:32] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.9338,	best estimator xgboost's best error=0.9338
[flaml.automl: 09-16 12:56:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 12:56:33] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.9338,	best estimator xgboost's best error=0.9338
[flaml.automl: 09-16 12:56:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 12:56:34] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.9338,	best estimator xgboost's best error=0.9338
[flaml.automl: 09-16 12:56:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 12:56:40] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.8520,	best estimator xgboost's best error=0.8520
[flaml.automl: 09-16 12:56:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 12:56:49] {3072} INFO -  at 38.9s,	estimator xgboost's best error=0.8366,	best estimator xgboost's best error=0.8366
[flaml.automl: 09-16 12:56:49] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 12:56:55] {3072} INFO -  at 44.5s,	estimator xgboost's best error=0.8366,	best estimator xgboost's best error=0.8366
[flaml.automl: 09-16 12:56:55] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 12:57:12] {3072} INFO -  at 61.2s,	estimator xgboost's best error=0.8366,	best estimator xgboost's best error=0.8366
[flaml.automl: 09-16 12:57:34] {3335} INFO - retrain xgboost for 22.7s
[flaml.automl: 09-16 12:57:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 12:57:34] {2636} INFO - fit succeeded
[flaml.automl: 09-16 12:57:34] {2637} INFO - Time taken to find the best model: 38.93343472480774
SO2(0)最佳参数：{'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
SO2(0)最佳损失：0.16337396005709126
SO2(0)最好结果：{'pred_time': 3.139974614161104e-05, 'wall_clock_time': 38.93343472480774, 'metric_for_logging': {'pred_time': 3.139974614161104e-05}, 'val_loss': 0.8366260399429087, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 17, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 9.874236583709717}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8244846536981765
SO2(0)的mse=1.6870756613543352
SO2(0)的mae=0.860772965517911
SO2(0)的mar=0.11961504964836281
总共花费的时间为：84.24
泰州市
1206A
1207A
3295A
3435A
[flaml.automl: 09-16 13:09:34] {2390} INFO - task = regression
[flaml.automl: 09-16 13:09:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:09:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:09:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:09:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:09:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:09:36] {3025} INFO - Estimated sufficient time budget=96703s. Estimated necessary time budget=97s.
[flaml.automl: 09-16 13:09:36] {3072} INFO -  at 2.5s,	estimator xgboost's best error=3.8273,	best estimator xgboost's best error=3.8273
[flaml.automl: 09-16 13:09:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:09:40] {3072} INFO -  at 6.4s,	estimator xgboost's best error=1.7961,	best estimator xgboost's best error=1.7961
[flaml.automl: 09-16 13:09:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:09:43] {3072} INFO -  at 8.7s,	estimator xgboost's best error=1.7961,	best estimator xgboost's best error=1.7961
[flaml.automl: 09-16 13:09:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:09:48] {3072} INFO -  at 13.7s,	estimator xgboost's best error=1.7961,	best estimator xgboost's best error=1.7961
[flaml.automl: 09-16 13:09:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:09:49] {3072} INFO -  at 14.8s,	estimator xgboost's best error=1.1127,	best estimator xgboost's best error=1.1127
[flaml.automl: 09-16 13:09:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:09:50] {3072} INFO -  at 16.4s,	estimator xgboost's best error=1.0131,	best estimator xgboost's best error=1.0131
[flaml.automl: 09-16 13:09:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:09:52] {3072} INFO -  at 18.0s,	estimator xgboost's best error=0.9186,	best estimator xgboost's best error=0.9186
[flaml.automl: 09-16 13:09:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:09:55] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.9186,	best estimator xgboost's best error=0.9186
[flaml.automl: 09-16 13:09:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:09:56] {3072} INFO -  at 22.2s,	estimator xgboost's best error=0.9186,	best estimator xgboost's best error=0.9186
[flaml.automl: 09-16 13:09:56] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:09:59] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.8558,	best estimator xgboost's best error=0.8558
[flaml.automl: 09-16 13:09:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:10:01] {3072} INFO -  at 26.8s,	estimator xgboost's best error=0.8558,	best estimator xgboost's best error=0.8558
[flaml.automl: 09-16 13:10:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:10:02] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.8558,	best estimator xgboost's best error=0.8558
[flaml.automl: 09-16 13:10:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:10:05] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.8558,	best estimator xgboost's best error=0.8558
[flaml.automl: 09-16 13:10:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:10:08] {3072} INFO -  at 33.8s,	estimator xgboost's best error=0.8558,	best estimator xgboost's best error=0.8558
[flaml.automl: 09-16 13:10:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:10:11] {3072} INFO -  at 36.9s,	estimator xgboost's best error=0.8558,	best estimator xgboost's best error=0.8558
[flaml.automl: 09-16 13:10:11] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 13:10:16] {3072} INFO -  at 41.8s,	estimator xgboost's best error=0.8558,	best estimator xgboost's best error=0.8558
[flaml.automl: 09-16 13:10:16] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 13:10:18] {3072} INFO -  at 43.7s,	estimator xgboost's best error=0.8439,	best estimator xgboost's best error=0.8439
[flaml.automl: 09-16 13:10:18] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 13:10:19] {3072} INFO -  at 45.3s,	estimator xgboost's best error=0.8439,	best estimator xgboost's best error=0.8439
[flaml.automl: 09-16 13:10:19] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 13:10:24] {3072} INFO -  at 49.7s,	estimator xgboost's best error=0.8250,	best estimator xgboost's best error=0.8250
[flaml.automl: 09-16 13:10:24] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 13:10:25] {3072} INFO -  at 51.3s,	estimator xgboost's best error=0.8250,	best estimator xgboost's best error=0.8250
[flaml.automl: 09-16 13:10:25] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 13:10:34] {3072} INFO -  at 59.6s,	estimator xgboost's best error=0.7948,	best estimator xgboost's best error=0.7948
[flaml.automl: 09-16 13:10:47] {3335} INFO - retrain xgboost for 13.3s
[flaml.automl: 09-16 13:10:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:10:47] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:10:47] {2637} INFO - Time taken to find the best model: 59.597126483917236
[flaml.automl: 09-16 13:10:47] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 43906}
SO2(0)最佳损失：0.2052292696494845
SO2(0)最好结果：{'pred_time': 8.62411652268091e-06, 'wall_clock_time': 59.597126483917236, 'metric_for_logging': {'pred_time': 8.62411652268091e-06}, 'val_loss': 0.7947707303505155, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.002212987346630184, 'learning_rate': 1.0, 'subsample': 0.9736629142498529, 'colsample_bylevel': 0.8717925041332704, 'colsample_bytree': 0.9857450191529009, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04619111740498511, 'FLAML_sample_size': 43906}, 'config/n_estimators': 13, 'config/max_leaves': 18, 'config/min_child_weight': 0.002212987346630184, 'config/learning_rate': 1.0, 'config/subsample': 0.9736629142498529, 'config/colsample_bylevel': 0.8717925041332704, 'config/colsample_bytree': 0.9857450191529009, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.04619111740498511, 'config/FLAML_sample_size': 43906, 'experiment_tag': 'exp', 'time_total_s': 8.271354913711548}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8717925041332704, colsample_bynode=1,
             colsample_bytree=0.9857450191529009, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.002212987346630184,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04619111740498511, scale_pos_weight=1,
             subsample=0.9736629142498529, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7979965120186694
SO2(0)的mse=1.6865569146620738
SO2(0)的mae=0.7841380753215444
SO2(0)的mar=0.138501855998742
总共花费的时间为：73.60
淮安市
1210A
1211A
1213A
1214A
3426A
[flaml.automl: 09-16 13:26:34] {2390} INFO - task = regression
[flaml.automl: 09-16 13:26:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:26:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:26:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:26:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:26:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:26:36] {3025} INFO - Estimated sufficient time budget=106065s. Estimated necessary time budget=106s.
[flaml.automl: 09-16 13:26:36] {3072} INFO -  at 2.2s,	estimator xgboost's best error=4.1910,	best estimator xgboost's best error=4.1910
[flaml.automl: 09-16 13:26:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:26:39] {3072} INFO -  at 5.6s,	estimator xgboost's best error=2.0116,	best estimator xgboost's best error=2.0116
[flaml.automl: 09-16 13:26:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:26:41] {3072} INFO -  at 7.5s,	estimator xgboost's best error=2.0116,	best estimator xgboost's best error=2.0116
[flaml.automl: 09-16 13:26:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:26:46] {3072} INFO -  at 12.1s,	estimator xgboost's best error=2.0116,	best estimator xgboost's best error=2.0116
[flaml.automl: 09-16 13:26:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:26:48] {3072} INFO -  at 14.2s,	estimator xgboost's best error=1.2713,	best estimator xgboost's best error=1.2713
[flaml.automl: 09-16 13:26:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:26:50] {3072} INFO -  at 16.5s,	estimator xgboost's best error=1.1341,	best estimator xgboost's best error=1.1341
[flaml.automl: 09-16 13:26:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:26:53] {3072} INFO -  at 19.0s,	estimator xgboost's best error=1.0454,	best estimator xgboost's best error=1.0454
[flaml.automl: 09-16 13:26:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:26:56] {3072} INFO -  at 22.1s,	estimator xgboost's best error=1.0454,	best estimator xgboost's best error=1.0454
[flaml.automl: 09-16 13:26:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:26:58] {3072} INFO -  at 24.5s,	estimator xgboost's best error=1.0454,	best estimator xgboost's best error=1.0454
[flaml.automl: 09-16 13:26:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:27:01] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.9603,	best estimator xgboost's best error=0.9603
[flaml.automl: 09-16 13:27:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:27:02] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.9603,	best estimator xgboost's best error=0.9603
[flaml.automl: 09-16 13:27:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:27:03] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.9603,	best estimator xgboost's best error=0.9603
[flaml.automl: 09-16 13:27:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:27:05] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.9603,	best estimator xgboost's best error=0.9603
[flaml.automl: 09-16 13:27:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:27:07] {3072} INFO -  at 33.4s,	estimator xgboost's best error=0.9603,	best estimator xgboost's best error=0.9603
[flaml.automl: 09-16 13:27:07] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:27:10] {3072} INFO -  at 36.4s,	estimator xgboost's best error=0.9603,	best estimator xgboost's best error=0.9603
[flaml.automl: 09-16 13:27:10] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 13:27:15] {3072} INFO -  at 41.2s,	estimator xgboost's best error=0.9603,	best estimator xgboost's best error=0.9603
[flaml.automl: 09-16 13:27:15] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 13:27:17] {3072} INFO -  at 43.1s,	estimator xgboost's best error=0.9603,	best estimator xgboost's best error=0.9603
[flaml.automl: 09-16 13:27:17] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 13:27:19] {3072} INFO -  at 44.7s,	estimator xgboost's best error=0.9603,	best estimator xgboost's best error=0.9603
[flaml.automl: 09-16 13:27:19] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 13:27:26] {3072} INFO -  at 52.0s,	estimator xgboost's best error=0.9603,	best estimator xgboost's best error=0.9603
[flaml.automl: 09-16 13:27:26] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 13:27:27] {3072} INFO -  at 53.2s,	estimator xgboost's best error=0.9603,	best estimator xgboost's best error=0.9603
[flaml.automl: 09-16 13:27:27] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 13:27:33] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.8924,	best estimator xgboost's best error=0.8924
[flaml.automl: 09-16 13:27:42] {3335} INFO - retrain xgboost for 8.9s
[flaml.automl: 09-16 13:27:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8105661245809422, colsample_bynode=1,
             colsample_bytree=0.9736248890816903, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.025507582899487728,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001173430399890411, reg_lambda=0.11802693660028775,
             scale_pos_weight=1, subsample=0.963672855918478,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 13:27:42] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:27:42] {2637} INFO - Time taken to find the best model: 59.65893530845642
[flaml.automl: 09-16 13:27:42] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 14, 'min_child_weight': 0.025507582899487728, 'learning_rate': 1.0, 'subsample': 0.963672855918478, 'colsample_bylevel': 0.8105661245809422, 'colsample_bytree': 0.9736248890816903, 'reg_alpha': 0.001173430399890411, 'reg_lambda': 0.11802693660028775, 'FLAML_sample_size': 55128}
SO2(0)最佳损失：0.10763724841520439
SO2(0)最好结果：{'pred_time': 6.908028490824362e-06, 'wall_clock_time': 59.65893530845642, 'metric_for_logging': {'pred_time': 6.908028490824362e-06}, 'val_loss': 0.8923627515847956, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 14, 'min_child_weight': 0.025507582899487728, 'learning_rate': 1.0, 'subsample': 0.963672855918478, 'colsample_bylevel': 0.8105661245809422, 'colsample_bytree': 0.9736248890816903, 'reg_alpha': 0.001173430399890411, 'reg_lambda': 0.11802693660028775, 'FLAML_sample_size': 55128}, 'config/n_estimators': 11, 'config/max_leaves': 14, 'config/min_child_weight': 0.025507582899487728, 'config/learning_rate': 1.0, 'config/subsample': 0.963672855918478, 'config/colsample_bylevel': 0.8105661245809422, 'config/colsample_bytree': 0.9736248890816903, 'config/reg_alpha': 0.001173430399890411, 'config/reg_lambda': 0.11802693660028775, 'config/FLAML_sample_size': 55128, 'experiment_tag': 'exp', 'time_total_s': 6.5016090869903564}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8105661245809422, colsample_bynode=1,
             colsample_bytree=0.9736248890816903, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.025507582899487728,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001173430399890411, reg_lambda=0.11802693660028775,
             scale_pos_weight=1, subsample=0.963672855918478,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.802407299692443
SO2(0)的mse=2.148331141094241
SO2(0)的mae=0.8784538432857233
SO2(0)的mar=0.15165891558748662
总共花费的时间为：69.92
盐城市
1215A
1216A
3293A
3436A
[flaml.automl: 09-16 13:40:22] {2390} INFO - task = regression
[flaml.automl: 09-16 13:40:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:40:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:40:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:40:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:40:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:40:25] {3025} INFO - Estimated sufficient time budget=95279s. Estimated necessary time budget=95s.
[flaml.automl: 09-16 13:40:25] {3072} INFO -  at 2.4s,	estimator xgboost's best error=3.2834,	best estimator xgboost's best error=3.2834
[flaml.automl: 09-16 13:40:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:40:28] {3072} INFO -  at 6.4s,	estimator xgboost's best error=1.5586,	best estimator xgboost's best error=1.5586
[flaml.automl: 09-16 13:40:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:40:31] {3072} INFO -  at 8.6s,	estimator xgboost's best error=1.5586,	best estimator xgboost's best error=1.5586
[flaml.automl: 09-16 13:40:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:40:36] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.5586,	best estimator xgboost's best error=1.5586
[flaml.automl: 09-16 13:40:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:40:38] {3072} INFO -  at 16.2s,	estimator xgboost's best error=0.9606,	best estimator xgboost's best error=0.9606
[flaml.automl: 09-16 13:40:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:40:43] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.9606,	best estimator xgboost's best error=0.9606
[flaml.automl: 09-16 13:40:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:40:46] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.8225,	best estimator xgboost's best error=0.8225
[flaml.automl: 09-16 13:40:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:40:49] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.8225,	best estimator xgboost's best error=0.8225
[flaml.automl: 09-16 13:40:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:40:52] {3072} INFO -  at 30.1s,	estimator xgboost's best error=0.8225,	best estimator xgboost's best error=0.8225
[flaml.automl: 09-16 13:40:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:40:55] {3072} INFO -  at 32.7s,	estimator xgboost's best error=0.8225,	best estimator xgboost's best error=0.8225
[flaml.automl: 09-16 13:40:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:40:57] {3072} INFO -  at 35.2s,	estimator xgboost's best error=0.8225,	best estimator xgboost's best error=0.8225
[flaml.automl: 09-16 13:40:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:41:02] {3072} INFO -  at 39.6s,	estimator xgboost's best error=0.6916,	best estimator xgboost's best error=0.6916
[flaml.automl: 09-16 13:41:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:41:04] {3072} INFO -  at 41.6s,	estimator xgboost's best error=0.6916,	best estimator xgboost's best error=0.6916
[flaml.automl: 09-16 13:41:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:41:20] {3072} INFO -  at 57.8s,	estimator xgboost's best error=0.5830,	best estimator xgboost's best error=0.5830
[flaml.automl: 09-16 13:41:39] {3335} INFO - retrain xgboost for 19.0s
[flaml.automl: 09-16 13:41:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:41:39] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:41:39] {2637} INFO - Time taken to find the best model: 57.84079885482788
[flaml.automl: 09-16 13:41:39] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 43822}
SO2(0)最佳损失：0.41699837169608056
SO2(0)最好结果：{'pred_time': 2.6185801386588407e-05, 'wall_clock_time': 57.84079885482788, 'metric_for_logging': {'pred_time': 2.6185801386588407e-05}, 'val_loss': 0.5830016283039194, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 43822}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 43822, 'experiment_tag': 'exp', 'time_total_s': 16.2004177570343}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8851273958760222
SO2(0)的mse=1.0383352061269455
SO2(0)的mae=0.5838274814971954
SO2(0)的mar=0.12445077446180486
总共花费的时间为：77.63
宿迁市
3191A
[flaml.automl: 09-16 13:45:29] {2390} INFO - task = regression
[flaml.automl: 09-16 13:45:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 13:45:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 13:45:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 13:45:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 13:45:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 13:45:31] {3025} INFO - Estimated sufficient time budget=20913s. Estimated necessary time budget=21s.
[flaml.automl: 09-16 13:45:31] {3072} INFO -  at 2.1s,	estimator xgboost's best error=3.3534,	best estimator xgboost's best error=3.3534
[flaml.automl: 09-16 13:45:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 13:45:34] {3072} INFO -  at 5.5s,	estimator xgboost's best error=1.7940,	best estimator xgboost's best error=1.7940
[flaml.automl: 09-16 13:45:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 13:45:36] {3072} INFO -  at 7.5s,	estimator xgboost's best error=1.7940,	best estimator xgboost's best error=1.7940
[flaml.automl: 09-16 13:45:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 13:45:45] {3072} INFO -  at 16.4s,	estimator xgboost's best error=1.7940,	best estimator xgboost's best error=1.7940
[flaml.automl: 09-16 13:45:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 13:45:46] {3072} INFO -  at 17.5s,	estimator xgboost's best error=0.9967,	best estimator xgboost's best error=0.9967
[flaml.automl: 09-16 13:45:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 13:45:48] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.8294,	best estimator xgboost's best error=0.8294
[flaml.automl: 09-16 13:45:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 13:45:50] {3072} INFO -  at 20.7s,	estimator xgboost's best error=0.8294,	best estimator xgboost's best error=0.8294
[flaml.automl: 09-16 13:45:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 13:45:52] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.8294,	best estimator xgboost's best error=0.8294
[flaml.automl: 09-16 13:45:52] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 13:45:53] {3072} INFO -  at 23.9s,	estimator xgboost's best error=0.8294,	best estimator xgboost's best error=0.8294
[flaml.automl: 09-16 13:45:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 13:45:55] {3072} INFO -  at 26.3s,	estimator xgboost's best error=0.8294,	best estimator xgboost's best error=0.8294
[flaml.automl: 09-16 13:45:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 13:45:56] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.8294,	best estimator xgboost's best error=0.8294
[flaml.automl: 09-16 13:45:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 13:45:57] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.8294,	best estimator xgboost's best error=0.8294
[flaml.automl: 09-16 13:45:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 13:46:03] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.7725,	best estimator xgboost's best error=0.7725
[flaml.automl: 09-16 13:46:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 13:46:15] {3072} INFO -  at 45.9s,	estimator xgboost's best error=0.7651,	best estimator xgboost's best error=0.7651
[flaml.automl: 09-16 13:46:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 13:46:24] {3072} INFO -  at 54.8s,	estimator xgboost's best error=0.7651,	best estimator xgboost's best error=0.7651
[flaml.automl: 09-16 13:46:41] {3335} INFO - retrain xgboost for 17.0s
[flaml.automl: 09-16 13:46:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 13:46:41] {2636} INFO - fit succeeded
[flaml.automl: 09-16 13:46:41] {2637} INFO - Time taken to find the best model: 45.86389374732971
[flaml.automl: 09-16 13:46:41] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
SO2(0)最佳损失：0.23494973124289997
SO2(0)最好结果：{'pred_time': 6.73066353311344e-05, 'wall_clock_time': 45.86389374732971, 'metric_for_logging': {'pred_time': 6.73066353311344e-05}, 'val_loss': 0.7650502687571, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 17, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 11.806933641433716}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7847862710254104
SO2(0)的mse=1.3696368629473619
SO2(0)的mae=0.7543205995055177
SO2(0)的mar=0.15248032444630472
总共花费的时间为：72.07
杭州市
1227A
1231A
1232A
3557A
3558A
3656A
[flaml.automl: 09-16 14:05:48] {2390} INFO - task = regression
[flaml.automl: 09-16 14:05:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:05:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:05:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:05:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:05:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:05:50] {3025} INFO - Estimated sufficient time budget=150297s. Estimated necessary time budget=150s.
[flaml.automl: 09-16 14:05:50] {3072} INFO -  at 2.5s,	estimator xgboost's best error=3.4202,	best estimator xgboost's best error=3.4202
[flaml.automl: 09-16 14:05:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:05:54] {3072} INFO -  at 6.4s,	estimator xgboost's best error=1.7692,	best estimator xgboost's best error=1.7692
[flaml.automl: 09-16 14:05:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:05:57] {3072} INFO -  at 9.6s,	estimator xgboost's best error=1.7692,	best estimator xgboost's best error=1.7692
[flaml.automl: 09-16 14:05:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:06:00] {3072} INFO -  at 12.7s,	estimator xgboost's best error=1.7692,	best estimator xgboost's best error=1.7692
[flaml.automl: 09-16 14:06:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:06:03] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.7185,	best estimator xgboost's best error=0.7185
[flaml.automl: 09-16 14:06:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:06:06] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.6876,	best estimator xgboost's best error=0.6876
[flaml.automl: 09-16 14:06:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:06:09] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.6876,	best estimator xgboost's best error=0.6876
[flaml.automl: 09-16 14:06:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:06:11] {3072} INFO -  at 23.7s,	estimator xgboost's best error=0.6876,	best estimator xgboost's best error=0.6876
[flaml.automl: 09-16 14:06:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:06:13] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.6876,	best estimator xgboost's best error=0.6876
[flaml.automl: 09-16 14:06:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:06:15] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.6876,	best estimator xgboost's best error=0.6876
[flaml.automl: 09-16 14:06:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:06:20] {3072} INFO -  at 32.7s,	estimator xgboost's best error=0.6415,	best estimator xgboost's best error=0.6415
[flaml.automl: 09-16 14:06:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:06:23] {3072} INFO -  at 35.4s,	estimator xgboost's best error=0.6415,	best estimator xgboost's best error=0.6415
[flaml.automl: 09-16 14:06:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:06:32] {3072} INFO -  at 44.6s,	estimator xgboost's best error=0.4293,	best estimator xgboost's best error=0.4293
[flaml.automl: 09-16 14:06:32] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:06:47] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.4206,	best estimator xgboost's best error=0.4206
[flaml.automl: 09-16 14:07:06] {3335} INFO - retrain xgboost for 18.7s
[flaml.automl: 09-16 14:07:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:07:06] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:07:06] {2637} INFO - Time taken to find the best model: 59.512980461120605
[flaml.automl: 09-16 14:07:06] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 67680}
SO2(0)最佳损失：0.5794104043354207
SO2(0)最好结果：{'pred_time': 9.280604070971073e-06, 'wall_clock_time': 59.512980461120605, 'metric_for_logging': {'pred_time': 9.280604070971073e-06}, 'val_loss': 0.4205895956645794, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 67680}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 67680, 'experiment_tag': 'exp', 'time_total_s': 14.917603731155396}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8893108026364288
SO2(0)的mse=0.4091277429893369
SO2(0)的mae=0.4212587544540288
SO2(0)的mar=0.07915432431929943
总共花费的时间为：79.80
宁波市
1235A
1236A
1239A
1240A
2871A
3710A
[flaml.automl: 09-16 14:25:22] {2390} INFO - task = regression
[flaml.automl: 09-16 14:25:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:25:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:25:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:25:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:25:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:25:23] {3025} INFO - Estimated sufficient time budget=78858s. Estimated necessary time budget=79s.
[flaml.automl: 09-16 14:25:23] {3072} INFO -  at 1.5s,	estimator xgboost's best error=4.9016,	best estimator xgboost's best error=4.9016
[flaml.automl: 09-16 14:25:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:25:25] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.2008,	best estimator xgboost's best error=2.2008
[flaml.automl: 09-16 14:25:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:25:26] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.2008,	best estimator xgboost's best error=2.2008
[flaml.automl: 09-16 14:25:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:25:30] {3072} INFO -  at 8.7s,	estimator xgboost's best error=2.2008,	best estimator xgboost's best error=2.2008
[flaml.automl: 09-16 14:25:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:25:31] {3072} INFO -  at 9.9s,	estimator xgboost's best error=1.0983,	best estimator xgboost's best error=1.0983
[flaml.automl: 09-16 14:25:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:25:33] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.8762,	best estimator xgboost's best error=0.8762
[flaml.automl: 09-16 14:25:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:25:34] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.8762,	best estimator xgboost's best error=0.8762
[flaml.automl: 09-16 14:25:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:25:37] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.8762,	best estimator xgboost's best error=0.8762
[flaml.automl: 09-16 14:25:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:25:38] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.8762,	best estimator xgboost's best error=0.8762
[flaml.automl: 09-16 14:25:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:25:40] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.8762,	best estimator xgboost's best error=0.8762
[flaml.automl: 09-16 14:25:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:25:42] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.8762,	best estimator xgboost's best error=0.8762
[flaml.automl: 09-16 14:25:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:25:43] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.8762,	best estimator xgboost's best error=0.8762
[flaml.automl: 09-16 14:25:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:25:50] {3072} INFO -  at 28.3s,	estimator xgboost's best error=0.8216,	best estimator xgboost's best error=0.8216
[flaml.automl: 09-16 14:25:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:26:05] {3072} INFO -  at 43.3s,	estimator xgboost's best error=0.7645,	best estimator xgboost's best error=0.7645
[flaml.automl: 09-16 14:26:05] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 14:26:16] {3072} INFO -  at 54.4s,	estimator xgboost's best error=0.7645,	best estimator xgboost's best error=0.7645
[flaml.automl: 09-16 14:26:35] {3335} INFO - retrain xgboost for 19.0s
[flaml.automl: 09-16 14:26:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:26:35] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:26:35] {2637} INFO - Time taken to find the best model: 43.31073045730591
[flaml.automl: 09-16 14:26:35] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 66141}
SO2(0)最佳损失：0.23553846120542365
SO2(0)最好结果：{'pred_time': 1.1068539548721553e-05, 'wall_clock_time': 43.31073045730591, 'metric_for_logging': {'pred_time': 1.1068539548721553e-05}, 'val_loss': 0.7644615387945763, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 66141}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 66141, 'experiment_tag': 'exp', 'time_total_s': 15.012617826461792}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7536502765781817
SO2(0)的mse=1.8381523166113478
SO2(0)的mae=0.791945652228315
SO2(0)的mar=0.09396500325969391
总共花费的时间为：74.83
温州市
1242A
1243A
1244A
[flaml.automl: 09-16 14:35:46] {2390} INFO - task = regression
[flaml.automl: 09-16 14:35:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:35:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:35:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:35:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:35:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:35:47] {3025} INFO - Estimated sufficient time budget=11817s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 14:35:47] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.2703,	best estimator xgboost's best error=3.2703
[flaml.automl: 09-16 14:35:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:35:50] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.4656,	best estimator xgboost's best error=1.4656
[flaml.automl: 09-16 14:35:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:35:51] {3072} INFO -  at 4.5s,	estimator xgboost's best error=1.4656,	best estimator xgboost's best error=1.4656
[flaml.automl: 09-16 14:35:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:36:00] {3072} INFO -  at 14.1s,	estimator xgboost's best error=1.4656,	best estimator xgboost's best error=1.4656
[flaml.automl: 09-16 14:36:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:36:01] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.6278,	best estimator xgboost's best error=0.6278
[flaml.automl: 09-16 14:36:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:36:03] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.4827,	best estimator xgboost's best error=0.4827
[flaml.automl: 09-16 14:36:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:36:05] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.4471,	best estimator xgboost's best error=0.4471
[flaml.automl: 09-16 14:36:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:36:07] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.4471,	best estimator xgboost's best error=0.4471
[flaml.automl: 09-16 14:36:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:36:09] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.3683,	best estimator xgboost's best error=0.3683
[flaml.automl: 09-16 14:36:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:36:12] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.3683,	best estimator xgboost's best error=0.3683
[flaml.automl: 09-16 14:36:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:36:13] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.3632,	best estimator xgboost's best error=0.3632
[flaml.automl: 09-16 14:36:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:36:14] {3072} INFO -  at 28.3s,	estimator xgboost's best error=0.3632,	best estimator xgboost's best error=0.3632
[flaml.automl: 09-16 14:36:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:36:20] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.3632,	best estimator xgboost's best error=0.3632
[flaml.automl: 09-16 14:36:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:36:23] {3072} INFO -  at 37.3s,	estimator xgboost's best error=0.3632,	best estimator xgboost's best error=0.3632
[flaml.automl: 09-16 14:36:23] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 14:36:25] {3072} INFO -  at 38.6s,	estimator xgboost's best error=0.3628,	best estimator xgboost's best error=0.3628
[flaml.automl: 09-16 14:36:25] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 14:36:27] {3072} INFO -  at 40.4s,	estimator xgboost's best error=0.3628,	best estimator xgboost's best error=0.3628
[flaml.automl: 09-16 14:36:27] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 14:36:28] {3072} INFO -  at 41.5s,	estimator xgboost's best error=0.3628,	best estimator xgboost's best error=0.3628
[flaml.automl: 09-16 14:36:28] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 14:36:29] {3072} INFO -  at 42.7s,	estimator xgboost's best error=0.3628,	best estimator xgboost's best error=0.3628
[flaml.automl: 09-16 14:36:29] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 14:36:31] {3072} INFO -  at 45.3s,	estimator xgboost's best error=0.3628,	best estimator xgboost's best error=0.3628
[flaml.automl: 09-16 14:36:31] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 14:36:32] {3072} INFO -  at 46.2s,	estimator xgboost's best error=0.3628,	best estimator xgboost's best error=0.3628
[flaml.automl: 09-16 14:36:32] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 14:36:36] {3072} INFO -  at 49.8s,	estimator xgboost's best error=0.3628,	best estimator xgboost's best error=0.3628
[flaml.automl: 09-16 14:36:36] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 14:36:41] {3072} INFO -  at 55.2s,	estimator xgboost's best error=0.3628,	best estimator xgboost's best error=0.3628
[flaml.automl: 09-16 14:36:41] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 14:36:42] {3072} INFO -  at 56.3s,	estimator xgboost's best error=0.3628,	best estimator xgboost's best error=0.3628
[flaml.automl: 09-16 14:36:42] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-16 14:36:46] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.3628,	best estimator xgboost's best error=0.3628
[flaml.automl: 09-16 14:36:47] {3335} INFO - retrain xgboost for 1.2s
[flaml.automl: 09-16 14:36:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6329062763927384, colsample_bynode=1,
             colsample_bytree=0.6884216748557935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=5, min_child_weight=42.237573796052445,
             missing=nan, monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.011845932712887964, reg_lambda=120.89490182618881,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:36:47] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:36:47] {2637} INFO - Time taken to find the best model: 38.55918550491333
SO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 5, 'min_child_weight': 42.237573796052445, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.6329062763927384, 'colsample_bytree': 0.6884216748557935, 'reg_alpha': 0.011845932712887964, 'reg_lambda': 120.89490182618881}
SO2(0)最佳损失：0.6372169376924541
SO2(0)最好结果：{'pred_time': 1.0585259789265263e-05, 'wall_clock_time': 38.55918550491333, 'metric_for_logging': {'pred_time': 1.0585259789265263e-05}, 'val_loss': 0.3627830623075458, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 5, 'min_child_weight': 42.237573796052445, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.6329062763927384, 'colsample_bytree': 0.6884216748557935, 'reg_alpha': 0.011845932712887964, 'reg_lambda': 120.89490182618881}, 'config/n_estimators': 4, 'config/max_leaves': 5, 'config/min_child_weight': 42.237573796052445, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.6329062763927384, 'config/colsample_bytree': 0.6884216748557935, 'config/reg_alpha': 0.011845932712887964, 'config/reg_lambda': 120.89490182618881, 'experiment_tag': 'exp', 'time_total_s': 1.2563755512237549}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6329062763927384, colsample_bynode=1,
             colsample_bytree=0.6884216748557935, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=5, min_child_weight=42.237573796052445,
             missing=nan, monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.011845932712887964, reg_lambda=120.89490182618881,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.823009161120665
SO2(0)的mse=0.31913819899608814
SO2(0)的mae=0.3469716762435736
SO2(0)的mar=0.06737089082509475
总共花费的时间为：61.38
绍兴市
2921A
3408A
3409A
3410A
3411A
3560A
3658A
[flaml.automl: 09-16 14:58:26] {2390} INFO - task = regression
[flaml.automl: 09-16 14:58:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 14:58:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 14:58:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 14:58:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 14:58:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 14:58:27] {3025} INFO - Estimated sufficient time budget=105426s. Estimated necessary time budget=105s.
[flaml.automl: 09-16 14:58:27] {3072} INFO -  at 1.8s,	estimator xgboost's best error=3.8260,	best estimator xgboost's best error=3.8260
[flaml.automl: 09-16 14:58:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 14:58:29] {3072} INFO -  at 3.9s,	estimator xgboost's best error=1.7154,	best estimator xgboost's best error=1.7154
[flaml.automl: 09-16 14:58:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 14:58:30] {3072} INFO -  at 5.0s,	estimator xgboost's best error=1.7154,	best estimator xgboost's best error=1.7154
[flaml.automl: 09-16 14:58:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 14:58:34] {3072} INFO -  at 8.3s,	estimator xgboost's best error=1.7154,	best estimator xgboost's best error=1.7154
[flaml.automl: 09-16 14:58:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 14:58:35] {3072} INFO -  at 9.4s,	estimator xgboost's best error=0.8086,	best estimator xgboost's best error=0.8086
[flaml.automl: 09-16 14:58:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 14:58:36] {3072} INFO -  at 10.9s,	estimator xgboost's best error=0.7396,	best estimator xgboost's best error=0.7396
[flaml.automl: 09-16 14:58:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 14:58:38] {3072} INFO -  at 12.6s,	estimator xgboost's best error=0.6052,	best estimator xgboost's best error=0.6052
[flaml.automl: 09-16 14:58:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 14:58:41] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.6052,	best estimator xgboost's best error=0.6052
[flaml.automl: 09-16 14:58:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 14:58:42] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.6052,	best estimator xgboost's best error=0.6052
[flaml.automl: 09-16 14:58:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 14:58:45] {3072} INFO -  at 19.4s,	estimator xgboost's best error=0.6052,	best estimator xgboost's best error=0.6052
[flaml.automl: 09-16 14:58:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 14:58:46] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.6052,	best estimator xgboost's best error=0.6052
[flaml.automl: 09-16 14:58:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 14:58:48] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.6052,	best estimator xgboost's best error=0.6052
[flaml.automl: 09-16 14:58:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 14:58:49] {3072} INFO -  at 23.7s,	estimator xgboost's best error=0.6052,	best estimator xgboost's best error=0.6052
[flaml.automl: 09-16 14:58:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 14:58:56] {3072} INFO -  at 30.6s,	estimator xgboost's best error=0.5153,	best estimator xgboost's best error=0.5153
[flaml.automl: 09-16 14:58:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 14:59:09] {3072} INFO -  at 43.5s,	estimator xgboost's best error=0.5134,	best estimator xgboost's best error=0.5134
[flaml.automl: 09-16 14:59:09] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 14:59:16] {3072} INFO -  at 50.4s,	estimator xgboost's best error=0.5134,	best estimator xgboost's best error=0.5134
[flaml.automl: 09-16 14:59:28] {3335} INFO - retrain xgboost for 12.6s
[flaml.automl: 09-16 14:59:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 14:59:28] {2636} INFO - fit succeeded
[flaml.automl: 09-16 14:59:28] {2637} INFO - Time taken to find the best model: 43.4883599281311
[flaml.automl: 09-16 14:59:28] {2648} WARNING - Time taken to find the best model is 72% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 77883}
SO2(0)最佳损失：0.4865838098889662
SO2(0)最好结果：{'pred_time': 4.618333728808797e-06, 'wall_clock_time': 43.4883599281311, 'metric_for_logging': {'pred_time': 4.618333728808797e-06}, 'val_loss': 0.5134161901110338, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 77883}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 77883, 'experiment_tag': 'exp', 'time_total_s': 12.874953985214233}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.790245833136105
SO2(0)的mse=0.7985306416558979
SO2(0)的mae=0.5213129955054123
SO2(0)的mar=0.08155121097836135
总共花费的时间为：64.26
湖州市
1250A
3562A
[flaml.automl: 09-16 15:06:09] {2390} INFO - task = regression
[flaml.automl: 09-16 15:06:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:06:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:06:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:06:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:06:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:06:11] {3025} INFO - Estimated sufficient time budget=22372s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 15:06:11] {3072} INFO -  at 2.3s,	estimator xgboost's best error=3.4806,	best estimator xgboost's best error=3.4806
[flaml.automl: 09-16 15:06:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:06:15] {3072} INFO -  at 6.2s,	estimator xgboost's best error=1.5881,	best estimator xgboost's best error=1.5881
[flaml.automl: 09-16 15:06:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:06:17] {3072} INFO -  at 8.6s,	estimator xgboost's best error=1.5881,	best estimator xgboost's best error=1.5881
[flaml.automl: 09-16 15:06:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:06:29] {3072} INFO -  at 20.1s,	estimator xgboost's best error=1.5881,	best estimator xgboost's best error=1.5881
[flaml.automl: 09-16 15:06:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:06:30] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.9322,	best estimator xgboost's best error=0.9322
[flaml.automl: 09-16 15:06:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:06:32] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.8490,	best estimator xgboost's best error=0.8490
[flaml.automl: 09-16 15:06:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:06:33] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.7784,	best estimator xgboost's best error=0.7784
[flaml.automl: 09-16 15:06:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:06:36] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.7784,	best estimator xgboost's best error=0.7784
[flaml.automl: 09-16 15:06:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:06:37] {3072} INFO -  at 28.7s,	estimator xgboost's best error=0.7784,	best estimator xgboost's best error=0.7784
[flaml.automl: 09-16 15:06:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:06:40] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.7403,	best estimator xgboost's best error=0.7403
[flaml.automl: 09-16 15:06:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:06:42] {3072} INFO -  at 33.2s,	estimator xgboost's best error=0.7403,	best estimator xgboost's best error=0.7403
[flaml.automl: 09-16 15:06:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:06:43] {3072} INFO -  at 34.4s,	estimator xgboost's best error=0.7403,	best estimator xgboost's best error=0.7403
[flaml.automl: 09-16 15:06:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:06:55] {3072} INFO -  at 46.1s,	estimator xgboost's best error=0.7149,	best estimator xgboost's best error=0.7149
[flaml.automl: 09-16 15:06:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 15:07:09] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.6848,	best estimator xgboost's best error=0.6848
[flaml.automl: 09-16 15:07:30] {3335} INFO - retrain xgboost for 21.8s
[flaml.automl: 09-16 15:07:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:07:30] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:07:30] {2637} INFO - Time taken to find the best model: 59.828344106674194
[flaml.automl: 09-16 15:07:30] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
SO2(0)最佳损失：0.31516448538992314
SO2(0)最好结果：{'pred_time': 1.5614094672264992e-05, 'wall_clock_time': 59.828344106674194, 'metric_for_logging': {'pred_time': 1.5614094672264992e-05}, 'val_loss': 0.6848355146100769, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 13.677821159362793}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7962557953211221
SO2(0)的mse=1.1035179464650138
SO2(0)的mae=0.6455566129430181
SO2(0)的mar=0.11226751698582457
总共花费的时间为：82.00
嘉兴市
1253A
3407A
[flaml.automl: 09-16 15:13:56] {2390} INFO - task = regression
[flaml.automl: 09-16 15:13:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:13:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:13:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:13:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:13:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:13:58] {3025} INFO - Estimated sufficient time budget=21972s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 15:13:58] {3072} INFO -  at 2.3s,	estimator xgboost's best error=4.1540,	best estimator xgboost's best error=4.1540
[flaml.automl: 09-16 15:13:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:14:02] {3072} INFO -  at 6.2s,	estimator xgboost's best error=1.8976,	best estimator xgboost's best error=1.8976
[flaml.automl: 09-16 15:14:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:14:04] {3072} INFO -  at 8.4s,	estimator xgboost's best error=1.8976,	best estimator xgboost's best error=1.8976
[flaml.automl: 09-16 15:14:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:14:20] {3072} INFO -  at 24.2s,	estimator xgboost's best error=1.8976,	best estimator xgboost's best error=1.8976
[flaml.automl: 09-16 15:14:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:14:21] {3072} INFO -  at 25.3s,	estimator xgboost's best error=1.0679,	best estimator xgboost's best error=1.0679
[flaml.automl: 09-16 15:14:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:14:22] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.9089,	best estimator xgboost's best error=0.9089
[flaml.automl: 09-16 15:14:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:14:24] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.8310,	best estimator xgboost's best error=0.8310
[flaml.automl: 09-16 15:14:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:14:27] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.8310,	best estimator xgboost's best error=0.8310
[flaml.automl: 09-16 15:14:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:14:28] {3072} INFO -  at 32.7s,	estimator xgboost's best error=0.8310,	best estimator xgboost's best error=0.8310
[flaml.automl: 09-16 15:14:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:14:31] {3072} INFO -  at 35.7s,	estimator xgboost's best error=0.7785,	best estimator xgboost's best error=0.7785
[flaml.automl: 09-16 15:14:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:14:33] {3072} INFO -  at 37.3s,	estimator xgboost's best error=0.7785,	best estimator xgboost's best error=0.7785
[flaml.automl: 09-16 15:14:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:14:34] {3072} INFO -  at 38.4s,	estimator xgboost's best error=0.7785,	best estimator xgboost's best error=0.7785
[flaml.automl: 09-16 15:14:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:14:46] {3072} INFO -  at 50.3s,	estimator xgboost's best error=0.7238,	best estimator xgboost's best error=0.7238
[flaml.automl: 09-16 15:14:58] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-16 15:14:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 15:14:58] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:14:58] {2637} INFO - Time taken to find the best model: 50.260695934295654
[flaml.automl: 09-16 15:14:58] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
SO2(0)最佳损失：0.2761610120937952
SO2(0)最好结果：{'pred_time': 1.5383404023891987e-05, 'wall_clock_time': 50.260695934295654, 'metric_for_logging': {'pred_time': 1.5383404023891987e-05}, 'val_loss': 0.7238389879062048, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 11.838871002197266}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8192421188753338
SO2(0)的mse=1.3557684941644224
SO2(0)的mae=0.7496207226061112
SO2(0)的mar=0.11709077441095508
总共花费的时间为：62.54
台州市
1256A
1257A
3564A
[flaml.automl: 09-16 15:24:22] {2390} INFO - task = regression
[flaml.automl: 09-16 15:24:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:24:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:24:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:24:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:24:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:24:24] {3025} INFO - Estimated sufficient time budget=22562s. Estimated necessary time budget=23s.
[flaml.automl: 09-16 15:24:24] {3072} INFO -  at 2.4s,	estimator xgboost's best error=2.9136,	best estimator xgboost's best error=2.9136
[flaml.automl: 09-16 15:24:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:24:28] {3072} INFO -  at 6.3s,	estimator xgboost's best error=1.3325,	best estimator xgboost's best error=1.3325
[flaml.automl: 09-16 15:24:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:24:31] {3072} INFO -  at 8.5s,	estimator xgboost's best error=1.3325,	best estimator xgboost's best error=1.3325
[flaml.automl: 09-16 15:24:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:24:49] {3072} INFO -  at 26.9s,	estimator xgboost's best error=1.3325,	best estimator xgboost's best error=1.3325
[flaml.automl: 09-16 15:24:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:24:51] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.6932,	best estimator xgboost's best error=0.6932
[flaml.automl: 09-16 15:24:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:24:54] {3072} INFO -  at 32.1s,	estimator xgboost's best error=0.5806,	best estimator xgboost's best error=0.5806
[flaml.automl: 09-16 15:24:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:24:57] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.5424,	best estimator xgboost's best error=0.5424
[flaml.automl: 09-16 15:24:57] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:25:02] {3072} INFO -  at 40.1s,	estimator xgboost's best error=0.5424,	best estimator xgboost's best error=0.5424
[flaml.automl: 09-16 15:25:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:25:05] {3072} INFO -  at 43.2s,	estimator xgboost's best error=0.5424,	best estimator xgboost's best error=0.5424
[flaml.automl: 09-16 15:25:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:25:08] {3072} INFO -  at 46.2s,	estimator xgboost's best error=0.5424,	best estimator xgboost's best error=0.5424
[flaml.automl: 09-16 15:25:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:25:10] {3072} INFO -  at 47.6s,	estimator xgboost's best error=0.5424,	best estimator xgboost's best error=0.5424
[flaml.automl: 09-16 15:25:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:25:11] {3072} INFO -  at 48.8s,	estimator xgboost's best error=0.5424,	best estimator xgboost's best error=0.5424
[flaml.automl: 09-16 15:25:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:25:18] {3072} INFO -  at 55.6s,	estimator xgboost's best error=0.4836,	best estimator xgboost's best error=0.4836
[flaml.automl: 09-16 15:25:24] {3335} INFO - retrain xgboost for 6.8s
[flaml.automl: 09-16 15:25:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 15:25:24] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:25:24] {2637} INFO - Time taken to find the best model: 55.55792427062988
[flaml.automl: 09-16 15:25:24] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386}
SO2(0)最佳损失：0.516371807207132
SO2(0)最好结果：{'pred_time': 1.0363805472723166e-05, 'wall_clock_time': 55.55792427062988, 'metric_for_logging': {'pred_time': 1.0363805472723166e-05}, 'val_loss': 0.48362819279286795, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'experiment_tag': 'exp', 'time_total_s': 6.7909324169158936}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8036530684560925
SO2(0)的mse=0.5625619862872114
SO2(0)的mae=0.4769283647681971
SO2(0)的mar=0.10822960821640684
总共花费的时间为：62.91
舟山市
1258A
1259A
[flaml.automl: 09-16 15:32:03] {2390} INFO - task = regression
[flaml.automl: 09-16 15:32:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:32:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:32:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:32:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:32:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:32:06] {3025} INFO - Estimated sufficient time budget=23010s. Estimated necessary time budget=23s.
[flaml.automl: 09-16 15:32:06] {3072} INFO -  at 2.4s,	estimator xgboost's best error=2.5221,	best estimator xgboost's best error=2.5221
[flaml.automl: 09-16 15:32:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:32:10] {3072} INFO -  at 6.3s,	estimator xgboost's best error=1.1622,	best estimator xgboost's best error=1.1622
[flaml.automl: 09-16 15:32:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:32:12] {3072} INFO -  at 8.5s,	estimator xgboost's best error=1.1622,	best estimator xgboost's best error=1.1622
[flaml.automl: 09-16 15:32:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:32:29] {3072} INFO -  at 26.0s,	estimator xgboost's best error=1.1622,	best estimator xgboost's best error=1.1622
[flaml.automl: 09-16 15:32:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:32:31] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.5618,	best estimator xgboost's best error=0.5618
[flaml.automl: 09-16 15:32:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:32:34] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.4591,	best estimator xgboost's best error=0.4591
[flaml.automl: 09-16 15:32:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:32:37] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.4175,	best estimator xgboost's best error=0.4175
[flaml.automl: 09-16 15:32:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:32:42] {3072} INFO -  at 38.9s,	estimator xgboost's best error=0.4175,	best estimator xgboost's best error=0.4175
[flaml.automl: 09-16 15:32:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:32:45] {3072} INFO -  at 41.9s,	estimator xgboost's best error=0.3981,	best estimator xgboost's best error=0.3981
[flaml.automl: 09-16 15:32:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:32:51] {3072} INFO -  at 47.6s,	estimator xgboost's best error=0.3879,	best estimator xgboost's best error=0.3879
[flaml.automl: 09-16 15:32:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:32:53] {3072} INFO -  at 49.7s,	estimator xgboost's best error=0.3879,	best estimator xgboost's best error=0.3879
[flaml.automl: 09-16 15:32:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:33:03] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.3587,	best estimator xgboost's best error=0.3587
[flaml.automl: 09-16 15:33:15] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-16 15:33:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=15, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 15:33:15] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:33:15] {2637} INFO - Time taken to find the best model: 59.26235342025757
[flaml.automl: 09-16 15:33:15] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
SO2(0)最佳损失：0.6412544194482317
SO2(0)最好结果：{'pred_time': 1.6251929709073403e-05, 'wall_clock_time': 59.26235342025757, 'metric_for_logging': {'pred_time': 1.6251929709073403e-05}, 'val_loss': 0.35874558055176825, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 9.511183738708496}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=15, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8752994675254424
SO2(0)的mse=0.2988769001432874
SO2(0)的mae=0.36448864262925074
SO2(0)的mar=0.10912765572429459
总共花费的时间为：71.62
金华市
金华市没有数据
衢州市
1264A
1265A
[flaml.automl: 09-16 15:40:12] {2390} INFO - task = regression
[flaml.automl: 09-16 15:40:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:40:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:40:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:40:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:40:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:40:15] {3025} INFO - Estimated sufficient time budget=22204s. Estimated necessary time budget=22s.
[flaml.automl: 09-16 15:40:15] {3072} INFO -  at 2.3s,	estimator xgboost's best error=3.5693,	best estimator xgboost's best error=3.5693
[flaml.automl: 09-16 15:40:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:40:19] {3072} INFO -  at 6.3s,	estimator xgboost's best error=1.7833,	best estimator xgboost's best error=1.7833
[flaml.automl: 09-16 15:40:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:40:21] {3072} INFO -  at 8.6s,	estimator xgboost's best error=1.7833,	best estimator xgboost's best error=1.7833
[flaml.automl: 09-16 15:40:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:40:40] {3072} INFO -  at 28.1s,	estimator xgboost's best error=1.7833,	best estimator xgboost's best error=1.7833
[flaml.automl: 09-16 15:40:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:40:43] {3072} INFO -  at 31.3s,	estimator xgboost's best error=1.3419,	best estimator xgboost's best error=1.3419
[flaml.automl: 09-16 15:40:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:40:48] {3072} INFO -  at 35.6s,	estimator xgboost's best error=1.3419,	best estimator xgboost's best error=1.3419
[flaml.automl: 09-16 15:40:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:40:52] {3072} INFO -  at 40.0s,	estimator xgboost's best error=1.1216,	best estimator xgboost's best error=1.1216
[flaml.automl: 09-16 15:40:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:40:59] {3072} INFO -  at 47.3s,	estimator xgboost's best error=1.1216,	best estimator xgboost's best error=1.1216
[flaml.automl: 09-16 15:40:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:41:04] {3072} INFO -  at 51.7s,	estimator xgboost's best error=1.1216,	best estimator xgboost's best error=1.1216
[flaml.automl: 09-16 15:41:04] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:41:11] {3072} INFO -  at 58.8s,	estimator xgboost's best error=1.1216,	best estimator xgboost's best error=1.1216
[flaml.automl: 09-16 15:41:16] {3335} INFO - retrain xgboost for 4.5s
[flaml.automl: 09-16 15:41:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 15:41:16] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:41:16] {2637} INFO - Time taken to find the best model: 40.01363158226013
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-0.1216254392933307
SO2(0)最好结果：{'pred_time': 4.351751025942072e-05, 'wall_clock_time': 40.01363158226013, 'metric_for_logging': {'pred_time': 4.351751025942072e-05}, 'val_loss': 1.1216254392933307, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.452642202377319}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.3918548749497527
SO2(0)的mse=4.236750590944265
SO2(0)的mae=1.126568987293765
SO2(0)的mar=0.18780263951602144
总共花费的时间为：63.83
丽水市
1267A
[flaml.automl: 09-16 15:44:30] {2390} INFO - task = regression
[flaml.automl: 09-16 15:44:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 15:44:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 15:44:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 15:44:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 15:44:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 15:44:31] {3025} INFO - Estimated sufficient time budget=11997s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 15:44:31] {3072} INFO -  at 1.2s,	estimator xgboost's best error=3.5813,	best estimator xgboost's best error=3.5813
[flaml.automl: 09-16 15:44:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 15:44:33] {3072} INFO -  at 3.1s,	estimator xgboost's best error=1.8634,	best estimator xgboost's best error=1.8634
[flaml.automl: 09-16 15:44:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 15:44:34] {3072} INFO -  at 4.3s,	estimator xgboost's best error=1.8634,	best estimator xgboost's best error=1.8634
[flaml.automl: 09-16 15:44:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 15:44:39] {3072} INFO -  at 8.8s,	estimator xgboost's best error=1.8634,	best estimator xgboost's best error=1.8634
[flaml.automl: 09-16 15:44:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 15:44:40] {3072} INFO -  at 9.9s,	estimator xgboost's best error=0.6974,	best estimator xgboost's best error=0.6974
[flaml.automl: 09-16 15:44:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 15:44:42] {3072} INFO -  at 11.6s,	estimator xgboost's best error=0.5211,	best estimator xgboost's best error=0.5211
[flaml.automl: 09-16 15:44:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 15:44:45] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.5024,	best estimator xgboost's best error=0.5024
[flaml.automl: 09-16 15:44:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 15:44:49] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.5024,	best estimator xgboost's best error=0.5024
[flaml.automl: 09-16 15:44:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 15:44:52] {3072} INFO -  at 22.1s,	estimator xgboost's best error=0.4693,	best estimator xgboost's best error=0.4693
[flaml.automl: 09-16 15:44:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 15:44:57] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.4674,	best estimator xgboost's best error=0.4674
[flaml.automl: 09-16 15:44:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 15:44:58] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.4674,	best estimator xgboost's best error=0.4674
[flaml.automl: 09-16 15:44:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 15:45:15] {3072} INFO -  at 45.0s,	estimator xgboost's best error=0.4674,	best estimator xgboost's best error=0.4674
[flaml.automl: 09-16 15:45:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 15:45:24] {3072} INFO -  at 53.8s,	estimator xgboost's best error=0.4462,	best estimator xgboost's best error=0.4462
[flaml.automl: 09-16 15:45:33] {3335} INFO - retrain xgboost for 8.8s
[flaml.automl: 09-16 15:45:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8136798788499957, colsample_bynode=1,
             colsample_bytree=0.9208991078026539, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33998503667532703,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=0.03277216125322573, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0018909006731756164, reg_lambda=1.8835414288003787,
             scale_pos_weight=1, subsample=0.7140919639229458,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 15:45:33] {2636} INFO - fit succeeded
[flaml.automl: 09-16 15:45:33] {2637} INFO - Time taken to find the best model: 53.77073645591736
[flaml.automl: 09-16 15:45:33] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 6, 'min_child_weight': 0.03277216125322573, 'learning_rate': 0.33998503667532703, 'subsample': 0.7140919639229458, 'colsample_bylevel': 0.8136798788499957, 'colsample_bytree': 0.9208991078026539, 'reg_alpha': 0.0018909006731756164, 'reg_lambda': 1.8835414288003787}
SO2(0)最佳损失：0.5537776595126733
SO2(0)最好结果：{'pred_time': 6.571140257675651e-05, 'wall_clock_time': 53.77073645591736, 'metric_for_logging': {'pred_time': 6.571140257675651e-05}, 'val_loss': 0.44622234048732673, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 6, 'min_child_weight': 0.03277216125322573, 'learning_rate': 0.33998503667532703, 'subsample': 0.7140919639229458, 'colsample_bylevel': 0.8136798788499957, 'colsample_bytree': 0.9208991078026539, 'reg_alpha': 0.0018909006731756164, 'reg_lambda': 1.8835414288003787}, 'config/n_estimators': 13, 'config/max_leaves': 6, 'config/min_child_weight': 0.03277216125322573, 'config/learning_rate': 0.33998503667532703, 'config/subsample': 0.7140919639229458, 'config/colsample_bylevel': 0.8136798788499957, 'config/colsample_bytree': 0.9208991078026539, 'config/reg_alpha': 0.0018909006731756164, 'config/reg_lambda': 1.8835414288003787, 'experiment_tag': 'exp', 'time_total_s': 8.756489038467407}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8136798788499957, colsample_bynode=1,
             colsample_bytree=0.9208991078026539, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33998503667532703,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=0.03277216125322573, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0018909006731756164, reg_lambda=1.8835414288003787,
             scale_pos_weight=1, subsample=0.7140919639229458,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.561120083453731
SO2(0)的mse=0.413661182222191
SO2(0)的mae=0.38638503012774505
SO2(0)的mar=0.06439262990651176
总共花费的时间为：62.79
合肥市
1273A
1274A
1275A
1277A
1278A
1279A
3464A
[flaml.automl: 09-16 16:07:49] {2390} INFO - task = regression
[flaml.automl: 09-16 16:07:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:07:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:07:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:07:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:07:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:07:51] {3025} INFO - Estimated sufficient time budget=90642s. Estimated necessary time budget=91s.
[flaml.automl: 09-16 16:07:51] {3072} INFO -  at 1.5s,	estimator xgboost's best error=4.2452,	best estimator xgboost's best error=4.2452
[flaml.automl: 09-16 16:07:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:07:53] {3072} INFO -  at 3.6s,	estimator xgboost's best error=1.9064,	best estimator xgboost's best error=1.9064
[flaml.automl: 09-16 16:07:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:07:54] {3072} INFO -  at 4.8s,	estimator xgboost's best error=1.9064,	best estimator xgboost's best error=1.9064
[flaml.automl: 09-16 16:07:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:07:57] {3072} INFO -  at 8.0s,	estimator xgboost's best error=1.9064,	best estimator xgboost's best error=1.9064
[flaml.automl: 09-16 16:07:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:07:58] {3072} INFO -  at 9.2s,	estimator xgboost's best error=0.9557,	best estimator xgboost's best error=0.9557
[flaml.automl: 09-16 16:07:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:08:00] {3072} INFO -  at 10.6s,	estimator xgboost's best error=0.6660,	best estimator xgboost's best error=0.6660
[flaml.automl: 09-16 16:08:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:08:01] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.6660,	best estimator xgboost's best error=0.6660
[flaml.automl: 09-16 16:08:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:08:04] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.6660,	best estimator xgboost's best error=0.6660
[flaml.automl: 09-16 16:08:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:08:05] {3072} INFO -  at 15.6s,	estimator xgboost's best error=0.6660,	best estimator xgboost's best error=0.6660
[flaml.automl: 09-16 16:08:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:08:08] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.6660,	best estimator xgboost's best error=0.6660
[flaml.automl: 09-16 16:08:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:08:11] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.6660,	best estimator xgboost's best error=0.6660
[flaml.automl: 09-16 16:08:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:08:13] {3072} INFO -  at 23.7s,	estimator xgboost's best error=0.6660,	best estimator xgboost's best error=0.6660
[flaml.automl: 09-16 16:08:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:08:25] {3072} INFO -  at 35.9s,	estimator xgboost's best error=0.5867,	best estimator xgboost's best error=0.5867
[flaml.automl: 09-16 16:08:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:08:47] {3072} INFO -  at 58.1s,	estimator xgboost's best error=0.5595,	best estimator xgboost's best error=0.5595
[flaml.automl: 09-16 16:09:09] {3335} INFO - retrain xgboost for 22.1s
[flaml.automl: 09-16 16:09:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 16:09:09] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:09:09] {2637} INFO - Time taken to find the best model: 58.11206078529358
[flaml.automl: 09-16 16:09:09] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 75402}
SO2(0)最佳损失：0.44047775189167193
SO2(0)最好结果：{'pred_time': 1.0459430747846528e-05, 'wall_clock_time': 58.11206078529358, 'metric_for_logging': {'pred_time': 1.0459430747846528e-05}, 'val_loss': 0.5595222481083281, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 75402}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 75402, 'experiment_tag': 'exp', 'time_total_s': 22.229697465896606}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8579867552416331
SO2(0)的mse=0.8740592072140355
SO2(0)的mae=0.5765789328616215
SO2(0)的mar=0.08489259403126491
总共花费的时间为：81.65
福州市
1280A
1285A
3048A
3526A
[flaml.automl: 09-16 16:21:31] {2390} INFO - task = regression
[flaml.automl: 09-16 16:21:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:21:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:21:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:21:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:21:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:21:33] {3025} INFO - Estimated sufficient time budget=49894s. Estimated necessary time budget=50s.
[flaml.automl: 09-16 16:21:33] {3072} INFO -  at 1.4s,	estimator xgboost's best error=2.2050,	best estimator xgboost's best error=2.2050
[flaml.automl: 09-16 16:21:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:21:35] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.0404,	best estimator xgboost's best error=1.0404
[flaml.automl: 09-16 16:21:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:21:36] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.0404,	best estimator xgboost's best error=1.0404
[flaml.automl: 09-16 16:21:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:21:42] {3072} INFO -  at 10.9s,	estimator xgboost's best error=1.0404,	best estimator xgboost's best error=1.0404
[flaml.automl: 09-16 16:21:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:21:43] {3072} INFO -  at 12.1s,	estimator xgboost's best error=0.6237,	best estimator xgboost's best error=0.6237
[flaml.automl: 09-16 16:21:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:21:45] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.5926,	best estimator xgboost's best error=0.5926
[flaml.automl: 09-16 16:21:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:21:47] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.4942,	best estimator xgboost's best error=0.4942
[flaml.automl: 09-16 16:21:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:21:49] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.4942,	best estimator xgboost's best error=0.4942
[flaml.automl: 09-16 16:21:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:21:51] {3072} INFO -  at 19.6s,	estimator xgboost's best error=0.4942,	best estimator xgboost's best error=0.4942
[flaml.automl: 09-16 16:21:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:21:54] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.4850,	best estimator xgboost's best error=0.4850
[flaml.automl: 09-16 16:21:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:21:56] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.4850,	best estimator xgboost's best error=0.4850
[flaml.automl: 09-16 16:21:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:21:57] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.4850,	best estimator xgboost's best error=0.4850
[flaml.automl: 09-16 16:21:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:22:01] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.4118,	best estimator xgboost's best error=0.4118
[flaml.automl: 09-16 16:22:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:22:04] {3072} INFO -  at 32.3s,	estimator xgboost's best error=0.4118,	best estimator xgboost's best error=0.4118
[flaml.automl: 09-16 16:22:04] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 16:22:06] {3072} INFO -  at 34.9s,	estimator xgboost's best error=0.3936,	best estimator xgboost's best error=0.3936
[flaml.automl: 09-16 16:22:06] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 16:22:10] {3072} INFO -  at 39.1s,	estimator xgboost's best error=0.3936,	best estimator xgboost's best error=0.3936
[flaml.automl: 09-16 16:22:10] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 16:22:12] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.3876,	best estimator xgboost's best error=0.3876
[flaml.automl: 09-16 16:22:12] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 16:22:14] {3072} INFO -  at 42.7s,	estimator xgboost's best error=0.3876,	best estimator xgboost's best error=0.3876
[flaml.automl: 09-16 16:22:14] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 16:22:17] {3072} INFO -  at 45.8s,	estimator xgboost's best error=0.3876,	best estimator xgboost's best error=0.3876
[flaml.automl: 09-16 16:22:17] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 16:22:18] {3072} INFO -  at 46.8s,	estimator xgboost's best error=0.3876,	best estimator xgboost's best error=0.3876
[flaml.automl: 09-16 16:22:18] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 16:22:21] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.3876,	best estimator xgboost's best error=0.3876
[flaml.automl: 09-16 16:22:21] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 16:22:30] {3072} INFO -  at 58.9s,	estimator xgboost's best error=0.3876,	best estimator xgboost's best error=0.3876
[flaml.automl: 09-16 16:22:42] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-16 16:22:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9482258695618703, colsample_bynode=1,
             colsample_bytree=0.8691252589173032, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.011539472184836785,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.012210314244058312, reg_lambda=0.5323731817831729,
             scale_pos_weight=1, subsample=0.9468514399123292,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 16:22:42] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:22:42] {2637} INFO - Time taken to find the best model: 40.675113916397095
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 12, 'min_child_weight': 0.011539472184836785, 'learning_rate': 1.0, 'subsample': 0.9468514399123292, 'colsample_bylevel': 0.9482258695618703, 'colsample_bytree': 0.8691252589173032, 'reg_alpha': 0.012210314244058312, 'reg_lambda': 0.5323731817831729, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：0.6123905085159058
SO2(0)最好结果：{'pred_time': 1.5281838592805892e-05, 'wall_clock_time': 40.675113916397095, 'metric_for_logging': {'pred_time': 1.5281838592805892e-05}, 'val_loss': 0.3876094914840942, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 12, 'min_child_weight': 0.011539472184836785, 'learning_rate': 1.0, 'subsample': 0.9468514399123292, 'colsample_bylevel': 0.9482258695618703, 'colsample_bytree': 0.8691252589173032, 'reg_alpha': 0.012210314244058312, 'reg_lambda': 0.5323731817831729, 'FLAML_sample_size': 10000}, 'config/n_estimators': 6, 'config/max_leaves': 12, 'config/min_child_weight': 0.011539472184836785, 'config/learning_rate': 1.0, 'config/subsample': 0.9468514399123292, 'config/colsample_bylevel': 0.9482258695618703, 'config/colsample_bytree': 0.8691252589173032, 'config/reg_alpha': 0.012210314244058312, 'config/reg_lambda': 0.5323731817831729, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 1.5884342193603516}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9482258695618703, colsample_bynode=1,
             colsample_bytree=0.8691252589173032, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.011539472184836785,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.012210314244058312, reg_lambda=0.5323731817831729,
             scale_pos_weight=1, subsample=0.9468514399123292,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8508363509843356
SO2(0)的mse=0.4257672702739215
SO2(0)的mae=0.42312868758761135
SO2(0)的mar=0.12680794066946036
总共花费的时间为：71.64
厦门市
1286A
3527A
3528A
[flaml.automl: 09-16 16:31:46] {2390} INFO - task = regression
[flaml.automl: 09-16 16:31:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:31:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:31:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:31:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:31:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:31:47] {3025} INFO - Estimated sufficient time budget=12158s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 16:31:47] {3072} INFO -  at 1.4s,	estimator xgboost's best error=2.0742,	best estimator xgboost's best error=2.0742
[flaml.automl: 09-16 16:31:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:31:50] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.1369,	best estimator xgboost's best error=1.1369
[flaml.automl: 09-16 16:31:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:31:51] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.1369,	best estimator xgboost's best error=1.1369
[flaml.automl: 09-16 16:31:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:32:01] {3072} INFO -  at 14.7s,	estimator xgboost's best error=1.1369,	best estimator xgboost's best error=1.1369
[flaml.automl: 09-16 16:32:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:32:02] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.8797,	best estimator xgboost's best error=0.8797
[flaml.automl: 09-16 16:32:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:32:03] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.8797,	best estimator xgboost's best error=0.8797
[flaml.automl: 09-16 16:32:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:32:05] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.7629,	best estimator xgboost's best error=0.7629
[flaml.automl: 09-16 16:32:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:32:09] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.7629,	best estimator xgboost's best error=0.7629
[flaml.automl: 09-16 16:32:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:32:12] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.7629,	best estimator xgboost's best error=0.7629
[flaml.automl: 09-16 16:32:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:32:18] {3072} INFO -  at 31.5s,	estimator xgboost's best error=0.7629,	best estimator xgboost's best error=0.7629
[flaml.automl: 09-16 16:32:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:32:20] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.7552,	best estimator xgboost's best error=0.7552
[flaml.automl: 09-16 16:32:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:32:22] {3072} INFO -  at 36.2s,	estimator xgboost's best error=0.7552,	best estimator xgboost's best error=0.7552
[flaml.automl: 09-16 16:32:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:32:33] {3072} INFO -  at 46.5s,	estimator xgboost's best error=0.7344,	best estimator xgboost's best error=0.7344
[flaml.automl: 09-16 16:32:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:32:46] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.7213,	best estimator xgboost's best error=0.7213
[flaml.automl: 09-16 16:33:03] {3335} INFO - retrain xgboost for 17.7s
[flaml.automl: 09-16 16:33:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=24, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 16:33:03] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:33:03] {2637} INFO - Time taken to find the best model: 59.463364362716675
[flaml.automl: 09-16 16:33:03] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 24, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
SO2(0)最佳损失：0.27869253308978326
SO2(0)最好结果：{'pred_time': 2.2201248108239702e-05, 'wall_clock_time': 59.463364362716675, 'metric_for_logging': {'pred_time': 2.2201248108239702e-05}, 'val_loss': 0.7213074669102167, 'training_iteration': 1, 'config': {'n_estimators': 24, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 24, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 13.000758647918701}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=24, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7053036408549063
SO2(0)的mse=1.3090168758626575
SO2(0)的mae=0.71919743581806
SO2(0)的mar=0.24237337055103628
总共花费的时间为：77.79
南昌市
1295A
1296A
1298A
3690A
[flaml.automl: 09-16 16:45:08] {2390} INFO - task = regression
[flaml.automl: 09-16 16:45:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 16:45:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 16:45:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 16:45:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 16:45:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 16:45:10] {3025} INFO - Estimated sufficient time budget=49263s. Estimated necessary time budget=49s.
[flaml.automl: 09-16 16:45:10] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.0107,	best estimator xgboost's best error=4.0107
[flaml.automl: 09-16 16:45:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 16:45:12] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.9448,	best estimator xgboost's best error=1.9448
[flaml.automl: 09-16 16:45:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 16:45:13] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.9448,	best estimator xgboost's best error=1.9448
[flaml.automl: 09-16 16:45:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 16:45:19] {3072} INFO -  at 11.0s,	estimator xgboost's best error=1.9448,	best estimator xgboost's best error=1.9448
[flaml.automl: 09-16 16:45:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 16:45:20] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.4580,	best estimator xgboost's best error=1.4580
[flaml.automl: 09-16 16:45:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 16:45:22] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.2779,	best estimator xgboost's best error=1.2779
[flaml.automl: 09-16 16:45:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 16:45:24] {3072} INFO -  at 15.4s,	estimator xgboost's best error=1.2247,	best estimator xgboost's best error=1.2247
[flaml.automl: 09-16 16:45:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 16:45:26] {3072} INFO -  at 18.1s,	estimator xgboost's best error=1.2247,	best estimator xgboost's best error=1.2247
[flaml.automl: 09-16 16:45:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 16:45:28] {3072} INFO -  at 19.7s,	estimator xgboost's best error=1.2247,	best estimator xgboost's best error=1.2247
[flaml.automl: 09-16 16:45:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 16:45:31] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.0710,	best estimator xgboost's best error=1.0710
[flaml.automl: 09-16 16:45:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 16:45:33] {3072} INFO -  at 24.3s,	estimator xgboost's best error=1.0710,	best estimator xgboost's best error=1.0710
[flaml.automl: 09-16 16:45:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 16:45:34] {3072} INFO -  at 25.5s,	estimator xgboost's best error=1.0710,	best estimator xgboost's best error=1.0710
[flaml.automl: 09-16 16:45:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 16:45:37] {3072} INFO -  at 29.2s,	estimator xgboost's best error=1.0710,	best estimator xgboost's best error=1.0710
[flaml.automl: 09-16 16:45:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 16:45:41] {3072} INFO -  at 32.7s,	estimator xgboost's best error=1.0710,	best estimator xgboost's best error=1.0710
[flaml.automl: 09-16 16:45:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 16:45:44] {3072} INFO -  at 35.7s,	estimator xgboost's best error=1.0710,	best estimator xgboost's best error=1.0710
[flaml.automl: 09-16 16:45:44] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 16:45:49] {3072} INFO -  at 40.9s,	estimator xgboost's best error=1.0710,	best estimator xgboost's best error=1.0710
[flaml.automl: 09-16 16:45:49] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 16:45:52] {3072} INFO -  at 44.0s,	estimator xgboost's best error=1.0585,	best estimator xgboost's best error=1.0585
[flaml.automl: 09-16 16:45:52] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 16:45:55] {3072} INFO -  at 46.7s,	estimator xgboost's best error=1.0585,	best estimator xgboost's best error=1.0585
[flaml.automl: 09-16 16:45:55] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 16:46:05] {3072} INFO -  at 56.8s,	estimator xgboost's best error=1.0585,	best estimator xgboost's best error=1.0585
[flaml.automl: 09-16 16:46:05] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 16:46:08] {3072} INFO -  at 59.4s,	estimator xgboost's best error=1.0585,	best estimator xgboost's best error=1.0585
[flaml.automl: 09-16 16:46:13] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-16 16:46:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 16:46:13] {2636} INFO - fit succeeded
[flaml.automl: 09-16 16:46:13] {2637} INFO - Time taken to find the best model: 43.98469924926758
[flaml.automl: 09-16 16:46:13] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 41391}
SO2(0)最佳损失：-0.058528235436107856
SO2(0)最好结果：{'pred_time': 1.822800500467047e-05, 'wall_clock_time': 43.98469924926758, 'metric_for_logging': {'pred_time': 1.822800500467047e-05}, 'val_loss': 1.0585282354361079, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 41391}, 'config/n_estimators': 7, 'config/max_leaves': 4, 'config/min_child_weight': 0.031257867156220746, 'config/learning_rate': 0.3417584818146937, 'config/subsample': 0.8188658116452241, 'config/colsample_bylevel': 0.9718368347363348, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.001562580531679857, 'config/reg_lambda': 0.36612402315272025, 'config/FLAML_sample_size': 41391, 'experiment_tag': 'exp', 'time_total_s': 3.0688822269439697}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7330353076120373
SO2(0)的mse=3.553707078829952
SO2(0)的mae=1.0583646989713045
SO2(0)的mar=0.19844550590451318
总共花费的时间为：65.47
济南市
1300A
1301A
1305A
1306A
1961A
3064A
3494A
3495A
3682A
[flaml.automl: 09-16 17:13:27] {2390} INFO - task = regression
[flaml.automl: 09-16 17:13:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 17:13:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 17:13:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 17:13:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 17:13:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 17:13:28] {3025} INFO - Estimated sufficient time budget=118052s. Estimated necessary time budget=118s.
[flaml.automl: 09-16 17:13:28] {3072} INFO -  at 1.6s,	estimator xgboost's best error=6.6879,	best estimator xgboost's best error=6.6879
[flaml.automl: 09-16 17:13:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 17:13:30] {3072} INFO -  at 3.7s,	estimator xgboost's best error=3.0775,	best estimator xgboost's best error=3.0775
[flaml.automl: 09-16 17:13:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 17:13:31] {3072} INFO -  at 4.9s,	estimator xgboost's best error=3.0775,	best estimator xgboost's best error=3.0775
[flaml.automl: 09-16 17:13:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 17:13:34] {3072} INFO -  at 7.2s,	estimator xgboost's best error=3.0775,	best estimator xgboost's best error=3.0775
[flaml.automl: 09-16 17:13:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 17:13:35] {3072} INFO -  at 8.3s,	estimator xgboost's best error=1.9023,	best estimator xgboost's best error=1.9023
[flaml.automl: 09-16 17:13:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 17:13:36] {3072} INFO -  at 9.9s,	estimator xgboost's best error=1.9023,	best estimator xgboost's best error=1.9023
[flaml.automl: 09-16 17:13:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 17:13:38] {3072} INFO -  at 11.6s,	estimator xgboost's best error=1.3865,	best estimator xgboost's best error=1.3865
[flaml.automl: 09-16 17:13:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 17:13:40] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.3865,	best estimator xgboost's best error=1.3865
[flaml.automl: 09-16 17:13:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 17:13:42] {3072} INFO -  at 15.5s,	estimator xgboost's best error=1.3865,	best estimator xgboost's best error=1.3865
[flaml.automl: 09-16 17:13:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 17:13:44] {3072} INFO -  at 17.3s,	estimator xgboost's best error=1.3865,	best estimator xgboost's best error=1.3865
[flaml.automl: 09-16 17:13:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 17:13:45] {3072} INFO -  at 18.7s,	estimator xgboost's best error=1.3865,	best estimator xgboost's best error=1.3865
[flaml.automl: 09-16 17:13:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 17:13:47] {3072} INFO -  at 20.5s,	estimator xgboost's best error=1.3865,	best estimator xgboost's best error=1.3865
[flaml.automl: 09-16 17:13:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 17:13:48] {3072} INFO -  at 21.7s,	estimator xgboost's best error=1.3865,	best estimator xgboost's best error=1.3865
[flaml.automl: 09-16 17:13:48] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:13:55] {3072} INFO -  at 28.8s,	estimator xgboost's best error=1.2592,	best estimator xgboost's best error=1.2592
[flaml.automl: 09-16 17:13:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 17:14:08] {3072} INFO -  at 41.7s,	estimator xgboost's best error=1.2367,	best estimator xgboost's best error=1.2367
[flaml.automl: 09-16 17:14:08] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 17:14:19] {3072} INFO -  at 52.6s,	estimator xgboost's best error=1.2367,	best estimator xgboost's best error=1.2367
[flaml.automl: 09-16 17:14:43] {3335} INFO - retrain xgboost for 23.8s
[flaml.automl: 09-16 17:14:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 17:14:43] {2636} INFO - fit succeeded
[flaml.automl: 09-16 17:14:43] {2637} INFO - Time taken to find the best model: 41.70165777206421
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 98519}
SO2(0)最佳损失：-0.2367129080631183
SO2(0)最好结果：{'pred_time': 4.013869450322524e-06, 'wall_clock_time': 41.70165777206421, 'metric_for_logging': {'pred_time': 4.013869450322524e-06}, 'val_loss': 1.2367129080631183, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 98519}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 98519, 'experiment_tag': 'exp', 'time_total_s': 12.903583288192749}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8432305950461525
SO2(0)的mse=4.112021570452596
SO2(0)的mae=1.2318690297909987
SO2(0)的mar=0.12177607124732447
总共花费的时间为：78.13
青岛市
1307A
1311A
3362A
3642A
3643A
[flaml.automl: 09-16 17:31:33] {2390} INFO - task = regression
[flaml.automl: 09-16 17:31:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 17:31:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 17:31:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 17:31:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 17:31:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 17:31:35] {3025} INFO - Estimated sufficient time budget=110827s. Estimated necessary time budget=111s.
[flaml.automl: 09-16 17:31:35] {3072} INFO -  at 2.3s,	estimator xgboost's best error=4.0752,	best estimator xgboost's best error=4.0752
[flaml.automl: 09-16 17:31:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 17:31:39] {3072} INFO -  at 6.2s,	estimator xgboost's best error=1.9221,	best estimator xgboost's best error=1.9221
[flaml.automl: 09-16 17:31:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 17:31:41] {3072} INFO -  at 8.5s,	estimator xgboost's best error=1.9221,	best estimator xgboost's best error=1.9221
[flaml.automl: 09-16 17:31:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 17:31:45] {3072} INFO -  at 12.5s,	estimator xgboost's best error=1.9221,	best estimator xgboost's best error=1.9221
[flaml.automl: 09-16 17:31:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 17:31:47] {3072} INFO -  at 14.6s,	estimator xgboost's best error=1.2168,	best estimator xgboost's best error=1.2168
[flaml.automl: 09-16 17:31:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 17:31:50] {3072} INFO -  at 17.5s,	estimator xgboost's best error=1.0783,	best estimator xgboost's best error=1.0783
[flaml.automl: 09-16 17:31:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 17:31:53] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.9883,	best estimator xgboost's best error=0.9883
[flaml.automl: 09-16 17:31:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 17:31:56] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.9883,	best estimator xgboost's best error=0.9883
[flaml.automl: 09-16 17:31:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 17:31:59] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.9883,	best estimator xgboost's best error=0.9883
[flaml.automl: 09-16 17:31:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 17:32:02] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.9883,	best estimator xgboost's best error=0.9883
[flaml.automl: 09-16 17:32:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 17:32:04] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.9883,	best estimator xgboost's best error=0.9883
[flaml.automl: 09-16 17:32:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 17:32:07] {3072} INFO -  at 34.8s,	estimator xgboost's best error=0.9883,	best estimator xgboost's best error=0.9883
[flaml.automl: 09-16 17:32:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 17:32:09] {3072} INFO -  at 37.0s,	estimator xgboost's best error=0.9883,	best estimator xgboost's best error=0.9883
[flaml.automl: 09-16 17:32:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:32:22] {3072} INFO -  at 49.8s,	estimator xgboost's best error=0.8517,	best estimator xgboost's best error=0.8517
[flaml.automl: 09-16 17:32:35] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-16 17:32:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 17:32:35] {2636} INFO - fit succeeded
[flaml.automl: 09-16 17:32:35] {2637} INFO - Time taken to find the best model: 49.80795407295227
[flaml.automl: 09-16 17:32:35] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 53098}
SO2(0)最佳损失：0.14830017754587077
SO2(0)最好结果：{'pred_time': 1.1133096985897776e-05, 'wall_clock_time': 49.80795407295227, 'metric_for_logging': {'pred_time': 1.1133096985897776e-05}, 'val_loss': 0.8516998224541292, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 53098}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 53098, 'experiment_tag': 'exp', 'time_total_s': 12.807369470596313}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7850712408457707
SO2(0)的mse=2.039807779597583
SO2(0)的mae=0.8723879301235252
SO2(0)的mar=0.13803300843002367
总共花费的时间为：63.39
郑州市
1318A
1320A
1321A
1323A
1324A
3471A
3590A
3591A
[flaml.automl: 09-16 17:57:51] {2390} INFO - task = regression
[flaml.automl: 09-16 17:57:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 17:57:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 17:57:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 17:57:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 17:57:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 17:57:53] {3025} INFO - Estimated sufficient time budget=102514s. Estimated necessary time budget=103s.
[flaml.automl: 09-16 17:57:53] {3072} INFO -  at 1.5s,	estimator xgboost's best error=4.4415,	best estimator xgboost's best error=4.4415
[flaml.automl: 09-16 17:57:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 17:57:55] {3072} INFO -  at 3.7s,	estimator xgboost's best error=2.0432,	best estimator xgboost's best error=2.0432
[flaml.automl: 09-16 17:57:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 17:57:56] {3072} INFO -  at 4.9s,	estimator xgboost's best error=2.0432,	best estimator xgboost's best error=2.0432
[flaml.automl: 09-16 17:57:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 17:57:59] {3072} INFO -  at 7.6s,	estimator xgboost's best error=2.0432,	best estimator xgboost's best error=2.0432
[flaml.automl: 09-16 17:57:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 17:58:00] {3072} INFO -  at 8.7s,	estimator xgboost's best error=1.3130,	best estimator xgboost's best error=1.3130
[flaml.automl: 09-16 17:58:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 17:58:01] {3072} INFO -  at 10.3s,	estimator xgboost's best error=1.3130,	best estimator xgboost's best error=1.3130
[flaml.automl: 09-16 17:58:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 17:58:03] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.9700,	best estimator xgboost's best error=0.9700
[flaml.automl: 09-16 17:58:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 17:58:06] {3072} INFO -  at 14.6s,	estimator xgboost's best error=0.9700,	best estimator xgboost's best error=0.9700
[flaml.automl: 09-16 17:58:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 17:58:07] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.9700,	best estimator xgboost's best error=0.9700
[flaml.automl: 09-16 17:58:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 17:58:09] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.9700,	best estimator xgboost's best error=0.9700
[flaml.automl: 09-16 17:58:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 17:58:11] {3072} INFO -  at 19.9s,	estimator xgboost's best error=0.9700,	best estimator xgboost's best error=0.9700
[flaml.automl: 09-16 17:58:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 17:58:13] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.9695,	best estimator xgboost's best error=0.9695
[flaml.automl: 09-16 17:58:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 17:58:14] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.9695,	best estimator xgboost's best error=0.9695
[flaml.automl: 09-16 17:58:14] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 17:58:21] {3072} INFO -  at 29.9s,	estimator xgboost's best error=0.8950,	best estimator xgboost's best error=0.8950
[flaml.automl: 09-16 17:58:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 17:58:34] {3072} INFO -  at 42.7s,	estimator xgboost's best error=0.8859,	best estimator xgboost's best error=0.8859
[flaml.automl: 09-16 17:58:34] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 17:58:41] {3072} INFO -  at 49.9s,	estimator xgboost's best error=0.8859,	best estimator xgboost's best error=0.8859
[flaml.automl: 09-16 17:58:54] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-16 17:58:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 17:58:54] {2636} INFO - fit succeeded
[flaml.automl: 09-16 17:58:54] {2637} INFO - Time taken to find the best model: 42.74352240562439
[flaml.automl: 09-16 17:58:54] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 86293}
SO2(0)最佳损失：0.11405249854821908
SO2(0)最好结果：{'pred_time': 4.7938320706119155e-06, 'wall_clock_time': 42.74352240562439, 'metric_for_logging': {'pred_time': 4.7938320706119155e-06}, 'val_loss': 0.8859475014517809, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 86293}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 86293, 'experiment_tag': 'exp', 'time_total_s': 12.892367362976074}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8404758001055637
SO2(0)的mse=2.430306337240247
SO2(0)的mae=0.8939475492328288
SO2(0)的mar=0.1339291342667545
总共花费的时间为：63.93
武汉市
1325A
1326A
1327A
1328A
1329A
1331A
3153A
[flaml.automl: 09-16 18:20:14] {2390} INFO - task = regression
[flaml.automl: 09-16 18:20:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 18:20:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 18:20:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 18:20:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 18:20:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 18:20:16] {3025} INFO - Estimated sufficient time budget=171661s. Estimated necessary time budget=172s.
[flaml.automl: 09-16 18:20:16] {3072} INFO -  at 2.6s,	estimator xgboost's best error=4.6323,	best estimator xgboost's best error=4.6323
[flaml.automl: 09-16 18:20:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 18:20:19] {3072} INFO -  at 6.1s,	estimator xgboost's best error=2.4290,	best estimator xgboost's best error=2.4290
[flaml.automl: 09-16 18:20:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 18:20:22] {3072} INFO -  at 8.3s,	estimator xgboost's best error=2.4290,	best estimator xgboost's best error=2.4290
[flaml.automl: 09-16 18:20:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 18:20:24] {3072} INFO -  at 11.1s,	estimator xgboost's best error=2.4290,	best estimator xgboost's best error=2.4290
[flaml.automl: 09-16 18:20:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 18:20:26] {3072} INFO -  at 13.2s,	estimator xgboost's best error=1.0771,	best estimator xgboost's best error=1.0771
[flaml.automl: 09-16 18:20:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 18:20:29] {3072} INFO -  at 15.9s,	estimator xgboost's best error=0.8196,	best estimator xgboost's best error=0.8196
[flaml.automl: 09-16 18:20:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 18:20:32] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.8196,	best estimator xgboost's best error=0.8196
[flaml.automl: 09-16 18:20:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 18:20:34] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.8196,	best estimator xgboost's best error=0.8196
[flaml.automl: 09-16 18:20:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 18:20:36] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.8196,	best estimator xgboost's best error=0.8196
[flaml.automl: 09-16 18:20:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 18:20:37] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.8196,	best estimator xgboost's best error=0.8196
[flaml.automl: 09-16 18:20:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 18:20:40] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.8196,	best estimator xgboost's best error=0.8196
[flaml.automl: 09-16 18:20:40] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 18:20:42] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.8196,	best estimator xgboost's best error=0.8196
[flaml.automl: 09-16 18:20:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 18:20:48] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.7069,	best estimator xgboost's best error=0.7069
[flaml.automl: 09-16 18:20:48] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 18:21:00] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.6920,	best estimator xgboost's best error=0.6920
[flaml.automl: 09-16 18:21:00] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 18:21:07] {3072} INFO -  at 53.7s,	estimator xgboost's best error=0.6920,	best estimator xgboost's best error=0.6920
[flaml.automl: 09-16 18:21:19] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 18:21:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 18:21:19] {2636} INFO - fit succeeded
[flaml.automl: 09-16 18:21:19] {2637} INFO - Time taken to find the best model: 47.16494393348694
[flaml.automl: 09-16 18:21:19] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 77462}
SO2(0)最佳损失：0.3080240477941669
SO2(0)最好结果：{'pred_time': 4.605547786520737e-06, 'wall_clock_time': 47.16494393348694, 'metric_for_logging': {'pred_time': 4.605547786520737e-06}, 'val_loss': 0.6919759522058331, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 77462}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 77462, 'experiment_tag': 'exp', 'time_total_s': 12.098364114761353}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8219893974277404
SO2(0)的mse=1.3725720531828418
SO2(0)的mae=0.6934600005784028
SO2(0)的mar=0.09012200606568385
总共花费的时间为：67.21
长沙市
1340A
1342A
1343A
1344A
[flaml.automl: 09-16 18:33:52] {2390} INFO - task = regression
[flaml.automl: 09-16 18:33:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 18:33:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 18:33:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 18:33:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 18:33:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 18:33:53] {3025} INFO - Estimated sufficient time budget=50164s. Estimated necessary time budget=50s.
[flaml.automl: 09-16 18:33:53] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.8519,	best estimator xgboost's best error=3.8519
[flaml.automl: 09-16 18:33:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 18:33:55] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.7561,	best estimator xgboost's best error=1.7561
[flaml.automl: 09-16 18:33:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 18:33:56] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.7561,	best estimator xgboost's best error=1.7561
[flaml.automl: 09-16 18:33:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 18:34:02] {3072} INFO -  at 10.9s,	estimator xgboost's best error=1.7561,	best estimator xgboost's best error=1.7561
[flaml.automl: 09-16 18:34:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 18:34:04] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.9157,	best estimator xgboost's best error=0.9157
[flaml.automl: 09-16 18:34:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 18:34:05] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.7563,	best estimator xgboost's best error=0.7563
[flaml.automl: 09-16 18:34:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 18:34:07] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.7100,	best estimator xgboost's best error=0.7100
[flaml.automl: 09-16 18:34:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 18:34:09] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.7100,	best estimator xgboost's best error=0.7100
[flaml.automl: 09-16 18:34:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 18:34:11] {3072} INFO -  at 19.4s,	estimator xgboost's best error=0.7100,	best estimator xgboost's best error=0.7100
[flaml.automl: 09-16 18:34:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 18:34:14] {3072} INFO -  at 22.4s,	estimator xgboost's best error=0.6608,	best estimator xgboost's best error=0.6608
[flaml.automl: 09-16 18:34:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 18:34:16] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.6608,	best estimator xgboost's best error=0.6608
[flaml.automl: 09-16 18:34:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 18:34:17] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.6608,	best estimator xgboost's best error=0.6608
[flaml.automl: 09-16 18:34:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 18:34:21] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.5734,	best estimator xgboost's best error=0.5734
[flaml.automl: 09-16 18:34:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 18:34:24] {3072} INFO -  at 32.6s,	estimator xgboost's best error=0.5734,	best estimator xgboost's best error=0.5734
[flaml.automl: 09-16 18:34:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 18:34:27] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.5477,	best estimator xgboost's best error=0.5477
[flaml.automl: 09-16 18:34:27] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 18:34:29] {3072} INFO -  at 37.4s,	estimator xgboost's best error=0.5477,	best estimator xgboost's best error=0.5477
[flaml.automl: 09-16 18:34:29] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 18:34:31] {3072} INFO -  at 39.6s,	estimator xgboost's best error=0.5477,	best estimator xgboost's best error=0.5477
[flaml.automl: 09-16 18:34:31] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 18:34:33] {3072} INFO -  at 41.8s,	estimator xgboost's best error=0.5477,	best estimator xgboost's best error=0.5477
[flaml.automl: 09-16 18:34:33] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 18:34:36] {3072} INFO -  at 44.6s,	estimator xgboost's best error=0.5477,	best estimator xgboost's best error=0.5477
[flaml.automl: 09-16 18:34:36] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 18:34:43] {3072} INFO -  at 51.9s,	estimator xgboost's best error=0.5477,	best estimator xgboost's best error=0.5477
[flaml.automl: 09-16 18:34:43] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 18:34:46] {3072} INFO -  at 54.6s,	estimator xgboost's best error=0.5477,	best estimator xgboost's best error=0.5477
[flaml.automl: 09-16 18:34:46] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 18:34:51] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.5399,	best estimator xgboost's best error=0.5399
[flaml.automl: 09-16 18:35:15] {3335} INFO - retrain xgboost for 24.3s
[flaml.automl: 09-16 18:35:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7869551594064775, colsample_bynode=1,
             colsample_bytree=0.7933397443475808, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=41, min_child_weight=0.009416638758491828,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009169417918441369, reg_lambda=0.1716204668378346,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 18:35:15] {2636} INFO - fit succeeded
[flaml.automl: 09-16 18:35:15] {2637} INFO - Time taken to find the best model: 59.38886094093323
[flaml.automl: 09-16 18:35:15] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 10, 'max_leaves': 41, 'min_child_weight': 0.009416638758491828, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7869551594064775, 'colsample_bytree': 0.7933397443475808, 'reg_alpha': 0.009169417918441369, 'reg_lambda': 0.1716204668378346, 'FLAML_sample_size': 41913}
SO2(0)最佳损失：0.4600716026723001
SO2(0)最好结果：{'pred_time': 8.662302845176477e-06, 'wall_clock_time': 59.38886094093323, 'metric_for_logging': {'pred_time': 8.662302845176477e-06}, 'val_loss': 0.5399283973276999, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 41, 'min_child_weight': 0.009416638758491828, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7869551594064775, 'colsample_bytree': 0.7933397443475808, 'reg_alpha': 0.009169417918441369, 'reg_lambda': 0.1716204668378346, 'FLAML_sample_size': 41913}, 'config/n_estimators': 10, 'config/max_leaves': 41, 'config/min_child_weight': 0.009416638758491828, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7869551594064775, 'config/colsample_bytree': 0.7933397443475808, 'config/reg_alpha': 0.009169417918441369, 'config/reg_lambda': 0.1716204668378346, 'config/FLAML_sample_size': 41913, 'experiment_tag': 'exp', 'time_total_s': 4.7756898403167725}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7869551594064775, colsample_bynode=1,
             colsample_bytree=0.7933397443475808, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=41, min_child_weight=0.009416638758491828,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009169417918441369, reg_lambda=0.1716204668378346,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8377980556557332
SO2(0)的mse=1.0457307586865365
SO2(0)的mae=0.5876760074366694
SO2(0)的mar=0.09167927642025116
总共花费的时间为：84.36
广州市
1345A
1346A
1349A
1351A
1352A
1354A
1355A
2846A
3299A
3300A
3301A
3302A
3303A
3304A
3443A
3445A
3446A
[flaml.automl: 09-16 19:25:13] {2390} INFO - task = regression
[flaml.automl: 09-16 19:25:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 19:25:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 19:25:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 19:25:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 19:25:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 19:25:15] {3025} INFO - Estimated sufficient time budget=256480s. Estimated necessary time budget=256s.
[flaml.automl: 09-16 19:25:15] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.1483,	best estimator xgboost's best error=5.1483
[flaml.automl: 09-16 19:25:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 19:25:16] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.6670,	best estimator xgboost's best error=4.6670
[flaml.automl: 09-16 19:25:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 19:25:17] {3072} INFO -  at 4.9s,	estimator xgboost's best error=4.6670,	best estimator xgboost's best error=4.6670
[flaml.automl: 09-16 19:25:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 19:25:19] {3072} INFO -  at 6.1s,	estimator xgboost's best error=4.6670,	best estimator xgboost's best error=4.6670
[flaml.automl: 09-16 19:25:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 19:25:20] {3072} INFO -  at 7.4s,	estimator xgboost's best error=1.9855,	best estimator xgboost's best error=1.9855
[flaml.automl: 09-16 19:25:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 19:25:21] {3072} INFO -  at 8.8s,	estimator xgboost's best error=1.3860,	best estimator xgboost's best error=1.3860
[flaml.automl: 09-16 19:25:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 19:25:22] {3072} INFO -  at 9.9s,	estimator xgboost's best error=1.3860,	best estimator xgboost's best error=1.3860
[flaml.automl: 09-16 19:25:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 19:25:24] {3072} INFO -  at 11.2s,	estimator xgboost's best error=1.3860,	best estimator xgboost's best error=1.3860
[flaml.automl: 09-16 19:25:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 19:25:25] {3072} INFO -  at 12.6s,	estimator xgboost's best error=0.6857,	best estimator xgboost's best error=0.6857
[flaml.automl: 09-16 19:25:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 19:25:26] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.6857,	best estimator xgboost's best error=0.6857
[flaml.automl: 09-16 19:25:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 19:25:27] {3072} INFO -  at 14.5s,	estimator xgboost's best error=0.6857,	best estimator xgboost's best error=0.6857
[flaml.automl: 09-16 19:25:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 19:25:28] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.6857,	best estimator xgboost's best error=0.6857
[flaml.automl: 09-16 19:25:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 19:25:30] {3072} INFO -  at 17.2s,	estimator xgboost's best error=0.6857,	best estimator xgboost's best error=0.6857
[flaml.automl: 09-16 19:25:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 19:25:32] {3072} INFO -  at 19.4s,	estimator xgboost's best error=0.6857,	best estimator xgboost's best error=0.6857
[flaml.automl: 09-16 19:25:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 19:25:36] {3072} INFO -  at 23.5s,	estimator xgboost's best error=0.6011,	best estimator xgboost's best error=0.6011
[flaml.automl: 09-16 19:25:36] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 19:25:39] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.6011,	best estimator xgboost's best error=0.6011
[flaml.automl: 09-16 19:25:39] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 19:25:42] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.6011,	best estimator xgboost's best error=0.6011
[flaml.automl: 09-16 19:25:42] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 19:25:45] {3072} INFO -  at 32.5s,	estimator xgboost's best error=0.4695,	best estimator xgboost's best error=0.4695
[flaml.automl: 09-16 19:25:45] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 19:25:48] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.4695,	best estimator xgboost's best error=0.4695
[flaml.automl: 09-16 19:25:48] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 19:25:50] {3072} INFO -  at 37.3s,	estimator xgboost's best error=0.4507,	best estimator xgboost's best error=0.4507
[flaml.automl: 09-16 19:25:50] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 19:25:52] {3072} INFO -  at 39.5s,	estimator xgboost's best error=0.4507,	best estimator xgboost's best error=0.4507
[flaml.automl: 09-16 19:25:52] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 19:25:54] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.4476,	best estimator xgboost's best error=0.4476
[flaml.automl: 09-16 19:25:54] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 19:25:56] {3072} INFO -  at 43.7s,	estimator xgboost's best error=0.4476,	best estimator xgboost's best error=0.4476
[flaml.automl: 09-16 19:25:56] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-16 19:25:58] {3072} INFO -  at 45.0s,	estimator xgboost's best error=0.4476,	best estimator xgboost's best error=0.4476
[flaml.automl: 09-16 19:25:58] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-16 19:25:59] {3072} INFO -  at 46.1s,	estimator xgboost's best error=0.4318,	best estimator xgboost's best error=0.4318
[flaml.automl: 09-16 19:25:59] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-16 19:26:00] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.4318,	best estimator xgboost's best error=0.4318
[flaml.automl: 09-16 19:26:00] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-16 19:26:01] {3072} INFO -  at 48.1s,	estimator xgboost's best error=0.4318,	best estimator xgboost's best error=0.4318
[flaml.automl: 09-16 19:26:01] {2897} INFO - iteration 27, current learner xgboost
[flaml.automl: 09-16 19:26:02] {3072} INFO -  at 49.0s,	estimator xgboost's best error=0.4314,	best estimator xgboost's best error=0.4314
[flaml.automl: 09-16 19:26:02] {2897} INFO - iteration 28, current learner xgboost
[flaml.automl: 09-16 19:26:02] {3072} INFO -  at 49.8s,	estimator xgboost's best error=0.4297,	best estimator xgboost's best error=0.4297
[flaml.automl: 09-16 19:26:02] {2897} INFO - iteration 29, current learner xgboost
[flaml.automl: 09-16 19:26:03] {3072} INFO -  at 50.7s,	estimator xgboost's best error=0.4297,	best estimator xgboost's best error=0.4297
[flaml.automl: 09-16 19:26:03] {2897} INFO - iteration 30, current learner xgboost
[flaml.automl: 09-16 19:26:05] {3072} INFO -  at 52.0s,	estimator xgboost's best error=0.4297,	best estimator xgboost's best error=0.4297
[flaml.automl: 09-16 19:26:05] {2897} INFO - iteration 31, current learner xgboost
[flaml.automl: 09-16 19:26:05] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.4297,	best estimator xgboost's best error=0.4297
[flaml.automl: 09-16 19:26:05] {2897} INFO - iteration 32, current learner xgboost
[flaml.automl: 09-16 19:26:06] {3072} INFO -  at 53.7s,	estimator xgboost's best error=0.4292,	best estimator xgboost's best error=0.4292
[flaml.automl: 09-16 19:26:06] {2897} INFO - iteration 33, current learner xgboost
[flaml.automl: 09-16 19:26:07] {3072} INFO -  at 54.5s,	estimator xgboost's best error=0.4292,	best estimator xgboost's best error=0.4292
[flaml.automl: 09-16 19:26:07] {2897} INFO - iteration 34, current learner xgboost
[flaml.automl: 09-16 19:26:08] {3072} INFO -  at 55.4s,	estimator xgboost's best error=0.4292,	best estimator xgboost's best error=0.4292
[flaml.automl: 09-16 19:26:08] {2897} INFO - iteration 35, current learner xgboost
[flaml.automl: 09-16 19:26:09] {3072} INFO -  at 56.1s,	estimator xgboost's best error=0.4292,	best estimator xgboost's best error=0.4292
[flaml.automl: 09-16 19:26:09] {2897} INFO - iteration 36, current learner xgboost
[flaml.automl: 09-16 19:26:09] {3072} INFO -  at 56.9s,	estimator xgboost's best error=0.4292,	best estimator xgboost's best error=0.4292
[flaml.automl: 09-16 19:26:09] {2897} INFO - iteration 37, current learner xgboost
[flaml.automl: 09-16 19:26:59] {3072} INFO -  at 106.8s,	estimator xgboost's best error=0.4292,	best estimator xgboost's best error=0.4292
[flaml.automl: 09-16 19:27:49] {3335} INFO - retrain xgboost for 49.7s
[flaml.automl: 09-16 19:27:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7970409577841507, colsample_bynode=1,
             colsample_bytree=0.5428779896657706, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=890, min_child_weight=0.03171566491085697,
             missing=nan, monotone_constraints='()', n_estimators=2, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.01487730509938369,
             reg_lambda=11.578212830029724, scale_pos_weight=1,
             subsample=0.8728746814430716, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 19:27:49] {2636} INFO - fit succeeded
[flaml.automl: 09-16 19:27:49] {2637} INFO - Time taken to find the best model: 53.65407919883728
[flaml.automl: 09-16 19:27:49] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 240, 'max_leaves': 890, 'min_child_weight': 0.03171566491085697, 'learning_rate': 1.0, 'subsample': 0.8728746814430716, 'colsample_bylevel': 0.7970409577841507, 'colsample_bytree': 0.5428779896657706, 'reg_alpha': 0.01487730509938369, 'reg_lambda': 11.578212830029724, 'FLAML_sample_size': 40000}
SO2(0)最佳损失：0.5707966219042944
SO2(0)最好结果：{'pred_time': 1.989438483016413e-06, 'wall_clock_time': 53.65407919883728, 'metric_for_logging': {'pred_time': 1.989438483016413e-06}, 'val_loss': 0.4292033780957056, 'training_iteration': 1, 'config': {'n_estimators': 240, 'max_leaves': 890, 'min_child_weight': 0.03171566491085697, 'learning_rate': 1.0, 'subsample': 0.8728746814430716, 'colsample_bylevel': 0.7970409577841507, 'colsample_bytree': 0.5428779896657706, 'reg_alpha': 0.01487730509938369, 'reg_lambda': 11.578212830029724, 'FLAML_sample_size': 40000}, 'config/n_estimators': 240, 'config/max_leaves': 890, 'config/min_child_weight': 0.03171566491085697, 'config/learning_rate': 1.0, 'config/subsample': 0.8728746814430716, 'config/colsample_bylevel': 0.7970409577841507, 'config/colsample_bytree': 0.5428779896657706, 'config/reg_alpha': 0.01487730509938369, 'config/reg_lambda': 11.578212830029724, 'config/FLAML_sample_size': 40000, 'experiment_tag': 'exp', 'time_total_s': 0.7558059692382812}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7970409577841507, colsample_bynode=1,
             colsample_bytree=0.5428779896657706, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=890, min_child_weight=0.03171566491085697,
             missing=nan, monotone_constraints='()', n_estimators=2, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.01487730509938369,
             reg_lambda=11.578212830029724, scale_pos_weight=1,
             subsample=0.8728746814430716, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8496599652163948
SO2(0)的mse=0.5443653056302407
SO2(0)的mae=0.4251828957986051
SO2(0)的mar=0.06022838327555149
总共花费的时间为：159.70
深圳市
1356A
1363A
1364A
1365A
1366A
3305A
3306A
3307A
3447A
3623A
[flaml.automl: 09-16 19:57:27] {2390} INFO - task = regression
[flaml.automl: 09-16 19:57:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 19:57:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 19:57:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 19:57:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 19:57:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 19:57:28] {3025} INFO - Estimated sufficient time budget=132325s. Estimated necessary time budget=132s.
[flaml.automl: 09-16 19:57:28] {3072} INFO -  at 1.6s,	estimator xgboost's best error=3.3808,	best estimator xgboost's best error=3.3808
[flaml.automl: 09-16 19:57:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 19:57:30] {3072} INFO -  at 3.8s,	estimator xgboost's best error=1.5258,	best estimator xgboost's best error=1.5258
[flaml.automl: 09-16 19:57:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 19:57:31] {3072} INFO -  at 5.0s,	estimator xgboost's best error=1.5258,	best estimator xgboost's best error=1.5258
[flaml.automl: 09-16 19:57:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 19:57:34] {3072} INFO -  at 7.1s,	estimator xgboost's best error=1.5258,	best estimator xgboost's best error=1.5258
[flaml.automl: 09-16 19:57:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 19:57:35] {3072} INFO -  at 8.2s,	estimator xgboost's best error=0.6616,	best estimator xgboost's best error=0.6616
[flaml.automl: 09-16 19:57:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 19:57:36] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.5517,	best estimator xgboost's best error=0.5517
[flaml.automl: 09-16 19:57:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 19:57:38] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.4383,	best estimator xgboost's best error=0.4383
[flaml.automl: 09-16 19:57:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 19:57:40] {3072} INFO -  at 13.2s,	estimator xgboost's best error=0.4383,	best estimator xgboost's best error=0.4383
[flaml.automl: 09-16 19:57:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 19:57:41] {3072} INFO -  at 14.8s,	estimator xgboost's best error=0.4383,	best estimator xgboost's best error=0.4383
[flaml.automl: 09-16 19:57:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 19:57:43] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.4383,	best estimator xgboost's best error=0.4383
[flaml.automl: 09-16 19:57:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 19:57:44] {3072} INFO -  at 18.0s,	estimator xgboost's best error=0.4365,	best estimator xgboost's best error=0.4365
[flaml.automl: 09-16 19:57:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 19:57:46] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.4365,	best estimator xgboost's best error=0.4365
[flaml.automl: 09-16 19:57:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 19:57:47] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.3250,	best estimator xgboost's best error=0.3250
[flaml.automl: 09-16 19:57:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 19:57:48] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.3250,	best estimator xgboost's best error=0.3250
[flaml.automl: 09-16 19:57:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 19:57:50] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.3250,	best estimator xgboost's best error=0.3250
[flaml.automl: 09-16 19:57:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 19:57:51] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.3250,	best estimator xgboost's best error=0.3250
[flaml.automl: 09-16 19:57:51] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 19:57:52] {3072} INFO -  at 25.4s,	estimator xgboost's best error=0.3250,	best estimator xgboost's best error=0.3250
[flaml.automl: 09-16 19:57:52] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 19:57:58] {3072} INFO -  at 31.4s,	estimator xgboost's best error=0.3250,	best estimator xgboost's best error=0.3250
[flaml.automl: 09-16 19:57:58] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 19:58:00] {3072} INFO -  at 33.9s,	estimator xgboost's best error=0.3250,	best estimator xgboost's best error=0.3250
[flaml.automl: 09-16 19:58:00] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 19:58:15] {3072} INFO -  at 48.7s,	estimator xgboost's best error=0.3250,	best estimator xgboost's best error=0.3250
[flaml.automl: 09-16 19:58:15] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 19:58:17] {3072} INFO -  at 51.0s,	estimator xgboost's best error=0.3250,	best estimator xgboost's best error=0.3250
[flaml.automl: 09-16 19:58:17] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 19:58:26] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.3250,	best estimator xgboost's best error=0.3250
[flaml.automl: 09-16 19:58:32] {3335} INFO - retrain xgboost for 6.0s
[flaml.automl: 09-16 19:58:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 19:58:32] {2636} INFO - fit succeeded
[flaml.automl: 09-16 19:58:32] {2637} INFO - Time taken to find the best model: 20.36698579788208
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：0.6750322539813665
SO2(0)最好结果：{'pred_time': 3.2005783285095204e-06, 'wall_clock_time': 20.36698579788208, 'metric_for_logging': {'pred_time': 3.2005783285095204e-06}, 'val_loss': 0.3249677460186336, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024, 'FLAML_sample_size': 10000}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 1.2372956275939941}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8998211891207131
SO2(0)的mse=0.30218640917821515
SO2(0)的mae=0.34184714467498
SO2(0)的mar=0.06914776017803605
总共花费的时间为：66.86
珠海市
1368A
1369A
1370A
3308A
3448A
[flaml.automl: 09-16 20:14:20] {2390} INFO - task = regression
[flaml.automl: 09-16 20:14:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:14:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:14:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:14:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:14:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:14:22] {3025} INFO - Estimated sufficient time budget=102196s. Estimated necessary time budget=102s.
[flaml.automl: 09-16 20:14:22] {3072} INFO -  at 2.1s,	estimator xgboost's best error=4.2021,	best estimator xgboost's best error=4.2021
[flaml.automl: 09-16 20:14:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:14:26] {3072} INFO -  at 5.4s,	estimator xgboost's best error=1.8977,	best estimator xgboost's best error=1.8977
[flaml.automl: 09-16 20:14:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:14:27] {3072} INFO -  at 7.3s,	estimator xgboost's best error=1.8977,	best estimator xgboost's best error=1.8977
[flaml.automl: 09-16 20:14:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:14:32] {3072} INFO -  at 11.7s,	estimator xgboost's best error=1.8977,	best estimator xgboost's best error=1.8977
[flaml.automl: 09-16 20:14:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:14:33] {3072} INFO -  at 13.2s,	estimator xgboost's best error=0.8402,	best estimator xgboost's best error=0.8402
[flaml.automl: 09-16 20:14:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:14:36] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.6073,	best estimator xgboost's best error=0.6073
[flaml.automl: 09-16 20:14:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:14:39] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.5849,	best estimator xgboost's best error=0.5849
[flaml.automl: 09-16 20:14:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:14:42] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.5849,	best estimator xgboost's best error=0.5849
[flaml.automl: 09-16 20:14:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:14:44] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.5590,	best estimator xgboost's best error=0.5590
[flaml.automl: 09-16 20:14:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:14:47] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.5590,	best estimator xgboost's best error=0.5590
[flaml.automl: 09-16 20:14:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:14:49] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.5590,	best estimator xgboost's best error=0.5590
[flaml.automl: 09-16 20:14:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:14:50] {3072} INFO -  at 29.9s,	estimator xgboost's best error=0.5590,	best estimator xgboost's best error=0.5590
[flaml.automl: 09-16 20:14:50] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:14:52] {3072} INFO -  at 31.8s,	estimator xgboost's best error=0.5068,	best estimator xgboost's best error=0.5068
[flaml.automl: 09-16 20:14:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:14:54] {3072} INFO -  at 33.6s,	estimator xgboost's best error=0.5068,	best estimator xgboost's best error=0.5068
[flaml.automl: 09-16 20:14:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:14:56] {3072} INFO -  at 35.6s,	estimator xgboost's best error=0.5068,	best estimator xgboost's best error=0.5068
[flaml.automl: 09-16 20:14:56] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 20:14:58] {3072} INFO -  at 38.0s,	estimator xgboost's best error=0.5065,	best estimator xgboost's best error=0.5065
[flaml.automl: 09-16 20:14:58] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 20:15:00] {3072} INFO -  at 39.7s,	estimator xgboost's best error=0.5045,	best estimator xgboost's best error=0.5045
[flaml.automl: 09-16 20:15:00] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 20:15:01] {3072} INFO -  at 41.3s,	estimator xgboost's best error=0.5045,	best estimator xgboost's best error=0.5045
[flaml.automl: 09-16 20:15:01] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 20:15:02] {3072} INFO -  at 42.2s,	estimator xgboost's best error=0.5045,	best estimator xgboost's best error=0.5045
[flaml.automl: 09-16 20:15:02] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 20:15:04] {3072} INFO -  at 43.5s,	estimator xgboost's best error=0.5045,	best estimator xgboost's best error=0.5045
[flaml.automl: 09-16 20:15:04] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 20:15:05] {3072} INFO -  at 44.6s,	estimator xgboost's best error=0.5045,	best estimator xgboost's best error=0.5045
[flaml.automl: 09-16 20:15:05] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 20:15:12] {3072} INFO -  at 51.5s,	estimator xgboost's best error=0.5045,	best estimator xgboost's best error=0.5045
[flaml.automl: 09-16 20:15:12] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 20:15:19] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.5045,	best estimator xgboost's best error=0.5045
[flaml.automl: 09-16 20:15:26] {3335} INFO - retrain xgboost for 6.8s
[flaml.automl: 09-16 20:15:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.25014912526093963,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466846,
             scale_pos_weight=1, subsample=0.9468514399123292,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 20:15:26] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:15:26] {2637} INFO - Time taken to find the best model: 39.65726137161255
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.25014912526093963, 'learning_rate': 1.0, 'subsample': 0.9468514399123292, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466846, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：0.4954794885639965
SO2(0)最好结果：{'pred_time': 6.964308028232602e-06, 'wall_clock_time': 39.65726137161255, 'metric_for_logging': {'pred_time': 6.964308028232602e-06}, 'val_loss': 0.5045205114360035, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.25014912526093963, 'learning_rate': 1.0, 'subsample': 0.9468514399123292, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466846, 'FLAML_sample_size': 10000}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.25014912526093963, 'config/learning_rate': 1.0, 'config/subsample': 0.9468514399123292, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466846, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 1.6359295845031738}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.25014912526093963,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466846,
             scale_pos_weight=1, subsample=0.9468514399123292,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8084704773350602
SO2(0)的mse=0.6695571604711628
SO2(0)的mae=0.5032933584380945
SO2(0)的mar=0.08123187193553853
总共花费的时间为：67.12
佛山市
1371A
1372A
1373A
1377A
1378A
3625A
[flaml.automl: 09-16 20:34:15] {2390} INFO - task = regression
[flaml.automl: 09-16 20:34:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:34:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:34:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:34:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:34:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:34:17] {3025} INFO - Estimated sufficient time budget=80191s. Estimated necessary time budget=80s.
[flaml.automl: 09-16 20:34:17] {3072} INFO -  at 1.5s,	estimator xgboost's best error=3.9994,	best estimator xgboost's best error=3.9994
[flaml.automl: 09-16 20:34:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:34:19] {3072} INFO -  at 3.6s,	estimator xgboost's best error=1.8345,	best estimator xgboost's best error=1.8345
[flaml.automl: 09-16 20:34:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:34:20] {3072} INFO -  at 4.8s,	estimator xgboost's best error=1.8345,	best estimator xgboost's best error=1.8345
[flaml.automl: 09-16 20:34:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:34:24] {3072} INFO -  at 8.6s,	estimator xgboost's best error=1.8345,	best estimator xgboost's best error=1.8345
[flaml.automl: 09-16 20:34:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:34:25] {3072} INFO -  at 9.7s,	estimator xgboost's best error=1.0644,	best estimator xgboost's best error=1.0644
[flaml.automl: 09-16 20:34:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:34:26] {3072} INFO -  at 11.3s,	estimator xgboost's best error=0.9369,	best estimator xgboost's best error=0.9369
[flaml.automl: 09-16 20:34:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:34:28] {3072} INFO -  at 12.9s,	estimator xgboost's best error=0.8661,	best estimator xgboost's best error=0.8661
[flaml.automl: 09-16 20:34:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:34:31] {3072} INFO -  at 15.6s,	estimator xgboost's best error=0.8661,	best estimator xgboost's best error=0.8661
[flaml.automl: 09-16 20:34:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:34:32] {3072} INFO -  at 17.2s,	estimator xgboost's best error=0.8583,	best estimator xgboost's best error=0.8583
[flaml.automl: 09-16 20:34:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:34:35] {3072} INFO -  at 20.2s,	estimator xgboost's best error=0.8154,	best estimator xgboost's best error=0.8154
[flaml.automl: 09-16 20:34:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:34:37] {3072} INFO -  at 21.4s,	estimator xgboost's best error=0.8154,	best estimator xgboost's best error=0.8154
[flaml.automl: 09-16 20:34:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:34:39] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.7746,	best estimator xgboost's best error=0.7746
[flaml.automl: 09-16 20:34:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:34:42] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.7746,	best estimator xgboost's best error=0.7746
[flaml.automl: 09-16 20:34:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:34:44] {3072} INFO -  at 29.2s,	estimator xgboost's best error=0.7746,	best estimator xgboost's best error=0.7746
[flaml.automl: 09-16 20:34:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:34:46] {3072} INFO -  at 30.8s,	estimator xgboost's best error=0.7746,	best estimator xgboost's best error=0.7746
[flaml.automl: 09-16 20:34:46] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 20:34:48] {3072} INFO -  at 32.5s,	estimator xgboost's best error=0.7743,	best estimator xgboost's best error=0.7743
[flaml.automl: 09-16 20:34:48] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 20:34:50] {3072} INFO -  at 34.4s,	estimator xgboost's best error=0.7743,	best estimator xgboost's best error=0.7743
[flaml.automl: 09-16 20:34:50] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 20:34:51] {3072} INFO -  at 35.7s,	estimator xgboost's best error=0.7743,	best estimator xgboost's best error=0.7743
[flaml.automl: 09-16 20:34:51] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 20:34:53] {3072} INFO -  at 37.3s,	estimator xgboost's best error=0.7743,	best estimator xgboost's best error=0.7743
[flaml.automl: 09-16 20:34:53] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 20:34:54] {3072} INFO -  at 38.5s,	estimator xgboost's best error=0.7743,	best estimator xgboost's best error=0.7743
[flaml.automl: 09-16 20:34:54] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 20:35:02] {3072} INFO -  at 46.5s,	estimator xgboost's best error=0.7674,	best estimator xgboost's best error=0.7674
[flaml.automl: 09-16 20:35:02] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 20:35:14] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.7639,	best estimator xgboost's best error=0.7639
[flaml.automl: 09-16 20:36:11] {3335} INFO - retrain xgboost for 57.0s
[flaml.automl: 09-16 20:36:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7567192013554584, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.003975665986684456, missing=nan,
             monotone_constraints='()', n_estimators=38, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0033439463336546946, reg_lambda=0.0901848624343786,
             scale_pos_weight=1, subsample=0.8352593723923232,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 20:36:11] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:36:11] {2637} INFO - Time taken to find the best model: 59.08914136886597
[flaml.automl: 09-16 20:36:11] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 38, 'max_leaves': 17, 'min_child_weight': 0.003975665986684456, 'learning_rate': 0.6023269513573992, 'subsample': 0.8352593723923232, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7567192013554584, 'reg_alpha': 0.0033439463336546946, 'reg_lambda': 0.0901848624343786, 'FLAML_sample_size': 66121}
SO2(0)最佳损失：0.2361347445569364
SO2(0)最好结果：{'pred_time': 5.518090437752415e-06, 'wall_clock_time': 59.08914136886597, 'metric_for_logging': {'pred_time': 5.518090437752415e-06}, 'val_loss': 0.7638652554430636, 'training_iteration': 1, 'config': {'n_estimators': 38, 'max_leaves': 17, 'min_child_weight': 0.003975665986684456, 'learning_rate': 0.6023269513573992, 'subsample': 0.8352593723923232, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7567192013554584, 'reg_alpha': 0.0033439463336546946, 'reg_lambda': 0.0901848624343786, 'FLAML_sample_size': 66121}, 'config/n_estimators': 38, 'config/max_leaves': 17, 'config/min_child_weight': 0.003975665986684456, 'config/learning_rate': 0.6023269513573992, 'config/subsample': 0.8352593723923232, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7567192013554584, 'config/reg_alpha': 0.0033439463336546946, 'config/reg_lambda': 0.0901848624343786, 'config/FLAML_sample_size': 66121, 'experiment_tag': 'exp', 'time_total_s': 12.59069538116455}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7567192013554584, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=17,
             min_child_weight=0.003975665986684456, missing=nan,
             monotone_constraints='()', n_estimators=38, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0033439463336546946, reg_lambda=0.0901848624343786,
             scale_pos_weight=1, subsample=0.8352593723923232,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7643789421341135
SO2(0)的mse=1.6224070527709005
SO2(0)的mae=0.7740064797624544
SO2(0)的mar=0.11692411831259654
总共花费的时间为：117.25
中山市
1379A
3454A
[flaml.automl: 09-16 20:42:53] {2390} INFO - task = regression
[flaml.automl: 09-16 20:42:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:42:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:42:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:42:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:42:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:42:54] {3025} INFO - Estimated sufficient time budget=12004s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 20:42:54] {3072} INFO -  at 1.3s,	estimator xgboost's best error=2.5393,	best estimator xgboost's best error=2.5393
[flaml.automl: 09-16 20:42:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:42:56] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.1881,	best estimator xgboost's best error=1.1881
[flaml.automl: 09-16 20:42:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:42:57] {3072} INFO -  at 4.6s,	estimator xgboost's best error=1.1881,	best estimator xgboost's best error=1.1881
[flaml.automl: 09-16 20:42:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:43:07] {3072} INFO -  at 14.0s,	estimator xgboost's best error=1.1881,	best estimator xgboost's best error=1.1881
[flaml.automl: 09-16 20:43:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:43:08] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.6718,	best estimator xgboost's best error=0.6718
[flaml.automl: 09-16 20:43:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:43:09] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.5802,	best estimator xgboost's best error=0.5802
[flaml.automl: 09-16 20:43:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:43:11] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.5546,	best estimator xgboost's best error=0.5546
[flaml.automl: 09-16 20:43:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:43:14] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.5546,	best estimator xgboost's best error=0.5546
[flaml.automl: 09-16 20:43:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:43:15] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.5312,	best estimator xgboost's best error=0.5312
[flaml.automl: 09-16 20:43:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:43:18] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.5312,	best estimator xgboost's best error=0.5312
[flaml.automl: 09-16 20:43:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:43:20] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.5312,	best estimator xgboost's best error=0.5312
[flaml.automl: 09-16 20:43:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:43:21] {3072} INFO -  at 28.6s,	estimator xgboost's best error=0.5312,	best estimator xgboost's best error=0.5312
[flaml.automl: 09-16 20:43:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:43:28] {3072} INFO -  at 34.9s,	estimator xgboost's best error=0.4772,	best estimator xgboost's best error=0.4772
[flaml.automl: 09-16 20:43:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:43:53] {3072} INFO -  at 60.6s,	estimator xgboost's best error=0.4708,	best estimator xgboost's best error=0.4708
[flaml.automl: 09-16 20:44:25] {3335} INFO - retrain xgboost for 31.4s
[flaml.automl: 09-16 20:44:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 20:44:25] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:44:25] {2637} INFO - Time taken to find the best model: 60.59881043434143
[flaml.automl: 09-16 20:44:25] {2648} WARNING - Time taken to find the best model is 101% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}
SO2(0)最佳损失：0.5292431928761383
SO2(0)最好结果：{'pred_time': 5.458709741831804e-05, 'wall_clock_time': 60.59881043434143, 'metric_for_logging': {'pred_time': 5.458709741831804e-05}, 'val_loss': 0.4707568071238617, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'experiment_tag': 'exp', 'time_total_s': 25.65446400642395}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8040903151099941
SO2(0)的mse=0.510839553922769
SO2(0)的mae=0.4523129018746253
SO2(0)的mar=0.11783695172116741
总共花费的时间为：92.55
江门市
1386A
3449A
[flaml.automl: 09-16 20:51:10] {2390} INFO - task = regression
[flaml.automl: 09-16 20:51:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 20:51:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 20:51:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 20:51:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 20:51:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 20:51:11] {3025} INFO - Estimated sufficient time budget=11883s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 20:51:11] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.9207,	best estimator xgboost's best error=3.9207
[flaml.automl: 09-16 20:51:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 20:51:14] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.7559,	best estimator xgboost's best error=1.7559
[flaml.automl: 09-16 20:51:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 20:51:15] {3072} INFO -  at 4.6s,	estimator xgboost's best error=1.7559,	best estimator xgboost's best error=1.7559
[flaml.automl: 09-16 20:51:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 20:51:24] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.7559,	best estimator xgboost's best error=1.7559
[flaml.automl: 09-16 20:51:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 20:51:25] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.8165,	best estimator xgboost's best error=0.8165
[flaml.automl: 09-16 20:51:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 20:51:27] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.6851,	best estimator xgboost's best error=0.6851
[flaml.automl: 09-16 20:51:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 20:51:28] {3072} INFO -  at 18.2s,	estimator xgboost's best error=0.5825,	best estimator xgboost's best error=0.5825
[flaml.automl: 09-16 20:51:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 20:51:31] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.5825,	best estimator xgboost's best error=0.5825
[flaml.automl: 09-16 20:51:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 20:51:33] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.5616,	best estimator xgboost's best error=0.5616
[flaml.automl: 09-16 20:51:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 20:51:36] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.5616,	best estimator xgboost's best error=0.5616
[flaml.automl: 09-16 20:51:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 20:51:37] {3072} INFO -  at 27.3s,	estimator xgboost's best error=0.5598,	best estimator xgboost's best error=0.5598
[flaml.automl: 09-16 20:51:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 20:51:39] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.5598,	best estimator xgboost's best error=0.5598
[flaml.automl: 09-16 20:51:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 20:51:45] {3072} INFO -  at 34.9s,	estimator xgboost's best error=0.4950,	best estimator xgboost's best error=0.4950
[flaml.automl: 09-16 20:51:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 20:51:56] {3072} INFO -  at 46.1s,	estimator xgboost's best error=0.4833,	best estimator xgboost's best error=0.4833
[flaml.automl: 09-16 20:51:56] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 20:52:03] {3072} INFO -  at 52.6s,	estimator xgboost's best error=0.4833,	best estimator xgboost's best error=0.4833
[flaml.automl: 09-16 20:52:20] {3335} INFO - retrain xgboost for 17.2s
[flaml.automl: 09-16 20:52:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 20:52:20] {2636} INFO - fit succeeded
[flaml.automl: 09-16 20:52:20] {2637} INFO - Time taken to find the best model: 46.070865869522095
[flaml.automl: 09-16 20:52:20] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}
SO2(0)最佳损失：0.5167373709398608
SO2(0)最好结果：{'pred_time': 1.6569036298093542e-05, 'wall_clock_time': 46.070865869522095, 'metric_for_logging': {'pred_time': 1.6569036298093542e-05}, 'val_loss': 0.4832626290601391, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 2.978767810916454, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532448312341252, 'config/reg_alpha': 0.0015779591896894877, 'config/reg_lambda': 1.3500086298278042, 'experiment_tag': 'exp', 'time_total_s': 11.154381513595581}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8294669564428703
SO2(0)的mse=0.6289530861338706
SO2(0)的mae=0.5073643851159363
SO2(0)的mar=0.07940585613167325
总共花费的时间为：70.18
东莞市
1389A
1391A
3319A
3626A
3627A
[flaml.automl: 09-16 21:08:54] {2390} INFO - task = regression
[flaml.automl: 09-16 21:08:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:08:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:08:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:08:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:08:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:08:55] {3025} INFO - Estimated sufficient time budget=66794s. Estimated necessary time budget=67s.
[flaml.automl: 09-16 21:08:55] {3072} INFO -  at 1.5s,	estimator xgboost's best error=4.8264,	best estimator xgboost's best error=4.8264
[flaml.automl: 09-16 21:08:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:08:57] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.1648,	best estimator xgboost's best error=2.1648
[flaml.automl: 09-16 21:08:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:08:58] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.1648,	best estimator xgboost's best error=2.1648
[flaml.automl: 09-16 21:08:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:09:03] {3072} INFO -  at 9.3s,	estimator xgboost's best error=2.1648,	best estimator xgboost's best error=2.1648
[flaml.automl: 09-16 21:09:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:09:04] {3072} INFO -  at 10.4s,	estimator xgboost's best error=0.9189,	best estimator xgboost's best error=0.9189
[flaml.automl: 09-16 21:09:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:09:05] {3072} INFO -  at 12.0s,	estimator xgboost's best error=0.7001,	best estimator xgboost's best error=0.7001
[flaml.automl: 09-16 21:09:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:09:07] {3072} INFO -  at 13.6s,	estimator xgboost's best error=0.6493,	best estimator xgboost's best error=0.6493
[flaml.automl: 09-16 21:09:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:09:10] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.6493,	best estimator xgboost's best error=0.6493
[flaml.automl: 09-16 21:09:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:09:11] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.6352,	best estimator xgboost's best error=0.6352
[flaml.automl: 09-16 21:09:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:09:14] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.6352,	best estimator xgboost's best error=0.6352
[flaml.automl: 09-16 21:09:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:09:16] {3072} INFO -  at 22.4s,	estimator xgboost's best error=0.6014,	best estimator xgboost's best error=0.6014
[flaml.automl: 09-16 21:09:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:09:17] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.6014,	best estimator xgboost's best error=0.6014
[flaml.automl: 09-16 21:09:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:09:19] {3072} INFO -  at 26.1s,	estimator xgboost's best error=0.5321,	best estimator xgboost's best error=0.5321
[flaml.automl: 09-16 21:09:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:09:22] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.5233,	best estimator xgboost's best error=0.5233
[flaml.automl: 09-16 21:09:22] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:09:25] {3072} INFO -  at 31.8s,	estimator xgboost's best error=0.5233,	best estimator xgboost's best error=0.5233
[flaml.automl: 09-16 21:09:25] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 21:09:27] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.5233,	best estimator xgboost's best error=0.5233
[flaml.automl: 09-16 21:09:27] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 21:09:29] {3072} INFO -  at 36.1s,	estimator xgboost's best error=0.5233,	best estimator xgboost's best error=0.5233
[flaml.automl: 09-16 21:09:29] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 21:09:31] {3072} INFO -  at 38.2s,	estimator xgboost's best error=0.5233,	best estimator xgboost's best error=0.5233
[flaml.automl: 09-16 21:09:31] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 21:09:42] {3072} INFO -  at 48.9s,	estimator xgboost's best error=0.5233,	best estimator xgboost's best error=0.5233
[flaml.automl: 09-16 21:09:42] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 21:09:46] {3072} INFO -  at 53.0s,	estimator xgboost's best error=0.5233,	best estimator xgboost's best error=0.5233
[flaml.automl: 09-16 21:09:46] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 21:09:53] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.5233,	best estimator xgboost's best error=0.5233
[flaml.automl: 09-16 21:10:04] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-16 21:10:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 21:10:04] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:10:04] {2637} INFO - Time taken to find the best model: 29.07208251953125
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：0.4767070710996212
SO2(0)最好结果：{'pred_time': 6.380230694147002e-06, 'wall_clock_time': 29.07208251953125, 'metric_for_logging': {'pred_time': 6.380230694147002e-06}, 'val_loss': 0.5232929289003788, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 2.978767810916454, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7532448312341252, 'reg_alpha': 0.0015779591896894877, 'reg_lambda': 1.3500086298278042, 'FLAML_sample_size': 10000}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 2.978767810916454, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7532448312341252, 'config/reg_alpha': 0.0015779591896894877, 'config/reg_lambda': 1.3500086298278042, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 2.9620871543884277}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7532448312341252, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=2.978767810916454, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015779591896894877, reg_lambda=1.3500086298278042,
             scale_pos_weight=1, subsample=0.8921566499079494,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.888314412844214
SO2(0)的mse=0.6242877261667716
SO2(0)的mae=0.5207127662359853
SO2(0)的mar=0.07444364670231347
总共花费的时间为：71.15
惠州市
1392A
1393A
1395A
1396A
3314A
3452A
[flaml.automl: 09-16 21:28:33] {2390} INFO - task = regression
[flaml.automl: 09-16 21:28:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:28:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:28:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:28:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:28:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:28:34] {3025} INFO - Estimated sufficient time budget=80001s. Estimated necessary time budget=80s.
[flaml.automl: 09-16 21:28:34] {3072} INFO -  at 1.5s,	estimator xgboost's best error=3.4249,	best estimator xgboost's best error=3.4249
[flaml.automl: 09-16 21:28:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:28:37] {3072} INFO -  at 3.6s,	estimator xgboost's best error=1.5924,	best estimator xgboost's best error=1.5924
[flaml.automl: 09-16 21:28:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:28:38] {3072} INFO -  at 4.8s,	estimator xgboost's best error=1.5924,	best estimator xgboost's best error=1.5924
[flaml.automl: 09-16 21:28:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:28:41] {3072} INFO -  at 8.5s,	estimator xgboost's best error=1.5924,	best estimator xgboost's best error=1.5924
[flaml.automl: 09-16 21:28:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:28:43] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.8356,	best estimator xgboost's best error=0.8356
[flaml.automl: 09-16 21:28:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:28:44] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.6484,	best estimator xgboost's best error=0.6484
[flaml.automl: 09-16 21:28:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:28:46] {3072} INFO -  at 12.7s,	estimator xgboost's best error=0.6484,	best estimator xgboost's best error=0.6484
[flaml.automl: 09-16 21:28:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:28:48] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.6484,	best estimator xgboost's best error=0.6484
[flaml.automl: 09-16 21:28:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:28:49] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.6484,	best estimator xgboost's best error=0.6484
[flaml.automl: 09-16 21:28:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:28:52] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.6484,	best estimator xgboost's best error=0.6484
[flaml.automl: 09-16 21:28:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:28:54] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.6484,	best estimator xgboost's best error=0.6484
[flaml.automl: 09-16 21:28:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:28:55] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.6484,	best estimator xgboost's best error=0.6484
[flaml.automl: 09-16 21:28:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:29:01] {3072} INFO -  at 28.3s,	estimator xgboost's best error=0.5573,	best estimator xgboost's best error=0.5573
[flaml.automl: 09-16 21:29:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:29:13] {3072} INFO -  at 40.4s,	estimator xgboost's best error=0.5449,	best estimator xgboost's best error=0.5449
[flaml.automl: 09-16 21:29:13] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:29:20] {3072} INFO -  at 47.0s,	estimator xgboost's best error=0.5449,	best estimator xgboost's best error=0.5449
[flaml.automl: 09-16 21:29:20] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 21:29:33] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.5449,	best estimator xgboost's best error=0.5449
[flaml.automl: 09-16 21:29:45] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-16 21:29:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:29:45] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:29:45] {2637} INFO - Time taken to find the best model: 40.43283772468567
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 66617}
SO2(0)最佳损失：0.45511848700751933
SO2(0)最好结果：{'pred_time': 5.427291991742229e-06, 'wall_clock_time': 40.43283772468567, 'metric_for_logging': {'pred_time': 5.427291991742229e-06}, 'val_loss': 0.5448815129924807, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 66617}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 66617, 'experiment_tag': 'exp', 'time_total_s': 12.110372304916382}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8911262559544365
SO2(0)的mse=0.7144099555983815
SO2(0)的mae=0.5265988840955369
SO2(0)的mar=0.11408335486870651
总共花费的时间为：72.70
肇庆市
1397A
1398A
1400A
3451A
[flaml.automl: 09-16 21:42:41] {2390} INFO - task = regression
[flaml.automl: 09-16 21:42:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:42:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:42:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:42:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:42:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:42:42] {3025} INFO - Estimated sufficient time budget=49845s. Estimated necessary time budget=50s.
[flaml.automl: 09-16 21:42:42] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.4867,	best estimator xgboost's best error=5.4867
[flaml.automl: 09-16 21:42:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:42:44] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.4999,	best estimator xgboost's best error=2.4999
[flaml.automl: 09-16 21:42:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:42:46] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.4999,	best estimator xgboost's best error=2.4999
[flaml.automl: 09-16 21:42:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:42:52] {3072} INFO -  at 10.8s,	estimator xgboost's best error=2.4999,	best estimator xgboost's best error=2.4999
[flaml.automl: 09-16 21:42:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:42:53] {3072} INFO -  at 12.0s,	estimator xgboost's best error=1.4481,	best estimator xgboost's best error=1.4481
[flaml.automl: 09-16 21:42:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:42:54] {3072} INFO -  at 13.4s,	estimator xgboost's best error=1.1540,	best estimator xgboost's best error=1.1540
[flaml.automl: 09-16 21:42:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:42:56] {3072} INFO -  at 15.1s,	estimator xgboost's best error=1.1540,	best estimator xgboost's best error=1.1540
[flaml.automl: 09-16 21:42:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:42:58] {3072} INFO -  at 17.5s,	estimator xgboost's best error=1.1540,	best estimator xgboost's best error=1.1540
[flaml.automl: 09-16 21:42:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:43:00] {3072} INFO -  at 18.7s,	estimator xgboost's best error=1.1540,	best estimator xgboost's best error=1.1540
[flaml.automl: 09-16 21:43:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:43:02] {3072} INFO -  at 21.3s,	estimator xgboost's best error=1.1540,	best estimator xgboost's best error=1.1540
[flaml.automl: 09-16 21:43:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:43:04] {3072} INFO -  at 22.9s,	estimator xgboost's best error=1.1540,	best estimator xgboost's best error=1.1540
[flaml.automl: 09-16 21:43:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:43:05] {3072} INFO -  at 24.1s,	estimator xgboost's best error=1.1540,	best estimator xgboost's best error=1.1540
[flaml.automl: 09-16 21:43:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:43:12] {3072} INFO -  at 30.6s,	estimator xgboost's best error=1.1030,	best estimator xgboost's best error=1.1030
[flaml.automl: 09-16 21:43:12] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:43:24] {3072} INFO -  at 42.7s,	estimator xgboost's best error=1.0530,	best estimator xgboost's best error=1.0530
[flaml.automl: 09-16 21:43:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:43:30] {3072} INFO -  at 49.3s,	estimator xgboost's best error=1.0530,	best estimator xgboost's best error=1.0530
[flaml.automl: 09-16 21:43:58] {3335} INFO - retrain xgboost for 27.5s
[flaml.automl: 09-16 21:43:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:43:58] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:43:58] {2637} INFO - Time taken to find the best model: 42.731873750686646
[flaml.automl: 09-16 21:43:58] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41403}
SO2(0)最佳损失：-0.05304339848713013
SO2(0)最好结果：{'pred_time': 1.0527053415140518e-05, 'wall_clock_time': 42.731873750686646, 'metric_for_logging': {'pred_time': 1.0527053415140518e-05}, 'val_loss': 1.0530433984871301, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 41403}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 41403, 'experiment_tag': 'exp', 'time_total_s': 12.100067138671875}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.745928039832109
SO2(0)的mse=3.215109018940879
SO2(0)的mae=1.0524064907818707
SO2(0)的mar=0.11522089285199288
总共花费的时间为：77.55
南宁市
1401A
1408A
[flaml.automl: 09-16 21:50:25] {2390} INFO - task = regression
[flaml.automl: 09-16 21:50:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 21:50:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 21:50:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 21:50:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 21:50:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 21:50:26] {3025} INFO - Estimated sufficient time budget=12065s. Estimated necessary time budget=12s.
[flaml.automl: 09-16 21:50:26] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.1987,	best estimator xgboost's best error=4.1987
[flaml.automl: 09-16 21:50:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 21:50:29] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.9385,	best estimator xgboost's best error=1.9385
[flaml.automl: 09-16 21:50:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 21:50:30] {3072} INFO -  at 4.6s,	estimator xgboost's best error=1.9385,	best estimator xgboost's best error=1.9385
[flaml.automl: 09-16 21:50:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 21:50:39] {3072} INFO -  at 14.1s,	estimator xgboost's best error=1.9385,	best estimator xgboost's best error=1.9385
[flaml.automl: 09-16 21:50:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 21:50:40] {3072} INFO -  at 15.2s,	estimator xgboost's best error=1.0476,	best estimator xgboost's best error=1.0476
[flaml.automl: 09-16 21:50:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 21:50:42] {3072} INFO -  at 16.7s,	estimator xgboost's best error=0.9239,	best estimator xgboost's best error=0.9239
[flaml.automl: 09-16 21:50:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 21:50:43] {3072} INFO -  at 18.3s,	estimator xgboost's best error=0.9239,	best estimator xgboost's best error=0.9239
[flaml.automl: 09-16 21:50:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 21:50:46] {3072} INFO -  at 20.8s,	estimator xgboost's best error=0.9239,	best estimator xgboost's best error=0.9239
[flaml.automl: 09-16 21:50:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 21:50:47] {3072} INFO -  at 21.9s,	estimator xgboost's best error=0.9239,	best estimator xgboost's best error=0.9239
[flaml.automl: 09-16 21:50:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 21:50:50] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.9239,	best estimator xgboost's best error=0.9239
[flaml.automl: 09-16 21:50:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 21:50:51] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.9239,	best estimator xgboost's best error=0.9239
[flaml.automl: 09-16 21:50:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 21:50:52] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.9239,	best estimator xgboost's best error=0.9239
[flaml.automl: 09-16 21:50:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 21:50:58] {3072} INFO -  at 32.7s,	estimator xgboost's best error=0.9239,	best estimator xgboost's best error=0.9239
[flaml.automl: 09-16 21:50:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 21:51:01] {3072} INFO -  at 35.6s,	estimator xgboost's best error=0.8783,	best estimator xgboost's best error=0.8783
[flaml.automl: 09-16 21:51:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 21:51:02] {3072} INFO -  at 37.0s,	estimator xgboost's best error=0.8783,	best estimator xgboost's best error=0.8783
[flaml.automl: 09-16 21:51:02] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 21:51:07] {3072} INFO -  at 41.8s,	estimator xgboost's best error=0.8783,	best estimator xgboost's best error=0.8783
[flaml.automl: 09-16 21:51:07] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 21:51:09] {3072} INFO -  at 44.2s,	estimator xgboost's best error=0.8783,	best estimator xgboost's best error=0.8783
[flaml.automl: 09-16 21:51:09] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 21:51:11] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.8783,	best estimator xgboost's best error=0.8783
[flaml.automl: 09-16 21:51:11] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 21:51:18] {3072} INFO -  at 53.3s,	estimator xgboost's best error=0.8783,	best estimator xgboost's best error=0.8783
[flaml.automl: 09-16 21:51:18] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 21:51:20] {3072} INFO -  at 54.9s,	estimator xgboost's best error=0.8783,	best estimator xgboost's best error=0.8783
[flaml.automl: 09-16 21:51:20] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 21:51:25] {3072} INFO -  at 59.6s,	estimator xgboost's best error=0.8783,	best estimator xgboost's best error=0.8783
[flaml.automl: 09-16 21:51:28] {3335} INFO - retrain xgboost for 2.8s
[flaml.automl: 09-16 21:51:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 21:51:28] {2636} INFO - fit succeeded
[flaml.automl: 09-16 21:51:28] {2637} INFO - Time taken to find the best model: 35.56524348258972
SO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}
SO2(0)最佳损失：0.12165962199054703
SO2(0)最好结果：{'pred_time': 1.61634599672472e-05, 'wall_clock_time': 35.56524348258972, 'metric_for_logging': {'pred_time': 1.61634599672472e-05}, 'val_loss': 0.878340378009453, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 5, 'min_child_weight': 0.758814211387459, 'learning_rate': 0.29313806474778853, 'subsample': 0.7520389032316431, 'colsample_bylevel': 0.8985205690300798, 'colsample_bytree': 0.7992659131995336, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.2266745236797676}, 'config/n_estimators': 9, 'config/max_leaves': 5, 'config/min_child_weight': 0.758814211387459, 'config/learning_rate': 0.29313806474778853, 'config/subsample': 0.7520389032316431, 'config/colsample_bylevel': 0.8985205690300798, 'config/colsample_bytree': 0.7992659131995336, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.2266745236797676, 'experiment_tag': 'exp', 'time_total_s': 2.833583116531372}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8985205690300798, colsample_bynode=1,
             colsample_bytree=0.7992659131995336, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.29313806474778853,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=0.758814211387459, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.2266745236797676, scale_pos_weight=1,
             subsample=0.7520389032316431, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.39217933016914086
SO2(0)的mse=3.1140646995995835
SO2(0)的mae=0.935198364240346
SO2(0)的mar=0.12304193718078575
总共花费的时间为：62.87
海口市
1409A
1411A
1413A
3539A
[flaml.automl: 09-16 22:03:33] {2390} INFO - task = regression
[flaml.automl: 09-16 22:03:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 22:03:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 22:03:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 22:03:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 22:03:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 22:03:34] {3025} INFO - Estimated sufficient time budget=49056s. Estimated necessary time budget=49s.
[flaml.automl: 09-16 22:03:34] {3072} INFO -  at 1.3s,	estimator xgboost's best error=2.2199,	best estimator xgboost's best error=2.2199
[flaml.automl: 09-16 22:03:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 22:03:36] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.0888,	best estimator xgboost's best error=1.0888
[flaml.automl: 09-16 22:03:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 22:03:37] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.0888,	best estimator xgboost's best error=1.0888
[flaml.automl: 09-16 22:03:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 22:03:44] {3072} INFO -  at 11.0s,	estimator xgboost's best error=1.0888,	best estimator xgboost's best error=1.0888
[flaml.automl: 09-16 22:03:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 22:03:45] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.7029,	best estimator xgboost's best error=0.7029
[flaml.automl: 09-16 22:03:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 22:03:46] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.6396,	best estimator xgboost's best error=0.6396
[flaml.automl: 09-16 22:03:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 22:03:48] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.6293,	best estimator xgboost's best error=0.6293
[flaml.automl: 09-16 22:03:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 22:03:51] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.6293,	best estimator xgboost's best error=0.6293
[flaml.automl: 09-16 22:03:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 22:03:52] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.6287,	best estimator xgboost's best error=0.6287
[flaml.automl: 09-16 22:03:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 22:03:55] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.6287,	best estimator xgboost's best error=0.6287
[flaml.automl: 09-16 22:03:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 22:03:57] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.6287,	best estimator xgboost's best error=0.6287
[flaml.automl: 09-16 22:03:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 22:03:58] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.6287,	best estimator xgboost's best error=0.6287
[flaml.automl: 09-16 22:03:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 22:04:02] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.6242,	best estimator xgboost's best error=0.6242
[flaml.automl: 09-16 22:04:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 22:04:05] {3072} INFO -  at 32.4s,	estimator xgboost's best error=0.6118,	best estimator xgboost's best error=0.6118
[flaml.automl: 09-16 22:04:05] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 22:04:08] {3072} INFO -  at 35.2s,	estimator xgboost's best error=0.6118,	best estimator xgboost's best error=0.6118
[flaml.automl: 09-16 22:04:08] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 22:04:11] {3072} INFO -  at 38.1s,	estimator xgboost's best error=0.6118,	best estimator xgboost's best error=0.6118
[flaml.automl: 09-16 22:04:11] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 22:04:13] {3072} INFO -  at 40.4s,	estimator xgboost's best error=0.6118,	best estimator xgboost's best error=0.6118
[flaml.automl: 09-16 22:04:13] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 22:04:15] {3072} INFO -  at 42.5s,	estimator xgboost's best error=0.6118,	best estimator xgboost's best error=0.6118
[flaml.automl: 09-16 22:04:15] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 22:04:28] {3072} INFO -  at 55.3s,	estimator xgboost's best error=0.5989,	best estimator xgboost's best error=0.5989
[flaml.automl: 09-16 22:04:41] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-16 22:04:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-16 22:04:41] {2636} INFO - fit succeeded
[flaml.automl: 09-16 22:04:41] {2637} INFO - Time taken to find the best model: 55.311084508895874
[flaml.automl: 09-16 22:04:41] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 41281}
SO2(0)最佳损失：0.4010918534657054
SO2(0)最好结果：{'pred_time': 8.77600753647792e-06, 'wall_clock_time': 55.311084508895874, 'metric_for_logging': {'pred_time': 8.77600753647792e-06}, 'val_loss': 0.5989081465342946, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 41281}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'config/FLAML_sample_size': 41281, 'experiment_tag': 'exp', 'time_total_s': 12.778532028198242}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6163171268259935
SO2(0)的mse=0.6757390783623322
SO2(0)的mae=0.5995455366128656
SO2(0)的mar=0.17807805225416456
总共花费的时间为：68.71
重庆市
1414A
1418A
1419A
1420A
1422A
1428A
1429A
3015A
3016A
3346A
3347A
3348A
3349A
3350A
3351A
3352A
3353A
3354A
3355A
3356A
3482A
3483A
3484A
3485A
3599A
3600A
3601A
3610A
[flaml.automl: 09-16 23:28:35] {2390} INFO - task = regression
[flaml.automl: 09-16 23:28:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 23:28:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 23:28:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 23:28:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 23:28:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 23:28:36] {3025} INFO - Estimated sufficient time budget=254591s. Estimated necessary time budget=255s.
[flaml.automl: 09-16 23:28:36] {3072} INFO -  at 2.5s,	estimator xgboost's best error=7.9217,	best estimator xgboost's best error=7.9217
[flaml.automl: 09-16 23:28:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 23:28:37] {3072} INFO -  at 3.3s,	estimator xgboost's best error=7.5409,	best estimator xgboost's best error=7.5409
[flaml.automl: 09-16 23:28:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 23:28:38] {3072} INFO -  at 4.1s,	estimator xgboost's best error=7.5409,	best estimator xgboost's best error=7.5409
[flaml.automl: 09-16 23:28:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 23:28:39] {3072} INFO -  at 5.4s,	estimator xgboost's best error=7.5409,	best estimator xgboost's best error=7.5409
[flaml.automl: 09-16 23:28:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 23:28:40] {3072} INFO -  at 6.2s,	estimator xgboost's best error=4.8959,	best estimator xgboost's best error=4.8959
[flaml.automl: 09-16 23:28:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 23:28:41] {3072} INFO -  at 7.2s,	estimator xgboost's best error=4.0676,	best estimator xgboost's best error=4.0676
[flaml.automl: 09-16 23:28:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 23:28:42] {3072} INFO -  at 8.0s,	estimator xgboost's best error=4.0676,	best estimator xgboost's best error=4.0676
[flaml.automl: 09-16 23:28:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 23:28:43] {3072} INFO -  at 9.4s,	estimator xgboost's best error=4.0676,	best estimator xgboost's best error=4.0676
[flaml.automl: 09-16 23:28:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 23:28:44] {3072} INFO -  at 10.0s,	estimator xgboost's best error=3.3134,	best estimator xgboost's best error=3.3134
[flaml.automl: 09-16 23:28:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 23:28:44] {3072} INFO -  at 10.6s,	estimator xgboost's best error=3.3134,	best estimator xgboost's best error=3.3134
[flaml.automl: 09-16 23:28:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 23:28:45] {3072} INFO -  at 11.4s,	estimator xgboost's best error=1.6751,	best estimator xgboost's best error=1.6751
[flaml.automl: 09-16 23:28:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 23:28:46] {3072} INFO -  at 12.0s,	estimator xgboost's best error=1.6751,	best estimator xgboost's best error=1.6751
[flaml.automl: 09-16 23:28:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 23:28:47] {3072} INFO -  at 13.1s,	estimator xgboost's best error=1.4258,	best estimator xgboost's best error=1.4258
[flaml.automl: 09-16 23:28:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 23:28:48] {3072} INFO -  at 14.2s,	estimator xgboost's best error=1.4258,	best estimator xgboost's best error=1.4258
[flaml.automl: 09-16 23:28:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 23:28:49] {3072} INFO -  at 14.8s,	estimator xgboost's best error=1.4258,	best estimator xgboost's best error=1.4258
[flaml.automl: 09-16 23:28:49] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-16 23:28:49] {3072} INFO -  at 15.4s,	estimator xgboost's best error=1.4258,	best estimator xgboost's best error=1.4258
[flaml.automl: 09-16 23:28:49] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-16 23:28:50] {3072} INFO -  at 16.2s,	estimator xgboost's best error=1.4258,	best estimator xgboost's best error=1.4258
[flaml.automl: 09-16 23:28:50] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-16 23:28:52] {3072} INFO -  at 18.4s,	estimator xgboost's best error=1.2329,	best estimator xgboost's best error=1.2329
[flaml.automl: 09-16 23:28:52] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-16 23:28:54] {3072} INFO -  at 20.7s,	estimator xgboost's best error=1.2329,	best estimator xgboost's best error=1.2329
[flaml.automl: 09-16 23:28:54] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-16 23:28:56] {3072} INFO -  at 22.5s,	estimator xgboost's best error=1.2329,	best estimator xgboost's best error=1.2329
[flaml.automl: 09-16 23:28:56] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-16 23:28:58] {3072} INFO -  at 24.6s,	estimator xgboost's best error=1.2329,	best estimator xgboost's best error=1.2329
[flaml.automl: 09-16 23:28:58] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-16 23:29:01] {3072} INFO -  at 26.9s,	estimator xgboost's best error=1.1657,	best estimator xgboost's best error=1.1657
[flaml.automl: 09-16 23:29:01] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-16 23:29:04] {3072} INFO -  at 30.6s,	estimator xgboost's best error=1.1657,	best estimator xgboost's best error=1.1657
[flaml.automl: 09-16 23:29:04] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-16 23:29:05] {3072} INFO -  at 31.7s,	estimator xgboost's best error=1.1657,	best estimator xgboost's best error=1.1657
[flaml.automl: 09-16 23:29:05] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-16 23:29:10] {3072} INFO -  at 36.6s,	estimator xgboost's best error=1.1657,	best estimator xgboost's best error=1.1657
[flaml.automl: 09-16 23:29:10] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-16 23:29:12] {3072} INFO -  at 37.8s,	estimator xgboost's best error=1.1657,	best estimator xgboost's best error=1.1657
[flaml.automl: 09-16 23:29:12] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-16 23:29:32] {3072} INFO -  at 58.3s,	estimator xgboost's best error=1.1236,	best estimator xgboost's best error=1.1236
[flaml.automl: 09-16 23:29:46] {3335} INFO - retrain xgboost for 13.5s
[flaml.automl: 09-16 23:29:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9488606198341824, colsample_bynode=1,
             colsample_bytree=0.45830999468859307, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.5399321478998305,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.008506127608334661, reg_lambda=6.27023622703691,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 23:29:46] {2636} INFO - fit succeeded
[flaml.automl: 09-16 23:29:46] {2637} INFO - Time taken to find the best model: 58.31184196472168
[flaml.automl: 09-16 23:29:46] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.5399321478998305, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.9488606198341824, 'colsample_bytree': 0.45830999468859307, 'reg_alpha': 0.008506127608334661, 'reg_lambda': 6.27023622703691, 'FLAML_sample_size': 306605}
SO2(0)最佳损失：-0.12356410745837065
SO2(0)最好结果：{'pred_time': 1.572388304893688e-06, 'wall_clock_time': 58.31184196472168, 'metric_for_logging': {'pred_time': 1.572388304893688e-06}, 'val_loss': 1.1235641074583707, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 18, 'min_child_weight': 0.5399321478998305, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.9488606198341824, 'colsample_bytree': 0.45830999468859307, 'reg_alpha': 0.008506127608334661, 'reg_lambda': 6.27023622703691, 'FLAML_sample_size': 306605}, 'config/n_estimators': 13, 'config/max_leaves': 18, 'config/min_child_weight': 0.5399321478998305, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9488606198341824, 'config/colsample_bytree': 0.45830999468859307, 'config/reg_alpha': 0.008506127608334661, 'config/reg_lambda': 6.27023622703691, 'config/FLAML_sample_size': 306605, 'experiment_tag': 'exp', 'time_total_s': 20.478635549545288}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9488606198341824, colsample_bynode=1,
             colsample_bytree=0.45830999468859307, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=18, min_child_weight=0.5399321478998305,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.008506127608334661, reg_lambda=6.27023622703691,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7789548385437317
SO2(0)的mse=4.128022688548826
SO2(0)的mae=1.1045985929220163
SO2(0)的mar=0.11871856852597792
总共花费的时间为：78.06
贵阳市
1440A
1442A
1443A
1444A
1445A
1446A
[flaml.automl: 09-16 23:47:43] {2390} INFO - task = regression
[flaml.automl: 09-16 23:47:43] {2392} INFO - Data split method: uniform
[flaml.automl: 09-16 23:47:43] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-16 23:47:43] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-16 23:47:43] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-16 23:47:43] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-16 23:47:45] {3025} INFO - Estimated sufficient time budget=147214s. Estimated necessary time budget=147s.
[flaml.automl: 09-16 23:47:45] {3072} INFO -  at 2.5s,	estimator xgboost's best error=4.9730,	best estimator xgboost's best error=4.9730
[flaml.automl: 09-16 23:47:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-16 23:47:49] {3072} INFO -  at 6.5s,	estimator xgboost's best error=2.3913,	best estimator xgboost's best error=2.3913
[flaml.automl: 09-16 23:47:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-16 23:47:51] {3072} INFO -  at 8.6s,	estimator xgboost's best error=2.3913,	best estimator xgboost's best error=2.3913
[flaml.automl: 09-16 23:47:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-16 23:47:54] {3072} INFO -  at 11.7s,	estimator xgboost's best error=2.3913,	best estimator xgboost's best error=2.3913
[flaml.automl: 09-16 23:47:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-16 23:47:56] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.7150,	best estimator xgboost's best error=1.7150
[flaml.automl: 09-16 23:47:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-16 23:47:59] {3072} INFO -  at 16.7s,	estimator xgboost's best error=1.7035,	best estimator xgboost's best error=1.7035
[flaml.automl: 09-16 23:47:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-16 23:48:02] {3072} INFO -  at 19.7s,	estimator xgboost's best error=1.3561,	best estimator xgboost's best error=1.3561
[flaml.automl: 09-16 23:48:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-16 23:48:05] {3072} INFO -  at 22.3s,	estimator xgboost's best error=1.3561,	best estimator xgboost's best error=1.3561
[flaml.automl: 09-16 23:48:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-16 23:48:07] {3072} INFO -  at 24.8s,	estimator xgboost's best error=1.3561,	best estimator xgboost's best error=1.3561
[flaml.automl: 09-16 23:48:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-16 23:48:09] {3072} INFO -  at 26.6s,	estimator xgboost's best error=1.3561,	best estimator xgboost's best error=1.3561
[flaml.automl: 09-16 23:48:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-16 23:48:11] {3072} INFO -  at 28.8s,	estimator xgboost's best error=1.3561,	best estimator xgboost's best error=1.3561
[flaml.automl: 09-16 23:48:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-16 23:48:14] {3072} INFO -  at 31.9s,	estimator xgboost's best error=1.3561,	best estimator xgboost's best error=1.3561
[flaml.automl: 09-16 23:48:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-16 23:48:17] {3072} INFO -  at 34.1s,	estimator xgboost's best error=1.3561,	best estimator xgboost's best error=1.3561
[flaml.automl: 09-16 23:48:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-16 23:48:26] {3072} INFO -  at 43.4s,	estimator xgboost's best error=1.1031,	best estimator xgboost's best error=1.1031
[flaml.automl: 09-16 23:48:26] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-16 23:48:39] {3072} INFO -  at 56.2s,	estimator xgboost's best error=1.0989,	best estimator xgboost's best error=1.0989
[flaml.automl: 09-16 23:48:51] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-16 23:48:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-16 23:48:51] {2636} INFO - fit succeeded
[flaml.automl: 09-16 23:48:51] {2637} INFO - Time taken to find the best model: 56.19308543205261
[flaml.automl: 09-16 23:48:51] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 66363}
SO2(0)最佳损失：-0.09890876457818587
SO2(0)最好结果：{'pred_time': 5.586670445400038e-06, 'wall_clock_time': 56.19308543205261, 'metric_for_logging': {'pred_time': 5.586670445400038e-06}, 'val_loss': 1.0989087645781859, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 66363}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 66363, 'experiment_tag': 'exp', 'time_total_s': 12.756211280822754}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8770883011128526
SO2(0)的mse=3.6196059307917956
SO2(0)的mae=1.061538759167089
SO2(0)的mar=0.14601730274200214
总共花费的时间为：70.11
昆明市
1452A
1453A
1455A
3179A
3375A
3550A
3551A
3552A
[flaml.automl: 09-17 00:13:53] {2390} INFO - task = regression
[flaml.automl: 09-17 00:13:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 00:13:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 00:13:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 00:13:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 00:13:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 00:13:54] {3025} INFO - Estimated sufficient time budget=107641s. Estimated necessary time budget=108s.
[flaml.automl: 09-17 00:13:54] {3072} INFO -  at 1.7s,	estimator xgboost's best error=4.9421,	best estimator xgboost's best error=4.9421
[flaml.automl: 09-17 00:13:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 00:13:57] {3072} INFO -  at 3.8s,	estimator xgboost's best error=2.2793,	best estimator xgboost's best error=2.2793
[flaml.automl: 09-17 00:13:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 00:13:58] {3072} INFO -  at 5.0s,	estimator xgboost's best error=2.2793,	best estimator xgboost's best error=2.2793
[flaml.automl: 09-17 00:13:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 00:14:00] {3072} INFO -  at 7.7s,	estimator xgboost's best error=2.2793,	best estimator xgboost's best error=2.2793
[flaml.automl: 09-17 00:14:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 00:14:02] {3072} INFO -  at 8.8s,	estimator xgboost's best error=1.2514,	best estimator xgboost's best error=1.2514
[flaml.automl: 09-17 00:14:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 00:14:03] {3072} INFO -  at 10.4s,	estimator xgboost's best error=1.1230,	best estimator xgboost's best error=1.1230
[flaml.automl: 09-17 00:14:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 00:14:05] {3072} INFO -  at 12.0s,	estimator xgboost's best error=1.0402,	best estimator xgboost's best error=1.0402
[flaml.automl: 09-17 00:14:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 00:14:07] {3072} INFO -  at 14.3s,	estimator xgboost's best error=1.0402,	best estimator xgboost's best error=1.0402
[flaml.automl: 09-17 00:14:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 00:14:09] {3072} INFO -  at 15.9s,	estimator xgboost's best error=1.0402,	best estimator xgboost's best error=1.0402
[flaml.automl: 09-17 00:14:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 00:14:11] {3072} INFO -  at 18.1s,	estimator xgboost's best error=1.0402,	best estimator xgboost's best error=1.0402
[flaml.automl: 09-17 00:14:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 00:14:12] {3072} INFO -  at 19.6s,	estimator xgboost's best error=1.0402,	best estimator xgboost's best error=1.0402
[flaml.automl: 09-17 00:14:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 00:14:14] {3072} INFO -  at 21.3s,	estimator xgboost's best error=1.0341,	best estimator xgboost's best error=1.0341
[flaml.automl: 09-17 00:14:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 00:14:15] {3072} INFO -  at 22.4s,	estimator xgboost's best error=1.0341,	best estimator xgboost's best error=1.0341
[flaml.automl: 09-17 00:14:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 00:14:22] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.9426,	best estimator xgboost's best error=0.9426
[flaml.automl: 09-17 00:14:22] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 00:14:35] {3072} INFO -  at 42.3s,	estimator xgboost's best error=0.9405,	best estimator xgboost's best error=0.9405
[flaml.automl: 09-17 00:14:35] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 00:14:42] {3072} INFO -  at 49.2s,	estimator xgboost's best error=0.9405,	best estimator xgboost's best error=0.9405
[flaml.automl: 09-17 00:14:55] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-17 00:14:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 00:14:55] {2636} INFO - fit succeeded
[flaml.automl: 09-17 00:14:55] {2637} INFO - Time taken to find the best model: 42.27908706665039
[flaml.automl: 09-17 00:14:55] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 88767}
SO2(0)最佳损失：0.059548497659253186
SO2(0)最好结果：{'pred_time': 4.768057364520269e-06, 'wall_clock_time': 42.27908706665039, 'metric_for_logging': {'pred_time': 4.768057364520269e-06}, 'val_loss': 0.9404515023407468, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 88767}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 88767, 'experiment_tag': 'exp', 'time_total_s': 12.897763967514038}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7077817188068732
SO2(0)的mse=2.1960828026887858
SO2(0)的mae=0.9549705225838362
SO2(0)的mar=0.12744042012677786
总共花费的时间为：63.34
拉萨市
1456A
1457A
1458A
1461A
[flaml.automl: 09-17 00:27:06] {2390} INFO - task = regression
[flaml.automl: 09-17 00:27:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 00:27:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 00:27:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 00:27:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 00:27:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 00:27:08] {3025} INFO - Estimated sufficient time budget=98560s. Estimated necessary time budget=99s.
[flaml.automl: 09-17 00:27:08] {3072} INFO -  at 2.5s,	estimator xgboost's best error=3.8621,	best estimator xgboost's best error=3.8621
[flaml.automl: 09-17 00:27:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 00:27:12] {3072} INFO -  at 6.0s,	estimator xgboost's best error=1.7359,	best estimator xgboost's best error=1.7359
[flaml.automl: 09-17 00:27:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 00:27:13] {3072} INFO -  at 7.9s,	estimator xgboost's best error=1.7359,	best estimator xgboost's best error=1.7359
[flaml.automl: 09-17 00:27:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 00:27:19] {3072} INFO -  at 13.1s,	estimator xgboost's best error=1.7359,	best estimator xgboost's best error=1.7359
[flaml.automl: 09-17 00:27:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 00:27:21] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.7463,	best estimator xgboost's best error=0.7463
[flaml.automl: 09-17 00:27:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 00:27:24] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.5615,	best estimator xgboost's best error=0.5615
[flaml.automl: 09-17 00:27:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 00:27:26] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.5400,	best estimator xgboost's best error=0.5400
[flaml.automl: 09-17 00:27:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 00:27:30] {3072} INFO -  at 24.4s,	estimator xgboost's best error=0.5400,	best estimator xgboost's best error=0.5400
[flaml.automl: 09-17 00:27:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 00:27:33] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.5173,	best estimator xgboost's best error=0.5173
[flaml.automl: 09-17 00:27:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 00:27:36] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.4940,	best estimator xgboost's best error=0.4940
[flaml.automl: 09-17 00:27:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 00:27:38] {3072} INFO -  at 32.6s,	estimator xgboost's best error=0.4940,	best estimator xgboost's best error=0.4940
[flaml.automl: 09-17 00:27:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 00:27:41] {3072} INFO -  at 35.2s,	estimator xgboost's best error=0.4197,	best estimator xgboost's best error=0.4197
[flaml.automl: 09-17 00:27:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 00:27:43] {3072} INFO -  at 37.6s,	estimator xgboost's best error=0.4197,	best estimator xgboost's best error=0.4197
[flaml.automl: 09-17 00:27:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 00:27:46] {3072} INFO -  at 40.8s,	estimator xgboost's best error=0.4197,	best estimator xgboost's best error=0.4197
[flaml.automl: 09-17 00:27:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 00:27:48] {3072} INFO -  at 42.3s,	estimator xgboost's best error=0.4197,	best estimator xgboost's best error=0.4197
[flaml.automl: 09-17 00:27:48] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 00:27:50] {3072} INFO -  at 44.2s,	estimator xgboost's best error=0.4197,	best estimator xgboost's best error=0.4197
[flaml.automl: 09-17 00:27:50] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 00:27:51] {3072} INFO -  at 45.9s,	estimator xgboost's best error=0.4197,	best estimator xgboost's best error=0.4197
[flaml.automl: 09-17 00:27:51] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 00:27:54] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.4197,	best estimator xgboost's best error=0.4197
[flaml.automl: 09-17 00:27:54] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 00:27:55] {3072} INFO -  at 49.1s,	estimator xgboost's best error=0.4197,	best estimator xgboost's best error=0.4197
[flaml.automl: 09-17 00:27:55] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 00:27:56] {3072} INFO -  at 50.8s,	estimator xgboost's best error=0.4197,	best estimator xgboost's best error=0.4197
[flaml.automl: 09-17 00:27:56] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 00:28:06] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.4197,	best estimator xgboost's best error=0.4197
[flaml.automl: 09-17 00:28:17] {3335} INFO - retrain xgboost for 11.7s
[flaml.automl: 09-17 00:28:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8205463396260846, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.0030307068193346307,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.01522976298101339,
             reg_lambda=1.408764162137561, scale_pos_weight=1,
             subsample=0.8067656270100493, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 00:28:17] {2636} INFO - fit succeeded
[flaml.automl: 09-17 00:28:17] {2637} INFO - Time taken to find the best model: 44.23094367980957
[flaml.automl: 09-17 00:28:17] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 9, 'min_child_weight': 0.0030307068193346307, 'learning_rate': 1.0, 'subsample': 0.8067656270100493, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8205463396260846, 'reg_alpha': 0.01522976298101339, 'reg_lambda': 1.408764162137561, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：0.5803436343078268
SO2(0)最好结果：{'pred_time': 1.7399495039119862e-05, 'wall_clock_time': 44.23094367980957, 'metric_for_logging': {'pred_time': 1.7399495039119862e-05}, 'val_loss': 0.41965636569217324, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 9, 'min_child_weight': 0.0030307068193346307, 'learning_rate': 1.0, 'subsample': 0.8067656270100493, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8205463396260846, 'reg_alpha': 0.01522976298101339, 'reg_lambda': 1.408764162137561, 'FLAML_sample_size': 10000}, 'config/n_estimators': 15, 'config/max_leaves': 9, 'config/min_child_weight': 0.0030307068193346307, 'config/learning_rate': 1.0, 'config/subsample': 0.8067656270100493, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8205463396260846, 'config/reg_alpha': 0.01522976298101339, 'config/reg_lambda': 1.408764162137561, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 1.967909574508667}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8205463396260846, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.0030307068193346307,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.01522976298101339,
             reg_lambda=1.408764162137561, scale_pos_weight=1,
             subsample=0.8067656270100493, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8405719868421706
SO2(0)的mse=0.4895899985562488
SO2(0)的mae=0.438936138502456
SO2(0)的mar=0.07734664143155619
总共花费的时间为：72.47
西安市
1462A
1463A
1464A
1465A
1466A
1468A
1474A
3524A
3605A
[flaml.automl: 09-17 00:55:27] {2390} INFO - task = regression
[flaml.automl: 09-17 00:55:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 00:55:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 00:55:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 00:55:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 00:55:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 00:55:28] {3025} INFO - Estimated sufficient time budget=119872s. Estimated necessary time budget=120s.
[flaml.automl: 09-17 00:55:28] {3072} INFO -  at 1.7s,	estimator xgboost's best error=4.4604,	best estimator xgboost's best error=4.4604
[flaml.automl: 09-17 00:55:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 00:55:31] {3072} INFO -  at 3.8s,	estimator xgboost's best error=2.0399,	best estimator xgboost's best error=2.0399
[flaml.automl: 09-17 00:55:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 00:55:32] {3072} INFO -  at 5.0s,	estimator xgboost's best error=2.0399,	best estimator xgboost's best error=2.0399
[flaml.automl: 09-17 00:55:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 00:55:34] {3072} INFO -  at 7.2s,	estimator xgboost's best error=2.0399,	best estimator xgboost's best error=2.0399
[flaml.automl: 09-17 00:55:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 00:55:35] {3072} INFO -  at 8.4s,	estimator xgboost's best error=1.1430,	best estimator xgboost's best error=1.1430
[flaml.automl: 09-17 00:55:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 00:55:37] {3072} INFO -  at 10.0s,	estimator xgboost's best error=1.1232,	best estimator xgboost's best error=1.1232
[flaml.automl: 09-17 00:55:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 00:55:38] {3072} INFO -  at 11.6s,	estimator xgboost's best error=0.8728,	best estimator xgboost's best error=0.8728
[flaml.automl: 09-17 00:55:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 00:55:41] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.8728,	best estimator xgboost's best error=0.8728
[flaml.automl: 09-17 00:55:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 00:55:42] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.8728,	best estimator xgboost's best error=0.8728
[flaml.automl: 09-17 00:55:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 00:55:44] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.8728,	best estimator xgboost's best error=0.8728
[flaml.automl: 09-17 00:55:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 00:55:45] {3072} INFO -  at 18.7s,	estimator xgboost's best error=0.8728,	best estimator xgboost's best error=0.8728
[flaml.automl: 09-17 00:55:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 00:55:47] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.8728,	best estimator xgboost's best error=0.8728
[flaml.automl: 09-17 00:55:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 00:55:48] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.8728,	best estimator xgboost's best error=0.8728
[flaml.automl: 09-17 00:55:48] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 00:55:55] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.7049,	best estimator xgboost's best error=0.7049
[flaml.automl: 09-17 00:55:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 00:56:08] {3072} INFO -  at 41.3s,	estimator xgboost's best error=0.6971,	best estimator xgboost's best error=0.6971
[flaml.automl: 09-17 00:56:08] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 00:56:15] {3072} INFO -  at 48.3s,	estimator xgboost's best error=0.6971,	best estimator xgboost's best error=0.6971
[flaml.automl: 09-17 00:56:28] {3335} INFO - retrain xgboost for 12.9s
[flaml.automl: 09-17 00:56:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 00:56:28] {2636} INFO - fit succeeded
[flaml.automl: 09-17 00:56:28] {2637} INFO - Time taken to find the best model: 41.277836084365845
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 99996}
SO2(0)最佳损失：0.3028529573008304
SO2(0)最好结果：{'pred_time': 3.591256291438647e-06, 'wall_clock_time': 41.277836084365845, 'metric_for_logging': {'pred_time': 3.591256291438647e-06}, 'val_loss': 0.6971470426991696, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 99996}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 99996, 'experiment_tag': 'exp', 'time_total_s': 12.79718565940857}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8765113461223496
SO2(0)的mse=1.2622673304042535
SO2(0)的mae=0.693665432617148
SO2(0)的mar=0.099438158324097
总共花费的时间为：62.58
兰州市
1478A
3186A
3241A
3242A
[flaml.automl: 09-17 01:08:41] {2390} INFO - task = regression
[flaml.automl: 09-17 01:08:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:08:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:08:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:08:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:08:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:08:42] {3025} INFO - Estimated sufficient time budget=51766s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 01:08:42] {3072} INFO -  at 1.4s,	estimator xgboost's best error=7.9074,	best estimator xgboost's best error=7.9074
[flaml.automl: 09-17 01:08:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:08:44] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.6717,	best estimator xgboost's best error=3.6717
[flaml.automl: 09-17 01:08:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:08:46] {3072} INFO -  at 4.6s,	estimator xgboost's best error=3.6717,	best estimator xgboost's best error=3.6717
[flaml.automl: 09-17 01:08:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:08:52] {3072} INFO -  at 10.8s,	estimator xgboost's best error=3.6717,	best estimator xgboost's best error=3.6717
[flaml.automl: 09-17 01:08:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:08:53] {3072} INFO -  at 11.9s,	estimator xgboost's best error=2.5220,	best estimator xgboost's best error=2.5220
[flaml.automl: 09-17 01:08:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:08:54] {3072} INFO -  at 13.5s,	estimator xgboost's best error=2.2449,	best estimator xgboost's best error=2.2449
[flaml.automl: 09-17 01:08:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:08:56] {3072} INFO -  at 15.1s,	estimator xgboost's best error=2.2010,	best estimator xgboost's best error=2.2010
[flaml.automl: 09-17 01:08:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:08:59] {3072} INFO -  at 17.8s,	estimator xgboost's best error=2.2010,	best estimator xgboost's best error=2.2010
[flaml.automl: 09-17 01:08:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:09:00] {3072} INFO -  at 19.4s,	estimator xgboost's best error=2.2010,	best estimator xgboost's best error=2.2010
[flaml.automl: 09-17 01:09:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:09:03] {3072} INFO -  at 22.4s,	estimator xgboost's best error=1.9935,	best estimator xgboost's best error=1.9935
[flaml.automl: 09-17 01:09:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:09:05] {3072} INFO -  at 24.0s,	estimator xgboost's best error=1.9935,	best estimator xgboost's best error=1.9935
[flaml.automl: 09-17 01:09:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:09:06] {3072} INFO -  at 25.2s,	estimator xgboost's best error=1.9935,	best estimator xgboost's best error=1.9935
[flaml.automl: 09-17 01:09:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:09:10] {3072} INFO -  at 28.9s,	estimator xgboost's best error=1.9935,	best estimator xgboost's best error=1.9935
[flaml.automl: 09-17 01:09:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:09:13] {3072} INFO -  at 32.4s,	estimator xgboost's best error=1.9935,	best estimator xgboost's best error=1.9935
[flaml.automl: 09-17 01:09:13] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:09:16] {3072} INFO -  at 35.4s,	estimator xgboost's best error=1.9935,	best estimator xgboost's best error=1.9935
[flaml.automl: 09-17 01:09:16] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 01:09:21] {3072} INFO -  at 40.3s,	estimator xgboost's best error=1.9935,	best estimator xgboost's best error=1.9935
[flaml.automl: 09-17 01:09:21] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 01:09:23] {3072} INFO -  at 42.2s,	estimator xgboost's best error=1.9935,	best estimator xgboost's best error=1.9935
[flaml.automl: 09-17 01:09:23] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 01:09:25] {3072} INFO -  at 43.9s,	estimator xgboost's best error=1.9935,	best estimator xgboost's best error=1.9935
[flaml.automl: 09-17 01:09:25] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 01:09:32] {3072} INFO -  at 51.2s,	estimator xgboost's best error=1.9616,	best estimator xgboost's best error=1.9616
[flaml.automl: 09-17 01:09:32] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 01:09:35] {3072} INFO -  at 53.7s,	estimator xgboost's best error=1.9616,	best estimator xgboost's best error=1.9616
[flaml.automl: 09-17 01:09:42] {3335} INFO - retrain xgboost for 7.3s
[flaml.automl: 09-17 01:09:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5492977310397356, colsample_bynode=1,
             colsample_bytree=0.9510761842589558, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.04321599195729943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5415707634193446, scale_pos_weight=1,
             subsample=0.9351529901519405, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:09:42] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:09:42] {2637} INFO - Time taken to find the best model: 51.24062919616699
[flaml.automl: 09-17 01:09:42] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.04321599195729943, 'learning_rate': 1.0, 'subsample': 0.9351529901519405, 'colsample_bylevel': 0.5492977310397356, 'colsample_bytree': 0.9510761842589558, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5415707634193446, 'FLAML_sample_size': 43456}
SO2(0)最佳损失：-0.9615520128035648
SO2(0)最好结果：{'pred_time': 8.115612416317754e-06, 'wall_clock_time': 51.24062919616699, 'metric_for_logging': {'pred_time': 8.115612416317754e-06}, 'val_loss': 1.9615520128035648, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.04321599195729943, 'learning_rate': 1.0, 'subsample': 0.9351529901519405, 'colsample_bylevel': 0.5492977310397356, 'colsample_bytree': 0.9510761842589558, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.5415707634193446, 'FLAML_sample_size': 43456}, 'config/n_estimators': 8, 'config/max_leaves': 16, 'config/min_child_weight': 0.04321599195729943, 'config/learning_rate': 1.0, 'config/subsample': 0.9351529901519405, 'config/colsample_bylevel': 0.5492977310397356, 'config/colsample_bytree': 0.9510761842589558, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.5415707634193446, 'config/FLAML_sample_size': 43456, 'experiment_tag': 'exp', 'time_total_s': 7.381642580032349}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.5492977310397356, colsample_bynode=1,
             colsample_bytree=0.9510761842589558, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.04321599195729943,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.5415707634193446, scale_pos_weight=1,
             subsample=0.9351529901519405, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7669543979111165
SO2(0)的mse=11.058812340837754
SO2(0)的mae=2.0325213197536804
SO2(0)的mar=0.1554077963438304
总共花费的时间为：61.84
西宁市
3629A
3630A
[flaml.automl: 09-17 01:16:44] {2390} INFO - task = regression
[flaml.automl: 09-17 01:16:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:16:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:16:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:16:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:16:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:16:46] {3025} INFO - Estimated sufficient time budget=20432s. Estimated necessary time budget=20s.
[flaml.automl: 09-17 01:16:46] {3072} INFO -  at 2.2s,	estimator xgboost's best error=8.8008,	best estimator xgboost's best error=8.8008
[flaml.automl: 09-17 01:16:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:16:50] {3072} INFO -  at 6.2s,	estimator xgboost's best error=4.3291,	best estimator xgboost's best error=4.3291
[flaml.automl: 09-17 01:16:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:16:53] {3072} INFO -  at 8.5s,	estimator xgboost's best error=4.3291,	best estimator xgboost's best error=4.3291
[flaml.automl: 09-17 01:16:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:17:09] {3072} INFO -  at 24.5s,	estimator xgboost's best error=4.3291,	best estimator xgboost's best error=4.3291
[flaml.automl: 09-17 01:17:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:17:11] {3072} INFO -  at 26.3s,	estimator xgboost's best error=3.4286,	best estimator xgboost's best error=3.4286
[flaml.automl: 09-17 01:17:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:17:13] {3072} INFO -  at 28.8s,	estimator xgboost's best error=3.2262,	best estimator xgboost's best error=3.2262
[flaml.automl: 09-17 01:17:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:17:16] {3072} INFO -  at 31.5s,	estimator xgboost's best error=3.1094,	best estimator xgboost's best error=3.1094
[flaml.automl: 09-17 01:17:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:17:18] {3072} INFO -  at 34.2s,	estimator xgboost's best error=3.1094,	best estimator xgboost's best error=3.1094
[flaml.automl: 09-17 01:17:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:17:20] {3072} INFO -  at 35.8s,	estimator xgboost's best error=3.1094,	best estimator xgboost's best error=3.1094
[flaml.automl: 09-17 01:17:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:17:23] {3072} INFO -  at 38.8s,	estimator xgboost's best error=2.8037,	best estimator xgboost's best error=2.8037
[flaml.automl: 09-17 01:17:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:17:25] {3072} INFO -  at 40.5s,	estimator xgboost's best error=2.8037,	best estimator xgboost's best error=2.8037
[flaml.automl: 09-17 01:17:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:17:26] {3072} INFO -  at 41.6s,	estimator xgboost's best error=2.8037,	best estimator xgboost's best error=2.8037
[flaml.automl: 09-17 01:17:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:17:38] {3072} INFO -  at 53.6s,	estimator xgboost's best error=2.8037,	best estimator xgboost's best error=2.8037
[flaml.automl: 09-17 01:17:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:17:44] {3072} INFO -  at 59.4s,	estimator xgboost's best error=2.7617,	best estimator xgboost's best error=2.7617
[flaml.automl: 09-17 01:17:49] {3335} INFO - retrain xgboost for 5.8s
[flaml.automl: 09-17 01:17:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:17:49] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:17:49] {2637} INFO - Time taken to find the best model: 59.351820945739746
[flaml.automl: 09-17 01:17:49] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 16, 'max_leaves': 6, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}
SO2(0)最佳损失：-1.7617116348416197
SO2(0)最好结果：{'pred_time': 1.6546070351916947e-05, 'wall_clock_time': 59.351820945739746, 'metric_for_logging': {'pred_time': 1.6546070351916947e-05}, 'val_loss': 2.7617116348416197, 'training_iteration': 1, 'config': {'n_estimators': 16, 'max_leaves': 6, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}, 'config/n_estimators': 16, 'config/max_leaves': 6, 'config/min_child_weight': 0.07186500551974488, 'config/learning_rate': 0.18147476059072173, 'config/subsample': 0.7641710216408443, 'config/colsample_bylevel': 0.8883913141026579, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.14043216453256416, 'experiment_tag': 'exp', 'time_total_s': 5.783533811569214}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5199272269511799
SO2(0)的mse=26.30017347109838
SO2(0)的mae=2.819326828173395
SO2(0)的mar=0.21625904206137034
总共花费的时间为：65.52
银川市
1484A
1488A
2925A
2926A
3523A
[flaml.automl: 09-17 01:34:01] {2390} INFO - task = regression
[flaml.automl: 09-17 01:34:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:34:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:34:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:34:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:34:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:34:02] {3025} INFO - Estimated sufficient time budget=63197s. Estimated necessary time budget=63s.
[flaml.automl: 09-17 01:34:02] {3072} INFO -  at 1.4s,	estimator xgboost's best error=7.5372,	best estimator xgboost's best error=7.5372
[flaml.automl: 09-17 01:34:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:34:05] {3072} INFO -  at 3.6s,	estimator xgboost's best error=3.7940,	best estimator xgboost's best error=3.7940
[flaml.automl: 09-17 01:34:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:34:06] {3072} INFO -  at 4.8s,	estimator xgboost's best error=3.7940,	best estimator xgboost's best error=3.7940
[flaml.automl: 09-17 01:34:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:34:11] {3072} INFO -  at 9.6s,	estimator xgboost's best error=3.7940,	best estimator xgboost's best error=3.7940
[flaml.automl: 09-17 01:34:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:34:12] {3072} INFO -  at 10.7s,	estimator xgboost's best error=3.2057,	best estimator xgboost's best error=3.2057
[flaml.automl: 09-17 01:34:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:34:13] {3072} INFO -  at 12.3s,	estimator xgboost's best error=2.9553,	best estimator xgboost's best error=2.9553
[flaml.automl: 09-17 01:34:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:34:15] {3072} INFO -  at 13.9s,	estimator xgboost's best error=2.8084,	best estimator xgboost's best error=2.8084
[flaml.automl: 09-17 01:34:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:34:18] {3072} INFO -  at 16.6s,	estimator xgboost's best error=2.8084,	best estimator xgboost's best error=2.8084
[flaml.automl: 09-17 01:34:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:34:19] {3072} INFO -  at 18.2s,	estimator xgboost's best error=2.8084,	best estimator xgboost's best error=2.8084
[flaml.automl: 09-17 01:34:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:34:22] {3072} INFO -  at 21.2s,	estimator xgboost's best error=2.5918,	best estimator xgboost's best error=2.5918
[flaml.automl: 09-17 01:34:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:34:24] {3072} INFO -  at 22.8s,	estimator xgboost's best error=2.5918,	best estimator xgboost's best error=2.5918
[flaml.automl: 09-17 01:34:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:34:25] {3072} INFO -  at 23.9s,	estimator xgboost's best error=2.5918,	best estimator xgboost's best error=2.5918
[flaml.automl: 09-17 01:34:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:34:28] {3072} INFO -  at 26.8s,	estimator xgboost's best error=2.5918,	best estimator xgboost's best error=2.5918
[flaml.automl: 09-17 01:34:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:34:31] {3072} INFO -  at 29.6s,	estimator xgboost's best error=2.5918,	best estimator xgboost's best error=2.5918
[flaml.automl: 09-17 01:34:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:34:34] {3072} INFO -  at 32.7s,	estimator xgboost's best error=2.5238,	best estimator xgboost's best error=2.5238
[flaml.automl: 09-17 01:34:34] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 01:34:39] {3072} INFO -  at 37.6s,	estimator xgboost's best error=2.5238,	best estimator xgboost's best error=2.5238
[flaml.automl: 09-17 01:34:39] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 01:34:40] {3072} INFO -  at 39.5s,	estimator xgboost's best error=2.5238,	best estimator xgboost's best error=2.5238
[flaml.automl: 09-17 01:34:40] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 01:34:42] {3072} INFO -  at 41.2s,	estimator xgboost's best error=2.5238,	best estimator xgboost's best error=2.5238
[flaml.automl: 09-17 01:34:42] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 01:34:49] {3072} INFO -  at 48.5s,	estimator xgboost's best error=2.5238,	best estimator xgboost's best error=2.5238
[flaml.automl: 09-17 01:34:49] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 01:34:51] {3072} INFO -  at 49.7s,	estimator xgboost's best error=2.5238,	best estimator xgboost's best error=2.5238
[flaml.automl: 09-17 01:34:51] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 01:34:59] {3072} INFO -  at 58.6s,	estimator xgboost's best error=2.5238,	best estimator xgboost's best error=2.5238
[flaml.automl: 09-17 01:35:02] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-17 01:35:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 01:35:02] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:35:02] {2637} INFO - Time taken to find the best model: 32.659849882125854
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978, 'FLAML_sample_size': 52430}
SO2(0)最佳损失：-1.5238168816201432
SO2(0)最好结果：{'pred_time': 6.78940487202606e-06, 'wall_clock_time': 32.659849882125854, 'metric_for_logging': {'pred_time': 6.78940487202606e-06}, 'val_loss': 2.523816881620143, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978, 'FLAML_sample_size': 52430}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'config/FLAML_sample_size': 52430, 'experiment_tag': 'exp', 'time_total_s': 3.023953676223755}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.4178698100772702
SO2(0)的mse=29.160355698709466
SO2(0)的mae=2.666923413003373
SO2(0)的mar=0.22152757902518497
总共花费的时间为：62.38
乌鲁木齐市
1491A
3033A
3437A
3438A
3439A
3440A
[flaml.automl: 09-17 01:54:23] {2390} INFO - task = regression
[flaml.automl: 09-17 01:54:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 01:54:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 01:54:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 01:54:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 01:54:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 01:54:24] {3025} INFO - Estimated sufficient time budget=78744s. Estimated necessary time budget=79s.
[flaml.automl: 09-17 01:54:24] {3072} INFO -  at 1.5s,	estimator xgboost's best error=3.9605,	best estimator xgboost's best error=3.9605
[flaml.automl: 09-17 01:54:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 01:54:26] {3072} INFO -  at 3.6s,	estimator xgboost's best error=1.8362,	best estimator xgboost's best error=1.8362
[flaml.automl: 09-17 01:54:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 01:54:27] {3072} INFO -  at 4.8s,	estimator xgboost's best error=1.8362,	best estimator xgboost's best error=1.8362
[flaml.automl: 09-17 01:54:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 01:54:31] {3072} INFO -  at 8.5s,	estimator xgboost's best error=1.8362,	best estimator xgboost's best error=1.8362
[flaml.automl: 09-17 01:54:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 01:54:32] {3072} INFO -  at 9.7s,	estimator xgboost's best error=1.0518,	best estimator xgboost's best error=1.0518
[flaml.automl: 09-17 01:54:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 01:54:34] {3072} INFO -  at 11.2s,	estimator xgboost's best error=1.0180,	best estimator xgboost's best error=1.0180
[flaml.automl: 09-17 01:54:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 01:54:35] {3072} INFO -  at 12.8s,	estimator xgboost's best error=0.8339,	best estimator xgboost's best error=0.8339
[flaml.automl: 09-17 01:54:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 01:54:38] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.8339,	best estimator xgboost's best error=0.8339
[flaml.automl: 09-17 01:54:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 01:54:40] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.8339,	best estimator xgboost's best error=0.8339
[flaml.automl: 09-17 01:54:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 01:54:43] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.8137,	best estimator xgboost's best error=0.8137
[flaml.automl: 09-17 01:54:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 01:54:44] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.8137,	best estimator xgboost's best error=0.8137
[flaml.automl: 09-17 01:54:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 01:54:45] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.8137,	best estimator xgboost's best error=0.8137
[flaml.automl: 09-17 01:54:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 01:54:47] {3072} INFO -  at 24.8s,	estimator xgboost's best error=0.7337,	best estimator xgboost's best error=0.7337
[flaml.automl: 09-17 01:54:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 01:54:50] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.7337,	best estimator xgboost's best error=0.7337
[flaml.automl: 09-17 01:54:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 01:54:51] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.6858,	best estimator xgboost's best error=0.6858
[flaml.automl: 09-17 01:54:51] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 01:54:53] {3072} INFO -  at 30.6s,	estimator xgboost's best error=0.6858,	best estimator xgboost's best error=0.6858
[flaml.automl: 09-17 01:54:53] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 01:54:55] {3072} INFO -  at 32.2s,	estimator xgboost's best error=0.6858,	best estimator xgboost's best error=0.6858
[flaml.automl: 09-17 01:54:55] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 01:54:56] {3072} INFO -  at 33.9s,	estimator xgboost's best error=0.6858,	best estimator xgboost's best error=0.6858
[flaml.automl: 09-17 01:54:56] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 01:54:59] {3072} INFO -  at 36.7s,	estimator xgboost's best error=0.6858,	best estimator xgboost's best error=0.6858
[flaml.automl: 09-17 01:54:59] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 01:55:07] {3072} INFO -  at 44.0s,	estimator xgboost's best error=0.6858,	best estimator xgboost's best error=0.6858
[flaml.automl: 09-17 01:55:07] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 01:55:09] {3072} INFO -  at 46.7s,	estimator xgboost's best error=0.6858,	best estimator xgboost's best error=0.6858
[flaml.automl: 09-17 01:55:09] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 01:55:21] {3072} INFO -  at 58.4s,	estimator xgboost's best error=0.6858,	best estimator xgboost's best error=0.6858
[flaml.automl: 09-17 01:55:21] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-17 01:55:23] {3072} INFO -  at 60.8s,	estimator xgboost's best error=0.6858,	best estimator xgboost's best error=0.6858
[flaml.automl: 09-17 01:55:31] {3335} INFO - retrain xgboost for 7.3s
[flaml.automl: 09-17 01:55:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7063431279374065, colsample_bynode=1,
             colsample_bytree=0.7891928823597631, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=21, min_child_weight=0.04606527892183358,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.011846412336189025, reg_lambda=1.0350015394258016,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 01:55:31] {2636} INFO - fit succeeded
[flaml.automl: 09-17 01:55:31] {2637} INFO - Time taken to find the best model: 28.381579875946045
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 21, 'min_child_weight': 0.04606527892183358, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7063431279374065, 'colsample_bytree': 0.7891928823597631, 'reg_alpha': 0.011846412336189025, 'reg_lambda': 1.0350015394258016, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：0.31420942996217893
SO2(0)最好结果：{'pred_time': 6.313757462935014e-06, 'wall_clock_time': 28.381579875946045, 'metric_for_logging': {'pred_time': 6.313757462935014e-06}, 'val_loss': 0.6857905700378211, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 21, 'min_child_weight': 0.04606527892183358, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7063431279374065, 'colsample_bytree': 0.7891928823597631, 'reg_alpha': 0.011846412336189025, 'reg_lambda': 1.0350015394258016, 'FLAML_sample_size': 10000}, 'config/n_estimators': 6, 'config/max_leaves': 21, 'config/min_child_weight': 0.04606527892183358, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7063431279374065, 'config/colsample_bytree': 0.7891928823597631, 'config/reg_alpha': 0.011846412336189025, 'config/reg_lambda': 1.0350015394258016, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 1.3660902976989746}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7063431279374065, colsample_bynode=1,
             colsample_bytree=0.7891928823597631, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=21, min_child_weight=0.04606527892183358,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.011846412336189025, reg_lambda=1.0350015394258016,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8337350438980481
SO2(0)的mse=1.308078601948535
SO2(0)的mae=0.7167510992151024
SO2(0)的mar=0.1258712959292611
总共花费的时间为：69.17
湘潭市
1508A
1511A
1512A
1513A
1514A
1564A
[flaml.automl: 09-17 02:14:08] {2390} INFO - task = regression
[flaml.automl: 09-17 02:14:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:14:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:14:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:14:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:14:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:14:10] {3025} INFO - Estimated sufficient time budget=89125s. Estimated necessary time budget=89s.
[flaml.automl: 09-17 02:14:10] {3072} INFO -  at 1.8s,	estimator xgboost's best error=4.3875,	best estimator xgboost's best error=4.3875
[flaml.automl: 09-17 02:14:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:14:12] {3072} INFO -  at 3.9s,	estimator xgboost's best error=2.1622,	best estimator xgboost's best error=2.1622
[flaml.automl: 09-17 02:14:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:14:13] {3072} INFO -  at 5.1s,	estimator xgboost's best error=2.1622,	best estimator xgboost's best error=2.1622
[flaml.automl: 09-17 02:14:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:14:17] {3072} INFO -  at 9.3s,	estimator xgboost's best error=2.1622,	best estimator xgboost's best error=2.1622
[flaml.automl: 09-17 02:14:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:14:18] {3072} INFO -  at 10.4s,	estimator xgboost's best error=1.5718,	best estimator xgboost's best error=1.5718
[flaml.automl: 09-17 02:14:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:14:20] {3072} INFO -  at 12.0s,	estimator xgboost's best error=1.5718,	best estimator xgboost's best error=1.5718
[flaml.automl: 09-17 02:14:20] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:14:22] {3072} INFO -  at 13.7s,	estimator xgboost's best error=1.2741,	best estimator xgboost's best error=1.2741
[flaml.automl: 09-17 02:14:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:14:24] {3072} INFO -  at 16.3s,	estimator xgboost's best error=1.2741,	best estimator xgboost's best error=1.2741
[flaml.automl: 09-17 02:14:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:14:26] {3072} INFO -  at 17.9s,	estimator xgboost's best error=1.2741,	best estimator xgboost's best error=1.2741
[flaml.automl: 09-17 02:14:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:14:29] {3072} INFO -  at 20.9s,	estimator xgboost's best error=1.2741,	best estimator xgboost's best error=1.2741
[flaml.automl: 09-17 02:14:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:14:30] {3072} INFO -  at 22.3s,	estimator xgboost's best error=1.2741,	best estimator xgboost's best error=1.2741
[flaml.automl: 09-17 02:14:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:14:32] {3072} INFO -  at 24.0s,	estimator xgboost's best error=1.2741,	best estimator xgboost's best error=1.2741
[flaml.automl: 09-17 02:14:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:14:33] {3072} INFO -  at 25.2s,	estimator xgboost's best error=1.2741,	best estimator xgboost's best error=1.2741
[flaml.automl: 09-17 02:14:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:14:40] {3072} INFO -  at 32.2s,	estimator xgboost's best error=1.2572,	best estimator xgboost's best error=1.2572
[flaml.automl: 09-17 02:14:40] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 02:14:53] {3072} INFO -  at 45.0s,	estimator xgboost's best error=1.2349,	best estimator xgboost's best error=1.2349
[flaml.automl: 09-17 02:14:53] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 02:15:00] {3072} INFO -  at 52.1s,	estimator xgboost's best error=1.2349,	best estimator xgboost's best error=1.2349
[flaml.automl: 09-17 02:15:13] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 02:15:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:15:13] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:15:13] {2637} INFO - Time taken to find the best model: 44.997153759002686
[flaml.automl: 09-17 02:15:13] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 62856}
SO2(0)最佳损失：-0.23494231342450034
SO2(0)最好结果：{'pred_time': 6.122209498699315e-06, 'wall_clock_time': 44.997153759002686, 'metric_for_logging': {'pred_time': 6.122209498699315e-06}, 'val_loss': 1.2349423134245003, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 62856}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 62856, 'experiment_tag': 'exp', 'time_total_s': 12.8236825466156}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6665388736912872
SO2(0)的mse=5.370463137107856
SO2(0)的mae=1.2295262526335322
SO2(0)的mar=0.1893608206327769
总共花费的时间为：65.98
株洲市
1515A
1518A
1520A
1524A
1559A
2031A
[flaml.automl: 09-17 02:32:45] {2390} INFO - task = regression
[flaml.automl: 09-17 02:32:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:32:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:32:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:32:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:32:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:32:47] {3025} INFO - Estimated sufficient time budget=135763s. Estimated necessary time budget=136s.
[flaml.automl: 09-17 02:32:47] {3072} INFO -  at 2.4s,	estimator xgboost's best error=3.9926,	best estimator xgboost's best error=3.9926
[flaml.automl: 09-17 02:32:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:32:50] {3072} INFO -  at 5.7s,	estimator xgboost's best error=1.8531,	best estimator xgboost's best error=1.8531
[flaml.automl: 09-17 02:32:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:32:52] {3072} INFO -  at 7.3s,	estimator xgboost's best error=1.8531,	best estimator xgboost's best error=1.8531
[flaml.automl: 09-17 02:32:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:32:55] {3072} INFO -  at 11.0s,	estimator xgboost's best error=1.8531,	best estimator xgboost's best error=1.8531
[flaml.automl: 09-17 02:32:55] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:32:57] {3072} INFO -  at 12.8s,	estimator xgboost's best error=1.0881,	best estimator xgboost's best error=1.0881
[flaml.automl: 09-17 02:32:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:32:59] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.9259,	best estimator xgboost's best error=0.9259
[flaml.automl: 09-17 02:32:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:33:02] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.8893,	best estimator xgboost's best error=0.8893
[flaml.automl: 09-17 02:33:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:33:05] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.8893,	best estimator xgboost's best error=0.8893
[flaml.automl: 09-17 02:33:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:33:07] {3072} INFO -  at 22.9s,	estimator xgboost's best error=0.8893,	best estimator xgboost's best error=0.8893
[flaml.automl: 09-17 02:33:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:33:09] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.8893,	best estimator xgboost's best error=0.8893
[flaml.automl: 09-17 02:33:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:33:12] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.8692,	best estimator xgboost's best error=0.8692
[flaml.automl: 09-17 02:33:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:33:13] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.8692,	best estimator xgboost's best error=0.8692
[flaml.automl: 09-17 02:33:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:33:15] {3072} INFO -  at 30.4s,	estimator xgboost's best error=0.8657,	best estimator xgboost's best error=0.8657
[flaml.automl: 09-17 02:33:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:33:16] {3072} INFO -  at 32.1s,	estimator xgboost's best error=0.8657,	best estimator xgboost's best error=0.8657
[flaml.automl: 09-17 02:33:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 02:33:18] {3072} INFO -  at 33.4s,	estimator xgboost's best error=0.8648,	best estimator xgboost's best error=0.8648
[flaml.automl: 09-17 02:33:18] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 02:33:19] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.8648,	best estimator xgboost's best error=0.8648
[flaml.automl: 09-17 02:33:19] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 02:33:21] {3072} INFO -  at 36.4s,	estimator xgboost's best error=0.8645,	best estimator xgboost's best error=0.8645
[flaml.automl: 09-17 02:33:21] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 02:33:22] {3072} INFO -  at 37.5s,	estimator xgboost's best error=0.8645,	best estimator xgboost's best error=0.8645
[flaml.automl: 09-17 02:33:22] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 02:33:23] {3072} INFO -  at 38.6s,	estimator xgboost's best error=0.8645,	best estimator xgboost's best error=0.8645
[flaml.automl: 09-17 02:33:23] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 02:33:24] {3072} INFO -  at 39.8s,	estimator xgboost's best error=0.8645,	best estimator xgboost's best error=0.8645
[flaml.automl: 09-17 02:33:24] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 02:33:25] {3072} INFO -  at 40.8s,	estimator xgboost's best error=0.8452,	best estimator xgboost's best error=0.8452
[flaml.automl: 09-17 02:33:25] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 02:33:27] {3072} INFO -  at 42.4s,	estimator xgboost's best error=0.8452,	best estimator xgboost's best error=0.8452
[flaml.automl: 09-17 02:33:27] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-17 02:33:28] {3072} INFO -  at 43.5s,	estimator xgboost's best error=0.8452,	best estimator xgboost's best error=0.8452
[flaml.automl: 09-17 02:33:28] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-17 02:33:30] {3072} INFO -  at 45.5s,	estimator xgboost's best error=0.8452,	best estimator xgboost's best error=0.8452
[flaml.automl: 09-17 02:33:30] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-17 02:33:31] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.8452,	best estimator xgboost's best error=0.8452
[flaml.automl: 09-17 02:33:31] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-17 02:33:37] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.8329,	best estimator xgboost's best error=0.8329
[flaml.automl: 09-17 02:33:37] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-17 02:33:38] {3072} INFO -  at 54.1s,	estimator xgboost's best error=0.8329,	best estimator xgboost's best error=0.8329
[flaml.automl: 09-17 02:33:45] {3335} INFO - retrain xgboost for 6.6s
[flaml.automl: 09-17 02:33:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8042002220332212, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=1.0084230260043807,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.006088066584734147, reg_lambda=0.22054851550171276,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 02:33:45] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:33:45] {2637} INFO - Time taken to find the best model: 52.94225096702576
[flaml.automl: 09-17 02:33:45] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 1.0084230260043807, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8042002220332212, 'reg_alpha': 0.006088066584734147, 'reg_lambda': 0.22054851550171276, 'FLAML_sample_size': 63463}
SO2(0)最佳损失：0.16711131747446473
SO2(0)最好结果：{'pred_time': 6.384391103085644e-06, 'wall_clock_time': 52.94225096702576, 'metric_for_logging': {'pred_time': 6.384391103085644e-06}, 'val_loss': 0.8328886825255353, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 1.0084230260043807, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8042002220332212, 'reg_alpha': 0.006088066584734147, 'reg_lambda': 0.22054851550171276, 'FLAML_sample_size': 63463}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 1.0084230260043807, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8042002220332212, 'config/reg_alpha': 0.006088066584734147, 'config/reg_lambda': 0.22054851550171276, 'config/FLAML_sample_size': 63463, 'experiment_tag': 'exp', 'time_total_s': 6.5524280071258545}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8042002220332212, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=1.0084230260043807,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.006088066584734147, reg_lambda=0.22054851550171276,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7346717142066372
SO2(0)的mse=1.9448192219170115
SO2(0)的mae=0.8575932730971131
SO2(0)的mar=0.13721790524233385
总共花费的时间为：61.75
包头市
1585A
3283A
3419A
3683A
[flaml.automl: 09-17 02:46:28] {2390} INFO - task = regression
[flaml.automl: 09-17 02:46:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 02:46:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 02:46:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 02:46:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 02:46:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 02:46:29] {3025} INFO - Estimated sufficient time budget=52096s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 02:46:29] {3072} INFO -  at 1.4s,	estimator xgboost's best error=7.9353,	best estimator xgboost's best error=7.9353
[flaml.automl: 09-17 02:46:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 02:46:31] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.2325,	best estimator xgboost's best error=4.2325
[flaml.automl: 09-17 02:46:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 02:46:32] {3072} INFO -  at 4.6s,	estimator xgboost's best error=4.2325,	best estimator xgboost's best error=4.2325
[flaml.automl: 09-17 02:46:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 02:46:38] {3072} INFO -  at 10.4s,	estimator xgboost's best error=4.2325,	best estimator xgboost's best error=4.2325
[flaml.automl: 09-17 02:46:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 02:46:39] {3072} INFO -  at 11.6s,	estimator xgboost's best error=3.5598,	best estimator xgboost's best error=3.5598
[flaml.automl: 09-17 02:46:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 02:46:41] {3072} INFO -  at 13.2s,	estimator xgboost's best error=3.5598,	best estimator xgboost's best error=3.5598
[flaml.automl: 09-17 02:46:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 02:46:42] {3072} INFO -  at 14.8s,	estimator xgboost's best error=3.2982,	best estimator xgboost's best error=3.2982
[flaml.automl: 09-17 02:46:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 02:46:45] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.2982,	best estimator xgboost's best error=3.2982
[flaml.automl: 09-17 02:46:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 02:46:47] {3072} INFO -  at 19.1s,	estimator xgboost's best error=3.2982,	best estimator xgboost's best error=3.2982
[flaml.automl: 09-17 02:46:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 02:46:50] {3072} INFO -  at 22.2s,	estimator xgboost's best error=3.2484,	best estimator xgboost's best error=3.2484
[flaml.automl: 09-17 02:46:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 02:46:51] {3072} INFO -  at 23.9s,	estimator xgboost's best error=3.2484,	best estimator xgboost's best error=3.2484
[flaml.automl: 09-17 02:46:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 02:46:52] {3072} INFO -  at 25.0s,	estimator xgboost's best error=3.2484,	best estimator xgboost's best error=3.2484
[flaml.automl: 09-17 02:46:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 02:46:56] {3072} INFO -  at 28.8s,	estimator xgboost's best error=3.2484,	best estimator xgboost's best error=3.2484
[flaml.automl: 09-17 02:46:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 02:47:00] {3072} INFO -  at 32.2s,	estimator xgboost's best error=3.2484,	best estimator xgboost's best error=3.2484
[flaml.automl: 09-17 02:47:00] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 02:47:03] {3072} INFO -  at 35.2s,	estimator xgboost's best error=3.2235,	best estimator xgboost's best error=3.2235
[flaml.automl: 09-17 02:47:03] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 02:47:08] {3072} INFO -  at 40.1s,	estimator xgboost's best error=3.2235,	best estimator xgboost's best error=3.2235
[flaml.automl: 09-17 02:47:08] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 02:47:10] {3072} INFO -  at 42.0s,	estimator xgboost's best error=3.2235,	best estimator xgboost's best error=3.2235
[flaml.automl: 09-17 02:47:10] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 02:47:11] {3072} INFO -  at 43.7s,	estimator xgboost's best error=3.2235,	best estimator xgboost's best error=3.2235
[flaml.automl: 09-17 02:47:11] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 02:47:19] {3072} INFO -  at 51.0s,	estimator xgboost's best error=3.2235,	best estimator xgboost's best error=3.2235
[flaml.automl: 09-17 02:47:19] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 02:47:20] {3072} INFO -  at 52.2s,	estimator xgboost's best error=3.2235,	best estimator xgboost's best error=3.2235
[flaml.automl: 09-17 02:47:20] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 02:47:27] {3072} INFO -  at 59.7s,	estimator xgboost's best error=3.2235,	best estimator xgboost's best error=3.2235
[flaml.automl: 09-17 02:47:30] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-17 02:47:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 02:47:30] {2636} INFO - fit succeeded
[flaml.automl: 09-17 02:47:30] {2637} INFO - Time taken to find the best model: 35.22439765930176
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768, 'FLAML_sample_size': 44120}
SO2(0)最佳损失：-2.2235371329797524
SO2(0)最好结果：{'pred_time': 8.313917953239129e-06, 'wall_clock_time': 35.22439765930176, 'metric_for_logging': {'pred_time': 8.313917953239129e-06}, 'val_loss': 3.2235371329797524, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768, 'FLAML_sample_size': 44120}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'config/FLAML_sample_size': 44120, 'experiment_tag': 'exp', 'time_total_s': 3.037426233291626}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.28962680463308776
SO2(0)的mse=34.16469567669917
SO2(0)的mae=3.108825497305982
SO2(0)的mar=0.24170068470897793
总共花费的时间为：63.35
鄂尔多斯市
1591A
1592A
1594A
1595A
[flaml.automl: 09-17 03:00:11] {2390} INFO - task = regression
[flaml.automl: 09-17 03:00:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:00:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:00:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:00:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:00:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:00:13] {3025} INFO - Estimated sufficient time budget=99220s. Estimated necessary time budget=99s.
[flaml.automl: 09-17 03:00:13] {3072} INFO -  at 2.5s,	estimator xgboost's best error=6.3391,	best estimator xgboost's best error=6.3391
[flaml.automl: 09-17 03:00:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:00:17] {3072} INFO -  at 6.1s,	estimator xgboost's best error=2.9975,	best estimator xgboost's best error=2.9975
[flaml.automl: 09-17 03:00:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:00:19] {3072} INFO -  at 8.1s,	estimator xgboost's best error=2.9975,	best estimator xgboost's best error=2.9975
[flaml.automl: 09-17 03:00:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:00:24] {3072} INFO -  at 13.2s,	estimator xgboost's best error=2.9975,	best estimator xgboost's best error=2.9975
[flaml.automl: 09-17 03:00:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:00:25] {3072} INFO -  at 14.4s,	estimator xgboost's best error=2.0182,	best estimator xgboost's best error=2.0182
[flaml.automl: 09-17 03:00:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:00:26] {3072} INFO -  at 16.0s,	estimator xgboost's best error=2.0182,	best estimator xgboost's best error=2.0182
[flaml.automl: 09-17 03:00:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:00:28] {3072} INFO -  at 17.6s,	estimator xgboost's best error=1.8236,	best estimator xgboost's best error=1.8236
[flaml.automl: 09-17 03:00:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:00:31] {3072} INFO -  at 20.3s,	estimator xgboost's best error=1.8236,	best estimator xgboost's best error=1.8236
[flaml.automl: 09-17 03:00:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:00:32] {3072} INFO -  at 21.8s,	estimator xgboost's best error=1.8236,	best estimator xgboost's best error=1.8236
[flaml.automl: 09-17 03:00:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:00:35] {3072} INFO -  at 24.8s,	estimator xgboost's best error=1.8236,	best estimator xgboost's best error=1.8236
[flaml.automl: 09-17 03:00:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:00:37] {3072} INFO -  at 26.2s,	estimator xgboost's best error=1.8236,	best estimator xgboost's best error=1.8236
[flaml.automl: 09-17 03:00:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:00:38] {3072} INFO -  at 27.8s,	estimator xgboost's best error=1.8229,	best estimator xgboost's best error=1.8229
[flaml.automl: 09-17 03:00:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:00:39] {3072} INFO -  at 29.0s,	estimator xgboost's best error=1.8229,	best estimator xgboost's best error=1.8229
[flaml.automl: 09-17 03:00:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:00:46] {3072} INFO -  at 35.9s,	estimator xgboost's best error=1.7989,	best estimator xgboost's best error=1.7989
[flaml.automl: 09-17 03:00:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 03:00:59] {3072} INFO -  at 48.4s,	estimator xgboost's best error=1.7536,	best estimator xgboost's best error=1.7536
[flaml.automl: 09-17 03:01:11] {3335} INFO - retrain xgboost for 12.4s
[flaml.automl: 09-17 03:01:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:01:11] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:01:11] {2637} INFO - Time taken to find the best model: 48.356632471084595
[flaml.automl: 09-17 03:01:11] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43765}
SO2(0)最佳损失：-0.7535551014981985
SO2(0)最好结果：{'pred_time': 8.12643083701427e-06, 'wall_clock_time': 48.356632471084595, 'metric_for_logging': {'pred_time': 8.12643083701427e-06}, 'val_loss': 1.7535551014981985, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43765}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 43765, 'experiment_tag': 'exp', 'time_total_s': 12.47792673110962}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5845032340881855
SO2(0)的mse=9.4435992921479
SO2(0)的mae=1.701342347063549
SO2(0)的mar=0.15850545583863043
总共花费的时间为：61.68
营口市
1598A
3378A
3379A
3866A
[flaml.automl: 09-17 03:13:39] {2390} INFO - task = regression
[flaml.automl: 09-17 03:13:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:13:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:13:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:13:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:13:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:13:40] {3025} INFO - Estimated sufficient time budget=12111s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:13:40] {3072} INFO -  at 1.3s,	estimator xgboost's best error=6.2448,	best estimator xgboost's best error=6.2448
[flaml.automl: 09-17 03:13:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:13:42] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.2344,	best estimator xgboost's best error=3.2344
[flaml.automl: 09-17 03:13:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:13:43] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.2344,	best estimator xgboost's best error=3.2344
[flaml.automl: 09-17 03:13:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:13:53] {3072} INFO -  at 14.8s,	estimator xgboost's best error=3.2344,	best estimator xgboost's best error=3.2344
[flaml.automl: 09-17 03:13:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:13:55] {3072} INFO -  at 15.9s,	estimator xgboost's best error=2.9593,	best estimator xgboost's best error=2.9593
[flaml.automl: 09-17 03:13:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:13:56] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.9593,	best estimator xgboost's best error=2.9593
[flaml.automl: 09-17 03:13:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:13:58] {3072} INFO -  at 19.3s,	estimator xgboost's best error=2.3412,	best estimator xgboost's best error=2.3412
[flaml.automl: 09-17 03:13:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:14:01] {3072} INFO -  at 22.0s,	estimator xgboost's best error=2.3412,	best estimator xgboost's best error=2.3412
[flaml.automl: 09-17 03:14:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:14:02] {3072} INFO -  at 23.6s,	estimator xgboost's best error=2.3412,	best estimator xgboost's best error=2.3412
[flaml.automl: 09-17 03:14:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:14:05] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.3328,	best estimator xgboost's best error=2.3328
[flaml.automl: 09-17 03:14:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:14:07] {3072} INFO -  at 28.3s,	estimator xgboost's best error=2.3328,	best estimator xgboost's best error=2.3328
[flaml.automl: 09-17 03:14:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:14:08] {3072} INFO -  at 29.5s,	estimator xgboost's best error=2.3328,	best estimator xgboost's best error=2.3328
[flaml.automl: 09-17 03:14:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:14:22] {3072} INFO -  at 43.2s,	estimator xgboost's best error=2.3328,	best estimator xgboost's best error=2.3328
[flaml.automl: 09-17 03:14:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:14:27] {3072} INFO -  at 48.4s,	estimator xgboost's best error=2.2960,	best estimator xgboost's best error=2.2960
[flaml.automl: 09-17 03:14:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 03:14:30] {3072} INFO -  at 51.1s,	estimator xgboost's best error=2.2960,	best estimator xgboost's best error=2.2960
[flaml.automl: 09-17 03:14:30] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 03:14:38] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.2960,	best estimator xgboost's best error=2.2960
[flaml.automl: 09-17 03:14:43] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-17 03:14:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9463124789373943, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.14901131280540222, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.14072354678420318, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0254117033542491, scale_pos_weight=1,
             subsample=0.8026867063371195, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:14:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:14:43] {2637} INFO - Time taken to find the best model: 48.410393953323364
[flaml.automl: 09-17 03:14:43] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 17, 'max_leaves': 5, 'min_child_weight': 0.14072354678420318, 'learning_rate': 0.14901131280540222, 'subsample': 0.8026867063371195, 'colsample_bylevel': 0.9463124789373943, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0254117033542491}
SO2(0)最佳损失：-1.2960431308746339
SO2(0)最好结果：{'pred_time': 1.1257923010623817e-05, 'wall_clock_time': 48.410393953323364, 'metric_for_logging': {'pred_time': 1.1257923010623817e-05}, 'val_loss': 2.296043130874634, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 5, 'min_child_weight': 0.14072354678420318, 'learning_rate': 0.14901131280540222, 'subsample': 0.8026867063371195, 'colsample_bylevel': 0.9463124789373943, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0254117033542491}, 'config/n_estimators': 17, 'config/max_leaves': 5, 'config/min_child_weight': 0.14072354678420318, 'config/learning_rate': 0.14901131280540222, 'config/subsample': 0.8026867063371195, 'config/colsample_bylevel': 0.9463124789373943, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.0254117033542491, 'experiment_tag': 'exp', 'time_total_s': 5.221319675445557}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9463124789373943, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.14901131280540222, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.14072354678420318, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0254117033542491, scale_pos_weight=1,
             subsample=0.8026867063371195, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5604885840747122
SO2(0)的mse=21.512053311626495
SO2(0)的mae=2.331880752836906
SO2(0)的mar=0.24280277089019048
总共花费的时间为：65.31
丹东市
1600A
1602A
1603A
[flaml.automl: 09-17 03:24:06] {2390} INFO - task = regression
[flaml.automl: 09-17 03:24:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:24:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:24:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:24:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:24:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:24:08] {3025} INFO - Estimated sufficient time budget=12253s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:24:08] {3072} INFO -  at 1.3s,	estimator xgboost's best error=6.8522,	best estimator xgboost's best error=6.8522
[flaml.automl: 09-17 03:24:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:24:10] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.2860,	best estimator xgboost's best error=3.2860
[flaml.automl: 09-17 03:24:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:24:11] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.2860,	best estimator xgboost's best error=3.2860
[flaml.automl: 09-17 03:24:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:24:21] {3072} INFO -  at 14.7s,	estimator xgboost's best error=3.2860,	best estimator xgboost's best error=3.2860
[flaml.automl: 09-17 03:24:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:24:22] {3072} INFO -  at 15.8s,	estimator xgboost's best error=2.2216,	best estimator xgboost's best error=2.2216
[flaml.automl: 09-17 03:24:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:24:24] {3072} INFO -  at 17.4s,	estimator xgboost's best error=2.1027,	best estimator xgboost's best error=2.1027
[flaml.automl: 09-17 03:24:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:24:25] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.0375,	best estimator xgboost's best error=2.0375
[flaml.automl: 09-17 03:24:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:24:28] {3072} INFO -  at 21.8s,	estimator xgboost's best error=2.0375,	best estimator xgboost's best error=2.0375
[flaml.automl: 09-17 03:24:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:24:30] {3072} INFO -  at 23.4s,	estimator xgboost's best error=2.0375,	best estimator xgboost's best error=2.0375
[flaml.automl: 09-17 03:24:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:24:33] {3072} INFO -  at 26.4s,	estimator xgboost's best error=1.8242,	best estimator xgboost's best error=1.8242
[flaml.automl: 09-17 03:24:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:24:34] {3072} INFO -  at 28.1s,	estimator xgboost's best error=1.8242,	best estimator xgboost's best error=1.8242
[flaml.automl: 09-17 03:24:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:24:36] {3072} INFO -  at 29.2s,	estimator xgboost's best error=1.8242,	best estimator xgboost's best error=1.8242
[flaml.automl: 09-17 03:24:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:24:49] {3072} INFO -  at 43.0s,	estimator xgboost's best error=1.8242,	best estimator xgboost's best error=1.8242
[flaml.automl: 09-17 03:24:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:24:55] {3072} INFO -  at 48.2s,	estimator xgboost's best error=1.7700,	best estimator xgboost's best error=1.7700
[flaml.automl: 09-17 03:24:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 03:24:57] {3072} INFO -  at 50.9s,	estimator xgboost's best error=1.7700,	best estimator xgboost's best error=1.7700
[flaml.automl: 09-17 03:24:57] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 03:25:06] {3072} INFO -  at 59.8s,	estimator xgboost's best error=1.7700,	best estimator xgboost's best error=1.7700
[flaml.automl: 09-17 03:25:11] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-17 03:25:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:25:11] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:25:11] {2637} INFO - Time taken to find the best model: 48.21724009513855
[flaml.automl: 09-17 03:25:11] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 17, 'max_leaves': 5, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}
SO2(0)最佳损失：-0.7700073989115794
SO2(0)最好结果：{'pred_time': 1.1608154350763686e-05, 'wall_clock_time': 48.21724009513855, 'metric_for_logging': {'pred_time': 1.1608154350763686e-05}, 'val_loss': 1.7700073989115794, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 5, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}, 'config/n_estimators': 17, 'config/max_leaves': 5, 'config/min_child_weight': 0.07186500551974488, 'config/learning_rate': 0.18147476059072173, 'config/subsample': 0.7641710216408443, 'config/colsample_bylevel': 0.8883913141026579, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.14043216453256416, 'experiment_tag': 'exp', 'time_total_s': 5.219210624694824}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6540449650023119
SO2(0)的mse=11.127252524132947
SO2(0)的mae=1.7805638787215963
SO2(0)的mar=0.15570398819334774
总共花费的时间为：65.57
盘锦市
1604A
1605A
[flaml.automl: 09-17 03:30:58] {2390} INFO - task = regression
[flaml.automl: 09-17 03:30:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:30:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:30:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:30:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:30:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:30:59] {3025} INFO - Estimated sufficient time budget=11924s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 03:30:59] {3072} INFO -  at 1.3s,	estimator xgboost's best error=6.5906,	best estimator xgboost's best error=6.5906
[flaml.automl: 09-17 03:30:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:31:01] {3072} INFO -  at 3.4s,	estimator xgboost's best error=3.2952,	best estimator xgboost's best error=3.2952
[flaml.automl: 09-17 03:31:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:31:02] {3072} INFO -  at 4.5s,	estimator xgboost's best error=3.2952,	best estimator xgboost's best error=3.2952
[flaml.automl: 09-17 03:31:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:31:11] {3072} INFO -  at 13.8s,	estimator xgboost's best error=3.2952,	best estimator xgboost's best error=3.2952
[flaml.automl: 09-17 03:31:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:31:13] {3072} INFO -  at 15.0s,	estimator xgboost's best error=2.5519,	best estimator xgboost's best error=2.5519
[flaml.automl: 09-17 03:31:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:31:14] {3072} INFO -  at 16.5s,	estimator xgboost's best error=2.5519,	best estimator xgboost's best error=2.5519
[flaml.automl: 09-17 03:31:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:31:16] {3072} INFO -  at 18.2s,	estimator xgboost's best error=2.1554,	best estimator xgboost's best error=2.1554
[flaml.automl: 09-17 03:31:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:31:18] {3072} INFO -  at 20.8s,	estimator xgboost's best error=2.1554,	best estimator xgboost's best error=2.1554
[flaml.automl: 09-17 03:31:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:31:20] {3072} INFO -  at 22.4s,	estimator xgboost's best error=2.1554,	best estimator xgboost's best error=2.1554
[flaml.automl: 09-17 03:31:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:31:23] {3072} INFO -  at 25.4s,	estimator xgboost's best error=2.1505,	best estimator xgboost's best error=2.1505
[flaml.automl: 09-17 03:31:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:31:25] {3072} INFO -  at 27.1s,	estimator xgboost's best error=2.1505,	best estimator xgboost's best error=2.1505
[flaml.automl: 09-17 03:31:25] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:31:26] {3072} INFO -  at 28.2s,	estimator xgboost's best error=2.1505,	best estimator xgboost's best error=2.1505
[flaml.automl: 09-17 03:31:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:31:38] {3072} INFO -  at 40.2s,	estimator xgboost's best error=2.1505,	best estimator xgboost's best error=2.1505
[flaml.automl: 09-17 03:31:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:31:44] {3072} INFO -  at 46.0s,	estimator xgboost's best error=2.0572,	best estimator xgboost's best error=2.0572
[flaml.automl: 09-17 03:31:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 03:31:47] {3072} INFO -  at 49.4s,	estimator xgboost's best error=2.0572,	best estimator xgboost's best error=2.0572
[flaml.automl: 09-17 03:31:47] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 03:31:56] {3072} INFO -  at 58.7s,	estimator xgboost's best error=2.0572,	best estimator xgboost's best error=2.0572
[flaml.automl: 09-17 03:32:02] {3335} INFO - retrain xgboost for 5.8s
[flaml.automl: 09-17 03:32:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9463124789373943, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.14901131280540222, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.14072354678420318, missing=nan,
             monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0254117033542491, scale_pos_weight=1,
             subsample=0.8026867063371195, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:32:02] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:32:02] {2637} INFO - Time taken to find the best model: 45.99739742279053
[flaml.automl: 09-17 03:32:02] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 16, 'max_leaves': 6, 'min_child_weight': 0.14072354678420318, 'learning_rate': 0.14901131280540222, 'subsample': 0.8026867063371195, 'colsample_bylevel': 0.9463124789373943, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0254117033542491}
SO2(0)最佳损失：-1.0571612661446586
SO2(0)最好结果：{'pred_time': 1.8345379936495525e-05, 'wall_clock_time': 45.99739742279053, 'metric_for_logging': {'pred_time': 1.8345379936495525e-05}, 'val_loss': 2.0571612661446586, 'training_iteration': 1, 'config': {'n_estimators': 16, 'max_leaves': 6, 'min_child_weight': 0.14072354678420318, 'learning_rate': 0.14901131280540222, 'subsample': 0.8026867063371195, 'colsample_bylevel': 0.9463124789373943, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0254117033542491}, 'config/n_estimators': 16, 'config/max_leaves': 6, 'config/min_child_weight': 0.14072354678420318, 'config/learning_rate': 0.14901131280540222, 'config/subsample': 0.8026867063371195, 'config/colsample_bylevel': 0.9463124789373943, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.0254117033542491, 'experiment_tag': 'exp', 'time_total_s': 5.814051151275635}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9463124789373943, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.14901131280540222, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.14072354678420318, missing=nan,
             monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0254117033542491, scale_pos_weight=1,
             subsample=0.8026867063371195, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.47361234202808944
SO2(0)的mse=20.568524897964114
SO2(0)的mae=2.1582924328158657
SO2(0)的mar=0.19310501656567902
总共花费的时间为：64.78
葫芦岛市
1607A
[flaml.automl: 09-17 03:35:34] {2390} INFO - task = regression
[flaml.automl: 09-17 03:35:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:35:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:35:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:35:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:35:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:35:37] {3025} INFO - Estimated sufficient time budget=32703s. Estimated necessary time budget=33s.
[flaml.automl: 09-17 03:35:37] {3072} INFO -  at 3.3s,	estimator xgboost's best error=14.3004,	best estimator xgboost's best error=14.3004
[flaml.automl: 09-17 03:35:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:35:42] {3072} INFO -  at 8.3s,	estimator xgboost's best error=9.3549,	best estimator xgboost's best error=9.3549
[flaml.automl: 09-17 03:35:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:35:46] {3072} INFO -  at 11.6s,	estimator xgboost's best error=9.3549,	best estimator xgboost's best error=9.3549
[flaml.automl: 09-17 03:35:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:35:58] {3072} INFO -  at 24.0s,	estimator xgboost's best error=9.3549,	best estimator xgboost's best error=9.3549
[flaml.automl: 09-17 03:35:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:35:59] {3072} INFO -  at 25.1s,	estimator xgboost's best error=8.7316,	best estimator xgboost's best error=8.7316
[flaml.automl: 09-17 03:35:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:36:01] {3072} INFO -  at 26.6s,	estimator xgboost's best error=8.5092,	best estimator xgboost's best error=8.5092
[flaml.automl: 09-17 03:36:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:36:02] {3072} INFO -  at 28.2s,	estimator xgboost's best error=8.5092,	best estimator xgboost's best error=8.5092
[flaml.automl: 09-17 03:36:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:36:04] {3072} INFO -  at 30.4s,	estimator xgboost's best error=8.5092,	best estimator xgboost's best error=8.5092
[flaml.automl: 09-17 03:36:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:36:06] {3072} INFO -  at 31.5s,	estimator xgboost's best error=8.5092,	best estimator xgboost's best error=8.5092
[flaml.automl: 09-17 03:36:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:36:08] {3072} INFO -  at 33.9s,	estimator xgboost's best error=8.3484,	best estimator xgboost's best error=8.3484
[flaml.automl: 09-17 03:36:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:36:10] {3072} INFO -  at 35.5s,	estimator xgboost's best error=8.3484,	best estimator xgboost's best error=8.3484
[flaml.automl: 09-17 03:36:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:36:11] {3072} INFO -  at 36.8s,	estimator xgboost's best error=8.3484,	best estimator xgboost's best error=8.3484
[flaml.automl: 09-17 03:36:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:36:20] {3072} INFO -  at 46.2s,	estimator xgboost's best error=8.3484,	best estimator xgboost's best error=8.3484
[flaml.automl: 09-17 03:36:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 03:36:25] {3072} INFO -  at 50.5s,	estimator xgboost's best error=8.0113,	best estimator xgboost's best error=8.0113
[flaml.automl: 09-17 03:36:25] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 03:36:27] {3072} INFO -  at 52.9s,	estimator xgboost's best error=8.0113,	best estimator xgboost's best error=8.0113
[flaml.automl: 09-17 03:36:27] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 03:36:34] {3072} INFO -  at 59.6s,	estimator xgboost's best error=8.0113,	best estimator xgboost's best error=8.0113
[flaml.automl: 09-17 03:36:38] {3335} INFO - retrain xgboost for 4.2s
[flaml.automl: 09-17 03:36:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8304701492679215, colsample_bynode=1,
             colsample_bytree=0.8338018626354835, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.22101066094537317,
             max_delta_step=0, max_depth=0, max_leaves=8,
             min_child_weight=0.036700176597117336, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.7760673324562323, scale_pos_weight=1,
             subsample=0.6625689596608133, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 03:36:38] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:36:38] {2637} INFO - Time taken to find the best model: 50.45886492729187
[flaml.automl: 09-17 03:36:38] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 8, 'min_child_weight': 0.036700176597117336, 'learning_rate': 0.22101066094537317, 'subsample': 0.6625689596608133, 'colsample_bylevel': 0.8304701492679215, 'colsample_bytree': 0.8338018626354835, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.7760673324562323}
SO2(0)最佳损失：-7.0112762163493265
SO2(0)最好结果：{'pred_time': 3.4639066901088746e-05, 'wall_clock_time': 50.45886492729187, 'metric_for_logging': {'pred_time': 3.4639066901088746e-05}, 'val_loss': 8.011276216349327, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 8, 'min_child_weight': 0.036700176597117336, 'learning_rate': 0.22101066094537317, 'subsample': 0.6625689596608133, 'colsample_bylevel': 0.8304701492679215, 'colsample_bytree': 0.8338018626354835, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.7760673324562323}, 'config/n_estimators': 9, 'config/max_leaves': 8, 'config/min_child_weight': 0.036700176597117336, 'config/learning_rate': 0.22101066094537317, 'config/subsample': 0.6625689596608133, 'config/colsample_bylevel': 0.8304701492679215, 'config/colsample_bytree': 0.8338018626354835, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.7760673324562323, 'experiment_tag': 'exp', 'time_total_s': 4.256192922592163}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8304701492679215, colsample_bynode=1,
             colsample_bytree=0.8338018626354835, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.22101066094537317,
             max_delta_step=0, max_depth=0, max_leaves=8,
             min_child_weight=0.036700176597117336, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.7760673324562323, scale_pos_weight=1,
             subsample=0.6625689596608133, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=-0.27539776327157206
SO2(0)的mse=369.48945488968985
SO2(0)的mae=8.15243217703842
SO2(0)的mar=0.4113526621979839
总共花费的时间为：64.03
泉州市
1614A
3529A
[flaml.automl: 09-17 03:43:11] {2390} INFO - task = regression
[flaml.automl: 09-17 03:43:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 03:43:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 03:43:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 03:43:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 03:43:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 03:43:14] {3025} INFO - Estimated sufficient time budget=32326s. Estimated necessary time budget=32s.
[flaml.automl: 09-17 03:43:14] {3072} INFO -  at 3.4s,	estimator xgboost's best error=3.2843,	best estimator xgboost's best error=3.2843
[flaml.automl: 09-17 03:43:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 03:43:19] {3072} INFO -  at 8.4s,	estimator xgboost's best error=1.7345,	best estimator xgboost's best error=1.7345
[flaml.automl: 09-17 03:43:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 03:43:22] {3072} INFO -  at 11.6s,	estimator xgboost's best error=1.7345,	best estimator xgboost's best error=1.7345
[flaml.automl: 09-17 03:43:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 03:43:45] {3072} INFO -  at 34.3s,	estimator xgboost's best error=1.7345,	best estimator xgboost's best error=1.7345
[flaml.automl: 09-17 03:43:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 03:43:48] {3072} INFO -  at 37.4s,	estimator xgboost's best error=0.8123,	best estimator xgboost's best error=0.8123
[flaml.automl: 09-17 03:43:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 03:43:52] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.8123,	best estimator xgboost's best error=0.8123
[flaml.automl: 09-17 03:43:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 03:43:56] {3072} INFO -  at 44.9s,	estimator xgboost's best error=0.6902,	best estimator xgboost's best error=0.6902
[flaml.automl: 09-17 03:43:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 03:43:58] {3072} INFO -  at 47.6s,	estimator xgboost's best error=0.6902,	best estimator xgboost's best error=0.6902
[flaml.automl: 09-17 03:43:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 03:44:00] {3072} INFO -  at 49.2s,	estimator xgboost's best error=0.6902,	best estimator xgboost's best error=0.6902
[flaml.automl: 09-17 03:44:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 03:44:03] {3072} INFO -  at 52.2s,	estimator xgboost's best error=0.6902,	best estimator xgboost's best error=0.6902
[flaml.automl: 09-17 03:44:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 03:44:05] {3072} INFO -  at 53.8s,	estimator xgboost's best error=0.6090,	best estimator xgboost's best error=0.6090
[flaml.automl: 09-17 03:44:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 03:44:06] {3072} INFO -  at 54.9s,	estimator xgboost's best error=0.6090,	best estimator xgboost's best error=0.6090
[flaml.automl: 09-17 03:44:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 03:44:11] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.6090,	best estimator xgboost's best error=0.6090
[flaml.automl: 09-17 03:44:12] {3335} INFO - retrain xgboost for 1.6s
[flaml.automl: 09-17 03:44:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.923976097470924, colsample_bynode=1,
             colsample_bytree=0.9654640505640502, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4847187609776744,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=104.45542841808965, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3217956171961422, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 03:44:12] {2636} INFO - fit succeeded
[flaml.automl: 09-17 03:44:12] {2637} INFO - Time taken to find the best model: 53.82068729400635
[flaml.automl: 09-17 03:44:12] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 104.45542841808965, 'learning_rate': 0.4847187609776744, 'subsample': 1.0, 'colsample_bylevel': 0.923976097470924, 'colsample_bytree': 0.9654640505640502, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3217956171961422}
SO2(0)最佳损失：0.3909952202916199
SO2(0)最好结果：{'pred_time': 1.7655264470995088e-05, 'wall_clock_time': 53.82068729400635, 'metric_for_logging': {'pred_time': 1.7655264470995088e-05}, 'val_loss': 0.6090047797083801, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 104.45542841808965, 'learning_rate': 0.4847187609776744, 'subsample': 1.0, 'colsample_bylevel': 0.923976097470924, 'colsample_bytree': 0.9654640505640502, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3217956171961422}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 104.45542841808965, 'config/learning_rate': 0.4847187609776744, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.923976097470924, 'config/colsample_bytree': 0.9654640505640502, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.3217956171961422, 'experiment_tag': 'exp', 'time_total_s': 1.664518117904663}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.923976097470924, colsample_bynode=1,
             colsample_bytree=0.9654640505640502, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4847187609776744,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=104.45542841808965, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3217956171961422, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7391256170937301
SO2(0)的mse=0.884307965977935
SO2(0)的mae=0.5810982440449913
SO2(0)的mar=0.11780233986962552
总共花费的时间为：62.05
临沂市
1618A
1619A
1620A
3496A
3860A
[flaml.automl: 09-17 04:01:06] {2390} INFO - task = regression
[flaml.automl: 09-17 04:01:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:01:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:01:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:01:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:01:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:01:09] {3025} INFO - Estimated sufficient time budget=108056s. Estimated necessary time budget=108s.
[flaml.automl: 09-17 04:01:09] {3072} INFO -  at 2.4s,	estimator xgboost's best error=6.3147,	best estimator xgboost's best error=6.3147
[flaml.automl: 09-17 04:01:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:01:11] {3072} INFO -  at 5.3s,	estimator xgboost's best error=3.0390,	best estimator xgboost's best error=3.0390
[flaml.automl: 09-17 04:01:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:01:13] {3072} INFO -  at 6.4s,	estimator xgboost's best error=3.0390,	best estimator xgboost's best error=3.0390
[flaml.automl: 09-17 04:01:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:01:18] {3072} INFO -  at 11.5s,	estimator xgboost's best error=3.0390,	best estimator xgboost's best error=3.0390
[flaml.automl: 09-17 04:01:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:01:20] {3072} INFO -  at 13.5s,	estimator xgboost's best error=2.1947,	best estimator xgboost's best error=2.1947
[flaml.automl: 09-17 04:01:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:01:23] {3072} INFO -  at 16.4s,	estimator xgboost's best error=2.1947,	best estimator xgboost's best error=2.1947
[flaml.automl: 09-17 04:01:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:01:26] {3072} INFO -  at 19.5s,	estimator xgboost's best error=1.8909,	best estimator xgboost's best error=1.8909
[flaml.automl: 09-17 04:01:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:01:29] {3072} INFO -  at 22.9s,	estimator xgboost's best error=1.8909,	best estimator xgboost's best error=1.8909
[flaml.automl: 09-17 04:01:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:01:32] {3072} INFO -  at 25.8s,	estimator xgboost's best error=1.8909,	best estimator xgboost's best error=1.8909
[flaml.automl: 09-17 04:01:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:01:35] {3072} INFO -  at 28.5s,	estimator xgboost's best error=1.8909,	best estimator xgboost's best error=1.8909
[flaml.automl: 09-17 04:01:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:01:37] {3072} INFO -  at 31.1s,	estimator xgboost's best error=1.8909,	best estimator xgboost's best error=1.8909
[flaml.automl: 09-17 04:01:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:01:41] {3072} INFO -  at 34.4s,	estimator xgboost's best error=1.8859,	best estimator xgboost's best error=1.8859
[flaml.automl: 09-17 04:01:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:01:43] {3072} INFO -  at 36.5s,	estimator xgboost's best error=1.8859,	best estimator xgboost's best error=1.8859
[flaml.automl: 09-17 04:01:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:01:56] {3072} INFO -  at 49.5s,	estimator xgboost's best error=1.7926,	best estimator xgboost's best error=1.7926
[flaml.automl: 09-17 04:02:08] {3335} INFO - retrain xgboost for 12.9s
[flaml.automl: 09-17 04:02:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:02:08] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:02:08] {2637} INFO - Time taken to find the best model: 49.45258712768555
[flaml.automl: 09-17 04:02:08] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 49170}
SO2(0)最佳损失：-0.7926386600881161
SO2(0)最好结果：{'pred_time': 1.5592374354819078e-05, 'wall_clock_time': 49.45258712768555, 'metric_for_logging': {'pred_time': 1.5592374354819078e-05}, 'val_loss': 1.7926386600881161, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 49170}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 49170, 'experiment_tag': 'exp', 'time_total_s': 12.904908895492554}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7767442762469966
SO2(0)的mse=8.606553836185903
SO2(0)的mae=1.8012309437861573
SO2(0)的mar=0.1906969689912835
总共花费的时间为：63.30
德州市
3066A
3372A
3511A
[flaml.automl: 09-17 04:11:41] {2390} INFO - task = regression
[flaml.automl: 09-17 04:11:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:11:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:11:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:11:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:11:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:11:43] {3025} INFO - Estimated sufficient time budget=11946s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 04:11:43] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.7556,	best estimator xgboost's best error=5.7556
[flaml.automl: 09-17 04:11:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:11:45] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.7192,	best estimator xgboost's best error=2.7192
[flaml.automl: 09-17 04:11:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:11:46] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.7192,	best estimator xgboost's best error=2.7192
[flaml.automl: 09-17 04:11:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:11:56] {3072} INFO -  at 14.7s,	estimator xgboost's best error=2.7192,	best estimator xgboost's best error=2.7192
[flaml.automl: 09-17 04:11:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:11:57] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.9572,	best estimator xgboost's best error=1.9572
[flaml.automl: 09-17 04:11:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:11:59] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.7641,	best estimator xgboost's best error=1.7641
[flaml.automl: 09-17 04:11:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:12:00] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.6058,	best estimator xgboost's best error=1.6058
[flaml.automl: 09-17 04:12:00] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:12:05] {3072} INFO -  at 24.2s,	estimator xgboost's best error=1.6058,	best estimator xgboost's best error=1.6058
[flaml.automl: 09-17 04:12:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:12:08] {3072} INFO -  at 27.1s,	estimator xgboost's best error=1.6058,	best estimator xgboost's best error=1.6058
[flaml.automl: 09-17 04:12:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:12:14] {3072} INFO -  at 32.8s,	estimator xgboost's best error=1.4569,	best estimator xgboost's best error=1.4569
[flaml.automl: 09-17 04:12:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:12:17] {3072} INFO -  at 35.9s,	estimator xgboost's best error=1.4569,	best estimator xgboost's best error=1.4569
[flaml.automl: 09-17 04:12:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:12:19] {3072} INFO -  at 38.1s,	estimator xgboost's best error=1.4569,	best estimator xgboost's best error=1.4569
[flaml.automl: 09-17 04:12:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:12:40] {3072} INFO -  at 58.4s,	estimator xgboost's best error=1.4490,	best estimator xgboost's best error=1.4490
[flaml.automl: 09-17 04:13:05] {3335} INFO - retrain xgboost for 25.0s
[flaml.automl: 09-17 04:13:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 04:13:05] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:13:05] {2637} INFO - Time taken to find the best model: 58.409278869628906
[flaml.automl: 09-17 04:13:05] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
SO2(0)最佳损失：-0.4489651545489981
SO2(0)最好结果：{'pred_time': 2.0465945959685747e-05, 'wall_clock_time': 58.409278869628906, 'metric_for_logging': {'pred_time': 2.0465945959685747e-05}, 'val_loss': 1.448965154548998, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 20.33700966835022}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.826345500460195
SO2(0)的mse=4.858757918583779
SO2(0)的mae=1.4483648096831363
SO2(0)的mar=0.18529969610250782
总共花费的时间为：83.96
聊城市
1625A
3513A
[flaml.automl: 09-17 04:19:09] {2390} INFO - task = regression
[flaml.automl: 09-17 04:19:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:19:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:19:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:19:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:19:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:19:11] {3025} INFO - Estimated sufficient time budget=23144s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 04:19:11] {3072} INFO -  at 2.4s,	estimator xgboost's best error=7.5000,	best estimator xgboost's best error=7.5000
[flaml.automl: 09-17 04:19:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:19:15] {3072} INFO -  at 6.3s,	estimator xgboost's best error=3.4975,	best estimator xgboost's best error=3.4975
[flaml.automl: 09-17 04:19:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:19:17] {3072} INFO -  at 7.9s,	estimator xgboost's best error=3.4975,	best estimator xgboost's best error=3.4975
[flaml.automl: 09-17 04:19:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:19:32] {3072} INFO -  at 22.5s,	estimator xgboost's best error=3.4975,	best estimator xgboost's best error=3.4975
[flaml.automl: 09-17 04:19:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:19:34] {3072} INFO -  at 24.6s,	estimator xgboost's best error=2.3879,	best estimator xgboost's best error=2.3879
[flaml.automl: 09-17 04:19:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:19:37] {3072} INFO -  at 27.5s,	estimator xgboost's best error=2.3879,	best estimator xgboost's best error=2.3879
[flaml.automl: 09-17 04:19:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:19:40] {3072} INFO -  at 30.7s,	estimator xgboost's best error=1.9591,	best estimator xgboost's best error=1.9591
[flaml.automl: 09-17 04:19:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:19:44] {3072} INFO -  at 35.0s,	estimator xgboost's best error=1.9591,	best estimator xgboost's best error=1.9591
[flaml.automl: 09-17 04:19:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:19:47] {3072} INFO -  at 38.0s,	estimator xgboost's best error=1.9591,	best estimator xgboost's best error=1.9591
[flaml.automl: 09-17 04:19:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:19:52] {3072} INFO -  at 43.4s,	estimator xgboost's best error=1.9591,	best estimator xgboost's best error=1.9591
[flaml.automl: 09-17 04:19:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:19:56] {3072} INFO -  at 46.6s,	estimator xgboost's best error=1.9591,	best estimator xgboost's best error=1.9591
[flaml.automl: 09-17 04:19:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:19:58] {3072} INFO -  at 48.7s,	estimator xgboost's best error=1.9591,	best estimator xgboost's best error=1.9591
[flaml.automl: 09-17 04:19:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:20:09] {3072} INFO -  at 59.7s,	estimator xgboost's best error=1.9445,	best estimator xgboost's best error=1.9445
[flaml.automl: 09-17 04:20:21] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-17 04:20:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:20:21] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:20:21] {2637} INFO - Time taken to find the best model: 59.74289011955261
[flaml.automl: 09-17 04:20:21] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
SO2(0)最佳损失：-0.9445354990492321
SO2(0)最好结果：{'pred_time': 3.1822101842546726e-05, 'wall_clock_time': 59.74289011955261, 'metric_for_logging': {'pred_time': 3.1822101842546726e-05}, 'val_loss': 1.944535499049232, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 11.0305495262146}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.623515600051858
SO2(0)的mse=11.32174713941374
SO2(0)的mae=1.9275170880677654
SO2(0)的mar=0.1623174245011458
总共花费的时间为：72.11
滨州市
1629A
1630A
3514A
3515A
3516A
[flaml.automl: 09-17 04:36:45] {2390} INFO - task = regression
[flaml.automl: 09-17 04:36:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:36:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:36:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:36:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:36:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:36:47] {3025} INFO - Estimated sufficient time budget=108085s. Estimated necessary time budget=108s.
[flaml.automl: 09-17 04:36:47] {3072} INFO -  at 2.2s,	estimator xgboost's best error=8.7633,	best estimator xgboost's best error=8.7633
[flaml.automl: 09-17 04:36:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:36:52] {3072} INFO -  at 7.3s,	estimator xgboost's best error=4.7952,	best estimator xgboost's best error=4.7952
[flaml.automl: 09-17 04:36:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:36:55] {3072} INFO -  at 10.4s,	estimator xgboost's best error=4.7952,	best estimator xgboost's best error=4.7952
[flaml.automl: 09-17 04:36:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:36:58] {3072} INFO -  at 13.5s,	estimator xgboost's best error=4.7952,	best estimator xgboost's best error=4.7952
[flaml.automl: 09-17 04:36:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:37:01] {3072} INFO -  at 16.7s,	estimator xgboost's best error=3.3501,	best estimator xgboost's best error=3.3501
[flaml.automl: 09-17 04:37:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:37:05] {3072} INFO -  at 20.0s,	estimator xgboost's best error=3.3501,	best estimator xgboost's best error=3.3501
[flaml.automl: 09-17 04:37:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:37:08] {3072} INFO -  at 23.3s,	estimator xgboost's best error=3.2018,	best estimator xgboost's best error=3.2018
[flaml.automl: 09-17 04:37:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:37:11] {3072} INFO -  at 26.1s,	estimator xgboost's best error=3.2018,	best estimator xgboost's best error=3.2018
[flaml.automl: 09-17 04:37:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:37:13] {3072} INFO -  at 28.7s,	estimator xgboost's best error=3.2018,	best estimator xgboost's best error=3.2018
[flaml.automl: 09-17 04:37:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:37:15] {3072} INFO -  at 30.4s,	estimator xgboost's best error=3.2018,	best estimator xgboost's best error=3.2018
[flaml.automl: 09-17 04:37:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:37:17] {3072} INFO -  at 32.4s,	estimator xgboost's best error=3.2018,	best estimator xgboost's best error=3.2018
[flaml.automl: 09-17 04:37:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:37:22] {3072} INFO -  at 37.2s,	estimator xgboost's best error=2.8028,	best estimator xgboost's best error=2.8028
[flaml.automl: 09-17 04:37:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:37:25] {3072} INFO -  at 40.6s,	estimator xgboost's best error=2.8028,	best estimator xgboost's best error=2.8028
[flaml.automl: 09-17 04:37:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:37:42] {3072} INFO -  at 57.6s,	estimator xgboost's best error=2.7409,	best estimator xgboost's best error=2.7409
[flaml.automl: 09-17 04:37:53] {3335} INFO - retrain xgboost for 10.2s
[flaml.automl: 09-17 04:37:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:37:53] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:37:53] {2637} INFO - Time taken to find the best model: 57.64421629905701
[flaml.automl: 09-17 04:37:53] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 54354}
SO2(0)最佳损失：-1.7408554494775683
SO2(0)最好结果：{'pred_time': 1.3483360113687072e-05, 'wall_clock_time': 57.64421629905701, 'metric_for_logging': {'pred_time': 1.3483360113687072e-05}, 'val_loss': 2.7408554494775683, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 54354}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 54354, 'experiment_tag': 'exp', 'time_total_s': 17.081177473068237}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6531497972321078
SO2(0)的mse=18.665789486566553
SO2(0)的mae=2.690849369852256
SO2(0)的mar=0.19784143978686713
总共花费的时间为：68.92
淄博市
1631A
3363A
3644A
3645A
[flaml.automl: 09-17 04:51:25] {2390} INFO - task = regression
[flaml.automl: 09-17 04:51:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 04:51:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 04:51:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 04:51:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 04:51:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 04:51:27] {3025} INFO - Estimated sufficient time budget=93270s. Estimated necessary time budget=93s.
[flaml.automl: 09-17 04:51:27] {3072} INFO -  at 2.3s,	estimator xgboost's best error=8.1698,	best estimator xgboost's best error=8.1698
[flaml.automl: 09-17 04:51:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 04:51:31] {3072} INFO -  at 6.2s,	estimator xgboost's best error=4.0190,	best estimator xgboost's best error=4.0190
[flaml.automl: 09-17 04:51:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 04:51:34] {3072} INFO -  at 8.5s,	estimator xgboost's best error=4.0190,	best estimator xgboost's best error=4.0190
[flaml.automl: 09-17 04:51:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 04:51:39] {3072} INFO -  at 13.5s,	estimator xgboost's best error=4.0190,	best estimator xgboost's best error=4.0190
[flaml.automl: 09-17 04:51:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 04:51:41] {3072} INFO -  at 15.7s,	estimator xgboost's best error=3.1123,	best estimator xgboost's best error=3.1123
[flaml.automl: 09-17 04:51:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 04:51:44] {3072} INFO -  at 18.6s,	estimator xgboost's best error=2.9397,	best estimator xgboost's best error=2.9397
[flaml.automl: 09-17 04:51:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 04:51:47] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.7331,	best estimator xgboost's best error=2.7331
[flaml.automl: 09-17 04:51:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 04:51:50] {3072} INFO -  at 25.3s,	estimator xgboost's best error=2.7331,	best estimator xgboost's best error=2.7331
[flaml.automl: 09-17 04:51:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 04:51:52] {3072} INFO -  at 26.9s,	estimator xgboost's best error=2.7331,	best estimator xgboost's best error=2.7331
[flaml.automl: 09-17 04:51:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 04:51:55] {3072} INFO -  at 29.9s,	estimator xgboost's best error=2.6218,	best estimator xgboost's best error=2.6218
[flaml.automl: 09-17 04:51:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 04:51:57] {3072} INFO -  at 31.6s,	estimator xgboost's best error=2.6218,	best estimator xgboost's best error=2.6218
[flaml.automl: 09-17 04:51:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 04:51:58] {3072} INFO -  at 32.7s,	estimator xgboost's best error=2.6218,	best estimator xgboost's best error=2.6218
[flaml.automl: 09-17 04:51:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 04:52:01] {3072} INFO -  at 35.6s,	estimator xgboost's best error=2.6071,	best estimator xgboost's best error=2.6071
[flaml.automl: 09-17 04:52:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 04:52:03] {3072} INFO -  at 37.8s,	estimator xgboost's best error=2.6071,	best estimator xgboost's best error=2.6071
[flaml.automl: 09-17 04:52:03] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 04:52:04] {3072} INFO -  at 39.2s,	estimator xgboost's best error=2.5996,	best estimator xgboost's best error=2.5996
[flaml.automl: 09-17 04:52:04] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 04:52:07] {3072} INFO -  at 41.4s,	estimator xgboost's best error=2.5996,	best estimator xgboost's best error=2.5996
[flaml.automl: 09-17 04:52:07] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 04:52:08] {3072} INFO -  at 42.9s,	estimator xgboost's best error=2.5790,	best estimator xgboost's best error=2.5790
[flaml.automl: 09-17 04:52:08] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 04:52:10] {3072} INFO -  at 44.6s,	estimator xgboost's best error=2.5790,	best estimator xgboost's best error=2.5790
[flaml.automl: 09-17 04:52:10] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 04:52:11] {3072} INFO -  at 46.3s,	estimator xgboost's best error=2.5790,	best estimator xgboost's best error=2.5790
[flaml.automl: 09-17 04:52:11] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 04:52:13] {3072} INFO -  at 47.5s,	estimator xgboost's best error=2.5790,	best estimator xgboost's best error=2.5790
[flaml.automl: 09-17 04:52:13] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 04:52:14] {3072} INFO -  at 49.0s,	estimator xgboost's best error=2.5345,	best estimator xgboost's best error=2.5345
[flaml.automl: 09-17 04:52:14] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 04:52:17] {3072} INFO -  at 51.7s,	estimator xgboost's best error=2.5345,	best estimator xgboost's best error=2.5345
[flaml.automl: 09-17 04:52:17] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-17 04:52:18] {3072} INFO -  at 52.6s,	estimator xgboost's best error=2.5345,	best estimator xgboost's best error=2.5345
[flaml.automl: 09-17 04:52:18] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-17 04:52:21] {3072} INFO -  at 55.9s,	estimator xgboost's best error=2.5345,	best estimator xgboost's best error=2.5345
[flaml.automl: 09-17 04:52:21] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-17 04:52:22] {3072} INFO -  at 56.7s,	estimator xgboost's best error=2.5345,	best estimator xgboost's best error=2.5345
[flaml.automl: 09-17 04:52:22] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-17 04:52:25] {3072} INFO -  at 59.6s,	estimator xgboost's best error=2.4996,	best estimator xgboost's best error=2.4996
[flaml.automl: 09-17 04:52:38] {3335} INFO - retrain xgboost for 13.7s
[flaml.automl: 09-17 04:52:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8732721209051209, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=24, min_child_weight=0.0023588925015011544,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009451087049979049, reg_lambda=0.08827632666156185,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 04:52:38] {2636} INFO - fit succeeded
[flaml.automl: 09-17 04:52:38] {2637} INFO - Time taken to find the best model: 59.60440635681152
[flaml.automl: 09-17 04:52:38] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 10, 'max_leaves': 24, 'min_child_weight': 0.0023588925015011544, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8732721209051209, 'reg_alpha': 0.009451087049979049, 'reg_lambda': 0.08827632666156185, 'FLAML_sample_size': 43617}
SO2(0)最佳损失：-1.4996183526031448
SO2(0)最好结果：{'pred_time': 8.076275651765308e-06, 'wall_clock_time': 59.60440635681152, 'metric_for_logging': {'pred_time': 8.076275651765308e-06}, 'val_loss': 2.499618352603145, 'training_iteration': 1, 'config': {'n_estimators': 10, 'max_leaves': 24, 'min_child_weight': 0.0023588925015011544, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8732721209051209, 'reg_alpha': 0.009451087049979049, 'reg_lambda': 0.08827632666156185, 'FLAML_sample_size': 43617}, 'config/n_estimators': 10, 'config/max_leaves': 24, 'config/min_child_weight': 0.0023588925015011544, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8732721209051209, 'config/reg_alpha': 0.009451087049979049, 'config/reg_lambda': 0.08827632666156185, 'config/FLAML_sample_size': 43617, 'experiment_tag': 'exp', 'time_total_s': 2.917661190032959}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8732721209051209, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=24, min_child_weight=0.0023588925015011544,
             missing=nan, monotone_constraints='()', n_estimators=10, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009451087049979049, reg_lambda=0.08827632666156185,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6997332570983443
SO2(0)的mse=15.727218081954513
SO2(0)的mae=2.4812175922960513
SO2(0)的mar=0.21666134148661909
总共花费的时间为：73.95
枣庄市
1637A
1638A
1639A
1640A
3364A
[flaml.automl: 09-17 05:08:09] {2390} INFO - task = regression
[flaml.automl: 09-17 05:08:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:08:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:08:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:08:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:08:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:08:10] {3025} INFO - Estimated sufficient time budget=64622s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 05:08:10] {3072} INFO -  at 1.4s,	estimator xgboost's best error=8.0106,	best estimator xgboost's best error=8.0106
[flaml.automl: 09-17 05:08:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:08:12] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.9843,	best estimator xgboost's best error=3.9843
[flaml.automl: 09-17 05:08:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:08:13] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.9843,	best estimator xgboost's best error=3.9843
[flaml.automl: 09-17 05:08:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:08:18] {3072} INFO -  at 9.4s,	estimator xgboost's best error=3.9843,	best estimator xgboost's best error=3.9843
[flaml.automl: 09-17 05:08:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:08:19] {3072} INFO -  at 10.5s,	estimator xgboost's best error=3.0999,	best estimator xgboost's best error=3.0999
[flaml.automl: 09-17 05:08:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:08:21] {3072} INFO -  at 12.1s,	estimator xgboost's best error=3.0999,	best estimator xgboost's best error=3.0999
[flaml.automl: 09-17 05:08:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:08:23] {3072} INFO -  at 13.8s,	estimator xgboost's best error=2.6253,	best estimator xgboost's best error=2.6253
[flaml.automl: 09-17 05:08:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:08:25] {3072} INFO -  at 16.5s,	estimator xgboost's best error=2.6253,	best estimator xgboost's best error=2.6253
[flaml.automl: 09-17 05:08:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:08:27] {3072} INFO -  at 18.1s,	estimator xgboost's best error=2.6253,	best estimator xgboost's best error=2.6253
[flaml.automl: 09-17 05:08:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:08:30] {3072} INFO -  at 21.1s,	estimator xgboost's best error=2.6253,	best estimator xgboost's best error=2.6253
[flaml.automl: 09-17 05:08:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:08:31] {3072} INFO -  at 22.5s,	estimator xgboost's best error=2.6253,	best estimator xgboost's best error=2.6253
[flaml.automl: 09-17 05:08:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:08:33] {3072} INFO -  at 24.2s,	estimator xgboost's best error=2.6252,	best estimator xgboost's best error=2.6252
[flaml.automl: 09-17 05:08:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:08:34] {3072} INFO -  at 25.4s,	estimator xgboost's best error=2.6252,	best estimator xgboost's best error=2.6252
[flaml.automl: 09-17 05:08:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:08:41] {3072} INFO -  at 32.5s,	estimator xgboost's best error=2.6252,	best estimator xgboost's best error=2.6252
[flaml.automl: 09-17 05:08:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 05:08:45] {3072} INFO -  at 36.1s,	estimator xgboost's best error=2.5697,	best estimator xgboost's best error=2.5697
[flaml.automl: 09-17 05:08:45] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 05:08:47] {3072} INFO -  at 38.2s,	estimator xgboost's best error=2.5697,	best estimator xgboost's best error=2.5697
[flaml.automl: 09-17 05:08:47] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 05:08:53] {3072} INFO -  at 44.0s,	estimator xgboost's best error=2.5625,	best estimator xgboost's best error=2.5625
[flaml.automl: 09-17 05:08:53] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 05:08:56] {3072} INFO -  at 47.6s,	estimator xgboost's best error=2.5625,	best estimator xgboost's best error=2.5625
[flaml.automl: 09-17 05:08:56] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 05:08:59] {3072} INFO -  at 50.6s,	estimator xgboost's best error=2.5625,	best estimator xgboost's best error=2.5625
[flaml.automl: 09-17 05:08:59] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 05:09:08] {3072} INFO -  at 59.1s,	estimator xgboost's best error=2.5625,	best estimator xgboost's best error=2.5625
[flaml.automl: 09-17 05:09:14] {3335} INFO - retrain xgboost for 5.9s
[flaml.automl: 09-17 05:09:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.19405630899231047,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=11.615070090947864, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.07808876344933313, scale_pos_weight=1,
             subsample=0.9453052099956202, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:09:14] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:09:14] {2637} INFO - Time taken to find the best model: 44.019572019577026
[flaml.automl: 09-17 05:09:14] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 7, 'min_child_weight': 11.615070090947864, 'learning_rate': 0.19405630899231047, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.07808876344933313, 'FLAML_sample_size': 54579}
SO2(0)最佳损失：-1.5624981259081054
SO2(0)最好结果：{'pred_time': 6.592656990170773e-06, 'wall_clock_time': 44.019572019577026, 'metric_for_logging': {'pred_time': 6.592656990170773e-06}, 'val_loss': 2.5624981259081054, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 7, 'min_child_weight': 11.615070090947864, 'learning_rate': 0.19405630899231047, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.07808876344933313, 'FLAML_sample_size': 54579}, 'config/n_estimators': 14, 'config/max_leaves': 7, 'config/min_child_weight': 11.615070090947864, 'config/learning_rate': 0.19405630899231047, 'config/subsample': 0.9453052099956202, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.92006762344246, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.07808876344933313, 'config/FLAML_sample_size': 54579, 'experiment_tag': 'exp', 'time_total_s': 5.8656721115112305}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.19405630899231047,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=11.615070090947864, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.07808876344933313, scale_pos_weight=1,
             subsample=0.9453052099956202, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.33796482592931365
SO2(0)的mse=23.955132309083513
SO2(0)的mae=2.479715653246987
SO2(0)的mar=0.19455819386594625
总共花费的时间为：65.84
烟台市
1642A
1643A
1644A
1646A
3366A
[flaml.automl: 09-17 05:24:28] {2390} INFO - task = regression
[flaml.automl: 09-17 05:24:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:24:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:24:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:24:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:24:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:24:30] {3025} INFO - Estimated sufficient time budget=64893s. Estimated necessary time budget=65s.
[flaml.automl: 09-17 05:24:30] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.7984,	best estimator xgboost's best error=4.7984
[flaml.automl: 09-17 05:24:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:24:32] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.2169,	best estimator xgboost's best error=2.2169
[flaml.automl: 09-17 05:24:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:24:33] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.2169,	best estimator xgboost's best error=2.2169
[flaml.automl: 09-17 05:24:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:24:38] {3072} INFO -  at 9.4s,	estimator xgboost's best error=2.2169,	best estimator xgboost's best error=2.2169
[flaml.automl: 09-17 05:24:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:24:39] {3072} INFO -  at 10.5s,	estimator xgboost's best error=1.3917,	best estimator xgboost's best error=1.3917
[flaml.automl: 09-17 05:24:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:24:40] {3072} INFO -  at 12.0s,	estimator xgboost's best error=1.3917,	best estimator xgboost's best error=1.3917
[flaml.automl: 09-17 05:24:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:24:42] {3072} INFO -  at 13.7s,	estimator xgboost's best error=1.0746,	best estimator xgboost's best error=1.0746
[flaml.automl: 09-17 05:24:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:24:45] {3072} INFO -  at 16.3s,	estimator xgboost's best error=1.0746,	best estimator xgboost's best error=1.0746
[flaml.automl: 09-17 05:24:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:24:46] {3072} INFO -  at 17.9s,	estimator xgboost's best error=1.0746,	best estimator xgboost's best error=1.0746
[flaml.automl: 09-17 05:24:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:24:49] {3072} INFO -  at 20.9s,	estimator xgboost's best error=1.0746,	best estimator xgboost's best error=1.0746
[flaml.automl: 09-17 05:24:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:24:50] {3072} INFO -  at 22.3s,	estimator xgboost's best error=1.0584,	best estimator xgboost's best error=1.0584
[flaml.automl: 09-17 05:24:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:24:52] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.0584,	best estimator xgboost's best error=1.0584
[flaml.automl: 09-17 05:24:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:24:54] {3072} INFO -  at 26.2s,	estimator xgboost's best error=1.0128,	best estimator xgboost's best error=1.0128
[flaml.automl: 09-17 05:24:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 05:24:57] {3072} INFO -  at 29.2s,	estimator xgboost's best error=0.9959,	best estimator xgboost's best error=0.9959
[flaml.automl: 09-17 05:24:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 05:25:00] {3072} INFO -  at 31.4s,	estimator xgboost's best error=0.9959,	best estimator xgboost's best error=0.9959
[flaml.automl: 09-17 05:25:00] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 05:25:02] {3072} INFO -  at 33.6s,	estimator xgboost's best error=0.9959,	best estimator xgboost's best error=0.9959
[flaml.automl: 09-17 05:25:02] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 05:25:04] {3072} INFO -  at 35.9s,	estimator xgboost's best error=0.9959,	best estimator xgboost's best error=0.9959
[flaml.automl: 09-17 05:25:04] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 05:25:06] {3072} INFO -  at 38.0s,	estimator xgboost's best error=0.9959,	best estimator xgboost's best error=0.9959
[flaml.automl: 09-17 05:25:06] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 05:25:17] {3072} INFO -  at 48.8s,	estimator xgboost's best error=0.9808,	best estimator xgboost's best error=0.9808
[flaml.automl: 09-17 05:25:17] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 05:25:21] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.9808,	best estimator xgboost's best error=0.9808
[flaml.automl: 09-17 05:25:32] {3335} INFO - retrain xgboost for 10.8s
[flaml.automl: 09-17 05:25:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:25:32] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:25:32] {2637} INFO - Time taken to find the best model: 48.781797885894775
[flaml.automl: 09-17 05:25:32] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 54813}
SO2(0)最佳损失：0.01916894766325783
SO2(0)最好结果：{'pred_time': 6.432291361875007e-06, 'wall_clock_time': 48.781797885894775, 'metric_for_logging': {'pred_time': 6.432291361875007e-06}, 'val_loss': 0.9808310523367422, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 54813}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 54813, 'experiment_tag': 'exp', 'time_total_s': 10.780868291854858}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7557693346255782
SO2(0)的mse=2.6858790617642567
SO2(0)的mae=0.9770020641580259
SO2(0)的mar=0.1351748194713184
总共花费的时间为：64.34
潍坊市
3178A
3368A
3416A
3861A
[flaml.automl: 09-17 05:38:01] {2390} INFO - task = regression
[flaml.automl: 09-17 05:38:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:38:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:38:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:38:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:38:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:38:04] {3025} INFO - Estimated sufficient time budget=32757s. Estimated necessary time budget=33s.
[flaml.automl: 09-17 05:38:04] {3072} INFO -  at 3.4s,	estimator xgboost's best error=4.5149,	best estimator xgboost's best error=4.5149
[flaml.automl: 09-17 05:38:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:38:10] {3072} INFO -  at 9.2s,	estimator xgboost's best error=2.1284,	best estimator xgboost's best error=2.1284
[flaml.automl: 09-17 05:38:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:38:13] {3072} INFO -  at 12.5s,	estimator xgboost's best error=2.1284,	best estimator xgboost's best error=2.1284
[flaml.automl: 09-17 05:38:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:38:36] {3072} INFO -  at 35.5s,	estimator xgboost's best error=2.1284,	best estimator xgboost's best error=2.1284
[flaml.automl: 09-17 05:38:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:38:37] {3072} INFO -  at 36.6s,	estimator xgboost's best error=1.4293,	best estimator xgboost's best error=1.4293
[flaml.automl: 09-17 05:38:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:38:39] {3072} INFO -  at 38.2s,	estimator xgboost's best error=1.2667,	best estimator xgboost's best error=1.2667
[flaml.automl: 09-17 05:38:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:38:40] {3072} INFO -  at 39.8s,	estimator xgboost's best error=1.1644,	best estimator xgboost's best error=1.1644
[flaml.automl: 09-17 05:38:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:38:43] {3072} INFO -  at 42.5s,	estimator xgboost's best error=1.1644,	best estimator xgboost's best error=1.1644
[flaml.automl: 09-17 05:38:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:38:45] {3072} INFO -  at 44.0s,	estimator xgboost's best error=1.1514,	best estimator xgboost's best error=1.1514
[flaml.automl: 09-17 05:38:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:38:48] {3072} INFO -  at 47.0s,	estimator xgboost's best error=1.0710,	best estimator xgboost's best error=1.0710
[flaml.automl: 09-17 05:38:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:38:49] {3072} INFO -  at 48.1s,	estimator xgboost's best error=1.0710,	best estimator xgboost's best error=1.0710
[flaml.automl: 09-17 05:38:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:39:01] {3072} INFO -  at 59.9s,	estimator xgboost's best error=1.0025,	best estimator xgboost's best error=1.0025
[flaml.automl: 09-17 05:39:14] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-17 05:39:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:39:14] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:39:14] {2637} INFO - Time taken to find the best model: 59.91152739524841
[flaml.automl: 09-17 05:39:14] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
SO2(0)最佳损失：-0.002481048091824034
SO2(0)最好结果：{'pred_time': 9.381864729792526e-06, 'wall_clock_time': 59.91152739524841, 'metric_for_logging': {'pred_time': 9.381864729792526e-06}, 'val_loss': 1.002481048091824, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 11.75993275642395}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8264500857787169
SO2(0)的mse=2.445473917769246
SO2(0)的mae=0.9800823551012904
SO2(0)的mar=0.14489123963691225
总共花费的时间为：74.20
济宁市
1653A
3501A
3678A
[flaml.automl: 09-17 05:48:33] {2390} INFO - task = regression
[flaml.automl: 09-17 05:48:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:48:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:48:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:48:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:48:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:48:37] {3025} INFO - Estimated sufficient time budget=32627s. Estimated necessary time budget=33s.
[flaml.automl: 09-17 05:48:37] {3072} INFO -  at 3.4s,	estimator xgboost's best error=6.1554,	best estimator xgboost's best error=6.1554
[flaml.automl: 09-17 05:48:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:48:42] {3072} INFO -  at 9.1s,	estimator xgboost's best error=2.8016,	best estimator xgboost's best error=2.8016
[flaml.automl: 09-17 05:48:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:48:46] {3072} INFO -  at 12.4s,	estimator xgboost's best error=2.8016,	best estimator xgboost's best error=2.8016
[flaml.automl: 09-17 05:48:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 05:49:09] {3072} INFO -  at 35.5s,	estimator xgboost's best error=2.8016,	best estimator xgboost's best error=2.8016
[flaml.automl: 09-17 05:49:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 05:49:10] {3072} INFO -  at 36.6s,	estimator xgboost's best error=1.5964,	best estimator xgboost's best error=1.5964
[flaml.automl: 09-17 05:49:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 05:49:11] {3072} INFO -  at 38.2s,	estimator xgboost's best error=1.5964,	best estimator xgboost's best error=1.5964
[flaml.automl: 09-17 05:49:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 05:49:13] {3072} INFO -  at 39.9s,	estimator xgboost's best error=1.1820,	best estimator xgboost's best error=1.1820
[flaml.automl: 09-17 05:49:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 05:49:16] {3072} INFO -  at 42.5s,	estimator xgboost's best error=1.1820,	best estimator xgboost's best error=1.1820
[flaml.automl: 09-17 05:49:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 05:49:17] {3072} INFO -  at 44.1s,	estimator xgboost's best error=1.1820,	best estimator xgboost's best error=1.1820
[flaml.automl: 09-17 05:49:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 05:49:20] {3072} INFO -  at 47.1s,	estimator xgboost's best error=1.1820,	best estimator xgboost's best error=1.1820
[flaml.automl: 09-17 05:49:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 05:49:22] {3072} INFO -  at 48.6s,	estimator xgboost's best error=1.1404,	best estimator xgboost's best error=1.1404
[flaml.automl: 09-17 05:49:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 05:49:23] {3072} INFO -  at 49.7s,	estimator xgboost's best error=1.1404,	best estimator xgboost's best error=1.1404
[flaml.automl: 09-17 05:49:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 05:49:29] {3072} INFO -  at 55.6s,	estimator xgboost's best error=1.0217,	best estimator xgboost's best error=1.0217
[flaml.automl: 09-17 05:49:35] {3335} INFO - retrain xgboost for 5.9s
[flaml.automl: 09-17 05:49:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 05:49:35] {2636} INFO - fit succeeded
[flaml.automl: 09-17 05:49:35] {2637} INFO - Time taken to find the best model: 55.56563758850098
[flaml.automl: 09-17 05:49:35] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
SO2(0)最佳损失：-0.02165635536041033
SO2(0)最好结果：{'pred_time': 1.1047640230149104e-05, 'wall_clock_time': 55.56563758850098, 'metric_for_logging': {'pred_time': 1.1047640230149104e-05}, 'val_loss': 1.0216563553604103, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 5.877272367477417}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8667164599513196
SO2(0)的mse=2.299536523932308
SO2(0)的mae=0.9944204456137813
SO2(0)的mar=0.10215675344951301
总共花费的时间为：62.05
泰安市
3502A
3503A
3504A
[flaml.automl: 09-17 05:59:15] {2390} INFO - task = regression
[flaml.automl: 09-17 05:59:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 05:59:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 05:59:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 05:59:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 05:59:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 05:59:19] {3025} INFO - Estimated sufficient time budget=31960s. Estimated necessary time budget=32s.
[flaml.automl: 09-17 05:59:19] {3072} INFO -  at 3.3s,	estimator xgboost's best error=6.5620,	best estimator xgboost's best error=6.5620
[flaml.automl: 09-17 05:59:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 05:59:24] {3072} INFO -  at 9.1s,	estimator xgboost's best error=3.0255,	best estimator xgboost's best error=3.0255
[flaml.automl: 09-17 05:59:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 05:59:29] {3072} INFO -  at 13.5s,	estimator xgboost's best error=3.0255,	best estimator xgboost's best error=3.0255
[flaml.automl: 09-17 05:59:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:00:01] {3072} INFO -  at 45.8s,	estimator xgboost's best error=3.0255,	best estimator xgboost's best error=3.0255
[flaml.automl: 09-17 06:00:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:00:04] {3072} INFO -  at 49.1s,	estimator xgboost's best error=1.9774,	best estimator xgboost's best error=1.9774
[flaml.automl: 09-17 06:00:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:00:09] {3072} INFO -  at 53.5s,	estimator xgboost's best error=1.9774,	best estimator xgboost's best error=1.9774
[flaml.automl: 09-17 06:00:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:00:14] {3072} INFO -  at 58.2s,	estimator xgboost's best error=1.4887,	best estimator xgboost's best error=1.4887
[flaml.automl: 09-17 06:00:18] {3335} INFO - retrain xgboost for 4.2s
[flaml.automl: 09-17 06:00:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 06:00:18] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:00:18] {2637} INFO - Time taken to find the best model: 58.226078271865845
[flaml.automl: 09-17 06:00:18] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-0.4887392659840051
SO2(0)最好结果：{'pred_time': 3.660301902649362e-05, 'wall_clock_time': 58.226078271865845, 'metric_for_logging': {'pred_time': 3.660301902649362e-05}, 'val_loss': 1.488739265984005, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.733215808868408}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6123564474417336
SO2(0)的mse=6.695846077062774
SO2(0)的mae=1.5443490990195636
SO2(0)的mar=0.14964519702735504
总共花费的时间为：63.15
日照市
1659A
1661A
3507A
3604A
[flaml.automl: 09-17 06:13:58] {2390} INFO - task = regression
[flaml.automl: 09-17 06:13:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:13:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:13:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:13:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:13:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:13:59] {3025} INFO - Estimated sufficient time budget=52055s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 06:13:59] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.6088,	best estimator xgboost's best error=4.6088
[flaml.automl: 09-17 06:13:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:14:02] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.2089,	best estimator xgboost's best error=2.2089
[flaml.automl: 09-17 06:14:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:14:03] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.2089,	best estimator xgboost's best error=2.2089
[flaml.automl: 09-17 06:14:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:14:09] {3072} INFO -  at 10.6s,	estimator xgboost's best error=2.2089,	best estimator xgboost's best error=2.2089
[flaml.automl: 09-17 06:14:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:14:10] {3072} INFO -  at 11.7s,	estimator xgboost's best error=1.6385,	best estimator xgboost's best error=1.6385
[flaml.automl: 09-17 06:14:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:14:11] {3072} INFO -  at 13.3s,	estimator xgboost's best error=1.6230,	best estimator xgboost's best error=1.6230
[flaml.automl: 09-17 06:14:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:14:13] {3072} INFO -  at 14.9s,	estimator xgboost's best error=1.3971,	best estimator xgboost's best error=1.3971
[flaml.automl: 09-17 06:14:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:14:16] {3072} INFO -  at 17.6s,	estimator xgboost's best error=1.3971,	best estimator xgboost's best error=1.3971
[flaml.automl: 09-17 06:14:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:14:17] {3072} INFO -  at 19.2s,	estimator xgboost's best error=1.3610,	best estimator xgboost's best error=1.3610
[flaml.automl: 09-17 06:14:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:14:20] {3072} INFO -  at 22.2s,	estimator xgboost's best error=1.3434,	best estimator xgboost's best error=1.3434
[flaml.automl: 09-17 06:14:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:14:21] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.3434,	best estimator xgboost's best error=1.3434
[flaml.automl: 09-17 06:14:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:14:26] {3072} INFO -  at 28.0s,	estimator xgboost's best error=1.2357,	best estimator xgboost's best error=1.2357
[flaml.automl: 09-17 06:14:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:14:29] {3072} INFO -  at 30.9s,	estimator xgboost's best error=1.2357,	best estimator xgboost's best error=1.2357
[flaml.automl: 09-17 06:14:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:14:32] {3072} INFO -  at 34.2s,	estimator xgboost's best error=1.2357,	best estimator xgboost's best error=1.2357
[flaml.automl: 09-17 06:14:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 06:14:36] {3072} INFO -  at 37.5s,	estimator xgboost's best error=1.2357,	best estimator xgboost's best error=1.2357
[flaml.automl: 09-17 06:14:36] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 06:14:38] {3072} INFO -  at 39.7s,	estimator xgboost's best error=1.2357,	best estimator xgboost's best error=1.2357
[flaml.automl: 09-17 06:14:38] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 06:14:57] {3072} INFO -  at 58.5s,	estimator xgboost's best error=1.1639,	best estimator xgboost's best error=1.1639
[flaml.automl: 09-17 06:15:22] {3335} INFO - retrain xgboost for 25.1s
[flaml.automl: 09-17 06:15:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:15:22] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:15:22] {2637} INFO - Time taken to find the best model: 58.52517580986023
[flaml.automl: 09-17 06:15:22] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858, 'FLAML_sample_size': 43746}
SO2(0)最佳损失：-0.1639324458286464
SO2(0)最好结果：{'pred_time': 1.386501396224715e-05, 'wall_clock_time': 58.52517580986023, 'metric_for_logging': {'pred_time': 1.386501396224715e-05}, 'val_loss': 1.1639324458286464, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858, 'FLAML_sample_size': 43746}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'config/FLAML_sample_size': 43746, 'experiment_tag': 'exp', 'time_total_s': 18.790632486343384}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7314362415926325
SO2(0)的mse=5.6383669445618985
SO2(0)的mae=1.190431305709271
SO2(0)的mar=0.15012965454821553
总共花费的时间为：84.39
威海市
1662A
1664A
1982A
3505A
[flaml.automl: 09-17 06:27:26] {2390} INFO - task = regression
[flaml.automl: 09-17 06:27:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:27:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:27:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:27:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:27:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:27:28] {3025} INFO - Estimated sufficient time budget=52771s. Estimated necessary time budget=53s.
[flaml.automl: 09-17 06:27:28] {3072} INFO -  at 1.4s,	estimator xgboost's best error=2.8090,	best estimator xgboost's best error=2.8090
[flaml.automl: 09-17 06:27:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:27:30] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.3034,	best estimator xgboost's best error=1.3034
[flaml.automl: 09-17 06:27:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:27:31] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.3034,	best estimator xgboost's best error=1.3034
[flaml.automl: 09-17 06:27:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:27:37] {3072} INFO -  at 10.6s,	estimator xgboost's best error=1.3034,	best estimator xgboost's best error=1.3034
[flaml.automl: 09-17 06:27:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:27:38] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.7527,	best estimator xgboost's best error=0.7527
[flaml.automl: 09-17 06:27:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:27:40] {3072} INFO -  at 13.3s,	estimator xgboost's best error=0.7527,	best estimator xgboost's best error=0.7527
[flaml.automl: 09-17 06:27:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:27:41] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.6260,	best estimator xgboost's best error=0.6260
[flaml.automl: 09-17 06:27:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:27:44] {3072} INFO -  at 17.7s,	estimator xgboost's best error=0.6260,	best estimator xgboost's best error=0.6260
[flaml.automl: 09-17 06:27:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:27:46] {3072} INFO -  at 19.3s,	estimator xgboost's best error=0.6260,	best estimator xgboost's best error=0.6260
[flaml.automl: 09-17 06:27:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:27:49] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.6260,	best estimator xgboost's best error=0.6260
[flaml.automl: 09-17 06:27:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:27:50] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.5879,	best estimator xgboost's best error=0.5879
[flaml.automl: 09-17 06:27:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:27:51] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.5879,	best estimator xgboost's best error=0.5879
[flaml.automl: 09-17 06:27:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:27:55] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.5367,	best estimator xgboost's best error=0.5367
[flaml.automl: 09-17 06:27:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:27:59] {3072} INFO -  at 32.2s,	estimator xgboost's best error=0.5284,	best estimator xgboost's best error=0.5284
[flaml.automl: 09-17 06:27:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 06:28:01] {3072} INFO -  at 35.0s,	estimator xgboost's best error=0.5284,	best estimator xgboost's best error=0.5284
[flaml.automl: 09-17 06:28:01] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 06:28:04] {3072} INFO -  at 37.2s,	estimator xgboost's best error=0.5284,	best estimator xgboost's best error=0.5284
[flaml.automl: 09-17 06:28:04] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 06:28:06] {3072} INFO -  at 39.7s,	estimator xgboost's best error=0.5284,	best estimator xgboost's best error=0.5284
[flaml.automl: 09-17 06:28:06] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 06:28:08] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.5284,	best estimator xgboost's best error=0.5284
[flaml.automl: 09-17 06:28:08] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 06:28:26] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.5257,	best estimator xgboost's best error=0.5257
[flaml.automl: 09-17 06:28:45] {3335} INFO - retrain xgboost for 19.8s
[flaml.automl: 09-17 06:28:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:28:45] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:28:45] {2637} INFO - Time taken to find the best model: 59.379382610321045
[flaml.automl: 09-17 06:28:45] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 43983}
SO2(0)最佳损失：0.4742522101863801
SO2(0)最好结果：{'pred_time': 1.738494418748981e-05, 'wall_clock_time': 59.379382610321045, 'metric_for_logging': {'pred_time': 1.738494418748981e-05}, 'val_loss': 0.5257477898136199, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 43983}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 43983, 'experiment_tag': 'exp', 'time_total_s': 17.657612562179565}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7540237918232165
SO2(0)的mse=0.6497211998948976
SO2(0)的mae=0.5256414879007116
SO2(0)的mar=0.12040034412177503
总共花费的时间为：80.14
东营市
3365A
3498A
3734A
[flaml.automl: 09-17 06:38:13] {2390} INFO - task = regression
[flaml.automl: 09-17 06:38:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:38:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:38:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:38:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:38:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:38:14] {3025} INFO - Estimated sufficient time budget=11966s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 06:38:14] {3072} INFO -  at 1.3s,	estimator xgboost's best error=8.0319,	best estimator xgboost's best error=8.0319
[flaml.automl: 09-17 06:38:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:38:16] {3072} INFO -  at 3.4s,	estimator xgboost's best error=3.6891,	best estimator xgboost's best error=3.6891
[flaml.automl: 09-17 06:38:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:38:17] {3072} INFO -  at 4.6s,	estimator xgboost's best error=3.6891,	best estimator xgboost's best error=3.6891
[flaml.automl: 09-17 06:38:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:38:27] {3072} INFO -  at 14.5s,	estimator xgboost's best error=3.6891,	best estimator xgboost's best error=3.6891
[flaml.automl: 09-17 06:38:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:38:28] {3072} INFO -  at 15.7s,	estimator xgboost's best error=2.2789,	best estimator xgboost's best error=2.2789
[flaml.automl: 09-17 06:38:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:38:30] {3072} INFO -  at 17.3s,	estimator xgboost's best error=2.1222,	best estimator xgboost's best error=2.1222
[flaml.automl: 09-17 06:38:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:38:32] {3072} INFO -  at 18.9s,	estimator xgboost's best error=1.9876,	best estimator xgboost's best error=1.9876
[flaml.automl: 09-17 06:38:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:38:34] {3072} INFO -  at 21.6s,	estimator xgboost's best error=1.9876,	best estimator xgboost's best error=1.9876
[flaml.automl: 09-17 06:38:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:38:36] {3072} INFO -  at 23.2s,	estimator xgboost's best error=1.9876,	best estimator xgboost's best error=1.9876
[flaml.automl: 09-17 06:38:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:38:39] {3072} INFO -  at 26.3s,	estimator xgboost's best error=1.7486,	best estimator xgboost's best error=1.7486
[flaml.automl: 09-17 06:38:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:38:41] {3072} INFO -  at 27.9s,	estimator xgboost's best error=1.7486,	best estimator xgboost's best error=1.7486
[flaml.automl: 09-17 06:38:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:38:42] {3072} INFO -  at 29.1s,	estimator xgboost's best error=1.7486,	best estimator xgboost's best error=1.7486
[flaml.automl: 09-17 06:38:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:38:55] {3072} INFO -  at 42.7s,	estimator xgboost's best error=1.7486,	best estimator xgboost's best error=1.7486
[flaml.automl: 09-17 06:38:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:39:01] {3072} INFO -  at 47.9s,	estimator xgboost's best error=1.6347,	best estimator xgboost's best error=1.6347
[flaml.automl: 09-17 06:39:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 06:39:03] {3072} INFO -  at 50.5s,	estimator xgboost's best error=1.6347,	best estimator xgboost's best error=1.6347
[flaml.automl: 09-17 06:39:03] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 06:39:12] {3072} INFO -  at 59.5s,	estimator xgboost's best error=1.6347,	best estimator xgboost's best error=1.6347
[flaml.automl: 09-17 06:39:17] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-17 06:39:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:39:17] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:39:17] {2637} INFO - Time taken to find the best model: 47.907904863357544
[flaml.automl: 09-17 06:39:17] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 17, 'max_leaves': 5, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}
SO2(0)最佳损失：-0.6346954002224245
SO2(0)最好结果：{'pred_time': 1.0983192579157519e-05, 'wall_clock_time': 47.907904863357544, 'metric_for_logging': {'pred_time': 1.0983192579157519e-05}, 'val_loss': 1.6346954002224245, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 5, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}, 'config/n_estimators': 17, 'config/max_leaves': 5, 'config/min_child_weight': 0.07186500551974488, 'config/learning_rate': 0.18147476059072173, 'config/subsample': 0.7641710216408443, 'config/colsample_bylevel': 0.8883913141026579, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.14043216453256416, 'experiment_tag': 'exp', 'time_total_s': 5.18534779548645}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6262235886251344
SO2(0)的mse=12.71010058702947
SO2(0)的mae=1.817352489310304
SO2(0)的mar=0.13815195942466763
总共花费的时间为：65.22
韶关市
1669A
1673A
3622A
[flaml.automl: 09-17 06:48:30] {2390} INFO - task = regression
[flaml.automl: 09-17 06:48:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:48:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:48:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:48:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:48:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:48:32] {3025} INFO - Estimated sufficient time budget=18919s. Estimated necessary time budget=19s.
[flaml.automl: 09-17 06:48:32] {3072} INFO -  at 2.1s,	estimator xgboost's best error=5.3045,	best estimator xgboost's best error=5.3045
[flaml.automl: 09-17 06:48:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:48:35] {3072} INFO -  at 5.8s,	estimator xgboost's best error=2.4369,	best estimator xgboost's best error=2.4369
[flaml.automl: 09-17 06:48:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:48:37] {3072} INFO -  at 7.7s,	estimator xgboost's best error=2.4369,	best estimator xgboost's best error=2.4369
[flaml.automl: 09-17 06:48:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 06:48:54] {3072} INFO -  at 23.9s,	estimator xgboost's best error=2.4369,	best estimator xgboost's best error=2.4369
[flaml.automl: 09-17 06:48:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 06:48:56] {3072} INFO -  at 25.8s,	estimator xgboost's best error=1.4379,	best estimator xgboost's best error=1.4379
[flaml.automl: 09-17 06:48:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 06:48:59] {3072} INFO -  at 28.8s,	estimator xgboost's best error=1.4379,	best estimator xgboost's best error=1.4379
[flaml.automl: 09-17 06:48:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 06:49:01] {3072} INFO -  at 31.7s,	estimator xgboost's best error=1.1982,	best estimator xgboost's best error=1.1982
[flaml.automl: 09-17 06:49:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 06:49:06] {3072} INFO -  at 36.2s,	estimator xgboost's best error=1.1982,	best estimator xgboost's best error=1.1982
[flaml.automl: 09-17 06:49:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 06:49:09] {3072} INFO -  at 39.1s,	estimator xgboost's best error=1.1982,	best estimator xgboost's best error=1.1982
[flaml.automl: 09-17 06:49:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 06:49:13] {3072} INFO -  at 43.5s,	estimator xgboost's best error=1.1982,	best estimator xgboost's best error=1.1982
[flaml.automl: 09-17 06:49:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 06:49:15] {3072} INFO -  at 45.0s,	estimator xgboost's best error=1.1904,	best estimator xgboost's best error=1.1904
[flaml.automl: 09-17 06:49:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 06:49:16] {3072} INFO -  at 46.1s,	estimator xgboost's best error=1.1904,	best estimator xgboost's best error=1.1904
[flaml.automl: 09-17 06:49:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 06:49:22] {3072} INFO -  at 52.1s,	estimator xgboost's best error=1.1713,	best estimator xgboost's best error=1.1713
[flaml.automl: 09-17 06:49:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 06:49:29] {3072} INFO -  at 59.5s,	estimator xgboost's best error=1.1653,	best estimator xgboost's best error=1.1653
[flaml.automl: 09-17 06:49:40] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-17 06:49:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 06:49:40] {2636} INFO - fit succeeded
[flaml.automl: 09-17 06:49:40] {2637} INFO - Time taken to find the best model: 59.54120063781738
[flaml.automl: 09-17 06:49:40] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
SO2(0)最佳损失：-0.16527496881882464
SO2(0)最好结果：{'pred_time': 1.0785859081213235e-05, 'wall_clock_time': 59.54120063781738, 'metric_for_logging': {'pred_time': 1.0785859081213235e-05}, 'val_loss': 1.1652749688188246, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 7.420433282852173}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5607333309881506
SO2(0)的mse=3.6071333750062067
SO2(0)的mae=1.1739013393184197
SO2(0)的mar=0.12816052237725364
总共花费的时间为：70.83
汕头市
1674A
1675A
3624A
[flaml.automl: 09-17 06:59:47] {2390} INFO - task = regression
[flaml.automl: 09-17 06:59:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 06:59:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 06:59:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 06:59:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 06:59:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 06:59:49] {3025} INFO - Estimated sufficient time budget=20367s. Estimated necessary time budget=20s.
[flaml.automl: 09-17 06:59:49] {3072} INFO -  at 2.2s,	estimator xgboost's best error=4.9243,	best estimator xgboost's best error=4.9243
[flaml.automl: 09-17 06:59:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 06:59:53] {3072} INFO -  at 5.6s,	estimator xgboost's best error=2.1935,	best estimator xgboost's best error=2.1935
[flaml.automl: 09-17 06:59:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 06:59:55] {3072} INFO -  at 7.7s,	estimator xgboost's best error=2.1935,	best estimator xgboost's best error=2.1935
[flaml.automl: 09-17 06:59:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:00:12] {3072} INFO -  at 24.3s,	estimator xgboost's best error=2.1935,	best estimator xgboost's best error=2.1935
[flaml.automl: 09-17 07:00:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:00:13] {3072} INFO -  at 26.1s,	estimator xgboost's best error=0.9898,	best estimator xgboost's best error=0.9898
[flaml.automl: 09-17 07:00:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:00:16] {3072} INFO -  at 28.7s,	estimator xgboost's best error=0.8704,	best estimator xgboost's best error=0.8704
[flaml.automl: 09-17 07:00:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:00:19] {3072} INFO -  at 31.3s,	estimator xgboost's best error=0.7287,	best estimator xgboost's best error=0.7287
[flaml.automl: 09-17 07:00:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:00:23] {3072} INFO -  at 35.8s,	estimator xgboost's best error=0.7287,	best estimator xgboost's best error=0.7287
[flaml.automl: 09-17 07:00:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:00:26] {3072} INFO -  at 38.5s,	estimator xgboost's best error=0.7287,	best estimator xgboost's best error=0.7287
[flaml.automl: 09-17 07:00:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:00:31] {3072} INFO -  at 43.3s,	estimator xgboost's best error=0.7287,	best estimator xgboost's best error=0.7287
[flaml.automl: 09-17 07:00:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:00:33] {3072} INFO -  at 45.8s,	estimator xgboost's best error=0.7287,	best estimator xgboost's best error=0.7287
[flaml.automl: 09-17 07:00:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:00:35] {3072} INFO -  at 47.6s,	estimator xgboost's best error=0.7287,	best estimator xgboost's best error=0.7287
[flaml.automl: 09-17 07:00:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:00:47] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.6551,	best estimator xgboost's best error=0.6551
[flaml.automl: 09-17 07:00:58] {3335} INFO - retrain xgboost for 11.2s
[flaml.automl: 09-17 07:00:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 07:00:58] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:00:58] {2637} INFO - Time taken to find the best model: 59.238173723220825
[flaml.automl: 09-17 07:00:58] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386}
SO2(0)最佳损失：0.34487929578048326
SO2(0)最好结果：{'pred_time': 1.8913154705993486e-05, 'wall_clock_time': 59.238173723220825, 'metric_for_logging': {'pred_time': 1.8913154705993486e-05}, 'val_loss': 0.6551207042195167, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'experiment_tag': 'exp', 'time_total_s': 11.681020259857178}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7732936758589665
SO2(0)的mse=1.1477861943861372
SO2(0)的mae=0.6393106226069837
SO2(0)的mar=0.07775463063113801
总共花费的时间为：70.94
湛江市
1680A
1681A
1682A
1684A
1685A
[flaml.automl: 09-17 07:16:29] {2390} INFO - task = regression
[flaml.automl: 09-17 07:16:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:16:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:16:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:16:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:16:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:16:30] {3025} INFO - Estimated sufficient time budget=65988s. Estimated necessary time budget=66s.
[flaml.automl: 09-17 07:16:30] {3072} INFO -  at 1.5s,	estimator xgboost's best error=4.2724,	best estimator xgboost's best error=4.2724
[flaml.automl: 09-17 07:16:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:16:32] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.0711,	best estimator xgboost's best error=2.0711
[flaml.automl: 09-17 07:16:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:16:34] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.0711,	best estimator xgboost's best error=2.0711
[flaml.automl: 09-17 07:16:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:16:38] {3072} INFO -  at 9.5s,	estimator xgboost's best error=2.0711,	best estimator xgboost's best error=2.0711
[flaml.automl: 09-17 07:16:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:16:39] {3072} INFO -  at 10.7s,	estimator xgboost's best error=1.5397,	best estimator xgboost's best error=1.5397
[flaml.automl: 09-17 07:16:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:16:41] {3072} INFO -  at 12.3s,	estimator xgboost's best error=1.5397,	best estimator xgboost's best error=1.5397
[flaml.automl: 09-17 07:16:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:16:43] {3072} INFO -  at 14.0s,	estimator xgboost's best error=1.2783,	best estimator xgboost's best error=1.2783
[flaml.automl: 09-17 07:16:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:16:45] {3072} INFO -  at 16.7s,	estimator xgboost's best error=1.2783,	best estimator xgboost's best error=1.2783
[flaml.automl: 09-17 07:16:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:16:47] {3072} INFO -  at 18.3s,	estimator xgboost's best error=1.2783,	best estimator xgboost's best error=1.2783
[flaml.automl: 09-17 07:16:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:16:50] {3072} INFO -  at 21.3s,	estimator xgboost's best error=1.2783,	best estimator xgboost's best error=1.2783
[flaml.automl: 09-17 07:16:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:16:51] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.2783,	best estimator xgboost's best error=1.2783
[flaml.automl: 09-17 07:16:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:16:53] {3072} INFO -  at 24.4s,	estimator xgboost's best error=1.2684,	best estimator xgboost's best error=1.2684
[flaml.automl: 09-17 07:16:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:16:54] {3072} INFO -  at 25.6s,	estimator xgboost's best error=1.2684,	best estimator xgboost's best error=1.2684
[flaml.automl: 09-17 07:16:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:17:01] {3072} INFO -  at 32.7s,	estimator xgboost's best error=1.2684,	best estimator xgboost's best error=1.2684
[flaml.automl: 09-17 07:17:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 07:17:05] {3072} INFO -  at 36.3s,	estimator xgboost's best error=1.2297,	best estimator xgboost's best error=1.2297
[flaml.automl: 09-17 07:17:05] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 07:17:07] {3072} INFO -  at 38.3s,	estimator xgboost's best error=1.2297,	best estimator xgboost's best error=1.2297
[flaml.automl: 09-17 07:17:07] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 07:17:13] {3072} INFO -  at 44.2s,	estimator xgboost's best error=1.2297,	best estimator xgboost's best error=1.2297
[flaml.automl: 09-17 07:17:13] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 07:17:16] {3072} INFO -  at 47.8s,	estimator xgboost's best error=1.2297,	best estimator xgboost's best error=1.2297
[flaml.automl: 09-17 07:17:16] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 07:17:20] {3072} INFO -  at 50.9s,	estimator xgboost's best error=1.2297,	best estimator xgboost's best error=1.2297
[flaml.automl: 09-17 07:17:20] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 07:17:29] {3072} INFO -  at 59.9s,	estimator xgboost's best error=1.2297,	best estimator xgboost's best error=1.2297
[flaml.automl: 09-17 07:17:32] {3335} INFO - retrain xgboost for 3.5s
[flaml.automl: 09-17 07:17:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:17:32] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:17:32] {2637} INFO - Time taken to find the best model: 36.2634699344635
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445, 'FLAML_sample_size': 55215}
SO2(0)最佳损失：-0.22965480272810734
SO2(0)最好结果：{'pred_time': 6.489143589209422e-06, 'wall_clock_time': 36.2634699344635, 'metric_for_logging': {'pred_time': 6.489143589209422e-06}, 'val_loss': 1.2296548027281073, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445, 'FLAML_sample_size': 55215}, 'config/n_estimators': 14, 'config/max_leaves': 4, 'config/min_child_weight': 2.909605268359626, 'config/learning_rate': 0.1976415421521206, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040166475000702445, 'config/FLAML_sample_size': 55215, 'experiment_tag': 'exp', 'time_total_s': 3.590785503387451}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.49074741665550814
SO2(0)的mse=6.4819432274962745
SO2(0)的mae=1.1891958089593184
SO2(0)的mar=0.14218675055765062
总共花费的时间为：64.20
茂名市
1686A
1688A
1689A
3450A
[flaml.automl: 09-17 07:29:52] {2390} INFO - task = regression
[flaml.automl: 09-17 07:29:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:29:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:29:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:29:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:29:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:29:53] {3025} INFO - Estimated sufficient time budget=51346s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 07:29:53] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.7337,	best estimator xgboost's best error=5.7337
[flaml.automl: 09-17 07:29:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:29:55] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.7920,	best estimator xgboost's best error=2.7920
[flaml.automl: 09-17 07:29:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:29:56] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.7920,	best estimator xgboost's best error=2.7920
[flaml.automl: 09-17 07:29:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:30:02] {3072} INFO -  at 10.5s,	estimator xgboost's best error=2.7920,	best estimator xgboost's best error=2.7920
[flaml.automl: 09-17 07:30:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:30:03] {3072} INFO -  at 11.7s,	estimator xgboost's best error=2.1933,	best estimator xgboost's best error=2.1933
[flaml.automl: 09-17 07:30:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:30:05] {3072} INFO -  at 13.2s,	estimator xgboost's best error=2.1933,	best estimator xgboost's best error=2.1933
[flaml.automl: 09-17 07:30:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:30:06] {3072} INFO -  at 14.9s,	estimator xgboost's best error=1.9419,	best estimator xgboost's best error=1.9419
[flaml.automl: 09-17 07:30:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:30:09] {3072} INFO -  at 17.6s,	estimator xgboost's best error=1.9419,	best estimator xgboost's best error=1.9419
[flaml.automl: 09-17 07:30:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:30:11] {3072} INFO -  at 19.3s,	estimator xgboost's best error=1.9419,	best estimator xgboost's best error=1.9419
[flaml.automl: 09-17 07:30:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:30:14] {3072} INFO -  at 22.3s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:30:15] {3072} INFO -  at 24.0s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:30:17] {3072} INFO -  at 25.1s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:30:20] {3072} INFO -  at 28.9s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:30:24] {3072} INFO -  at 32.3s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 07:30:27] {3072} INFO -  at 35.4s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:27] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 07:30:32] {3072} INFO -  at 40.3s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:32] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 07:30:34] {3072} INFO -  at 42.2s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:34] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 07:30:35] {3072} INFO -  at 43.9s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:35] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 07:30:43] {3072} INFO -  at 51.2s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:43] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 07:30:44] {3072} INFO -  at 52.4s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:44] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 07:30:51] {3072} INFO -  at 59.8s,	estimator xgboost's best error=1.9231,	best estimator xgboost's best error=1.9231
[flaml.automl: 09-17 07:30:54] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-17 07:30:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 07:30:54] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:30:54] {2637} INFO - Time taken to find the best model: 22.281686067581177
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：-0.9230590028093224
SO2(0)最好结果：{'pred_time': 9.285546854714115e-06, 'wall_clock_time': 22.281686067581177, 'metric_for_logging': {'pred_time': 9.285546854714115e-06}, 'val_loss': 1.9230590028093224, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768, 'FLAML_sample_size': 10000}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.0264930725097656}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.3507136262655096
SO2(0)的mse=13.337718319352994
SO2(0)的mae=1.959183294646492
SO2(0)的mar=0.1785247801796031
总共花费的时间为：63.44
梅州市
1690A
1692A
3315A
[flaml.automl: 09-17 07:40:40] {2390} INFO - task = regression
[flaml.automl: 09-17 07:40:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:40:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:40:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:40:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:40:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:40:42] {3025} INFO - Estimated sufficient time budget=12122s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 07:40:42] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.7607,	best estimator xgboost's best error=3.7607
[flaml.automl: 09-17 07:40:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:40:44] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.7174,	best estimator xgboost's best error=1.7174
[flaml.automl: 09-17 07:40:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:40:45] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.7174,	best estimator xgboost's best error=1.7174
[flaml.automl: 09-17 07:40:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:40:59] {3072} INFO -  at 19.3s,	estimator xgboost's best error=1.7174,	best estimator xgboost's best error=1.7174
[flaml.automl: 09-17 07:40:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:41:01] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.9347,	best estimator xgboost's best error=0.9347
[flaml.automl: 09-17 07:41:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:41:04] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.9347,	best estimator xgboost's best error=0.9347
[flaml.automl: 09-17 07:41:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:41:07] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.7713,	best estimator xgboost's best error=0.7713
[flaml.automl: 09-17 07:41:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:41:12] {3072} INFO -  at 31.6s,	estimator xgboost's best error=0.7713,	best estimator xgboost's best error=0.7713
[flaml.automl: 09-17 07:41:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:41:15] {3072} INFO -  at 34.5s,	estimator xgboost's best error=0.7713,	best estimator xgboost's best error=0.7713
[flaml.automl: 09-17 07:41:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:41:20] {3072} INFO -  at 39.6s,	estimator xgboost's best error=0.7713,	best estimator xgboost's best error=0.7713
[flaml.automl: 09-17 07:41:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:41:22] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.7464,	best estimator xgboost's best error=0.7464
[flaml.automl: 09-17 07:41:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:41:24] {3072} INFO -  at 44.0s,	estimator xgboost's best error=0.7464,	best estimator xgboost's best error=0.7464
[flaml.automl: 09-17 07:41:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:41:36] {3072} INFO -  at 55.4s,	estimator xgboost's best error=0.7464,	best estimator xgboost's best error=0.7464
[flaml.automl: 09-17 07:41:36] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:41:40] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.7464,	best estimator xgboost's best error=0.7464
[flaml.automl: 09-17 07:41:43] {3335} INFO - retrain xgboost for 3.1s
[flaml.automl: 09-17 07:41:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.923976097470924, colsample_bynode=1,
             colsample_bytree=0.9654640505640502, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4847187609776744,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=104.45542841808965, missing=nan,
             monotone_constraints='()', n_estimators=5, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3217956171961422, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 07:41:43] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:41:43] {2637} INFO - Time taken to find the best model: 41.975218772888184
SO2(0)最佳参数：{'n_estimators': 5, 'max_leaves': 4, 'min_child_weight': 104.45542841808965, 'learning_rate': 0.4847187609776744, 'subsample': 1.0, 'colsample_bylevel': 0.923976097470924, 'colsample_bytree': 0.9654640505640502, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3217956171961422}
SO2(0)最佳损失：0.2535694806534612
SO2(0)最好结果：{'pred_time': 1.9216249988124155e-05, 'wall_clock_time': 41.975218772888184, 'metric_for_logging': {'pred_time': 1.9216249988124155e-05}, 'val_loss': 0.7464305193465388, 'training_iteration': 1, 'config': {'n_estimators': 5, 'max_leaves': 4, 'min_child_weight': 104.45542841808965, 'learning_rate': 0.4847187609776744, 'subsample': 1.0, 'colsample_bylevel': 0.923976097470924, 'colsample_bytree': 0.9654640505640502, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3217956171961422}, 'config/n_estimators': 5, 'config/max_leaves': 4, 'config/min_child_weight': 104.45542841808965, 'config/learning_rate': 0.4847187609776744, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.923976097470924, 'config/colsample_bytree': 0.9654640505640502, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.3217956171961422, 'experiment_tag': 'exp', 'time_total_s': 2.3657476902008057}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.923976097470924, colsample_bynode=1,
             colsample_bytree=0.9654640505640502, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4847187609776744,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=104.45542841808965, missing=nan,
             monotone_constraints='()', n_estimators=5, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3217956171961422, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.4516278271380212
SO2(0)的mse=1.8959562916908916
SO2(0)的mae=0.7614482557435512
SO2(0)的mar=0.11582133153635962
总共花费的时间为：63.47
汕尾市
1694A
1695A
[flaml.automl: 09-17 07:47:48] {2390} INFO - task = regression
[flaml.automl: 09-17 07:47:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:47:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:47:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:47:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:47:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:47:50] {3025} INFO - Estimated sufficient time budget=22165s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 07:47:50] {3072} INFO -  at 2.3s,	estimator xgboost's best error=4.3019,	best estimator xgboost's best error=4.3019
[flaml.automl: 09-17 07:47:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:47:53] {3072} INFO -  at 5.2s,	estimator xgboost's best error=1.8942,	best estimator xgboost's best error=1.8942
[flaml.automl: 09-17 07:47:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:47:54] {3072} INFO -  at 6.4s,	estimator xgboost's best error=1.8942,	best estimator xgboost's best error=1.8942
[flaml.automl: 09-17 07:47:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:48:01] {3072} INFO -  at 13.6s,	estimator xgboost's best error=1.8942,	best estimator xgboost's best error=1.8942
[flaml.automl: 09-17 07:48:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:48:03] {3072} INFO -  at 14.7s,	estimator xgboost's best error=0.6869,	best estimator xgboost's best error=0.6869
[flaml.automl: 09-17 07:48:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:48:04] {3072} INFO -  at 16.2s,	estimator xgboost's best error=0.4498,	best estimator xgboost's best error=0.4498
[flaml.automl: 09-17 07:48:04] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:48:06] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.4116,	best estimator xgboost's best error=0.4116
[flaml.automl: 09-17 07:48:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:48:08] {3072} INFO -  at 20.5s,	estimator xgboost's best error=0.4116,	best estimator xgboost's best error=0.4116
[flaml.automl: 09-17 07:48:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:48:10] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.3354,	best estimator xgboost's best error=0.3354
[flaml.automl: 09-17 07:48:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:48:13] {3072} INFO -  at 25.0s,	estimator xgboost's best error=0.3354,	best estimator xgboost's best error=0.3354
[flaml.automl: 09-17 07:48:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:48:14] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.3354,	best estimator xgboost's best error=0.3354
[flaml.automl: 09-17 07:48:14] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:48:15] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.3354,	best estimator xgboost's best error=0.3354
[flaml.automl: 09-17 07:48:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:48:22] {3072} INFO -  at 33.8s,	estimator xgboost's best error=0.3342,	best estimator xgboost's best error=0.3342
[flaml.automl: 09-17 07:48:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 07:48:39] {3072} INFO -  at 50.8s,	estimator xgboost's best error=0.3298,	best estimator xgboost's best error=0.3298
[flaml.automl: 09-17 07:48:58] {3335} INFO - retrain xgboost for 19.1s
[flaml.automl: 09-17 07:48:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 07:48:58] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:48:58] {2637} INFO - Time taken to find the best model: 50.83461666107178
[flaml.automl: 09-17 07:48:58] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}
SO2(0)最佳损失：0.670200903137304
SO2(0)最好结果：{'pred_time': 3.32261571058496e-05, 'wall_clock_time': 50.83461666107178, 'metric_for_logging': {'pred_time': 3.32261571058496e-05}, 'val_loss': 0.329799096862696, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'experiment_tag': 'exp', 'time_total_s': 17.00529456138611}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8954439006706356
SO2(0)的mse=0.2654053313634418
SO2(0)的mae=0.33922833837182514
SO2(0)的mar=0.04883182744982601
总共花费的时间为：70.35
河源市
1696A
1697A
[flaml.automl: 09-17 07:55:41] {2390} INFO - task = regression
[flaml.automl: 09-17 07:55:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 07:55:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 07:55:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 07:55:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 07:55:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 07:55:44] {3025} INFO - Estimated sufficient time budget=32323s. Estimated necessary time budget=32s.
[flaml.automl: 09-17 07:55:44] {3072} INFO -  at 3.3s,	estimator xgboost's best error=3.1584,	best estimator xgboost's best error=3.1584
[flaml.automl: 09-17 07:55:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 07:55:49] {3072} INFO -  at 8.6s,	estimator xgboost's best error=1.6241,	best estimator xgboost's best error=1.6241
[flaml.automl: 09-17 07:55:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 07:55:53] {3072} INFO -  at 11.9s,	estimator xgboost's best error=1.6241,	best estimator xgboost's best error=1.6241
[flaml.automl: 09-17 07:55:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 07:56:09] {3072} INFO -  at 28.5s,	estimator xgboost's best error=1.6241,	best estimator xgboost's best error=1.6241
[flaml.automl: 09-17 07:56:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 07:56:12] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.6777,	best estimator xgboost's best error=0.6777
[flaml.automl: 09-17 07:56:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 07:56:14] {3072} INFO -  at 33.2s,	estimator xgboost's best error=0.5372,	best estimator xgboost's best error=0.5372
[flaml.automl: 09-17 07:56:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 07:56:16] {3072} INFO -  at 35.6s,	estimator xgboost's best error=0.4788,	best estimator xgboost's best error=0.4788
[flaml.automl: 09-17 07:56:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 07:56:20] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.4788,	best estimator xgboost's best error=0.4788
[flaml.automl: 09-17 07:56:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 07:56:23] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.4788,	best estimator xgboost's best error=0.4788
[flaml.automl: 09-17 07:56:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 07:56:26] {3072} INFO -  at 45.5s,	estimator xgboost's best error=0.4788,	best estimator xgboost's best error=0.4788
[flaml.automl: 09-17 07:56:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 07:56:28] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.4777,	best estimator xgboost's best error=0.4777
[flaml.automl: 09-17 07:56:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 07:56:29] {3072} INFO -  at 48.4s,	estimator xgboost's best error=0.4777,	best estimator xgboost's best error=0.4777
[flaml.automl: 09-17 07:56:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 07:56:36] {3072} INFO -  at 54.9s,	estimator xgboost's best error=0.3873,	best estimator xgboost's best error=0.3873
[flaml.automl: 09-17 07:56:42] {3335} INFO - retrain xgboost for 6.4s
[flaml.automl: 09-17 07:56:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 07:56:42] {2636} INFO - fit succeeded
[flaml.automl: 09-17 07:56:42] {2637} INFO - Time taken to find the best model: 54.90567469596863
[flaml.automl: 09-17 07:56:42] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}
SO2(0)最佳损失：0.6127446241965326
SO2(0)最好结果：{'pred_time': 2.0258301754475727e-05, 'wall_clock_time': 54.90567469596863, 'metric_for_logging': {'pred_time': 2.0258301754475727e-05}, 'val_loss': 0.38725537580346736, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'experiment_tag': 'exp', 'time_total_s': 6.522572994232178}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8720707170501988
SO2(0)的mse=0.4149929331544899
SO2(0)的mae=0.3939191519407622
SO2(0)的mar=0.0734385398245746
总共花费的时间为：61.75
阳江市
1699A
3453A
[flaml.automl: 09-17 08:03:29] {2390} INFO - task = regression
[flaml.automl: 09-17 08:03:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:03:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:03:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:03:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:03:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:03:31] {3025} INFO - Estimated sufficient time budget=22679s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 08:03:31] {3072} INFO -  at 2.4s,	estimator xgboost's best error=3.6752,	best estimator xgboost's best error=3.6752
[flaml.automl: 09-17 08:03:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:03:35] {3072} INFO -  at 5.9s,	estimator xgboost's best error=1.9383,	best estimator xgboost's best error=1.9383
[flaml.automl: 09-17 08:03:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:03:37] {3072} INFO -  at 8.2s,	estimator xgboost's best error=1.9383,	best estimator xgboost's best error=1.9383
[flaml.automl: 09-17 08:03:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:03:45] {3072} INFO -  at 16.9s,	estimator xgboost's best error=1.9383,	best estimator xgboost's best error=1.9383
[flaml.automl: 09-17 08:03:45] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:03:47] {3072} INFO -  at 18.0s,	estimator xgboost's best error=0.9369,	best estimator xgboost's best error=0.9369
[flaml.automl: 09-17 08:03:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:03:48] {3072} INFO -  at 19.4s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-17 08:03:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:03:50] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-17 08:03:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:03:52] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-17 08:03:52] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:03:53] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-17 08:03:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:03:56] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-17 08:03:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:03:57] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-17 08:03:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:03:58] {3072} INFO -  at 29.2s,	estimator xgboost's best error=0.7042,	best estimator xgboost's best error=0.7042
[flaml.automl: 09-17 08:03:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:04:04] {3072} INFO -  at 35.2s,	estimator xgboost's best error=0.6302,	best estimator xgboost's best error=0.6302
[flaml.automl: 09-17 08:04:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:04:14] {3072} INFO -  at 45.6s,	estimator xgboost's best error=0.6159,	best estimator xgboost's best error=0.6159
[flaml.automl: 09-17 08:04:14] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 08:04:20] {3072} INFO -  at 51.7s,	estimator xgboost's best error=0.6159,	best estimator xgboost's best error=0.6159
[flaml.automl: 09-17 08:04:31] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-17 08:04:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:04:31] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:04:31] {2637} INFO - Time taken to find the best model: 45.61478543281555
[flaml.automl: 09-17 08:04:31] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
SO2(0)最佳损失：0.3840649479342485
SO2(0)最好结果：{'pred_time': 2.034411071916376e-05, 'wall_clock_time': 45.61478543281555, 'metric_for_logging': {'pred_time': 2.034411071916376e-05}, 'val_loss': 0.6159350520657515, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 18, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 10.37331748008728}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8373034749523104
SO2(0)的mse=0.8864333530604239
SO2(0)的mae=0.5900999769702908
SO2(0)的mar=0.09847384722586094
总共花费的时间为：62.51
清远市
1702A
3318A
3455A
[flaml.automl: 09-17 08:14:09] {2390} INFO - task = regression
[flaml.automl: 09-17 08:14:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:14:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:14:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:14:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:14:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:14:12] {3025} INFO - Estimated sufficient time budget=31568s. Estimated necessary time budget=32s.
[flaml.automl: 09-17 08:14:12] {3072} INFO -  at 3.3s,	estimator xgboost's best error=4.0660,	best estimator xgboost's best error=4.0660
[flaml.automl: 09-17 08:14:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:14:16] {3072} INFO -  at 7.2s,	estimator xgboost's best error=1.8105,	best estimator xgboost's best error=1.8105
[flaml.automl: 09-17 08:14:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:14:19] {3072} INFO -  at 9.4s,	estimator xgboost's best error=1.8105,	best estimator xgboost's best error=1.8105
[flaml.automl: 09-17 08:14:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:14:37] {3072} INFO -  at 27.6s,	estimator xgboost's best error=1.8105,	best estimator xgboost's best error=1.8105
[flaml.automl: 09-17 08:14:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:14:39] {3072} INFO -  at 29.7s,	estimator xgboost's best error=0.8301,	best estimator xgboost's best error=0.8301
[flaml.automl: 09-17 08:14:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:14:42] {3072} INFO -  at 32.7s,	estimator xgboost's best error=0.6410,	best estimator xgboost's best error=0.6410
[flaml.automl: 09-17 08:14:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:14:44] {3072} INFO -  at 34.7s,	estimator xgboost's best error=0.6002,	best estimator xgboost's best error=0.6002
[flaml.automl: 09-17 08:14:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:14:47] {3072} INFO -  at 37.4s,	estimator xgboost's best error=0.6002,	best estimator xgboost's best error=0.6002
[flaml.automl: 09-17 08:14:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:14:48] {3072} INFO -  at 39.0s,	estimator xgboost's best error=0.6002,	best estimator xgboost's best error=0.6002
[flaml.automl: 09-17 08:14:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:14:51] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.5966,	best estimator xgboost's best error=0.5966
[flaml.automl: 09-17 08:14:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:14:53] {3072} INFO -  at 43.6s,	estimator xgboost's best error=0.5966,	best estimator xgboost's best error=0.5966
[flaml.automl: 09-17 08:14:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:14:54] {3072} INFO -  at 44.8s,	estimator xgboost's best error=0.5966,	best estimator xgboost's best error=0.5966
[flaml.automl: 09-17 08:14:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:15:08] {3072} INFO -  at 58.4s,	estimator xgboost's best error=0.4833,	best estimator xgboost's best error=0.4833
[flaml.automl: 09-17 08:15:21] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-17 08:15:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 08:15:21] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:15:21] {2637} INFO - Time taken to find the best model: 58.36591076850891
[flaml.automl: 09-17 08:15:21] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
SO2(0)最佳损失：0.5166978358530341
SO2(0)最好结果：{'pred_time': 1.191836829109358e-05, 'wall_clock_time': 58.36591076850891, 'metric_for_logging': {'pred_time': 1.191836829109358e-05}, 'val_loss': 0.4833021641469659, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 13.614842891693115}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8951541760506824
SO2(0)的mse=0.5654703374428269
SO2(0)的mae=0.46778548246516205
SO2(0)的mar=0.06999418030645549
总共花费的时间为：72.62
潮州市
1705A
1706A
3026A
[flaml.automl: 09-17 08:24:53] {2390} INFO - task = regression
[flaml.automl: 09-17 08:24:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:24:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:24:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:24:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:24:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:24:55] {3025} INFO - Estimated sufficient time budget=22385s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 08:24:55] {3072} INFO -  at 2.4s,	estimator xgboost's best error=5.4721,	best estimator xgboost's best error=5.4721
[flaml.automl: 09-17 08:24:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:24:59] {3072} INFO -  at 6.3s,	estimator xgboost's best error=2.5046,	best estimator xgboost's best error=2.5046
[flaml.automl: 09-17 08:24:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:25:01] {3072} INFO -  at 8.6s,	estimator xgboost's best error=2.5046,	best estimator xgboost's best error=2.5046
[flaml.automl: 09-17 08:25:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:25:19] {3072} INFO -  at 27.1s,	estimator xgboost's best error=2.5046,	best estimator xgboost's best error=2.5046
[flaml.automl: 09-17 08:25:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:25:22] {3072} INFO -  at 29.2s,	estimator xgboost's best error=1.5254,	best estimator xgboost's best error=1.5254
[flaml.automl: 09-17 08:25:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:25:24] {3072} INFO -  at 32.1s,	estimator xgboost's best error=1.4156,	best estimator xgboost's best error=1.4156
[flaml.automl: 09-17 08:25:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:25:28] {3072} INFO -  at 35.2s,	estimator xgboost's best error=1.3638,	best estimator xgboost's best error=1.3638
[flaml.automl: 09-17 08:25:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:25:32] {3072} INFO -  at 40.0s,	estimator xgboost's best error=1.3638,	best estimator xgboost's best error=1.3638
[flaml.automl: 09-17 08:25:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:25:34] {3072} INFO -  at 41.8s,	estimator xgboost's best error=1.3638,	best estimator xgboost's best error=1.3638
[flaml.automl: 09-17 08:25:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:25:37] {3072} INFO -  at 44.8s,	estimator xgboost's best error=1.3042,	best estimator xgboost's best error=1.3042
[flaml.automl: 09-17 08:25:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:25:39] {3072} INFO -  at 46.4s,	estimator xgboost's best error=1.3042,	best estimator xgboost's best error=1.3042
[flaml.automl: 09-17 08:25:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:25:40] {3072} INFO -  at 47.5s,	estimator xgboost's best error=1.3042,	best estimator xgboost's best error=1.3042
[flaml.automl: 09-17 08:25:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:25:52] {3072} INFO -  at 59.2s,	estimator xgboost's best error=1.3042,	best estimator xgboost's best error=1.3042
[flaml.automl: 09-17 08:25:55] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-17 08:25:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 08:25:55] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:25:55] {2637} INFO - Time taken to find the best model: 44.795778036117554
[flaml.automl: 09-17 08:25:55] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-0.3041737891968379
SO2(0)最好结果：{'pred_time': 1.1925727928811499e-05, 'wall_clock_time': 44.795778036117554, 'metric_for_logging': {'pred_time': 1.1925727928811499e-05}, 'val_loss': 1.304173789196838, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 2.9827988147735596}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.3342852987279762
SO2(0)的mse=4.8725084155182445
SO2(0)的mae=1.3011297706419729
SO2(0)的mar=0.13086330124049167
总共花费的时间为：62.74
揭阳市
1708A
1709A
1710A
3320A
[flaml.automl: 09-17 08:38:12] {2390} INFO - task = regression
[flaml.automl: 09-17 08:38:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:38:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:38:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:38:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:38:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:38:14] {3025} INFO - Estimated sufficient time budget=98081s. Estimated necessary time budget=98s.
[flaml.automl: 09-17 08:38:14] {3072} INFO -  at 2.4s,	estimator xgboost's best error=4.2899,	best estimator xgboost's best error=4.2899
[flaml.automl: 09-17 08:38:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:38:18] {3072} INFO -  at 6.3s,	estimator xgboost's best error=2.1161,	best estimator xgboost's best error=2.1161
[flaml.automl: 09-17 08:38:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:38:21] {3072} INFO -  at 8.5s,	estimator xgboost's best error=2.1161,	best estimator xgboost's best error=2.1161
[flaml.automl: 09-17 08:38:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:38:26] {3072} INFO -  at 13.7s,	estimator xgboost's best error=2.1161,	best estimator xgboost's best error=2.1161
[flaml.automl: 09-17 08:38:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:38:28] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.5425,	best estimator xgboost's best error=1.5425
[flaml.automl: 09-17 08:38:28] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:38:31] {3072} INFO -  at 18.8s,	estimator xgboost's best error=1.4607,	best estimator xgboost's best error=1.4607
[flaml.automl: 09-17 08:38:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:38:34] {3072} INFO -  at 21.9s,	estimator xgboost's best error=1.3858,	best estimator xgboost's best error=1.3858
[flaml.automl: 09-17 08:38:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:38:37] {3072} INFO -  at 25.3s,	estimator xgboost's best error=1.3858,	best estimator xgboost's best error=1.3858
[flaml.automl: 09-17 08:38:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:38:40] {3072} INFO -  at 28.3s,	estimator xgboost's best error=1.3858,	best estimator xgboost's best error=1.3858
[flaml.automl: 09-17 08:38:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:38:44] {3072} INFO -  at 31.5s,	estimator xgboost's best error=1.3858,	best estimator xgboost's best error=1.3858
[flaml.automl: 09-17 08:38:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:38:46] {3072} INFO -  at 34.3s,	estimator xgboost's best error=1.3858,	best estimator xgboost's best error=1.3858
[flaml.automl: 09-17 08:38:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:38:49] {3072} INFO -  at 37.2s,	estimator xgboost's best error=1.3828,	best estimator xgboost's best error=1.3828
[flaml.automl: 09-17 08:38:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:38:51] {3072} INFO -  at 38.5s,	estimator xgboost's best error=1.3828,	best estimator xgboost's best error=1.3828
[flaml.automl: 09-17 08:38:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:38:59] {3072} INFO -  at 47.0s,	estimator xgboost's best error=1.3030,	best estimator xgboost's best error=1.3030
[flaml.automl: 09-17 08:38:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 08:39:11] {3072} INFO -  at 59.4s,	estimator xgboost's best error=1.2934,	best estimator xgboost's best error=1.2934
[flaml.automl: 09-17 08:39:41] {3335} INFO - retrain xgboost for 29.5s
[flaml.automl: 09-17 08:39:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:39:41] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:39:41] {2637} INFO - Time taken to find the best model: 59.43134427070618
[flaml.automl: 09-17 08:39:41] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 44085}
SO2(0)最佳损失：-0.29335265116780174
SO2(0)最好结果：{'pred_time': 1.93091309004887e-05, 'wall_clock_time': 59.43134427070618, 'metric_for_logging': {'pred_time': 1.93091309004887e-05}, 'val_loss': 1.2933526511678017, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 44085}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 44085, 'experiment_tag': 'exp', 'time_total_s': 12.45401668548584}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6568514116348889
SO2(0)的mse=4.110540821022037
SO2(0)的mae=1.2998266032945358
SO2(0)的mar=0.21716755037039578
总共花费的时间为：89.71
云浮市
1712A
[flaml.automl: 09-17 08:42:57] {2390} INFO - task = regression
[flaml.automl: 09-17 08:42:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:42:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:42:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:42:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:42:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:43:00] {3025} INFO - Estimated sufficient time budget=22262s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 08:43:00] {3072} INFO -  at 2.3s,	estimator xgboost's best error=4.7991,	best estimator xgboost's best error=4.7991
[flaml.automl: 09-17 08:43:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:43:03] {3072} INFO -  at 5.8s,	estimator xgboost's best error=2.5016,	best estimator xgboost's best error=2.5016
[flaml.automl: 09-17 08:43:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:43:05] {3072} INFO -  at 8.1s,	estimator xgboost's best error=2.5016,	best estimator xgboost's best error=2.5016
[flaml.automl: 09-17 08:43:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:43:18] {3072} INFO -  at 21.0s,	estimator xgboost's best error=2.5016,	best estimator xgboost's best error=2.5016
[flaml.automl: 09-17 08:43:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:43:20] {3072} INFO -  at 23.1s,	estimator xgboost's best error=1.1169,	best estimator xgboost's best error=1.1169
[flaml.automl: 09-17 08:43:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:43:23] {3072} INFO -  at 26.1s,	estimator xgboost's best error=0.9901,	best estimator xgboost's best error=0.9901
[flaml.automl: 09-17 08:43:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:43:26] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.9901,	best estimator xgboost's best error=0.9901
[flaml.automl: 09-17 08:43:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:43:31] {3072} INFO -  at 33.3s,	estimator xgboost's best error=0.9901,	best estimator xgboost's best error=0.9901
[flaml.automl: 09-17 08:43:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:43:33] {3072} INFO -  at 35.3s,	estimator xgboost's best error=0.9901,	best estimator xgboost's best error=0.9901
[flaml.automl: 09-17 08:43:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:43:37] {3072} INFO -  at 39.9s,	estimator xgboost's best error=0.9901,	best estimator xgboost's best error=0.9901
[flaml.automl: 09-17 08:43:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:43:39] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.9901,	best estimator xgboost's best error=0.9901
[flaml.automl: 09-17 08:43:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:43:41] {3072} INFO -  at 44.0s,	estimator xgboost's best error=0.9901,	best estimator xgboost's best error=0.9901
[flaml.automl: 09-17 08:43:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:43:51] {3072} INFO -  at 53.9s,	estimator xgboost's best error=0.9561,	best estimator xgboost's best error=0.9561
[flaml.automl: 09-17 08:44:01] {3335} INFO - retrain xgboost for 9.6s
[flaml.automl: 09-17 08:44:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 08:44:01] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:44:01] {2637} INFO - Time taken to find the best model: 53.85783267021179
[flaml.automl: 09-17 08:44:01] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
SO2(0)最佳损失：0.04389264436112239
SO2(0)最好结果：{'pred_time': 7.719231635024867e-05, 'wall_clock_time': 53.85783267021179, 'metric_for_logging': {'pred_time': 7.719231635024867e-05}, 'val_loss': 0.9561073556388776, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 12, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.883583307266235}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6319231105476694
SO2(0)的mse=3.2905899920520882
SO2(0)的mae=0.942658874232163
SO2(0)的mar=0.107051363089243
总共花费的时间为：63.69
玉溪市
2882A
2883A
[flaml.automl: 09-17 08:51:03] {2390} INFO - task = regression
[flaml.automl: 09-17 08:51:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:51:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:51:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:51:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:51:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:51:05] {3025} INFO - Estimated sufficient time budget=12048s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 08:51:05] {3072} INFO -  at 1.3s,	estimator xgboost's best error=5.4489,	best estimator xgboost's best error=5.4489
[flaml.automl: 09-17 08:51:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:51:07] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.4373,	best estimator xgboost's best error=2.4373
[flaml.automl: 09-17 08:51:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:51:08] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.4373,	best estimator xgboost's best error=2.4373
[flaml.automl: 09-17 08:51:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:51:19] {3072} INFO -  at 15.2s,	estimator xgboost's best error=2.4373,	best estimator xgboost's best error=2.4373
[flaml.automl: 09-17 08:51:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:51:21] {3072} INFO -  at 17.1s,	estimator xgboost's best error=1.3357,	best estimator xgboost's best error=1.3357
[flaml.automl: 09-17 08:51:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:51:23] {3072} INFO -  at 19.9s,	estimator xgboost's best error=1.3296,	best estimator xgboost's best error=1.3296
[flaml.automl: 09-17 08:51:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:51:26] {3072} INFO -  at 22.9s,	estimator xgboost's best error=1.1778,	best estimator xgboost's best error=1.1778
[flaml.automl: 09-17 08:51:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:51:31] {3072} INFO -  at 27.3s,	estimator xgboost's best error=1.1778,	best estimator xgboost's best error=1.1778
[flaml.automl: 09-17 08:51:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:51:34] {3072} INFO -  at 30.4s,	estimator xgboost's best error=1.1778,	best estimator xgboost's best error=1.1778
[flaml.automl: 09-17 08:51:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:51:39] {3072} INFO -  at 35.5s,	estimator xgboost's best error=1.1252,	best estimator xgboost's best error=1.1252
[flaml.automl: 09-17 08:51:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:51:42] {3072} INFO -  at 38.4s,	estimator xgboost's best error=1.1252,	best estimator xgboost's best error=1.1252
[flaml.automl: 09-17 08:51:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:51:44] {3072} INFO -  at 40.2s,	estimator xgboost's best error=1.1252,	best estimator xgboost's best error=1.1252
[flaml.automl: 09-17 08:51:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:52:03] {3072} INFO -  at 59.8s,	estimator xgboost's best error=1.1252,	best estimator xgboost's best error=1.1252
[flaml.automl: 09-17 08:52:08] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-17 08:52:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 08:52:08] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:52:08] {2637} INFO - Time taken to find the best model: 35.53824234008789
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-0.12520880809779267
SO2(0)最好结果：{'pred_time': 3.338209438673306e-05, 'wall_clock_time': 35.53824234008789, 'metric_for_logging': {'pred_time': 3.338209438673306e-05}, 'val_loss': 1.1252088080977927, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 5.137038707733154}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.4181514020810009
SO2(0)的mse=3.9460950268130612
SO2(0)的mae=1.2121977598239213
SO2(0)的mar=0.1197652107711458
总共花费的时间为：65.35
菏泽市
1719A
[flaml.automl: 09-17 08:55:29] {2390} INFO - task = regression
[flaml.automl: 09-17 08:55:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 08:55:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 08:55:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 08:55:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 08:55:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 08:55:30] {3025} INFO - Estimated sufficient time budget=11878s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 08:55:30] {3072} INFO -  at 1.2s,	estimator xgboost's best error=5.9713,	best estimator xgboost's best error=5.9713
[flaml.automl: 09-17 08:55:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 08:55:32] {3072} INFO -  at 3.1s,	estimator xgboost's best error=3.1612,	best estimator xgboost's best error=3.1612
[flaml.automl: 09-17 08:55:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 08:55:33] {3072} INFO -  at 4.3s,	estimator xgboost's best error=3.1612,	best estimator xgboost's best error=3.1612
[flaml.automl: 09-17 08:55:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 08:55:40] {3072} INFO -  at 11.4s,	estimator xgboost's best error=3.1612,	best estimator xgboost's best error=3.1612
[flaml.automl: 09-17 08:55:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 08:55:42] {3072} INFO -  at 12.5s,	estimator xgboost's best error=1.7255,	best estimator xgboost's best error=1.7255
[flaml.automl: 09-17 08:55:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 08:55:43] {3072} INFO -  at 14.1s,	estimator xgboost's best error=1.4444,	best estimator xgboost's best error=1.4444
[flaml.automl: 09-17 08:55:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 08:55:45] {3072} INFO -  at 15.7s,	estimator xgboost's best error=1.4444,	best estimator xgboost's best error=1.4444
[flaml.automl: 09-17 08:55:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 08:55:47] {3072} INFO -  at 17.9s,	estimator xgboost's best error=1.4444,	best estimator xgboost's best error=1.4444
[flaml.automl: 09-17 08:55:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 08:55:48] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.4444,	best estimator xgboost's best error=1.4444
[flaml.automl: 09-17 08:55:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 08:55:51] {3072} INFO -  at 21.5s,	estimator xgboost's best error=1.4444,	best estimator xgboost's best error=1.4444
[flaml.automl: 09-17 08:55:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 08:55:52] {3072} INFO -  at 22.6s,	estimator xgboost's best error=1.4444,	best estimator xgboost's best error=1.4444
[flaml.automl: 09-17 08:55:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 08:55:53] {3072} INFO -  at 23.8s,	estimator xgboost's best error=1.4444,	best estimator xgboost's best error=1.4444
[flaml.automl: 09-17 08:55:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 08:56:00] {3072} INFO -  at 30.6s,	estimator xgboost's best error=1.3914,	best estimator xgboost's best error=1.3914
[flaml.automl: 09-17 08:56:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 08:56:18] {3072} INFO -  at 48.7s,	estimator xgboost's best error=1.3771,	best estimator xgboost's best error=1.3771
[flaml.automl: 09-17 08:56:41] {3335} INFO - retrain xgboost for 23.3s
[flaml.automl: 09-17 08:56:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 08:56:41] {2636} INFO - fit succeeded
[flaml.automl: 09-17 08:56:41] {2637} INFO - Time taken to find the best model: 48.65646433830261
[flaml.automl: 09-17 08:56:41] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}
SO2(0)最佳损失：-0.3770873765655969
SO2(0)最好结果：{'pred_time': 5.263535090408137e-05, 'wall_clock_time': 48.65646433830261, 'metric_for_logging': {'pred_time': 5.263535090408137e-05}, 'val_loss': 1.3770873765655969, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 10, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176}, 'config/n_estimators': 17, 'config/max_leaves': 10, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'experiment_tag': 'exp', 'time_total_s': 18.0149667263031}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=10,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7742757309357118
SO2(0)的mse=5.226826934080513
SO2(0)的mae=1.3804318968267173
SO2(0)的mar=0.13951373346438262
总共花费的时间为：72.32
大同市
1721A
1725A
1726A
3565A
3566A
3567A
[flaml.automl: 09-17 09:15:17] {2390} INFO - task = regression
[flaml.automl: 09-17 09:15:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:15:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:15:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:15:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:15:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:15:20] {3025} INFO - Estimated sufficient time budget=216925s. Estimated necessary time budget=217s.
[flaml.automl: 09-17 09:15:20] {3072} INFO -  at 3.7s,	estimator xgboost's best error=11.6734,	best estimator xgboost's best error=11.6734
[flaml.automl: 09-17 09:15:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:15:24] {3072} INFO -  at 7.4s,	estimator xgboost's best error=8.4981,	best estimator xgboost's best error=8.4981
[flaml.automl: 09-17 09:15:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:15:28] {3072} INFO -  at 10.8s,	estimator xgboost's best error=8.4981,	best estimator xgboost's best error=8.4981
[flaml.automl: 09-17 09:15:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:15:31] {3072} INFO -  at 14.3s,	estimator xgboost's best error=8.4981,	best estimator xgboost's best error=8.4981
[flaml.automl: 09-17 09:15:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:15:34] {3072} INFO -  at 16.9s,	estimator xgboost's best error=6.0195,	best estimator xgboost's best error=6.0195
[flaml.automl: 09-17 09:15:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:15:36] {3072} INFO -  at 19.3s,	estimator xgboost's best error=5.8880,	best estimator xgboost's best error=5.8880
[flaml.automl: 09-17 09:15:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:15:39] {3072} INFO -  at 21.9s,	estimator xgboost's best error=5.8880,	best estimator xgboost's best error=5.8880
[flaml.automl: 09-17 09:15:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:15:41] {3072} INFO -  at 24.0s,	estimator xgboost's best error=5.8880,	best estimator xgboost's best error=5.8880
[flaml.automl: 09-17 09:15:41] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:15:43] {3072} INFO -  at 25.9s,	estimator xgboost's best error=5.8880,	best estimator xgboost's best error=5.8880
[flaml.automl: 09-17 09:15:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:15:45] {3072} INFO -  at 28.2s,	estimator xgboost's best error=5.8880,	best estimator xgboost's best error=5.8880
[flaml.automl: 09-17 09:15:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:15:50] {3072} INFO -  at 32.9s,	estimator xgboost's best error=5.5305,	best estimator xgboost's best error=5.5305
[flaml.automl: 09-17 09:15:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:15:53] {3072} INFO -  at 36.2s,	estimator xgboost's best error=5.5305,	best estimator xgboost's best error=5.5305
[flaml.automl: 09-17 09:15:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:16:08] {3072} INFO -  at 50.7s,	estimator xgboost's best error=4.5884,	best estimator xgboost's best error=4.5884
[flaml.automl: 09-17 09:16:14] {3335} INFO - retrain xgboost for 6.5s
[flaml.automl: 09-17 09:16:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 09:16:14] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:16:14] {2637} INFO - Time taken to find the best model: 50.68893647193909
[flaml.automl: 09-17 09:16:14] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 64381}
SO2(0)最佳损失：-3.5883820144730905
SO2(0)最好结果：{'pred_time': 1.1495581411735486e-05, 'wall_clock_time': 50.68893647193909, 'metric_for_logging': {'pred_time': 1.1495581411735486e-05}, 'val_loss': 4.5883820144730905, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 64381}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 64381, 'experiment_tag': 'exp', 'time_total_s': 14.45344352722168}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6388152740777882
SO2(0)的mse=58.856712594541825
SO2(0)的mae=4.510250634450645
SO2(0)的mar=0.259913250343335
总共花费的时间为：58.35
长治市
1728A
1731A
2845A
3568A
3569A
3570A
[flaml.automl: 09-17 09:35:45] {2390} INFO - task = regression
[flaml.automl: 09-17 09:35:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:35:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:35:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:35:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:35:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:35:47] {3025} INFO - Estimated sufficient time budget=140944s. Estimated necessary time budget=141s.
[flaml.automl: 09-17 09:35:47] {3072} INFO -  at 2.6s,	estimator xgboost's best error=7.9224,	best estimator xgboost's best error=7.9224
[flaml.automl: 09-17 09:35:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:35:51] {3072} INFO -  at 6.5s,	estimator xgboost's best error=3.8076,	best estimator xgboost's best error=3.8076
[flaml.automl: 09-17 09:35:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:35:53] {3072} INFO -  at 8.7s,	estimator xgboost's best error=3.8076,	best estimator xgboost's best error=3.8076
[flaml.automl: 09-17 09:35:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:35:56] {3072} INFO -  at 11.8s,	estimator xgboost's best error=3.8076,	best estimator xgboost's best error=3.8076
[flaml.automl: 09-17 09:35:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:35:58] {3072} INFO -  at 13.9s,	estimator xgboost's best error=2.5266,	best estimator xgboost's best error=2.5266
[flaml.automl: 09-17 09:35:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:36:01] {3072} INFO -  at 16.9s,	estimator xgboost's best error=2.5266,	best estimator xgboost's best error=2.5266
[flaml.automl: 09-17 09:36:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:36:04] {3072} INFO -  at 20.0s,	estimator xgboost's best error=2.3440,	best estimator xgboost's best error=2.3440
[flaml.automl: 09-17 09:36:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:36:07] {3072} INFO -  at 22.6s,	estimator xgboost's best error=2.3440,	best estimator xgboost's best error=2.3440
[flaml.automl: 09-17 09:36:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:36:09] {3072} INFO -  at 25.2s,	estimator xgboost's best error=2.3440,	best estimator xgboost's best error=2.3440
[flaml.automl: 09-17 09:36:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:36:11] {3072} INFO -  at 27.1s,	estimator xgboost's best error=2.3440,	best estimator xgboost's best error=2.3440
[flaml.automl: 09-17 09:36:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:36:13] {3072} INFO -  at 29.3s,	estimator xgboost's best error=2.3440,	best estimator xgboost's best error=2.3440
[flaml.automl: 09-17 09:36:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:36:17] {3072} INFO -  at 32.5s,	estimator xgboost's best error=2.3360,	best estimator xgboost's best error=2.3360
[flaml.automl: 09-17 09:36:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:36:19] {3072} INFO -  at 34.7s,	estimator xgboost's best error=2.3360,	best estimator xgboost's best error=2.3360
[flaml.automl: 09-17 09:36:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:36:32] {3072} INFO -  at 47.8s,	estimator xgboost's best error=2.3141,	best estimator xgboost's best error=2.3141
[flaml.automl: 09-17 09:36:44] {3335} INFO - retrain xgboost for 11.9s
[flaml.automl: 09-17 09:36:44] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:36:44] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:36:44] {2637} INFO - Time taken to find the best model: 47.78358292579651
[flaml.automl: 09-17 09:36:44] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 64708}
SO2(0)最佳损失：-1.3140939609066002
SO2(0)最好结果：{'pred_time': 1.0958964701985451e-05, 'wall_clock_time': 47.78358292579651, 'metric_for_logging': {'pred_time': 1.0958964701985451e-05}, 'val_loss': 2.3140939609066002, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 64708}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 64708, 'experiment_tag': 'exp', 'time_total_s': 13.088481187820435}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5181136725139797
SO2(0)的mse=22.318026869272703
SO2(0)的mae=2.356487773329911
SO2(0)的mar=0.167727717402391
总共花费的时间为：61.55
临汾市
3668A
[flaml.automl: 09-17 09:40:22] {2390} INFO - task = regression
[flaml.automl: 09-17 09:40:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:40:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:40:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:40:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:40:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:40:25] {3025} INFO - Estimated sufficient time budget=22004s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 09:40:25] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.2811,	best estimator xgboost's best error=5.2811
[flaml.automl: 09-17 09:40:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:40:28] {3072} INFO -  at 5.6s,	estimator xgboost's best error=3.0495,	best estimator xgboost's best error=3.0495
[flaml.automl: 09-17 09:40:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:40:30] {3072} INFO -  at 7.8s,	estimator xgboost's best error=3.0495,	best estimator xgboost's best error=3.0495
[flaml.automl: 09-17 09:40:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:40:43] {3072} INFO -  at 20.4s,	estimator xgboost's best error=3.0495,	best estimator xgboost's best error=3.0495
[flaml.automl: 09-17 09:40:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:40:45] {3072} INFO -  at 22.6s,	estimator xgboost's best error=2.2904,	best estimator xgboost's best error=2.2904
[flaml.automl: 09-17 09:40:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:40:48] {3072} INFO -  at 25.6s,	estimator xgboost's best error=2.1836,	best estimator xgboost's best error=2.1836
[flaml.automl: 09-17 09:40:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:40:51] {3072} INFO -  at 28.6s,	estimator xgboost's best error=2.1233,	best estimator xgboost's best error=2.1233
[flaml.automl: 09-17 09:40:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:40:55] {3072} INFO -  at 33.0s,	estimator xgboost's best error=2.1233,	best estimator xgboost's best error=2.1233
[flaml.automl: 09-17 09:40:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:40:58] {3072} INFO -  at 35.9s,	estimator xgboost's best error=2.0869,	best estimator xgboost's best error=2.0869
[flaml.automl: 09-17 09:40:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:41:05] {3072} INFO -  at 42.3s,	estimator xgboost's best error=2.0456,	best estimator xgboost's best error=2.0456
[flaml.automl: 09-17 09:41:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:41:08] {3072} INFO -  at 45.5s,	estimator xgboost's best error=2.0456,	best estimator xgboost's best error=2.0456
[flaml.automl: 09-17 09:41:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:41:21] {3072} INFO -  at 59.0s,	estimator xgboost's best error=1.9406,	best estimator xgboost's best error=1.9406
[flaml.automl: 09-17 09:41:41] {3335} INFO - retrain xgboost for 19.5s
[flaml.automl: 09-17 09:41:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:41:41] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:41:41] {2637} INFO - Time taken to find the best model: 59.01751255989075
[flaml.automl: 09-17 09:41:41] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
SO2(0)最佳损失：-0.9406064447286904
SO2(0)最好结果：{'pred_time': 9.320409882147581e-05, 'wall_clock_time': 59.01751255989075, 'metric_for_logging': {'pred_time': 9.320409882147581e-05}, 'val_loss': 1.9406064447286904, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 14, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 12, 'config/max_leaves': 14, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 13.521743059158325}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7935659563691496
SO2(0)的mse=9.21651652212789
SO2(0)的mae=1.844264109248376
SO2(0)的mar=0.2539176519292564
总共花费的时间为：78.73
阳泉市
1738A
1739A
1743A
3619A
[flaml.automl: 09-17 09:55:32] {2390} INFO - task = regression
[flaml.automl: 09-17 09:55:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 09:55:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 09:55:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 09:55:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 09:55:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 09:55:34] {3025} INFO - Estimated sufficient time budget=50526s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 09:55:34] {3072} INFO -  at 1.4s,	estimator xgboost's best error=10.2163,	best estimator xgboost's best error=10.2163
[flaml.automl: 09-17 09:55:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 09:55:36] {3072} INFO -  at 3.4s,	estimator xgboost's best error=5.1251,	best estimator xgboost's best error=5.1251
[flaml.automl: 09-17 09:55:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 09:55:37] {3072} INFO -  at 4.6s,	estimator xgboost's best error=5.1251,	best estimator xgboost's best error=5.1251
[flaml.automl: 09-17 09:55:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 09:55:43] {3072} INFO -  at 10.8s,	estimator xgboost's best error=5.1251,	best estimator xgboost's best error=5.1251
[flaml.automl: 09-17 09:55:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 09:55:44] {3072} INFO -  at 11.9s,	estimator xgboost's best error=4.3260,	best estimator xgboost's best error=4.3260
[flaml.automl: 09-17 09:55:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 09:55:46] {3072} INFO -  at 13.4s,	estimator xgboost's best error=4.3260,	best estimator xgboost's best error=4.3260
[flaml.automl: 09-17 09:55:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 09:55:47] {3072} INFO -  at 15.1s,	estimator xgboost's best error=3.5645,	best estimator xgboost's best error=3.5645
[flaml.automl: 09-17 09:55:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 09:55:50] {3072} INFO -  at 17.7s,	estimator xgboost's best error=3.5645,	best estimator xgboost's best error=3.5645
[flaml.automl: 09-17 09:55:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 09:55:52] {3072} INFO -  at 19.3s,	estimator xgboost's best error=3.5645,	best estimator xgboost's best error=3.5645
[flaml.automl: 09-17 09:55:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 09:55:55] {3072} INFO -  at 22.3s,	estimator xgboost's best error=3.5645,	best estimator xgboost's best error=3.5645
[flaml.automl: 09-17 09:55:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 09:55:56] {3072} INFO -  at 23.8s,	estimator xgboost's best error=3.5645,	best estimator xgboost's best error=3.5645
[flaml.automl: 09-17 09:55:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 09:55:58] {3072} INFO -  at 25.5s,	estimator xgboost's best error=3.5645,	best estimator xgboost's best error=3.5645
[flaml.automl: 09-17 09:55:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 09:55:59] {3072} INFO -  at 26.7s,	estimator xgboost's best error=3.5645,	best estimator xgboost's best error=3.5645
[flaml.automl: 09-17 09:55:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 09:56:06] {3072} INFO -  at 33.6s,	estimator xgboost's best error=3.5366,	best estimator xgboost's best error=3.5366
[flaml.automl: 09-17 09:56:06] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 09:56:19] {3072} INFO -  at 46.4s,	estimator xgboost's best error=3.4285,	best estimator xgboost's best error=3.4285
[flaml.automl: 09-17 09:56:19] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 09:56:26] {3072} INFO -  at 53.5s,	estimator xgboost's best error=3.4285,	best estimator xgboost's best error=3.4285
[flaml.automl: 09-17 09:56:39] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 09:56:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 09:56:39] {2636} INFO - fit succeeded
[flaml.automl: 09-17 09:56:39] {2637} INFO - Time taken to find the best model: 46.444804430007935
[flaml.automl: 09-17 09:56:39] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43560}
SO2(0)最佳损失：-2.428455251307527
SO2(0)最好结果：{'pred_time': 8.753360795580651e-06, 'wall_clock_time': 46.444804430007935, 'metric_for_logging': {'pred_time': 8.753360795580651e-06}, 'val_loss': 3.428455251307527, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43560}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 43560, 'experiment_tag': 'exp', 'time_total_s': 12.796729326248169}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6611568296574832
SO2(0)的mse=38.88225195196294
SO2(0)的mae=3.4647647341461862
SO2(0)的mar=0.2120607971216185
总共花费的时间为：66.92
赤峰市
1744A
1745A
3286A
[flaml.automl: 09-17 10:07:08] {2390} INFO - task = regression
[flaml.automl: 09-17 10:07:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:07:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:07:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:07:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:07:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:07:09] {3025} INFO - Estimated sufficient time budget=11953s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 10:07:09] {3072} INFO -  at 1.3s,	estimator xgboost's best error=7.9085,	best estimator xgboost's best error=7.9085
[flaml.automl: 09-17 10:07:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:07:11] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.9647,	best estimator xgboost's best error=3.9647
[flaml.automl: 09-17 10:07:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:07:12] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.9647,	best estimator xgboost's best error=3.9647
[flaml.automl: 09-17 10:07:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:07:24] {3072} INFO -  at 16.6s,	estimator xgboost's best error=3.9647,	best estimator xgboost's best error=3.9647
[flaml.automl: 09-17 10:07:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:07:26] {3072} INFO -  at 18.6s,	estimator xgboost's best error=2.9808,	best estimator xgboost's best error=2.9808
[flaml.automl: 09-17 10:07:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:07:29] {3072} INFO -  at 21.2s,	estimator xgboost's best error=2.9385,	best estimator xgboost's best error=2.9385
[flaml.automl: 09-17 10:07:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:07:31] {3072} INFO -  at 24.1s,	estimator xgboost's best error=2.8222,	best estimator xgboost's best error=2.8222
[flaml.automl: 09-17 10:07:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:07:36] {3072} INFO -  at 28.1s,	estimator xgboost's best error=2.8222,	best estimator xgboost's best error=2.8222
[flaml.automl: 09-17 10:07:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:07:39] {3072} INFO -  at 31.1s,	estimator xgboost's best error=2.8222,	best estimator xgboost's best error=2.8222
[flaml.automl: 09-17 10:07:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:07:45] {3072} INFO -  at 37.3s,	estimator xgboost's best error=2.6804,	best estimator xgboost's best error=2.6804
[flaml.automl: 09-17 10:07:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:07:48] {3072} INFO -  at 40.9s,	estimator xgboost's best error=2.6804,	best estimator xgboost's best error=2.6804
[flaml.automl: 09-17 10:07:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:07:51] {3072} INFO -  at 43.7s,	estimator xgboost's best error=2.6804,	best estimator xgboost's best error=2.6804
[flaml.automl: 09-17 10:07:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:08:05] {3072} INFO -  at 58.1s,	estimator xgboost's best error=2.6804,	best estimator xgboost's best error=2.6804
[flaml.automl: 09-17 10:08:12] {3335} INFO - retrain xgboost for 6.7s
[flaml.automl: 09-17 10:08:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 10:08:12] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:08:12] {2637} INFO - Time taken to find the best model: 37.306984663009644
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-1.6804164392236531
SO2(0)最好结果：{'pred_time': 2.155383168222598e-05, 'wall_clock_time': 37.306984663009644, 'metric_for_logging': {'pred_time': 2.155383168222598e-05}, 'val_loss': 2.680416439223653, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 6.1941094398498535}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.2717283967294032
SO2(0)的mse=30.099689973399347
SO2(0)的mae=2.6228150345531165
SO2(0)的mar=0.19294412136380026
总共花费的时间为：65.32
鞍山市
1749A
1750A
1751A
1752A
1753A
1754A
[flaml.automl: 09-17 10:26:40] {2390} INFO - task = regression
[flaml.automl: 09-17 10:26:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:26:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:26:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:26:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:26:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:26:41] {3025} INFO - Estimated sufficient time budget=77131s. Estimated necessary time budget=77s.
[flaml.automl: 09-17 10:26:41] {3072} INFO -  at 1.5s,	estimator xgboost's best error=7.2479,	best estimator xgboost's best error=7.2479
[flaml.automl: 09-17 10:26:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:26:44] {3072} INFO -  at 3.6s,	estimator xgboost's best error=3.6162,	best estimator xgboost's best error=3.6162
[flaml.automl: 09-17 10:26:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:26:45] {3072} INFO -  at 4.9s,	estimator xgboost's best error=3.6162,	best estimator xgboost's best error=3.6162
[flaml.automl: 09-17 10:26:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:26:49] {3072} INFO -  at 8.6s,	estimator xgboost's best error=3.6162,	best estimator xgboost's best error=3.6162
[flaml.automl: 09-17 10:26:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:26:50] {3072} INFO -  at 9.8s,	estimator xgboost's best error=2.9183,	best estimator xgboost's best error=2.9183
[flaml.automl: 09-17 10:26:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:26:51] {3072} INFO -  at 11.4s,	estimator xgboost's best error=2.7733,	best estimator xgboost's best error=2.7733
[flaml.automl: 09-17 10:26:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:26:53] {3072} INFO -  at 13.0s,	estimator xgboost's best error=2.6141,	best estimator xgboost's best error=2.6141
[flaml.automl: 09-17 10:26:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:26:56] {3072} INFO -  at 15.7s,	estimator xgboost's best error=2.6141,	best estimator xgboost's best error=2.6141
[flaml.automl: 09-17 10:26:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:26:57] {3072} INFO -  at 17.3s,	estimator xgboost's best error=2.6141,	best estimator xgboost's best error=2.6141
[flaml.automl: 09-17 10:26:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:27:00] {3072} INFO -  at 20.3s,	estimator xgboost's best error=2.3794,	best estimator xgboost's best error=2.3794
[flaml.automl: 09-17 10:27:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:27:02] {3072} INFO -  at 22.0s,	estimator xgboost's best error=2.3794,	best estimator xgboost's best error=2.3794
[flaml.automl: 09-17 10:27:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:27:03] {3072} INFO -  at 23.1s,	estimator xgboost's best error=2.3794,	best estimator xgboost's best error=2.3794
[flaml.automl: 09-17 10:27:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:27:05] {3072} INFO -  at 25.1s,	estimator xgboost's best error=2.3794,	best estimator xgboost's best error=2.3794
[flaml.automl: 09-17 10:27:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:27:08] {3072} INFO -  at 27.7s,	estimator xgboost's best error=2.3794,	best estimator xgboost's best error=2.3794
[flaml.automl: 09-17 10:27:08] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 10:27:11] {3072} INFO -  at 30.7s,	estimator xgboost's best error=2.3794,	best estimator xgboost's best error=2.3794
[flaml.automl: 09-17 10:27:11] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 10:27:16] {3072} INFO -  at 35.7s,	estimator xgboost's best error=2.3794,	best estimator xgboost's best error=2.3794
[flaml.automl: 09-17 10:27:16] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 10:27:18] {3072} INFO -  at 37.6s,	estimator xgboost's best error=2.3679,	best estimator xgboost's best error=2.3679
[flaml.automl: 09-17 10:27:18] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 10:27:19] {3072} INFO -  at 39.3s,	estimator xgboost's best error=2.3679,	best estimator xgboost's best error=2.3679
[flaml.automl: 09-17 10:27:19] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 10:27:24] {3072} INFO -  at 43.7s,	estimator xgboost's best error=2.3679,	best estimator xgboost's best error=2.3679
[flaml.automl: 09-17 10:27:24] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 10:27:25] {3072} INFO -  at 44.8s,	estimator xgboost's best error=2.3679,	best estimator xgboost's best error=2.3679
[flaml.automl: 09-17 10:27:25] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 10:27:30] {3072} INFO -  at 50.1s,	estimator xgboost's best error=2.3665,	best estimator xgboost's best error=2.3665
[flaml.automl: 09-17 10:27:30] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 10:27:40] {3072} INFO -  at 59.6s,	estimator xgboost's best error=2.3393,	best estimator xgboost's best error=2.3393
[flaml.automl: 09-17 10:28:03] {3335} INFO - retrain xgboost for 23.8s
[flaml.automl: 09-17 10:28:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.008381997180108987, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003886437262068607, scale_pos_weight=1,
             subsample=0.9390180412130811, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 10:28:03] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:28:03] {2637} INFO - Time taken to find the best model: 59.599026918411255
[flaml.automl: 09-17 10:28:03] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 28, 'max_leaves': 15, 'min_child_weight': 0.008381997180108987, 'learning_rate': 0.6023269513573992, 'subsample': 0.9390180412130811, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003886437262068607, 'FLAML_sample_size': 64068}
SO2(0)最佳损失：-1.3393069859853135
SO2(0)最好结果：{'pred_time': 5.859959609455934e-06, 'wall_clock_time': 59.599026918411255, 'metric_for_logging': {'pred_time': 5.859959609455934e-06}, 'val_loss': 2.3393069859853135, 'training_iteration': 1, 'config': {'n_estimators': 28, 'max_leaves': 15, 'min_child_weight': 0.008381997180108987, 'learning_rate': 0.6023269513573992, 'subsample': 0.9390180412130811, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003886437262068607, 'FLAML_sample_size': 64068}, 'config/n_estimators': 28, 'config/max_leaves': 15, 'config/min_child_weight': 0.008381997180108987, 'config/learning_rate': 0.6023269513573992, 'config/subsample': 0.9390180412130811, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.936172861729374, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.003886437262068607, 'config/FLAML_sample_size': 64068, 'experiment_tag': 'exp', 'time_total_s': 9.509191989898682}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.008381997180108987, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003886437262068607, scale_pos_weight=1,
             subsample=0.9390180412130811, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6849575893281854
SO2(0)的mse=14.788276617683076
SO2(0)的mae=2.2316129950898334
SO2(0)的mar=0.18236926691339622
总共花费的时间为：84.38
抚顺市
1755A
1756A
1757A
1758A
1760A
[flaml.automl: 09-17 10:43:04] {2390} INFO - task = regression
[flaml.automl: 09-17 10:43:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:43:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:43:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:43:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:43:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:43:06] {3025} INFO - Estimated sufficient time budget=122390s. Estimated necessary time budget=122s.
[flaml.automl: 09-17 10:43:06] {3072} INFO -  at 2.5s,	estimator xgboost's best error=7.2038,	best estimator xgboost's best error=7.2038
[flaml.automl: 09-17 10:43:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:43:11] {3072} INFO -  at 7.4s,	estimator xgboost's best error=3.9435,	best estimator xgboost's best error=3.9435
[flaml.automl: 09-17 10:43:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:43:14] {3072} INFO -  at 10.8s,	estimator xgboost's best error=3.9435,	best estimator xgboost's best error=3.9435
[flaml.automl: 09-17 10:43:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:43:18] {3072} INFO -  at 14.3s,	estimator xgboost's best error=3.9435,	best estimator xgboost's best error=3.9435
[flaml.automl: 09-17 10:43:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:43:21] {3072} INFO -  at 17.6s,	estimator xgboost's best error=2.4990,	best estimator xgboost's best error=2.4990
[flaml.automl: 09-17 10:43:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:43:25] {3072} INFO -  at 21.1s,	estimator xgboost's best error=2.4990,	best estimator xgboost's best error=2.4990
[flaml.automl: 09-17 10:43:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:43:27] {3072} INFO -  at 23.9s,	estimator xgboost's best error=2.4990,	best estimator xgboost's best error=2.4990
[flaml.automl: 09-17 10:43:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:43:30] {3072} INFO -  at 26.9s,	estimator xgboost's best error=2.4990,	best estimator xgboost's best error=2.4990
[flaml.automl: 09-17 10:43:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:43:33] {3072} INFO -  at 29.5s,	estimator xgboost's best error=2.4823,	best estimator xgboost's best error=2.4823
[flaml.automl: 09-17 10:43:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:43:35] {3072} INFO -  at 31.1s,	estimator xgboost's best error=2.4823,	best estimator xgboost's best error=2.4823
[flaml.automl: 09-17 10:43:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:43:37] {3072} INFO -  at 32.9s,	estimator xgboost's best error=2.4823,	best estimator xgboost's best error=2.4823
[flaml.automl: 09-17 10:43:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:43:38] {3072} INFO -  at 34.8s,	estimator xgboost's best error=2.4823,	best estimator xgboost's best error=2.4823
[flaml.automl: 09-17 10:43:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:43:40] {3072} INFO -  at 36.8s,	estimator xgboost's best error=2.1544,	best estimator xgboost's best error=2.1544
[flaml.automl: 09-17 10:43:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:43:42] {3072} INFO -  at 38.5s,	estimator xgboost's best error=2.1544,	best estimator xgboost's best error=2.1544
[flaml.automl: 09-17 10:43:42] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 10:43:44] {3072} INFO -  at 40.2s,	estimator xgboost's best error=2.1544,	best estimator xgboost's best error=2.1544
[flaml.automl: 09-17 10:43:44] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 10:43:45] {3072} INFO -  at 41.3s,	estimator xgboost's best error=2.1544,	best estimator xgboost's best error=2.1544
[flaml.automl: 09-17 10:43:45] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 10:43:46] {3072} INFO -  at 42.6s,	estimator xgboost's best error=2.1544,	best estimator xgboost's best error=2.1544
[flaml.automl: 09-17 10:43:46] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 10:43:54] {3072} INFO -  at 50.1s,	estimator xgboost's best error=2.0140,	best estimator xgboost's best error=2.0140
[flaml.automl: 09-17 10:43:54] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 10:43:57] {3072} INFO -  at 53.5s,	estimator xgboost's best error=2.0140,	best estimator xgboost's best error=2.0140
[flaml.automl: 09-17 10:44:05] {3335} INFO - retrain xgboost for 7.4s
[flaml.automl: 09-17 10:44:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 10:44:05] {2636} INFO - fit succeeded
[flaml.automl: 09-17 10:44:05] {2637} INFO - Time taken to find the best model: 50.07979106903076
[flaml.automl: 09-17 10:44:05] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 53873}
SO2(0)最佳损失：-1.0140140286823836
SO2(0)最好结果：{'pred_time': 8.411324625623852e-06, 'wall_clock_time': 50.07979106903076, 'metric_for_logging': {'pred_time': 8.411324625623852e-06}, 'val_loss': 2.0140140286823836, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 53873}, 'config/n_estimators': 8, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609401, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466859, 'config/FLAML_sample_size': 53873, 'experiment_tag': 'exp', 'time_total_s': 7.467823505401611}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6662168475797456
SO2(0)的mse=11.437143211101588
SO2(0)的mae=2.005618080935848
SO2(0)的mar=0.1803832621531154
总共花费的时间为：61.98
本溪市
1761A
1762A
1763A
1764A
1765A
[flaml.automl: 09-17 10:59:09] {2390} INFO - task = regression
[flaml.automl: 09-17 10:59:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 10:59:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 10:59:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 10:59:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 10:59:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 10:59:12] {3025} INFO - Estimated sufficient time budget=120828s. Estimated necessary time budget=121s.
[flaml.automl: 09-17 10:59:12] {3072} INFO -  at 2.5s,	estimator xgboost's best error=8.4127,	best estimator xgboost's best error=8.4127
[flaml.automl: 09-17 10:59:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 10:59:15] {3072} INFO -  at 6.4s,	estimator xgboost's best error=3.9950,	best estimator xgboost's best error=3.9950
[flaml.automl: 09-17 10:59:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 10:59:18] {3072} INFO -  at 8.6s,	estimator xgboost's best error=3.9950,	best estimator xgboost's best error=3.9950
[flaml.automl: 09-17 10:59:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 10:59:22] {3072} INFO -  at 12.7s,	estimator xgboost's best error=3.9950,	best estimator xgboost's best error=3.9950
[flaml.automl: 09-17 10:59:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 10:59:24] {3072} INFO -  at 14.9s,	estimator xgboost's best error=2.9863,	best estimator xgboost's best error=2.9863
[flaml.automl: 09-17 10:59:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 10:59:27] {3072} INFO -  at 17.9s,	estimator xgboost's best error=2.8877,	best estimator xgboost's best error=2.8877
[flaml.automl: 09-17 10:59:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 10:59:30] {3072} INFO -  at 20.9s,	estimator xgboost's best error=2.6580,	best estimator xgboost's best error=2.6580
[flaml.automl: 09-17 10:59:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 10:59:33] {3072} INFO -  at 23.5s,	estimator xgboost's best error=2.6580,	best estimator xgboost's best error=2.6580
[flaml.automl: 09-17 10:59:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 10:59:36] {3072} INFO -  at 26.5s,	estimator xgboost's best error=2.6580,	best estimator xgboost's best error=2.6580
[flaml.automl: 09-17 10:59:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 10:59:38] {3072} INFO -  at 28.8s,	estimator xgboost's best error=2.6580,	best estimator xgboost's best error=2.6580
[flaml.automl: 09-17 10:59:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 10:59:39] {3072} INFO -  at 30.2s,	estimator xgboost's best error=2.6580,	best estimator xgboost's best error=2.6580
[flaml.automl: 09-17 10:59:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 10:59:41] {3072} INFO -  at 31.8s,	estimator xgboost's best error=2.6328,	best estimator xgboost's best error=2.6328
[flaml.automl: 09-17 10:59:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 10:59:42] {3072} INFO -  at 33.0s,	estimator xgboost's best error=2.6328,	best estimator xgboost's best error=2.6328
[flaml.automl: 09-17 10:59:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 10:59:49] {3072} INFO -  at 39.8s,	estimator xgboost's best error=2.2925,	best estimator xgboost's best error=2.2925
[flaml.automl: 09-17 10:59:49] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 11:00:02] {3072} INFO -  at 52.6s,	estimator xgboost's best error=2.2409,	best estimator xgboost's best error=2.2409
[flaml.automl: 09-17 11:00:16] {3335} INFO - retrain xgboost for 14.7s
[flaml.automl: 09-17 11:00:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:00:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:00:16] {2637} INFO - Time taken to find the best model: 52.570653676986694
[flaml.automl: 09-17 11:00:16] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 54818}
SO2(0)最佳损失：-1.2408838083035172
SO2(0)最好结果：{'pred_time': 6.627770328224244e-06, 'wall_clock_time': 52.570653676986694, 'metric_for_logging': {'pred_time': 6.627770328224244e-06}, 'val_loss': 2.240883808303517, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 54818}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 54818, 'experiment_tag': 'exp', 'time_total_s': 12.72379446029663}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6870143660553563
SO2(0)的mse=23.31257499428034
SO2(0)的mae=2.26945679872594
SO2(0)的mar=0.1445356625642138
总共花费的时间为：68.14
锦州市
1767A
1768A
1770A
1771A
[flaml.automl: 09-17 11:13:22] {2390} INFO - task = regression
[flaml.automl: 09-17 11:13:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:13:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:13:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:13:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:13:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:13:25] {3025} INFO - Estimated sufficient time budget=96241s. Estimated necessary time budget=96s.
[flaml.automl: 09-17 11:13:25] {3072} INFO -  at 2.5s,	estimator xgboost's best error=11.0973,	best estimator xgboost's best error=11.0973
[flaml.automl: 09-17 11:13:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:13:29] {3072} INFO -  at 6.4s,	estimator xgboost's best error=6.1662,	best estimator xgboost's best error=6.1662
[flaml.automl: 09-17 11:13:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:13:31] {3072} INFO -  at 8.6s,	estimator xgboost's best error=6.1662,	best estimator xgboost's best error=6.1662
[flaml.automl: 09-17 11:13:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:13:36] {3072} INFO -  at 13.8s,	estimator xgboost's best error=6.1662,	best estimator xgboost's best error=6.1662
[flaml.automl: 09-17 11:13:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:13:38] {3072} INFO -  at 15.9s,	estimator xgboost's best error=5.3435,	best estimator xgboost's best error=5.3435
[flaml.automl: 09-17 11:13:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:13:40] {3072} INFO -  at 18.0s,	estimator xgboost's best error=5.3435,	best estimator xgboost's best error=5.3435
[flaml.automl: 09-17 11:13:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:13:42] {3072} INFO -  at 19.7s,	estimator xgboost's best error=5.0041,	best estimator xgboost's best error=5.0041
[flaml.automl: 09-17 11:13:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:13:45] {3072} INFO -  at 22.3s,	estimator xgboost's best error=5.0041,	best estimator xgboost's best error=5.0041
[flaml.automl: 09-17 11:13:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:13:46] {3072} INFO -  at 24.0s,	estimator xgboost's best error=5.0041,	best estimator xgboost's best error=5.0041
[flaml.automl: 09-17 11:13:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:13:49] {3072} INFO -  at 27.0s,	estimator xgboost's best error=4.8974,	best estimator xgboost's best error=4.8974
[flaml.automl: 09-17 11:13:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:13:51] {3072} INFO -  at 28.7s,	estimator xgboost's best error=4.8974,	best estimator xgboost's best error=4.8974
[flaml.automl: 09-17 11:13:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:13:52] {3072} INFO -  at 29.8s,	estimator xgboost's best error=4.8974,	best estimator xgboost's best error=4.8974
[flaml.automl: 09-17 11:13:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:13:55] {3072} INFO -  at 32.7s,	estimator xgboost's best error=4.8974,	best estimator xgboost's best error=4.8974
[flaml.automl: 09-17 11:13:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:13:58] {3072} INFO -  at 35.5s,	estimator xgboost's best error=4.8974,	best estimator xgboost's best error=4.8974
[flaml.automl: 09-17 11:13:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 11:14:01] {3072} INFO -  at 38.5s,	estimator xgboost's best error=4.8427,	best estimator xgboost's best error=4.8427
[flaml.automl: 09-17 11:14:01] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 11:14:06] {3072} INFO -  at 43.4s,	estimator xgboost's best error=4.8427,	best estimator xgboost's best error=4.8427
[flaml.automl: 09-17 11:14:06] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 11:14:07] {3072} INFO -  at 45.3s,	estimator xgboost's best error=4.8427,	best estimator xgboost's best error=4.8427
[flaml.automl: 09-17 11:14:07] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 11:14:09] {3072} INFO -  at 46.9s,	estimator xgboost's best error=4.8427,	best estimator xgboost's best error=4.8427
[flaml.automl: 09-17 11:14:09] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 11:14:17] {3072} INFO -  at 54.3s,	estimator xgboost's best error=4.8427,	best estimator xgboost's best error=4.8427
[flaml.automl: 09-17 11:14:17] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 11:14:18] {3072} INFO -  at 55.5s,	estimator xgboost's best error=4.8427,	best estimator xgboost's best error=4.8427
[flaml.automl: 09-17 11:14:18] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 11:14:22] {3072} INFO -  at 59.7s,	estimator xgboost's best error=4.8427,	best estimator xgboost's best error=4.8427
[flaml.automl: 09-17 11:14:25] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-17 11:14:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 11:14:25] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:14:25] {2637} INFO - Time taken to find the best model: 38.4787757396698
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768, 'FLAML_sample_size': 43675}
SO2(0)最佳损失：-3.842707576983593
SO2(0)最好结果：{'pred_time': 8.199968068790809e-06, 'wall_clock_time': 38.4787757396698, 'metric_for_logging': {'pred_time': 8.199968068790809e-06}, 'val_loss': 4.842707576983593, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768, 'FLAML_sample_size': 43675}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'config/FLAML_sample_size': 43675, 'experiment_tag': 'exp', 'time_total_s': 2.995448350906372}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.2616525186245109
SO2(0)的mse=68.42601656774474
SO2(0)的mae=4.545461084987245
SO2(0)的mar=0.2774655008176353
总共花费的时间为：63.38
吉林市
1772A
1774A
1775A
1776A
2868A
[flaml.automl: 09-17 11:30:02] {2390} INFO - task = regression
[flaml.automl: 09-17 11:30:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:30:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:30:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:30:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:30:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:30:03] {3025} INFO - Estimated sufficient time budget=64465s. Estimated necessary time budget=64s.
[flaml.automl: 09-17 11:30:03] {3072} INFO -  at 1.4s,	estimator xgboost's best error=6.1362,	best estimator xgboost's best error=6.1362
[flaml.automl: 09-17 11:30:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:30:05] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.9530,	best estimator xgboost's best error=2.9530
[flaml.automl: 09-17 11:30:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:30:06] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.9530,	best estimator xgboost's best error=2.9530
[flaml.automl: 09-17 11:30:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:30:11] {3072} INFO -  at 9.4s,	estimator xgboost's best error=2.9530,	best estimator xgboost's best error=2.9530
[flaml.automl: 09-17 11:30:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:30:12] {3072} INFO -  at 10.6s,	estimator xgboost's best error=2.0664,	best estimator xgboost's best error=2.0664
[flaml.automl: 09-17 11:30:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:30:14] {3072} INFO -  at 12.1s,	estimator xgboost's best error=1.9506,	best estimator xgboost's best error=1.9506
[flaml.automl: 09-17 11:30:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:30:15] {3072} INFO -  at 13.7s,	estimator xgboost's best error=1.9160,	best estimator xgboost's best error=1.9160
[flaml.automl: 09-17 11:30:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:30:18] {3072} INFO -  at 16.4s,	estimator xgboost's best error=1.9160,	best estimator xgboost's best error=1.9160
[flaml.automl: 09-17 11:30:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:30:20] {3072} INFO -  at 18.0s,	estimator xgboost's best error=1.9160,	best estimator xgboost's best error=1.9160
[flaml.automl: 09-17 11:30:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:30:23] {3072} INFO -  at 21.0s,	estimator xgboost's best error=1.8157,	best estimator xgboost's best error=1.8157
[flaml.automl: 09-17 11:30:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:30:24] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.8157,	best estimator xgboost's best error=1.8157
[flaml.automl: 09-17 11:30:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:30:26] {3072} INFO -  at 23.8s,	estimator xgboost's best error=1.8157,	best estimator xgboost's best error=1.8157
[flaml.automl: 09-17 11:30:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:30:28] {3072} INFO -  at 26.7s,	estimator xgboost's best error=1.8157,	best estimator xgboost's best error=1.8157
[flaml.automl: 09-17 11:30:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:30:31] {3072} INFO -  at 29.6s,	estimator xgboost's best error=1.8157,	best estimator xgboost's best error=1.8157
[flaml.automl: 09-17 11:30:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 11:30:35] {3072} INFO -  at 33.0s,	estimator xgboost's best error=1.7870,	best estimator xgboost's best error=1.7870
[flaml.automl: 09-17 11:30:35] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 11:30:43] {3072} INFO -  at 41.7s,	estimator xgboost's best error=1.7870,	best estimator xgboost's best error=1.7870
[flaml.automl: 09-17 11:30:43] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 11:30:47] {3072} INFO -  at 45.1s,	estimator xgboost's best error=1.7276,	best estimator xgboost's best error=1.7276
[flaml.automl: 09-17 11:30:47] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 11:30:50] {3072} INFO -  at 48.2s,	estimator xgboost's best error=1.7276,	best estimator xgboost's best error=1.7276
[flaml.automl: 09-17 11:30:50] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 11:30:58] {3072} INFO -  at 56.1s,	estimator xgboost's best error=1.7276,	best estimator xgboost's best error=1.7276
[flaml.automl: 09-17 11:30:58] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 11:31:00] {3072} INFO -  at 57.9s,	estimator xgboost's best error=1.7276,	best estimator xgboost's best error=1.7276
[flaml.automl: 09-17 11:31:03] {3335} INFO - retrain xgboost for 3.3s
[flaml.automl: 09-17 11:31:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 11:31:03] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:31:03] {2637} INFO - Time taken to find the best model: 45.12528371810913
[flaml.automl: 09-17 11:31:03] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 54647}
SO2(0)最佳损失：-0.7275559709006147
SO2(0)最好结果：{'pred_time': 9.371397209418777e-06, 'wall_clock_time': 45.12528371810913, 'metric_for_logging': {'pred_time': 9.371397209418777e-06}, 'val_loss': 1.7275559709006147, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 54647}, 'config/n_estimators': 7, 'config/max_leaves': 4, 'config/min_child_weight': 0.031257867156220746, 'config/learning_rate': 0.3417584818146937, 'config/subsample': 0.8188658116452241, 'config/colsample_bylevel': 0.9718368347363348, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.001562580531679857, 'config/reg_lambda': 0.36612402315272025, 'config/FLAML_sample_size': 54647, 'experiment_tag': 'exp', 'time_total_s': 3.4682023525238037}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.4515975904975046
SO2(0)的mse=14.144772051784003
SO2(0)的mae=1.6996994169090207
SO2(0)的mar=0.15053796542977538
总共花费的时间为：61.92
齐齐哈尔市
1779A
1781A
3662A
[flaml.automl: 09-17 11:41:10] {2390} INFO - task = regression
[flaml.automl: 09-17 11:41:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:41:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:41:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:41:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:41:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:41:12] {3025} INFO - Estimated sufficient time budget=20824s. Estimated necessary time budget=21s.
[flaml.automl: 09-17 11:41:12] {3072} INFO -  at 2.3s,	estimator xgboost's best error=6.6244,	best estimator xgboost's best error=6.6244
[flaml.automl: 09-17 11:41:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:41:16] {3072} INFO -  at 6.1s,	estimator xgboost's best error=4.2505,	best estimator xgboost's best error=4.2505
[flaml.automl: 09-17 11:41:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:41:18] {3072} INFO -  at 7.9s,	estimator xgboost's best error=4.2505,	best estimator xgboost's best error=4.2505
[flaml.automl: 09-17 11:41:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:41:34] {3072} INFO -  at 23.8s,	estimator xgboost's best error=4.2505,	best estimator xgboost's best error=4.2505
[flaml.automl: 09-17 11:41:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:41:36] {3072} INFO -  at 25.5s,	estimator xgboost's best error=4.2505,	best estimator xgboost's best error=4.2505
[flaml.automl: 09-17 11:41:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:41:39] {3072} INFO -  at 28.7s,	estimator xgboost's best error=4.2505,	best estimator xgboost's best error=4.2505
[flaml.automl: 09-17 11:41:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:41:44] {3072} INFO -  at 34.3s,	estimator xgboost's best error=3.9854,	best estimator xgboost's best error=3.9854
[flaml.automl: 09-17 11:41:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:41:54] {3072} INFO -  at 43.9s,	estimator xgboost's best error=3.9854,	best estimator xgboost's best error=3.9854
[flaml.automl: 09-17 11:41:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:42:00] {3072} INFO -  at 49.8s,	estimator xgboost's best error=3.9846,	best estimator xgboost's best error=3.9846
[flaml.automl: 09-17 11:42:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:42:10] {3072} INFO -  at 59.3s,	estimator xgboost's best error=3.7134,	best estimator xgboost's best error=3.7134
[flaml.automl: 09-17 11:42:19] {3335} INFO - retrain xgboost for 9.3s
[flaml.automl: 09-17 11:42:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7772588597645637, colsample_bynode=1,
             colsample_bytree=0.9042542631072509, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.16640263709615458,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.09973741821783534, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=6.327226987904219,
             scale_pos_weight=1, subsample=0.8604509987112716,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 11:42:19] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:42:19] {2637} INFO - Time taken to find the best model: 59.32044172286987
[flaml.automl: 09-17 11:42:19] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 7, 'min_child_weight': 0.09973741821783534, 'learning_rate': 0.16640263709615458, 'subsample': 0.8604509987112716, 'colsample_bylevel': 0.7772588597645637, 'colsample_bytree': 0.9042542631072509, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 6.327226987904219}
SO2(0)最佳损失：-2.713432137474351
SO2(0)最好结果：{'pred_time': 2.2189232266002706e-05, 'wall_clock_time': 59.32044172286987, 'metric_for_logging': {'pred_time': 2.2189232266002706e-05}, 'val_loss': 3.713432137474351, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 7, 'min_child_weight': 0.09973741821783534, 'learning_rate': 0.16640263709615458, 'subsample': 0.8604509987112716, 'colsample_bylevel': 0.7772588597645637, 'colsample_bytree': 0.9042542631072509, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 6.327226987904219}, 'config/n_estimators': 13, 'config/max_leaves': 7, 'config/min_child_weight': 0.09973741821783534, 'config/learning_rate': 0.16640263709615458, 'config/subsample': 0.8604509987112716, 'config/colsample_bylevel': 0.7772588597645637, 'config/colsample_bytree': 0.9042542631072509, 'config/reg_alpha': 0.007347191482940116, 'config/reg_lambda': 6.327226987904219, 'experiment_tag': 'exp', 'time_total_s': 9.554390668869019}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7772588597645637, colsample_bynode=1,
             colsample_bytree=0.9042542631072509, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.16640263709615458,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.09973741821783534, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=6.327226987904219,
             scale_pos_weight=1, subsample=0.8604509987112716,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.21780545168752863
SO2(0)的mse=87.02013510401922
SO2(0)的mae=3.62438135515188
SO2(0)的mar=0.2995993228115932
总共花费的时间为：69.27
牡丹江市
1784A
1785A
1786A
1787A
[flaml.automl: 09-17 11:54:15] {2390} INFO - task = regression
[flaml.automl: 09-17 11:54:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 11:54:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 11:54:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 11:54:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 11:54:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 11:54:17] {3025} INFO - Estimated sufficient time budget=12007s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 11:54:17] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.1687,	best estimator xgboost's best error=4.1687
[flaml.automl: 09-17 11:54:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 11:54:19] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.1749,	best estimator xgboost's best error=2.1749
[flaml.automl: 09-17 11:54:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 11:54:20] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.1749,	best estimator xgboost's best error=2.1749
[flaml.automl: 09-17 11:54:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 11:54:30] {3072} INFO -  at 14.6s,	estimator xgboost's best error=2.1749,	best estimator xgboost's best error=2.1749
[flaml.automl: 09-17 11:54:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 11:54:31] {3072} INFO -  at 15.7s,	estimator xgboost's best error=1.7348,	best estimator xgboost's best error=1.7348
[flaml.automl: 09-17 11:54:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 11:54:33] {3072} INFO -  at 17.3s,	estimator xgboost's best error=1.7348,	best estimator xgboost's best error=1.7348
[flaml.automl: 09-17 11:54:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 11:54:34] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.4959,	best estimator xgboost's best error=1.4959
[flaml.automl: 09-17 11:54:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 11:54:37] {3072} INFO -  at 21.8s,	estimator xgboost's best error=1.4959,	best estimator xgboost's best error=1.4959
[flaml.automl: 09-17 11:54:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 11:54:39] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.4959,	best estimator xgboost's best error=1.4959
[flaml.automl: 09-17 11:54:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 11:54:42] {3072} INFO -  at 26.4s,	estimator xgboost's best error=1.4860,	best estimator xgboost's best error=1.4860
[flaml.automl: 09-17 11:54:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 11:54:43] {3072} INFO -  at 28.1s,	estimator xgboost's best error=1.4860,	best estimator xgboost's best error=1.4860
[flaml.automl: 09-17 11:54:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 11:54:45] {3072} INFO -  at 29.3s,	estimator xgboost's best error=1.4860,	best estimator xgboost's best error=1.4860
[flaml.automl: 09-17 11:54:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 11:54:58] {3072} INFO -  at 43.0s,	estimator xgboost's best error=1.4207,	best estimator xgboost's best error=1.4207
[flaml.automl: 09-17 11:54:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 11:55:15] {3072} INFO -  at 59.5s,	estimator xgboost's best error=1.3837,	best estimator xgboost's best error=1.3837
[flaml.automl: 09-17 11:55:39] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-17 11:55:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.3638133431214387,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.029920148019616434, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.007290129763188211, scale_pos_weight=1,
             subsample=0.840665579419843, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 11:55:39] {2636} INFO - fit succeeded
[flaml.automl: 09-17 11:55:39] {2637} INFO - Time taken to find the best model: 59.520883321762085
[flaml.automl: 09-17 11:55:39] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.029920148019616434, 'learning_rate': 0.3638133431214387, 'subsample': 0.840665579419843, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.007290129763188211}
SO2(0)最佳损失：-0.38374291740633315
SO2(0)最好结果：{'pred_time': 1.0459567105264896e-05, 'wall_clock_time': 59.520883321762085, 'metric_for_logging': {'pred_time': 1.0459567105264896e-05}, 'val_loss': 1.3837429174063332, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.029920148019616434, 'learning_rate': 0.3638133431214387, 'subsample': 0.840665579419843, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.007290129763188211}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.029920148019616434, 'config/learning_rate': 0.3638133431214387, 'config/subsample': 0.840665579419843, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.007290129763188211, 'experiment_tag': 'exp', 'time_total_s': 16.52734637260437}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.3638133431214387,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.029920148019616434, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.007290129763188211, scale_pos_weight=1,
             subsample=0.840665579419843, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7345637340915329
SO2(0)的mse=7.288451050447864
SO2(0)的mae=1.4680088280567316
SO2(0)的mar=0.2442668947612908
总共花费的时间为：84.06
大庆市
1789A
1790A
1792A
1793A
3481A
[flaml.automl: 09-17 12:10:27] {2390} INFO - task = regression
[flaml.automl: 09-17 12:10:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:10:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:10:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:10:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:10:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:10:29] {3025} INFO - Estimated sufficient time budget=120783s. Estimated necessary time budget=121s.
[flaml.automl: 09-17 12:10:29] {3072} INFO -  at 2.4s,	estimator xgboost's best error=4.6877,	best estimator xgboost's best error=4.6877
[flaml.automl: 09-17 12:10:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:10:31] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.3226,	best estimator xgboost's best error=2.3226
[flaml.automl: 09-17 12:10:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:10:32] {3072} INFO -  at 6.0s,	estimator xgboost's best error=2.3226,	best estimator xgboost's best error=2.3226
[flaml.automl: 09-17 12:10:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:10:37] {3072} INFO -  at 10.8s,	estimator xgboost's best error=2.3226,	best estimator xgboost's best error=2.3226
[flaml.automl: 09-17 12:10:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:10:38] {3072} INFO -  at 12.0s,	estimator xgboost's best error=1.6776,	best estimator xgboost's best error=1.6776
[flaml.automl: 09-17 12:10:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:10:40] {3072} INFO -  at 13.5s,	estimator xgboost's best error=1.5404,	best estimator xgboost's best error=1.5404
[flaml.automl: 09-17 12:10:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:10:41] {3072} INFO -  at 15.1s,	estimator xgboost's best error=1.5041,	best estimator xgboost's best error=1.5041
[flaml.automl: 09-17 12:10:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:10:44] {3072} INFO -  at 17.8s,	estimator xgboost's best error=1.5041,	best estimator xgboost's best error=1.5041
[flaml.automl: 09-17 12:10:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:10:46] {3072} INFO -  at 19.4s,	estimator xgboost's best error=1.5041,	best estimator xgboost's best error=1.5041
[flaml.automl: 09-17 12:10:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:10:49] {3072} INFO -  at 22.4s,	estimator xgboost's best error=1.3589,	best estimator xgboost's best error=1.3589
[flaml.automl: 09-17 12:10:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:10:50] {3072} INFO -  at 24.1s,	estimator xgboost's best error=1.3589,	best estimator xgboost's best error=1.3589
[flaml.automl: 09-17 12:10:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:10:52] {3072} INFO -  at 25.2s,	estimator xgboost's best error=1.3589,	best estimator xgboost's best error=1.3589
[flaml.automl: 09-17 12:10:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:10:54] {3072} INFO -  at 28.1s,	estimator xgboost's best error=1.3589,	best estimator xgboost's best error=1.3589
[flaml.automl: 09-17 12:10:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:10:57] {3072} INFO -  at 30.6s,	estimator xgboost's best error=1.3589,	best estimator xgboost's best error=1.3589
[flaml.automl: 09-17 12:10:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 12:11:00] {3072} INFO -  at 33.7s,	estimator xgboost's best error=1.3589,	best estimator xgboost's best error=1.3589
[flaml.automl: 09-17 12:11:00] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 12:11:05] {3072} INFO -  at 38.6s,	estimator xgboost's best error=1.3589,	best estimator xgboost's best error=1.3589
[flaml.automl: 09-17 12:11:05] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 12:11:07] {3072} INFO -  at 40.5s,	estimator xgboost's best error=1.3390,	best estimator xgboost's best error=1.3390
[flaml.automl: 09-17 12:11:07] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 12:11:08] {3072} INFO -  at 42.2s,	estimator xgboost's best error=1.3390,	best estimator xgboost's best error=1.3390
[flaml.automl: 09-17 12:11:08] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 12:11:13] {3072} INFO -  at 46.5s,	estimator xgboost's best error=1.3390,	best estimator xgboost's best error=1.3390
[flaml.automl: 09-17 12:11:13] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 12:11:14] {3072} INFO -  at 47.7s,	estimator xgboost's best error=1.3390,	best estimator xgboost's best error=1.3390
[flaml.automl: 09-17 12:11:14] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 12:11:19] {3072} INFO -  at 52.9s,	estimator xgboost's best error=1.3088,	best estimator xgboost's best error=1.3088
[flaml.automl: 09-17 12:11:19] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 12:11:26] {3072} INFO -  at 59.9s,	estimator xgboost's best error=1.2917,	best estimator xgboost's best error=1.2917
[flaml.automl: 09-17 12:11:50] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-17 12:11:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.008381997180108987, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003886437262068607, scale_pos_weight=1,
             subsample=0.9390180412130811, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:11:50] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:11:50] {2637} INFO - Time taken to find the best model: 59.88362956047058
[flaml.automl: 09-17 12:11:50] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 28, 'max_leaves': 15, 'min_child_weight': 0.008381997180108987, 'learning_rate': 0.6023269513573992, 'subsample': 0.9390180412130811, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003886437262068607, 'FLAML_sample_size': 55015}
SO2(0)最佳损失：-0.291686112452076
SO2(0)最好结果：{'pred_time': 6.929037517520774e-06, 'wall_clock_time': 59.88362956047058, 'metric_for_logging': {'pred_time': 6.929037517520774e-06}, 'val_loss': 1.291686112452076, 'training_iteration': 1, 'config': {'n_estimators': 28, 'max_leaves': 15, 'min_child_weight': 0.008381997180108987, 'learning_rate': 0.6023269513573992, 'subsample': 0.9390180412130811, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003886437262068607, 'FLAML_sample_size': 55015}, 'config/n_estimators': 28, 'config/max_leaves': 15, 'config/min_child_weight': 0.008381997180108987, 'config/learning_rate': 0.6023269513573992, 'config/subsample': 0.9390180412130811, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.936172861729374, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.003886437262068607, 'config/FLAML_sample_size': 55015, 'experiment_tag': 'exp', 'time_total_s': 6.940187454223633}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.008381997180108987, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003886437262068607, scale_pos_weight=1,
             subsample=0.9390180412130811, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7057935908662882
SO2(0)的mse=6.715153461415394
SO2(0)的mae=1.3549876629744597
SO2(0)的mar=0.1756726092911648
总共花费的时间为：84.64
芜湖市
1795A
1796A
3328A
3465A
3466A
[flaml.automl: 09-17 12:27:05] {2390} INFO - task = regression
[flaml.automl: 09-17 12:27:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:27:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:27:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:27:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:27:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:27:08] {3025} INFO - Estimated sufficient time budget=115790s. Estimated necessary time budget=116s.
[flaml.automl: 09-17 12:27:08] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.1505,	best estimator xgboost's best error=5.1505
[flaml.automl: 09-17 12:27:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:27:12] {3072} INFO -  at 6.3s,	estimator xgboost's best error=2.2917,	best estimator xgboost's best error=2.2917
[flaml.automl: 09-17 12:27:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:27:14] {3072} INFO -  at 8.4s,	estimator xgboost's best error=2.2917,	best estimator xgboost's best error=2.2917
[flaml.automl: 09-17 12:27:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:27:18] {3072} INFO -  at 12.9s,	estimator xgboost's best error=2.2917,	best estimator xgboost's best error=2.2917
[flaml.automl: 09-17 12:27:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:27:20] {3072} INFO -  at 15.1s,	estimator xgboost's best error=1.1145,	best estimator xgboost's best error=1.1145
[flaml.automl: 09-17 12:27:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:27:23] {3072} INFO -  at 17.9s,	estimator xgboost's best error=0.8304,	best estimator xgboost's best error=0.8304
[flaml.automl: 09-17 12:27:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:27:26] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.8304,	best estimator xgboost's best error=0.8304
[flaml.automl: 09-17 12:27:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:27:29] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.8304,	best estimator xgboost's best error=0.8304
[flaml.automl: 09-17 12:27:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:27:30] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.8304,	best estimator xgboost's best error=0.8304
[flaml.automl: 09-17 12:27:30] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:27:33] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.8304,	best estimator xgboost's best error=0.8304
[flaml.automl: 09-17 12:27:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:27:36] {3072} INFO -  at 30.6s,	estimator xgboost's best error=0.8304,	best estimator xgboost's best error=0.8304
[flaml.automl: 09-17 12:27:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:27:38] {3072} INFO -  at 32.8s,	estimator xgboost's best error=0.8304,	best estimator xgboost's best error=0.8304
[flaml.automl: 09-17 12:27:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:27:50] {3072} INFO -  at 44.9s,	estimator xgboost's best error=0.6932,	best estimator xgboost's best error=0.6932
[flaml.automl: 09-17 12:27:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:28:04] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.6794,	best estimator xgboost's best error=0.6794
[flaml.automl: 09-17 12:28:16] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 12:28:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 12:28:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:28:16] {2637} INFO - Time taken to find the best model: 59.04752564430237
[flaml.automl: 09-17 12:28:16] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 55692}
SO2(0)最佳损失：0.320576319058017
SO2(0)最好结果：{'pred_time': 6.827833425944747e-06, 'wall_clock_time': 59.04752564430237, 'metric_for_logging': {'pred_time': 6.827833425944747e-06}, 'val_loss': 0.679423680941983, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 55692}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 55692, 'experiment_tag': 'exp', 'time_total_s': 14.144711256027222}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8490671696279352
SO2(0)的mse=1.0938353368003886
SO2(0)的mae=0.6694817015984642
SO2(0)的mar=0.08118854630014255
总共花费的时间为：71.99
马鞍山市
1798A
1800A
3633A
[flaml.automl: 09-17 12:37:48] {2390} INFO - task = regression
[flaml.automl: 09-17 12:37:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:37:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:37:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:37:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:37:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:37:50] {3025} INFO - Estimated sufficient time budget=21264s. Estimated necessary time budget=21s.
[flaml.automl: 09-17 12:37:50] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.3856,	best estimator xgboost's best error=5.3856
[flaml.automl: 09-17 12:37:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:37:54] {3072} INFO -  at 6.1s,	estimator xgboost's best error=2.5415,	best estimator xgboost's best error=2.5415
[flaml.automl: 09-17 12:37:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:37:57] {3072} INFO -  at 8.4s,	estimator xgboost's best error=2.5415,	best estimator xgboost's best error=2.5415
[flaml.automl: 09-17 12:37:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:38:15] {3072} INFO -  at 26.7s,	estimator xgboost's best error=2.5415,	best estimator xgboost's best error=2.5415
[flaml.automl: 09-17 12:38:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:38:17] {3072} INFO -  at 28.8s,	estimator xgboost's best error=1.7296,	best estimator xgboost's best error=1.7296
[flaml.automl: 09-17 12:38:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:38:20] {3072} INFO -  at 31.7s,	estimator xgboost's best error=1.7296,	best estimator xgboost's best error=1.7296
[flaml.automl: 09-17 12:38:20] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:38:23] {3072} INFO -  at 34.9s,	estimator xgboost's best error=1.4097,	best estimator xgboost's best error=1.4097
[flaml.automl: 09-17 12:38:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:38:28] {3072} INFO -  at 39.8s,	estimator xgboost's best error=1.4097,	best estimator xgboost's best error=1.4097
[flaml.automl: 09-17 12:38:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:38:31] {3072} INFO -  at 42.9s,	estimator xgboost's best error=1.4097,	best estimator xgboost's best error=1.4097
[flaml.automl: 09-17 12:38:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:38:40] {3072} INFO -  at 51.4s,	estimator xgboost's best error=1.4097,	best estimator xgboost's best error=1.4097
[flaml.automl: 09-17 12:38:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:38:44] {3072} INFO -  at 55.5s,	estimator xgboost's best error=1.4097,	best estimator xgboost's best error=1.4097
[flaml.automl: 09-17 12:38:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:38:47] {3072} INFO -  at 58.8s,	estimator xgboost's best error=1.4097,	best estimator xgboost's best error=1.4097
[flaml.automl: 09-17 12:38:52] {3335} INFO - retrain xgboost for 4.7s
[flaml.automl: 09-17 12:38:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 12:38:52] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:38:52] {2637} INFO - Time taken to find the best model: 34.88622450828552
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-0.40968310056746127
SO2(0)最好结果：{'pred_time': 2.126510072593446e-05, 'wall_clock_time': 34.88622450828552, 'metric_for_logging': {'pred_time': 2.126510072593446e-05}, 'val_loss': 1.4096831005674613, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 3.191291570663452}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.5272998581006852
SO2(0)的mse=5.953266051246294
SO2(0)的mae=1.3961532658175042
SO2(0)的mar=0.15844740848019
总共花费的时间为：64.09
九江市
1803A
1804A
1805A
1806A
1810A
[flaml.automl: 09-17 12:54:35] {2390} INFO - task = regression
[flaml.automl: 09-17 12:54:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 12:54:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 12:54:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 12:54:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 12:54:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 12:54:36] {3025} INFO - Estimated sufficient time budget=62711s. Estimated necessary time budget=63s.
[flaml.automl: 09-17 12:54:36] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.1677,	best estimator xgboost's best error=4.1677
[flaml.automl: 09-17 12:54:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 12:54:38] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.0231,	best estimator xgboost's best error=2.0231
[flaml.automl: 09-17 12:54:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 12:54:39] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.0231,	best estimator xgboost's best error=2.0231
[flaml.automl: 09-17 12:54:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 12:54:44] {3072} INFO -  at 9.6s,	estimator xgboost's best error=2.0231,	best estimator xgboost's best error=2.0231
[flaml.automl: 09-17 12:54:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 12:54:45] {3072} INFO -  at 10.7s,	estimator xgboost's best error=1.2885,	best estimator xgboost's best error=1.2885
[flaml.automl: 09-17 12:54:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 12:54:47] {3072} INFO -  at 12.3s,	estimator xgboost's best error=1.1600,	best estimator xgboost's best error=1.1600
[flaml.automl: 09-17 12:54:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 12:54:48] {3072} INFO -  at 13.9s,	estimator xgboost's best error=1.0990,	best estimator xgboost's best error=1.0990
[flaml.automl: 09-17 12:54:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 12:54:51] {3072} INFO -  at 16.6s,	estimator xgboost's best error=1.0990,	best estimator xgboost's best error=1.0990
[flaml.automl: 09-17 12:54:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 12:54:53] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.0990,	best estimator xgboost's best error=1.0990
[flaml.automl: 09-17 12:54:53] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 12:54:56] {3072} INFO -  at 21.6s,	estimator xgboost's best error=1.0800,	best estimator xgboost's best error=1.0800
[flaml.automl: 09-17 12:54:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 12:54:59] {3072} INFO -  at 24.6s,	estimator xgboost's best error=1.0800,	best estimator xgboost's best error=1.0800
[flaml.automl: 09-17 12:54:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 12:55:01] {3072} INFO -  at 26.7s,	estimator xgboost's best error=1.0800,	best estimator xgboost's best error=1.0800
[flaml.automl: 09-17 12:55:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 12:55:03] {3072} INFO -  at 28.7s,	estimator xgboost's best error=1.0800,	best estimator xgboost's best error=1.0800
[flaml.automl: 09-17 12:55:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 12:55:06] {3072} INFO -  at 31.3s,	estimator xgboost's best error=1.0800,	best estimator xgboost's best error=1.0800
[flaml.automl: 09-17 12:55:06] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 12:55:12] {3072} INFO -  at 37.0s,	estimator xgboost's best error=1.0800,	best estimator xgboost's best error=1.0800
[flaml.automl: 09-17 12:55:12] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 12:55:22] {3072} INFO -  at 47.1s,	estimator xgboost's best error=1.0800,	best estimator xgboost's best error=1.0800
[flaml.automl: 09-17 12:55:22] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 12:55:27] {3072} INFO -  at 52.4s,	estimator xgboost's best error=1.0766,	best estimator xgboost's best error=1.0766
[flaml.automl: 09-17 12:55:27] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 12:55:32] {3072} INFO -  at 57.2s,	estimator xgboost's best error=1.0766,	best estimator xgboost's best error=1.0766
[flaml.automl: 09-17 12:55:37] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-17 12:55:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 12:55:37] {2636} INFO - fit succeeded
[flaml.automl: 09-17 12:55:37] {2637} INFO - Time taken to find the best model: 52.44087243080139
[flaml.automl: 09-17 12:55:37] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 52201}
SO2(0)最佳损失：-0.07656087461904582
SO2(0)最好结果：{'pred_time': 2.019924452501707e-05, 'wall_clock_time': 52.44087243080139, 'metric_for_logging': {'pred_time': 2.019924452501707e-05}, 'val_loss': 1.0765608746190458, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 52201}, 'config/n_estimators': 7, 'config/max_leaves': 4, 'config/min_child_weight': 0.031257867156220746, 'config/learning_rate': 0.3417584818146937, 'config/subsample': 0.8188658116452241, 'config/colsample_bylevel': 0.9718368347363348, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.001562580531679857, 'config/reg_lambda': 0.36612402315272025, 'config/FLAML_sample_size': 52201, 'experiment_tag': 'exp', 'time_total_s': 5.352496862411499}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6023584563706743
SO2(0)的mse=2.5517829200701314
SO2(0)的mae=1.072198077742678
SO2(0)的mar=0.19859706088293752
总共花费的时间为：63.31
洛阳市
1815A
1817A
3341A
3593A
3635A
3636A
[flaml.automl: 09-17 13:14:01] {2390} INFO - task = regression
[flaml.automl: 09-17 13:14:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:14:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:14:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:14:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:14:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:14:03] {3025} INFO - Estimated sufficient time budget=121803s. Estimated necessary time budget=122s.
[flaml.automl: 09-17 13:14:03] {3072} INFO -  at 2.2s,	estimator xgboost's best error=3.5210,	best estimator xgboost's best error=3.5210
[flaml.automl: 09-17 13:14:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:14:06] {3072} INFO -  at 5.5s,	estimator xgboost's best error=1.7096,	best estimator xgboost's best error=1.7096
[flaml.automl: 09-17 13:14:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:14:09] {3072} INFO -  at 7.8s,	estimator xgboost's best error=1.7096,	best estimator xgboost's best error=1.7096
[flaml.automl: 09-17 13:14:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:14:12] {3072} INFO -  at 10.9s,	estimator xgboost's best error=1.7096,	best estimator xgboost's best error=1.7096
[flaml.automl: 09-17 13:14:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:14:14] {3072} INFO -  at 13.1s,	estimator xgboost's best error=1.2830,	best estimator xgboost's best error=1.2830
[flaml.automl: 09-17 13:14:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:14:17] {3072} INFO -  at 15.7s,	estimator xgboost's best error=1.2153,	best estimator xgboost's best error=1.2153
[flaml.automl: 09-17 13:14:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:14:20] {3072} INFO -  at 18.7s,	estimator xgboost's best error=1.0799,	best estimator xgboost's best error=1.0799
[flaml.automl: 09-17 13:14:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:14:22] {3072} INFO -  at 21.0s,	estimator xgboost's best error=1.0799,	best estimator xgboost's best error=1.0799
[flaml.automl: 09-17 13:14:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:14:24] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.0799,	best estimator xgboost's best error=1.0799
[flaml.automl: 09-17 13:14:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:14:26] {3072} INFO -  at 25.5s,	estimator xgboost's best error=1.0799,	best estimator xgboost's best error=1.0799
[flaml.automl: 09-17 13:14:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:14:29] {3072} INFO -  at 27.9s,	estimator xgboost's best error=1.0799,	best estimator xgboost's best error=1.0799
[flaml.automl: 09-17 13:14:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:14:32] {3072} INFO -  at 30.8s,	estimator xgboost's best error=1.0792,	best estimator xgboost's best error=1.0792
[flaml.automl: 09-17 13:14:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:14:34] {3072} INFO -  at 33.0s,	estimator xgboost's best error=1.0792,	best estimator xgboost's best error=1.0792
[flaml.automl: 09-17 13:14:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:14:48] {3072} INFO -  at 47.3s,	estimator xgboost's best error=0.9438,	best estimator xgboost's best error=0.9438
[flaml.automl: 09-17 13:15:01] {3335} INFO - retrain xgboost for 12.5s
[flaml.automl: 09-17 13:15:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 13:15:01] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:15:01] {2637} INFO - Time taken to find the best model: 47.34092044830322
[flaml.automl: 09-17 13:15:01] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 65759}
SO2(0)最佳损失：0.056173332358770534
SO2(0)最好结果：{'pred_time': 1.2605122082925485e-05, 'wall_clock_time': 47.34092044830322, 'metric_for_logging': {'pred_time': 1.2605122082925485e-05}, 'val_loss': 0.9438266676412295, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 65759}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 65759, 'experiment_tag': 'exp', 'time_total_s': 14.305696725845337}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7835578537316341
SO2(0)的mse=2.2590992219631407
SO2(0)的mae=0.9557745376747955
SO2(0)的mar=0.19808271807262773
总共花费的时间为：60.97
安阳市
1818A
1819A
3141A
3669A
[flaml.automl: 09-17 13:27:28] {2390} INFO - task = regression
[flaml.automl: 09-17 13:27:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:27:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:27:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:27:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:27:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:27:29] {3025} INFO - Estimated sufficient time budget=50606s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 13:27:29] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.6728,	best estimator xgboost's best error=4.6728
[flaml.automl: 09-17 13:27:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:27:31] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.2467,	best estimator xgboost's best error=2.2467
[flaml.automl: 09-17 13:27:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:27:32] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.2467,	best estimator xgboost's best error=2.2467
[flaml.automl: 09-17 13:27:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:27:38] {3072} INFO -  at 11.1s,	estimator xgboost's best error=2.2467,	best estimator xgboost's best error=2.2467
[flaml.automl: 09-17 13:27:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:27:40] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.6483,	best estimator xgboost's best error=1.6483
[flaml.automl: 09-17 13:27:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:27:41] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.5809,	best estimator xgboost's best error=1.5809
[flaml.automl: 09-17 13:27:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:27:43] {3072} INFO -  at 15.4s,	estimator xgboost's best error=1.4487,	best estimator xgboost's best error=1.4487
[flaml.automl: 09-17 13:27:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:27:46] {3072} INFO -  at 18.1s,	estimator xgboost's best error=1.4487,	best estimator xgboost's best error=1.4487
[flaml.automl: 09-17 13:27:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:27:47] {3072} INFO -  at 19.8s,	estimator xgboost's best error=1.4487,	best estimator xgboost's best error=1.4487
[flaml.automl: 09-17 13:27:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:27:50] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.3780,	best estimator xgboost's best error=1.3780
[flaml.automl: 09-17 13:27:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:27:52] {3072} INFO -  at 24.4s,	estimator xgboost's best error=1.3780,	best estimator xgboost's best error=1.3780
[flaml.automl: 09-17 13:27:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:27:53] {3072} INFO -  at 25.6s,	estimator xgboost's best error=1.3780,	best estimator xgboost's best error=1.3780
[flaml.automl: 09-17 13:27:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:27:57] {3072} INFO -  at 29.3s,	estimator xgboost's best error=1.3713,	best estimator xgboost's best error=1.3713
[flaml.automl: 09-17 13:27:57] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:28:00] {3072} INFO -  at 32.3s,	estimator xgboost's best error=1.3669,	best estimator xgboost's best error=1.3669
[flaml.automl: 09-17 13:28:00] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 13:28:02] {3072} INFO -  at 34.2s,	estimator xgboost's best error=1.3669,	best estimator xgboost's best error=1.3669
[flaml.automl: 09-17 13:28:02] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 13:28:04] {3072} INFO -  at 36.7s,	estimator xgboost's best error=1.3669,	best estimator xgboost's best error=1.3669
[flaml.automl: 09-17 13:28:04] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 13:28:07] {3072} INFO -  at 39.3s,	estimator xgboost's best error=1.3669,	best estimator xgboost's best error=1.3669
[flaml.automl: 09-17 13:28:07] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 13:28:09] {3072} INFO -  at 41.3s,	estimator xgboost's best error=1.3669,	best estimator xgboost's best error=1.3669
[flaml.automl: 09-17 13:28:09] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 13:28:27] {3072} INFO -  at 59.4s,	estimator xgboost's best error=1.2914,	best estimator xgboost's best error=1.2914
[flaml.automl: 09-17 13:29:08] {3335} INFO - retrain xgboost for 41.6s
[flaml.automl: 09-17 13:29:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:29:08] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:29:08] {2637} INFO - Time taken to find the best model: 59.353755235672
[flaml.automl: 09-17 13:29:08] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306, 'FLAML_sample_size': 42233}
SO2(0)最佳损失：-0.29140425627607036
SO2(0)最好结果：{'pred_time': 1.5506148973355638e-05, 'wall_clock_time': 59.353755235672, 'metric_for_logging': {'pred_time': 1.5506148973355638e-05}, 'val_loss': 1.2914042562760704, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306, 'FLAML_sample_size': 42233}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'config/FLAML_sample_size': 42233, 'experiment_tag': 'exp', 'time_total_s': 18.072616577148438}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7226955790248628
SO2(0)的mse=4.640204956446029
SO2(0)的mae=1.3173738626579603
SO2(0)的mar=0.1870743828297554
总共花费的时间为：101.62
开封市
3210A
3473A
3592A
[flaml.automl: 09-17 13:39:17] {2390} INFO - task = regression
[flaml.automl: 09-17 13:39:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:39:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:39:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:39:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:39:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:39:18] {3025} INFO - Estimated sufficient time budget=12103s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 13:39:18] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.6523,	best estimator xgboost's best error=4.6523
[flaml.automl: 09-17 13:39:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:39:21] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.1757,	best estimator xgboost's best error=2.1757
[flaml.automl: 09-17 13:39:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:39:22] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.1757,	best estimator xgboost's best error=2.1757
[flaml.automl: 09-17 13:39:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:39:32] {3072} INFO -  at 14.7s,	estimator xgboost's best error=2.1757,	best estimator xgboost's best error=2.1757
[flaml.automl: 09-17 13:39:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:39:33] {3072} INFO -  at 15.9s,	estimator xgboost's best error=1.3331,	best estimator xgboost's best error=1.3331
[flaml.automl: 09-17 13:39:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:39:35] {3072} INFO -  at 17.5s,	estimator xgboost's best error=1.1938,	best estimator xgboost's best error=1.1938
[flaml.automl: 09-17 13:39:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:39:36] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.1467,	best estimator xgboost's best error=1.1467
[flaml.automl: 09-17 13:39:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:39:39] {3072} INFO -  at 21.8s,	estimator xgboost's best error=1.1467,	best estimator xgboost's best error=1.1467
[flaml.automl: 09-17 13:39:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:39:41] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.1467,	best estimator xgboost's best error=1.1467
[flaml.automl: 09-17 13:39:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:39:44] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.0457,	best estimator xgboost's best error=1.0457
[flaml.automl: 09-17 13:39:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:39:45] {3072} INFO -  at 28.1s,	estimator xgboost's best error=1.0457,	best estimator xgboost's best error=1.0457
[flaml.automl: 09-17 13:39:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:39:46] {3072} INFO -  at 29.2s,	estimator xgboost's best error=1.0457,	best estimator xgboost's best error=1.0457
[flaml.automl: 09-17 13:39:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:40:00] {3072} INFO -  at 43.3s,	estimator xgboost's best error=0.9986,	best estimator xgboost's best error=0.9986
[flaml.automl: 09-17 13:40:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:40:16] {3072} INFO -  at 58.8s,	estimator xgboost's best error=0.9649,	best estimator xgboost's best error=0.9649
[flaml.automl: 09-17 13:40:59] {3335} INFO - retrain xgboost for 42.9s
[flaml.automl: 09-17 13:40:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 13:40:59] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:40:59] {2637} INFO - Time taken to find the best model: 58.773693561553955
[flaml.automl: 09-17 13:40:59] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
SO2(0)最佳损失：0.03510918142741837
SO2(0)最好结果：{'pred_time': 2.2719549219989597e-05, 'wall_clock_time': 58.773693561553955, 'metric_for_logging': {'pred_time': 2.2719549219989597e-05}, 'val_loss': 0.9648908185725816, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 15.480987787246704}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7830566977195127
SO2(0)的mse=3.0066337884908445
SO2(0)的mae=0.9622583622283526
SO2(0)的mar=0.12305592336493781
总共花费的时间为：102.23
焦作市
1830A
3169A
3335A
3336A
3477A
[flaml.automl: 09-17 13:58:53] {2390} INFO - task = regression
[flaml.automl: 09-17 13:58:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 13:58:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 13:58:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 13:58:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 13:58:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 13:58:56] {3025} INFO - Estimated sufficient time budget=179041s. Estimated necessary time budget=179s.
[flaml.automl: 09-17 13:58:56] {3072} INFO -  at 3.7s,	estimator xgboost's best error=5.5640,	best estimator xgboost's best error=5.5640
[flaml.automl: 09-17 13:58:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 13:59:01] {3072} INFO -  at 8.2s,	estimator xgboost's best error=3.5121,	best estimator xgboost's best error=3.5121
[flaml.automl: 09-17 13:59:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 13:59:04] {3072} INFO -  at 11.5s,	estimator xgboost's best error=3.5121,	best estimator xgboost's best error=3.5121
[flaml.automl: 09-17 13:59:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 13:59:07] {3072} INFO -  at 15.0s,	estimator xgboost's best error=3.5121,	best estimator xgboost's best error=3.5121
[flaml.automl: 09-17 13:59:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 13:59:11] {3072} INFO -  at 18.3s,	estimator xgboost's best error=2.1715,	best estimator xgboost's best error=2.1715
[flaml.automl: 09-17 13:59:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 13:59:14] {3072} INFO -  at 21.5s,	estimator xgboost's best error=2.1715,	best estimator xgboost's best error=2.1715
[flaml.automl: 09-17 13:59:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 13:59:16] {3072} INFO -  at 24.1s,	estimator xgboost's best error=2.1715,	best estimator xgboost's best error=2.1715
[flaml.automl: 09-17 13:59:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 13:59:19] {3072} INFO -  at 27.1s,	estimator xgboost's best error=2.1715,	best estimator xgboost's best error=2.1715
[flaml.automl: 09-17 13:59:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 13:59:22] {3072} INFO -  at 29.7s,	estimator xgboost's best error=2.1536,	best estimator xgboost's best error=2.1536
[flaml.automl: 09-17 13:59:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 13:59:24] {3072} INFO -  at 31.3s,	estimator xgboost's best error=2.1536,	best estimator xgboost's best error=2.1536
[flaml.automl: 09-17 13:59:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 13:59:26] {3072} INFO -  at 33.2s,	estimator xgboost's best error=2.1536,	best estimator xgboost's best error=2.1536
[flaml.automl: 09-17 13:59:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 13:59:27] {3072} INFO -  at 35.1s,	estimator xgboost's best error=2.1536,	best estimator xgboost's best error=2.1536
[flaml.automl: 09-17 13:59:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 13:59:29] {3072} INFO -  at 37.0s,	estimator xgboost's best error=1.9052,	best estimator xgboost's best error=1.9052
[flaml.automl: 09-17 13:59:29] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 13:59:31] {3072} INFO -  at 38.5s,	estimator xgboost's best error=1.9052,	best estimator xgboost's best error=1.9052
[flaml.automl: 09-17 13:59:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 13:59:33] {3072} INFO -  at 40.9s,	estimator xgboost's best error=1.9052,	best estimator xgboost's best error=1.9052
[flaml.automl: 09-17 13:59:33] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 13:59:36] {3072} INFO -  at 43.8s,	estimator xgboost's best error=1.9052,	best estimator xgboost's best error=1.9052
[flaml.automl: 09-17 13:59:36] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 13:59:37] {3072} INFO -  at 45.1s,	estimator xgboost's best error=1.9052,	best estimator xgboost's best error=1.9052
[flaml.automl: 09-17 13:59:37] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 13:59:50] {3072} INFO -  at 57.4s,	estimator xgboost's best error=1.7955,	best estimator xgboost's best error=1.7955
[flaml.automl: 09-17 13:59:59] {3335} INFO - retrain xgboost for 8.9s
[flaml.automl: 09-17 13:59:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 13:59:59] {2636} INFO - fit succeeded
[flaml.automl: 09-17 13:59:59] {2637} INFO - Time taken to find the best model: 57.397722005844116
[flaml.automl: 09-17 13:59:59] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 53154}
SO2(0)最佳损失：-0.7954544384439961
SO2(0)最好结果：{'pred_time': 2.250512333349014e-05, 'wall_clock_time': 57.397722005844116, 'metric_for_logging': {'pred_time': 2.250512333349014e-05}, 'val_loss': 1.795454438443996, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 53154}, 'config/n_estimators': 8, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609401, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466859, 'config/FLAML_sample_size': 53154, 'experiment_tag': 'exp', 'time_total_s': 12.330265283584595}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6327277414659664
SO2(0)的mse=8.199132662533977
SO2(0)的mae=1.7744993119637231
SO2(0)的mar=0.21464153563014982
总共花费的时间为：67.29
平顶山市
1833A
3204A
3594A
[flaml.automl: 09-17 14:09:55] {2390} INFO - task = regression
[flaml.automl: 09-17 14:09:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:09:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:09:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:09:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:09:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:09:56] {3025} INFO - Estimated sufficient time budget=11760s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 14:09:56] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.0754,	best estimator xgboost's best error=4.0754
[flaml.automl: 09-17 14:09:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:09:59] {3072} INFO -  at 3.8s,	estimator xgboost's best error=2.0433,	best estimator xgboost's best error=2.0433
[flaml.automl: 09-17 14:09:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:10:01] {3072} INFO -  at 6.0s,	estimator xgboost's best error=2.0433,	best estimator xgboost's best error=2.0433
[flaml.automl: 09-17 14:10:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:10:19] {3072} INFO -  at 24.3s,	estimator xgboost's best error=2.0433,	best estimator xgboost's best error=2.0433
[flaml.automl: 09-17 14:10:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:10:22] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.5318,	best estimator xgboost's best error=1.5318
[flaml.automl: 09-17 14:10:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:10:25] {3072} INFO -  at 29.5s,	estimator xgboost's best error=1.5318,	best estimator xgboost's best error=1.5318
[flaml.automl: 09-17 14:10:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:10:28] {3072} INFO -  at 32.6s,	estimator xgboost's best error=1.2583,	best estimator xgboost's best error=1.2583
[flaml.automl: 09-17 14:10:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:10:33] {3072} INFO -  at 37.6s,	estimator xgboost's best error=1.2583,	best estimator xgboost's best error=1.2583
[flaml.automl: 09-17 14:10:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:10:36] {3072} INFO -  at 40.6s,	estimator xgboost's best error=1.2583,	best estimator xgboost's best error=1.2583
[flaml.automl: 09-17 14:10:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:10:41] {3072} INFO -  at 46.1s,	estimator xgboost's best error=1.2583,	best estimator xgboost's best error=1.2583
[flaml.automl: 09-17 14:10:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:10:44] {3072} INFO -  at 49.3s,	estimator xgboost's best error=1.2583,	best estimator xgboost's best error=1.2583
[flaml.automl: 09-17 14:10:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:10:47] {3072} INFO -  at 51.4s,	estimator xgboost's best error=1.2583,	best estimator xgboost's best error=1.2583
[flaml.automl: 09-17 14:10:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:10:55] {3072} INFO -  at 59.6s,	estimator xgboost's best error=1.2568,	best estimator xgboost's best error=1.2568
[flaml.automl: 09-17 14:11:07] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 14:11:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:11:07] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:11:07] {2637} INFO - Time taken to find the best model: 59.550864458084106
[flaml.automl: 09-17 14:11:07] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
SO2(0)最佳损失：-0.256784804830678
SO2(0)最好结果：{'pred_time': 2.6897737842484922e-05, 'wall_clock_time': 59.550864458084106, 'metric_for_logging': {'pred_time': 2.6897737842484922e-05}, 'val_loss': 1.256784804830678, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 8.167360782623291}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5885049250613352
SO2(0)的mse=5.159801315812489
SO2(0)的mae=1.2877805010142507
SO2(0)的mar=0.20550992409296745
总共花费的时间为：72.21
三门峡市
1835A
1836A
1838A
3598A
[flaml.automl: 09-17 14:24:06] {2390} INFO - task = regression
[flaml.automl: 09-17 14:24:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:24:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:24:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:24:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:24:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:24:07] {3025} INFO - Estimated sufficient time budget=50962s. Estimated necessary time budget=51s.
[flaml.automl: 09-17 14:24:07] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.4438,	best estimator xgboost's best error=4.4438
[flaml.automl: 09-17 14:24:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:24:09] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.1360,	best estimator xgboost's best error=2.1360
[flaml.automl: 09-17 14:24:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:24:10] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.1360,	best estimator xgboost's best error=2.1360
[flaml.automl: 09-17 14:24:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:24:17] {3072} INFO -  at 11.1s,	estimator xgboost's best error=2.1360,	best estimator xgboost's best error=2.1360
[flaml.automl: 09-17 14:24:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:24:18] {3072} INFO -  at 12.6s,	estimator xgboost's best error=1.5230,	best estimator xgboost's best error=1.5230
[flaml.automl: 09-17 14:24:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:24:21] {3072} INFO -  at 15.5s,	estimator xgboost's best error=1.5230,	best estimator xgboost's best error=1.5230
[flaml.automl: 09-17 14:24:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:24:26] {3072} INFO -  at 20.1s,	estimator xgboost's best error=1.2175,	best estimator xgboost's best error=1.2175
[flaml.automl: 09-17 14:24:26] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:24:30] {3072} INFO -  at 24.0s,	estimator xgboost's best error=1.2175,	best estimator xgboost's best error=1.2175
[flaml.automl: 09-17 14:24:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:24:34] {3072} INFO -  at 27.8s,	estimator xgboost's best error=1.2175,	best estimator xgboost's best error=1.2175
[flaml.automl: 09-17 14:24:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:24:36] {3072} INFO -  at 30.5s,	estimator xgboost's best error=1.2175,	best estimator xgboost's best error=1.2175
[flaml.automl: 09-17 14:24:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:24:39] {3072} INFO -  at 33.1s,	estimator xgboost's best error=1.2175,	best estimator xgboost's best error=1.2175
[flaml.automl: 09-17 14:24:39] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:24:43] {3072} INFO -  at 37.7s,	estimator xgboost's best error=1.2175,	best estimator xgboost's best error=1.2175
[flaml.automl: 09-17 14:24:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:24:47] {3072} INFO -  at 40.8s,	estimator xgboost's best error=1.2175,	best estimator xgboost's best error=1.2175
[flaml.automl: 09-17 14:24:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:25:04] {3072} INFO -  at 58.4s,	estimator xgboost's best error=1.1648,	best estimator xgboost's best error=1.1648
[flaml.automl: 09-17 14:25:23] {3335} INFO - retrain xgboost for 18.7s
[flaml.automl: 09-17 14:25:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:25:23] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:25:23] {2637} INFO - Time taken to find the best model: 58.37345910072327
[flaml.automl: 09-17 14:25:23] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 42647}
SO2(0)最佳损失：-0.16479953104092826
SO2(0)最好结果：{'pred_time': 2.552427101497485e-05, 'wall_clock_time': 58.37345910072327, 'metric_for_logging': {'pred_time': 2.552427101497485e-05}, 'val_loss': 1.1647995310409283, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 42647}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 42647, 'experiment_tag': 'exp', 'time_total_s': 17.581324338912964}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7669458591658096
SO2(0)的mse=3.293202694997296
SO2(0)的mae=1.1837470582624465
SO2(0)的mar=0.17398282489759948
总共花费的时间为：77.88
宜昌市
1840A
1841A
1842A
1843A
3546A
3653A
[flaml.automl: 09-17 14:43:39] {2390} INFO - task = regression
[flaml.automl: 09-17 14:43:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:43:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:43:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:43:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:43:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:43:40] {3025} INFO - Estimated sufficient time budget=79186s. Estimated necessary time budget=79s.
[flaml.automl: 09-17 14:43:40] {3072} INFO -  at 1.5s,	estimator xgboost's best error=3.9499,	best estimator xgboost's best error=3.9499
[flaml.automl: 09-17 14:43:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:43:42] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.7654,	best estimator xgboost's best error=1.7654
[flaml.automl: 09-17 14:43:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:43:44] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.7654,	best estimator xgboost's best error=1.7654
[flaml.automl: 09-17 14:43:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:43:47] {3072} INFO -  at 8.5s,	estimator xgboost's best error=1.7654,	best estimator xgboost's best error=1.7654
[flaml.automl: 09-17 14:43:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:43:49] {3072} INFO -  at 9.6s,	estimator xgboost's best error=0.7824,	best estimator xgboost's best error=0.7824
[flaml.automl: 09-17 14:43:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:43:50] {3072} INFO -  at 11.1s,	estimator xgboost's best error=0.6039,	best estimator xgboost's best error=0.6039
[flaml.automl: 09-17 14:43:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:43:52] {3072} INFO -  at 12.7s,	estimator xgboost's best error=0.6039,	best estimator xgboost's best error=0.6039
[flaml.automl: 09-17 14:43:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:43:54] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.6039,	best estimator xgboost's best error=0.6039
[flaml.automl: 09-17 14:43:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:43:55] {3072} INFO -  at 16.1s,	estimator xgboost's best error=0.6039,	best estimator xgboost's best error=0.6039
[flaml.automl: 09-17 14:43:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:43:58] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.6039,	best estimator xgboost's best error=0.6039
[flaml.automl: 09-17 14:43:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:43:59] {3072} INFO -  at 20.4s,	estimator xgboost's best error=0.6039,	best estimator xgboost's best error=0.6039
[flaml.automl: 09-17 14:43:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:44:01] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.6039,	best estimator xgboost's best error=0.6039
[flaml.automl: 09-17 14:44:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:44:07] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.5697,	best estimator xgboost's best error=0.5697
[flaml.automl: 09-17 14:44:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:44:19] {3072} INFO -  at 40.2s,	estimator xgboost's best error=0.5629,	best estimator xgboost's best error=0.5629
[flaml.automl: 09-17 14:44:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 14:44:26] {3072} INFO -  at 46.8s,	estimator xgboost's best error=0.5629,	best estimator xgboost's best error=0.5629
[flaml.automl: 09-17 14:44:26] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 14:44:38] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.5629,	best estimator xgboost's best error=0.5629
[flaml.automl: 09-17 14:44:50] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-17 14:44:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:44:50] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:44:50] {2637} INFO - Time taken to find the best model: 40.21592617034912
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 66241}
SO2(0)最佳损失：0.4370698280720554
SO2(0)最好结果：{'pred_time': 5.506299918897287e-06, 'wall_clock_time': 40.21592617034912, 'metric_for_logging': {'pred_time': 5.506299918897287e-06}, 'val_loss': 0.5629301719279446, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 66241}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 66241, 'experiment_tag': 'exp', 'time_total_s': 12.10103726387024}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6729044421695451
SO2(0)的mse=0.9031075032440807
SO2(0)的mae=0.5463536101807765
SO2(0)的mar=0.08208720734876516
总共花费的时间为：72.43
荆州市
1845A
3548A
[flaml.automl: 09-17 14:51:00] {2390} INFO - task = regression
[flaml.automl: 09-17 14:51:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 14:51:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 14:51:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 14:51:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 14:51:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 14:51:01] {3025} INFO - Estimated sufficient time budget=11893s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 14:51:01] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.7940,	best estimator xgboost's best error=4.7940
[flaml.automl: 09-17 14:51:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 14:51:03] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.2351,	best estimator xgboost's best error=2.2351
[flaml.automl: 09-17 14:51:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 14:51:04] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.2351,	best estimator xgboost's best error=2.2351
[flaml.automl: 09-17 14:51:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 14:51:14] {3072} INFO -  at 14.0s,	estimator xgboost's best error=2.2351,	best estimator xgboost's best error=2.2351
[flaml.automl: 09-17 14:51:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 14:51:15] {3072} INFO -  at 15.2s,	estimator xgboost's best error=1.3079,	best estimator xgboost's best error=1.3079
[flaml.automl: 09-17 14:51:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 14:51:16] {3072} INFO -  at 16.8s,	estimator xgboost's best error=1.2872,	best estimator xgboost's best error=1.2872
[flaml.automl: 09-17 14:51:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 14:51:18] {3072} INFO -  at 18.4s,	estimator xgboost's best error=1.1197,	best estimator xgboost's best error=1.1197
[flaml.automl: 09-17 14:51:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 14:51:21] {3072} INFO -  at 21.1s,	estimator xgboost's best error=1.1197,	best estimator xgboost's best error=1.1197
[flaml.automl: 09-17 14:51:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 14:51:22] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.1197,	best estimator xgboost's best error=1.1197
[flaml.automl: 09-17 14:51:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 14:51:25] {3072} INFO -  at 25.7s,	estimator xgboost's best error=1.0504,	best estimator xgboost's best error=1.0504
[flaml.automl: 09-17 14:51:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 14:51:27] {3072} INFO -  at 27.4s,	estimator xgboost's best error=1.0504,	best estimator xgboost's best error=1.0504
[flaml.automl: 09-17 14:51:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 14:51:28] {3072} INFO -  at 28.5s,	estimator xgboost's best error=1.0504,	best estimator xgboost's best error=1.0504
[flaml.automl: 09-17 14:51:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 14:51:40] {3072} INFO -  at 40.4s,	estimator xgboost's best error=1.0008,	best estimator xgboost's best error=1.0008
[flaml.automl: 09-17 14:51:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 14:51:59] {3072} INFO -  at 59.6s,	estimator xgboost's best error=0.9732,	best estimator xgboost's best error=0.9732
[flaml.automl: 09-17 14:52:21] {3335} INFO - retrain xgboost for 21.9s
[flaml.automl: 09-17 14:52:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 14:52:21] {2636} INFO - fit succeeded
[flaml.automl: 09-17 14:52:21] {2637} INFO - Time taken to find the best model: 59.574092388153076
[flaml.automl: 09-17 14:52:21] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
SO2(0)最佳损失：0.026842543515083306
SO2(0)最好结果：{'pred_time': 1.6369526762409125e-05, 'wall_clock_time': 59.574092388153076, 'metric_for_logging': {'pred_time': 1.6369526762409125e-05}, 'val_loss': 0.9731574564849167, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 19.13700771331787}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7695312103742579
SO2(0)的mse=2.4896893108149882
SO2(0)的mae=0.9753333657032492
SO2(0)的mar=0.1386278450031054
总共花费的时间为：81.87
岳阳市
1847A
1848A
1850A
1851A
1852A
[flaml.automl: 09-17 15:07:28] {2390} INFO - task = regression
[flaml.automl: 09-17 15:07:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:07:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:07:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:07:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:07:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:07:29] {3025} INFO - Estimated sufficient time budget=62321s. Estimated necessary time budget=62s.
[flaml.automl: 09-17 15:07:29] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.4805,	best estimator xgboost's best error=5.4805
[flaml.automl: 09-17 15:07:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:07:32] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.4957,	best estimator xgboost's best error=2.4957
[flaml.automl: 09-17 15:07:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:07:33] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.4957,	best estimator xgboost's best error=2.4957
[flaml.automl: 09-17 15:07:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:07:38] {3072} INFO -  at 9.9s,	estimator xgboost's best error=2.4957,	best estimator xgboost's best error=2.4957
[flaml.automl: 09-17 15:07:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:07:39] {3072} INFO -  at 11.0s,	estimator xgboost's best error=1.3816,	best estimator xgboost's best error=1.3816
[flaml.automl: 09-17 15:07:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:07:41] {3072} INFO -  at 12.6s,	estimator xgboost's best error=1.3811,	best estimator xgboost's best error=1.3811
[flaml.automl: 09-17 15:07:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:07:42] {3072} INFO -  at 14.2s,	estimator xgboost's best error=1.1388,	best estimator xgboost's best error=1.1388
[flaml.automl: 09-17 15:07:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:07:45] {3072} INFO -  at 16.9s,	estimator xgboost's best error=1.1388,	best estimator xgboost's best error=1.1388
[flaml.automl: 09-17 15:07:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:07:46] {3072} INFO -  at 18.5s,	estimator xgboost's best error=1.1388,	best estimator xgboost's best error=1.1388
[flaml.automl: 09-17 15:07:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:07:50] {3072} INFO -  at 21.5s,	estimator xgboost's best error=1.0958,	best estimator xgboost's best error=1.0958
[flaml.automl: 09-17 15:07:50] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:07:51] {3072} INFO -  at 23.2s,	estimator xgboost's best error=1.0958,	best estimator xgboost's best error=1.0958
[flaml.automl: 09-17 15:07:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:07:52] {3072} INFO -  at 24.3s,	estimator xgboost's best error=1.0958,	best estimator xgboost's best error=1.0958
[flaml.automl: 09-17 15:07:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:07:55] {3072} INFO -  at 27.2s,	estimator xgboost's best error=1.0890,	best estimator xgboost's best error=1.0890
[flaml.automl: 09-17 15:07:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:07:58] {3072} INFO -  at 30.1s,	estimator xgboost's best error=1.0890,	best estimator xgboost's best error=1.0890
[flaml.automl: 09-17 15:07:58] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 15:08:01] {3072} INFO -  at 32.6s,	estimator xgboost's best error=1.0755,	best estimator xgboost's best error=1.0755
[flaml.automl: 09-17 15:08:01] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 15:08:02] {3072} INFO -  at 34.2s,	estimator xgboost's best error=1.0755,	best estimator xgboost's best error=1.0755
[flaml.automl: 09-17 15:08:02] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 15:08:04] {3072} INFO -  at 36.4s,	estimator xgboost's best error=1.0666,	best estimator xgboost's best error=1.0666
[flaml.automl: 09-17 15:08:04] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 15:08:06] {3072} INFO -  at 38.0s,	estimator xgboost's best error=1.0666,	best estimator xgboost's best error=1.0666
[flaml.automl: 09-17 15:08:06] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 15:08:08] {3072} INFO -  at 39.7s,	estimator xgboost's best error=1.0666,	best estimator xgboost's best error=1.0666
[flaml.automl: 09-17 15:08:08] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 15:08:09] {3072} INFO -  at 41.3s,	estimator xgboost's best error=1.0666,	best estimator xgboost's best error=1.0666
[flaml.automl: 09-17 15:08:09] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 15:08:11] {3072} INFO -  at 42.8s,	estimator xgboost's best error=1.0627,	best estimator xgboost's best error=1.0627
[flaml.automl: 09-17 15:08:11] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 15:08:13] {3072} INFO -  at 45.5s,	estimator xgboost's best error=1.0627,	best estimator xgboost's best error=1.0627
[flaml.automl: 09-17 15:08:13] {2897} INFO - iteration 22, current learner xgboost
[flaml.automl: 09-17 15:08:14] {3072} INFO -  at 46.4s,	estimator xgboost's best error=1.0606,	best estimator xgboost's best error=1.0606
[flaml.automl: 09-17 15:08:14] {2897} INFO - iteration 23, current learner xgboost
[flaml.automl: 09-17 15:08:16] {3072} INFO -  at 48.3s,	estimator xgboost's best error=1.0603,	best estimator xgboost's best error=1.0603
[flaml.automl: 09-17 15:08:16] {2897} INFO - iteration 24, current learner xgboost
[flaml.automl: 09-17 15:08:17] {3072} INFO -  at 49.2s,	estimator xgboost's best error=1.0603,	best estimator xgboost's best error=1.0603
[flaml.automl: 09-17 15:08:17] {2897} INFO - iteration 25, current learner xgboost
[flaml.automl: 09-17 15:08:18] {3072} INFO -  at 49.9s,	estimator xgboost's best error=1.0603,	best estimator xgboost's best error=1.0603
[flaml.automl: 09-17 15:08:18] {2897} INFO - iteration 26, current learner xgboost
[flaml.automl: 09-17 15:08:19] {3072} INFO -  at 51.4s,	estimator xgboost's best error=1.0603,	best estimator xgboost's best error=1.0603
[flaml.automl: 09-17 15:08:19] {2897} INFO - iteration 27, current learner xgboost
[flaml.automl: 09-17 15:08:21] {3072} INFO -  at 52.9s,	estimator xgboost's best error=1.0603,	best estimator xgboost's best error=1.0603
[flaml.automl: 09-17 15:08:21] {2897} INFO - iteration 28, current learner xgboost
[flaml.automl: 09-17 15:08:27] {3072} INFO -  at 58.6s,	estimator xgboost's best error=1.0312,	best estimator xgboost's best error=1.0312
[flaml.automl: 09-17 15:08:45] {3335} INFO - retrain xgboost for 18.2s
[flaml.automl: 09-17 15:08:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9071096091677163, colsample_bynode=1,
             colsample_bytree=0.8171119593656715, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=31, min_child_weight=0.001, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0027872733064379708, reg_lambda=1.1932963980484825,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:08:45] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:08:45] {2637} INFO - Time taken to find the best model: 58.62539887428284
[flaml.automl: 09-17 15:08:45] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 31, 'min_child_weight': 0.001, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.9071096091677163, 'colsample_bytree': 0.8171119593656715, 'reg_alpha': 0.0027872733064379708, 'reg_lambda': 1.1932963980484825, 'FLAML_sample_size': 52430}
SO2(0)最佳损失：-0.031186801663715347
SO2(0)最好结果：{'pred_time': 1.4797789183753158e-05, 'wall_clock_time': 58.62539887428284, 'metric_for_logging': {'pred_time': 1.4797789183753158e-05}, 'val_loss': 1.0311868016637153, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 31, 'min_child_weight': 0.001, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.9071096091677163, 'colsample_bytree': 0.8171119593656715, 'reg_alpha': 0.0027872733064379708, 'reg_lambda': 1.1932963980484825, 'FLAML_sample_size': 52430}, 'config/n_estimators': 6, 'config/max_leaves': 31, 'config/min_child_weight': 0.001, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9071096091677163, 'config/colsample_bytree': 0.8171119593656715, 'config/reg_alpha': 0.0027872733064379708, 'config/reg_lambda': 1.1932963980484825, 'config/FLAML_sample_size': 52430, 'experiment_tag': 'exp', 'time_total_s': 5.726457357406616}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9071096091677163, colsample_bynode=1,
             colsample_bytree=0.8171119593656715, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=31, min_child_weight=0.001, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0027872733064379708, reg_lambda=1.1932963980484825,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7020336612675006
SO2(0)的mse=2.6873474530769013
SO2(0)的mae=1.0307922647077814
SO2(0)的mar=0.11304337784403838
总共花费的时间为：77.70
常德市
1854A
1857A
3138A
3139A
3140A
[flaml.automl: 09-17 15:24:15] {2390} INFO - task = regression
[flaml.automl: 09-17 15:24:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:24:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:24:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:24:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:24:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:24:17] {3025} INFO - Estimated sufficient time budget=62856s. Estimated necessary time budget=63s.
[flaml.automl: 09-17 15:24:17] {3072} INFO -  at 1.5s,	estimator xgboost's best error=4.7611,	best estimator xgboost's best error=4.7611
[flaml.automl: 09-17 15:24:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:24:19] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.1981,	best estimator xgboost's best error=2.1981
[flaml.automl: 09-17 15:24:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:24:20] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.1981,	best estimator xgboost's best error=2.1981
[flaml.automl: 09-17 15:24:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:24:25] {3072} INFO -  at 9.5s,	estimator xgboost's best error=2.1981,	best estimator xgboost's best error=2.1981
[flaml.automl: 09-17 15:24:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:24:26] {3072} INFO -  at 10.6s,	estimator xgboost's best error=1.2089,	best estimator xgboost's best error=1.2089
[flaml.automl: 09-17 15:24:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:24:27] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.2089,	best estimator xgboost's best error=1.2089
[flaml.automl: 09-17 15:24:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:24:29] {3072} INFO -  at 13.9s,	estimator xgboost's best error=1.0051,	best estimator xgboost's best error=1.0051
[flaml.automl: 09-17 15:24:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:24:32] {3072} INFO -  at 16.6s,	estimator xgboost's best error=1.0051,	best estimator xgboost's best error=1.0051
[flaml.automl: 09-17 15:24:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:24:33] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.0051,	best estimator xgboost's best error=1.0051
[flaml.automl: 09-17 15:24:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:24:36] {3072} INFO -  at 21.2s,	estimator xgboost's best error=1.0051,	best estimator xgboost's best error=1.0051
[flaml.automl: 09-17 15:24:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:24:38] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.9694,	best estimator xgboost's best error=0.9694
[flaml.automl: 09-17 15:24:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:24:39] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.9694,	best estimator xgboost's best error=0.9694
[flaml.automl: 09-17 15:24:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:24:42] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.9626,	best estimator xgboost's best error=0.9626
[flaml.automl: 09-17 15:24:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:24:45] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.9363,	best estimator xgboost's best error=0.9363
[flaml.automl: 09-17 15:24:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 15:24:47] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.9363,	best estimator xgboost's best error=0.9363
[flaml.automl: 09-17 15:24:47] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 15:24:49] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.9363,	best estimator xgboost's best error=0.9363
[flaml.automl: 09-17 15:24:49] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 15:24:52] {3072} INFO -  at 36.5s,	estimator xgboost's best error=0.9363,	best estimator xgboost's best error=0.9363
[flaml.automl: 09-17 15:24:52] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 15:24:54] {3072} INFO -  at 38.6s,	estimator xgboost's best error=0.9363,	best estimator xgboost's best error=0.9363
[flaml.automl: 09-17 15:24:54] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 15:25:04] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.9330,	best estimator xgboost's best error=0.9330
[flaml.automl: 09-17 15:25:15] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-17 15:25:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:25:15] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:25:15] {2637} INFO - Time taken to find the best model: 49.380027055740356
[flaml.automl: 09-17 15:25:15] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52778}
SO2(0)最佳损失：0.06695543766428236
SO2(0)最好结果：{'pred_time': 6.728322711238877e-06, 'wall_clock_time': 49.380027055740356, 'metric_for_logging': {'pred_time': 6.728322711238877e-06}, 'val_loss': 0.9330445623357176, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52778}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 52778, 'experiment_tag': 'exp', 'time_total_s': 10.773710012435913}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6230458292660673
SO2(0)的mse=2.606615095018181
SO2(0)的mae=0.9396979888207053
SO2(0)的mar=0.1192733655783803
总共花费的时间为：60.95
张家界市
1858A
1859A
1861A
[flaml.automl: 09-17 15:34:46] {2390} INFO - task = regression
[flaml.automl: 09-17 15:34:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:34:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:34:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:34:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:34:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:34:48] {3025} INFO - Estimated sufficient time budget=19009s. Estimated necessary time budget=19s.
[flaml.automl: 09-17 15:34:48] {3072} INFO -  at 2.0s,	estimator xgboost's best error=1.5561,	best estimator xgboost's best error=1.5561
[flaml.automl: 09-17 15:34:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:34:51] {3072} INFO -  at 5.8s,	estimator xgboost's best error=0.8149,	best estimator xgboost's best error=0.8149
[flaml.automl: 09-17 15:34:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:34:54] {3072} INFO -  at 8.0s,	estimator xgboost's best error=0.8149,	best estimator xgboost's best error=0.8149
[flaml.automl: 09-17 15:34:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:35:11] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.8149,	best estimator xgboost's best error=0.8149
[flaml.automl: 09-17 15:35:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:35:13] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.5123,	best estimator xgboost's best error=0.5123
[flaml.automl: 09-17 15:35:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:35:15] {3072} INFO -  at 29.8s,	estimator xgboost's best error=0.4966,	best estimator xgboost's best error=0.4966
[flaml.automl: 09-17 15:35:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:35:18] {3072} INFO -  at 32.7s,	estimator xgboost's best error=0.4101,	best estimator xgboost's best error=0.4101
[flaml.automl: 09-17 15:35:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:35:23] {3072} INFO -  at 37.2s,	estimator xgboost's best error=0.4101,	best estimator xgboost's best error=0.4101
[flaml.automl: 09-17 15:35:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:35:25] {3072} INFO -  at 39.3s,	estimator xgboost's best error=0.4014,	best estimator xgboost's best error=0.4014
[flaml.automl: 09-17 15:35:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:35:28] {3072} INFO -  at 42.3s,	estimator xgboost's best error=0.3708,	best estimator xgboost's best error=0.3708
[flaml.automl: 09-17 15:35:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:35:29] {3072} INFO -  at 43.4s,	estimator xgboost's best error=0.3708,	best estimator xgboost's best error=0.3708
[flaml.automl: 09-17 15:35:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:35:43] {3072} INFO -  at 57.0s,	estimator xgboost's best error=0.3551,	best estimator xgboost's best error=0.3551
[flaml.automl: 09-17 15:35:56] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-17 15:35:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:35:56] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:35:56] {2637} INFO - Time taken to find the best model: 57.004109382629395
[flaml.automl: 09-17 15:35:56] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
SO2(0)最佳损失：0.6449182743912384
SO2(0)最好结果：{'pred_time': 1.1912358811692623e-05, 'wall_clock_time': 57.004109382629395, 'metric_for_logging': {'pred_time': 1.1912358811692623e-05}, 'val_loss': 0.3550817256087616, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 13.619898796081543}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=16, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8310492079144778
SO2(0)的mse=0.3312919083729141
SO2(0)的mae=0.3856098424339092
SO2(0)的mar=0.19088959367961308
总共花费的时间为：71.21
桂林市
1862A
1864A
1865A
3403A
3531A
[flaml.automl: 09-17 15:51:29] {2390} INFO - task = regression
[flaml.automl: 09-17 15:51:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 15:51:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 15:51:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 15:51:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 15:51:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 15:51:31] {3025} INFO - Estimated sufficient time budget=108189s. Estimated necessary time budget=108s.
[flaml.automl: 09-17 15:51:31] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.5232,	best estimator xgboost's best error=5.5232
[flaml.automl: 09-17 15:51:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 15:51:35] {3072} INFO -  at 6.1s,	estimator xgboost's best error=2.7635,	best estimator xgboost's best error=2.7635
[flaml.automl: 09-17 15:51:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 15:51:37] {3072} INFO -  at 8.2s,	estimator xgboost's best error=2.7635,	best estimator xgboost's best error=2.7635
[flaml.automl: 09-17 15:51:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 15:51:42] {3072} INFO -  at 12.8s,	estimator xgboost's best error=2.7635,	best estimator xgboost's best error=2.7635
[flaml.automl: 09-17 15:51:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 15:51:44] {3072} INFO -  at 14.5s,	estimator xgboost's best error=2.1425,	best estimator xgboost's best error=2.1425
[flaml.automl: 09-17 15:51:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 15:51:46] {3072} INFO -  at 17.0s,	estimator xgboost's best error=2.0381,	best estimator xgboost's best error=2.0381
[flaml.automl: 09-17 15:51:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 15:51:49] {3072} INFO -  at 20.0s,	estimator xgboost's best error=2.0381,	best estimator xgboost's best error=2.0381
[flaml.automl: 09-17 15:51:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 15:51:53] {3072} INFO -  at 23.6s,	estimator xgboost's best error=2.0381,	best estimator xgboost's best error=2.0381
[flaml.automl: 09-17 15:51:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 15:51:55] {3072} INFO -  at 25.5s,	estimator xgboost's best error=2.0381,	best estimator xgboost's best error=2.0381
[flaml.automl: 09-17 15:51:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 15:51:58] {3072} INFO -  at 28.4s,	estimator xgboost's best error=2.0381,	best estimator xgboost's best error=2.0381
[flaml.automl: 09-17 15:51:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 15:52:00] {3072} INFO -  at 31.3s,	estimator xgboost's best error=2.0381,	best estimator xgboost's best error=2.0381
[flaml.automl: 09-17 15:52:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 15:52:02] {3072} INFO -  at 32.8s,	estimator xgboost's best error=2.0381,	best estimator xgboost's best error=2.0381
[flaml.automl: 09-17 15:52:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 15:52:09] {3072} INFO -  at 39.7s,	estimator xgboost's best error=2.0001,	best estimator xgboost's best error=2.0001
[flaml.automl: 09-17 15:52:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 15:52:21] {3072} INFO -  at 51.8s,	estimator xgboost's best error=1.9210,	best estimator xgboost's best error=1.9210
[flaml.automl: 09-17 15:52:33] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 15:52:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 15:52:33] {2636} INFO - fit succeeded
[flaml.automl: 09-17 15:52:33] {2637} INFO - Time taken to find the best model: 51.821797609329224
[flaml.automl: 09-17 15:52:33] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52649}
SO2(0)最佳损失：-0.9210424787162717
SO2(0)最好结果：{'pred_time': 6.769701965853699e-06, 'wall_clock_time': 51.821797609329224, 'metric_for_logging': {'pred_time': 6.769701965853699e-06}, 'val_loss': 1.9210424787162717, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52649}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52649, 'experiment_tag': 'exp', 'time_total_s': 12.074242353439331}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.41012251050621107
SO2(0)的mse=18.5978563110269
SO2(0)的mae=1.9005085790707514
SO2(0)的mar=0.16942677192996589
总共花费的时间为：64.87
北海市
1866A
1868A
1869A
3400A
[flaml.automl: 09-17 16:05:04] {2390} INFO - task = regression
[flaml.automl: 09-17 16:05:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:05:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:05:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:05:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:05:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:05:05] {3025} INFO - Estimated sufficient time budget=50064s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 16:05:05] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.0137,	best estimator xgboost's best error=4.0137
[flaml.automl: 09-17 16:05:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:05:07] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.8591,	best estimator xgboost's best error=1.8591
[flaml.automl: 09-17 16:05:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:05:08] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.8591,	best estimator xgboost's best error=1.8591
[flaml.automl: 09-17 16:05:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:05:15] {3072} INFO -  at 11.3s,	estimator xgboost's best error=1.8591,	best estimator xgboost's best error=1.8591
[flaml.automl: 09-17 16:05:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:05:16] {3072} INFO -  at 12.5s,	estimator xgboost's best error=1.1685,	best estimator xgboost's best error=1.1685
[flaml.automl: 09-17 16:05:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:05:18] {3072} INFO -  at 14.0s,	estimator xgboost's best error=1.0446,	best estimator xgboost's best error=1.0446
[flaml.automl: 09-17 16:05:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:05:19] {3072} INFO -  at 15.7s,	estimator xgboost's best error=1.0197,	best estimator xgboost's best error=1.0197
[flaml.automl: 09-17 16:05:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:05:22] {3072} INFO -  at 18.4s,	estimator xgboost's best error=1.0197,	best estimator xgboost's best error=1.0197
[flaml.automl: 09-17 16:05:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:05:24] {3072} INFO -  at 20.0s,	estimator xgboost's best error=1.0197,	best estimator xgboost's best error=1.0197
[flaml.automl: 09-17 16:05:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:05:27] {3072} INFO -  at 23.0s,	estimator xgboost's best error=1.0197,	best estimator xgboost's best error=1.0197
[flaml.automl: 09-17 16:05:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:05:28] {3072} INFO -  at 24.4s,	estimator xgboost's best error=1.0197,	best estimator xgboost's best error=1.0197
[flaml.automl: 09-17 16:05:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:05:30] {3072} INFO -  at 26.1s,	estimator xgboost's best error=0.9922,	best estimator xgboost's best error=0.9922
[flaml.automl: 09-17 16:05:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:05:31] {3072} INFO -  at 27.3s,	estimator xgboost's best error=0.9922,	best estimator xgboost's best error=0.9922
[flaml.automl: 09-17 16:05:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:05:38] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.9347,	best estimator xgboost's best error=0.9347
[flaml.automl: 09-17 16:05:38] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 16:05:50] {3072} INFO -  at 46.9s,	estimator xgboost's best error=0.9313,	best estimator xgboost's best error=0.9313
[flaml.automl: 09-17 16:05:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 16:05:57] {3072} INFO -  at 53.9s,	estimator xgboost's best error=0.9313,	best estimator xgboost's best error=0.9313
[flaml.automl: 09-17 16:06:10] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 16:06:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:06:10] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:06:10] {2637} INFO - Time taken to find the best model: 46.94804286956787
[flaml.automl: 09-17 16:06:10] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 41563}
SO2(0)最佳损失：0.0687098101016943
SO2(0)最好结果：{'pred_time': 9.099857506232747e-06, 'wall_clock_time': 46.94804286956787, 'metric_for_logging': {'pred_time': 9.099857506232747e-06}, 'val_loss': 0.9312901898983057, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 41563}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 41563, 'experiment_tag': 'exp', 'time_total_s': 12.777086734771729}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6689469285996597
SO2(0)的mse=2.7348792390030283
SO2(0)的mae=0.9069404347929654
SO2(0)的mar=0.12300012012863488
总共花费的时间为：67.30
柳州市
1870A
1872A
1873A
1874A
1875A
3402A
[flaml.automl: 09-17 16:26:00] {2390} INFO - task = regression
[flaml.automl: 09-17 16:26:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:26:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:26:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:26:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:26:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:26:02] {3025} INFO - Estimated sufficient time budget=79732s. Estimated necessary time budget=80s.
[flaml.automl: 09-17 16:26:02] {3072} INFO -  at 1.5s,	estimator xgboost's best error=5.8692,	best estimator xgboost's best error=5.8692
[flaml.automl: 09-17 16:26:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:26:04] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.8359,	best estimator xgboost's best error=2.8359
[flaml.automl: 09-17 16:26:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:26:05] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.8359,	best estimator xgboost's best error=2.8359
[flaml.automl: 09-17 16:26:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:26:09] {3072} INFO -  at 8.6s,	estimator xgboost's best error=2.8359,	best estimator xgboost's best error=2.8359
[flaml.automl: 09-17 16:26:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:26:10] {3072} INFO -  at 9.7s,	estimator xgboost's best error=2.2530,	best estimator xgboost's best error=2.2530
[flaml.automl: 09-17 16:26:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:26:11] {3072} INFO -  at 11.3s,	estimator xgboost's best error=2.2530,	best estimator xgboost's best error=2.2530
[flaml.automl: 09-17 16:26:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:26:13] {3072} INFO -  at 13.0s,	estimator xgboost's best error=1.8512,	best estimator xgboost's best error=1.8512
[flaml.automl: 09-17 16:26:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:26:16] {3072} INFO -  at 15.7s,	estimator xgboost's best error=1.8512,	best estimator xgboost's best error=1.8512
[flaml.automl: 09-17 16:26:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:26:17] {3072} INFO -  at 17.3s,	estimator xgboost's best error=1.8512,	best estimator xgboost's best error=1.8512
[flaml.automl: 09-17 16:26:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:26:20] {3072} INFO -  at 20.4s,	estimator xgboost's best error=1.8512,	best estimator xgboost's best error=1.8512
[flaml.automl: 09-17 16:26:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:26:22] {3072} INFO -  at 21.8s,	estimator xgboost's best error=1.8512,	best estimator xgboost's best error=1.8512
[flaml.automl: 09-17 16:26:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:26:24] {3072} INFO -  at 23.5s,	estimator xgboost's best error=1.8512,	best estimator xgboost's best error=1.8512
[flaml.automl: 09-17 16:26:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:26:25] {3072} INFO -  at 24.7s,	estimator xgboost's best error=1.8512,	best estimator xgboost's best error=1.8512
[flaml.automl: 09-17 16:26:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:26:32] {3072} INFO -  at 31.8s,	estimator xgboost's best error=1.8512,	best estimator xgboost's best error=1.8512
[flaml.automl: 09-17 16:26:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 16:26:38] {3072} INFO -  at 38.3s,	estimator xgboost's best error=1.8284,	best estimator xgboost's best error=1.8284
[flaml.automl: 09-17 16:26:38] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 16:26:42] {3072} INFO -  at 42.1s,	estimator xgboost's best error=1.8284,	best estimator xgboost's best error=1.8284
[flaml.automl: 09-17 16:26:42] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 16:26:53] {3072} INFO -  at 52.9s,	estimator xgboost's best error=1.8259,	best estimator xgboost's best error=1.8259
[flaml.automl: 09-17 16:27:04] {3335} INFO - retrain xgboost for 10.8s
[flaml.automl: 09-17 16:27:04] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.19405630899231047,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=11.615070090947864, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.07808876344933313, scale_pos_weight=1,
             subsample=0.9453052099956202, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:27:04] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:27:04] {2637} INFO - Time taken to find the best model: 52.94304180145264
[flaml.automl: 09-17 16:27:04] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 7, 'min_child_weight': 11.615070090947864, 'learning_rate': 0.19405630899231047, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.07808876344933313, 'FLAML_sample_size': 65872}
SO2(0)最佳损失：-0.8259471864322496
SO2(0)最好结果：{'pred_time': 1.046546821385785e-05, 'wall_clock_time': 52.94304180145264, 'metric_for_logging': {'pred_time': 1.046546821385785e-05}, 'val_loss': 1.8259471864322496, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 7, 'min_child_weight': 11.615070090947864, 'learning_rate': 0.19405630899231047, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.07808876344933313, 'FLAML_sample_size': 65872}, 'config/n_estimators': 14, 'config/max_leaves': 7, 'config/min_child_weight': 11.615070090947864, 'config/learning_rate': 0.19405630899231047, 'config/subsample': 0.9453052099956202, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.92006762344246, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.07808876344933313, 'config/FLAML_sample_size': 65872, 'experiment_tag': 'exp', 'time_total_s': 10.79215145111084}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.19405630899231047,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=11.615070090947864, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.07808876344933313, scale_pos_weight=1,
             subsample=0.9453052099956202, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5017771576621766
SO2(0)的mse=15.42449226784022
SO2(0)的mae=1.9865684287949388
SO2(0)的mar=0.16931442218577997
总共花费的时间为：64.74
三亚市
1876A
3540A
[flaml.automl: 09-17 16:33:33] {2390} INFO - task = regression
[flaml.automl: 09-17 16:33:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:33:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:33:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:33:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:33:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:33:34] {3025} INFO - Estimated sufficient time budget=12179s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 16:33:34] {3072} INFO -  at 1.3s,	estimator xgboost's best error=1.8308,	best estimator xgboost's best error=1.8308
[flaml.automl: 09-17 16:33:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:33:36] {3072} INFO -  at 3.4s,	estimator xgboost's best error=0.8490,	best estimator xgboost's best error=0.8490
[flaml.automl: 09-17 16:33:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:33:37] {3072} INFO -  at 4.6s,	estimator xgboost's best error=0.8490,	best estimator xgboost's best error=0.8490
[flaml.automl: 09-17 16:33:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:33:46] {3072} INFO -  at 13.4s,	estimator xgboost's best error=0.8490,	best estimator xgboost's best error=0.8490
[flaml.automl: 09-17 16:33:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:33:47] {3072} INFO -  at 14.5s,	estimator xgboost's best error=0.4389,	best estimator xgboost's best error=0.4389
[flaml.automl: 09-17 16:33:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:33:49] {3072} INFO -  at 16.0s,	estimator xgboost's best error=0.3245,	best estimator xgboost's best error=0.3245
[flaml.automl: 09-17 16:33:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:33:50] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.3245,	best estimator xgboost's best error=0.3245
[flaml.automl: 09-17 16:33:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:33:53] {3072} INFO -  at 20.1s,	estimator xgboost's best error=0.3245,	best estimator xgboost's best error=0.3245
[flaml.automl: 09-17 16:33:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:33:54] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.3245,	best estimator xgboost's best error=0.3245
[flaml.automl: 09-17 16:33:54] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:33:56] {3072} INFO -  at 23.6s,	estimator xgboost's best error=0.3245,	best estimator xgboost's best error=0.3245
[flaml.automl: 09-17 16:33:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:33:57] {3072} INFO -  at 24.7s,	estimator xgboost's best error=0.3245,	best estimator xgboost's best error=0.3245
[flaml.automl: 09-17 16:33:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:33:59] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.3245,	best estimator xgboost's best error=0.3245
[flaml.automl: 09-17 16:33:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:34:05] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.2969,	best estimator xgboost's best error=0.2969
[flaml.automl: 09-17 16:34:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:34:20] {3072} INFO -  at 47.6s,	estimator xgboost's best error=0.2969,	best estimator xgboost's best error=0.2969
[flaml.automl: 09-17 16:34:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 16:34:26] {3072} INFO -  at 53.8s,	estimator xgboost's best error=0.2844,	best estimator xgboost's best error=0.2844
[flaml.automl: 09-17 16:34:26] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 16:34:34] {3072} INFO -  at 61.0s,	estimator xgboost's best error=0.2844,	best estimator xgboost's best error=0.2844
[flaml.automl: 09-17 16:34:40] {3335} INFO - retrain xgboost for 6.1s
[flaml.automl: 09-17 16:34:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7164723828648284, colsample_bynode=1,
             colsample_bytree=0.5372780014279438, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=17, min_child_weight=0.4863979073627235,
             missing=nan, monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.013677698859125073, reg_lambda=9.040735251848712,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:34:40] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:34:40] {2637} INFO - Time taken to find the best model: 53.77961468696594
[flaml.automl: 09-17 16:34:40] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 17, 'min_child_weight': 0.4863979073627235, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7164723828648284, 'colsample_bytree': 0.5372780014279438, 'reg_alpha': 0.013677698859125073, 'reg_lambda': 9.040735251848712}
SO2(0)最佳损失：0.715578726346163
SO2(0)最好结果：{'pred_time': 2.6370600449571984e-05, 'wall_clock_time': 53.77961468696594, 'metric_for_logging': {'pred_time': 2.6370600449571984e-05}, 'val_loss': 0.28442127365383696, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 17, 'min_child_weight': 0.4863979073627235, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7164723828648284, 'colsample_bytree': 0.5372780014279438, 'reg_alpha': 0.013677698859125073, 'reg_lambda': 9.040735251848712}, 'config/n_estimators': 4, 'config/max_leaves': 17, 'config/min_child_weight': 0.4863979073627235, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7164723828648284, 'config/colsample_bytree': 0.5372780014279438, 'config/reg_alpha': 0.013677698859125073, 'config/reg_lambda': 9.040735251848712, 'experiment_tag': 'exp', 'time_total_s': 6.177292585372925}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7164723828648284, colsample_bynode=1,
             colsample_bytree=0.5372780014279438, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=17, min_child_weight=0.4863979073627235,
             missing=nan, monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.013677698859125073, reg_lambda=9.040735251848712,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7384596088210013
SO2(0)的mse=0.24650127516817033
SO2(0)的mae=0.27362089431543146
SO2(0)的mar=0.09585976012063867
总共花费的时间为：67.51
德阳市
1902A
3639A
[flaml.automl: 09-17 16:41:09] {2390} INFO - task = regression
[flaml.automl: 09-17 16:41:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:41:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:41:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:41:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:41:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:41:10] {3025} INFO - Estimated sufficient time budget=12082s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 16:41:10] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.7505,	best estimator xgboost's best error=3.7505
[flaml.automl: 09-17 16:41:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:41:12] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.7411,	best estimator xgboost's best error=1.7411
[flaml.automl: 09-17 16:41:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:41:13] {3072} INFO -  at 4.6s,	estimator xgboost's best error=1.7411,	best estimator xgboost's best error=1.7411
[flaml.automl: 09-17 16:41:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:41:22] {3072} INFO -  at 14.0s,	estimator xgboost's best error=1.7411,	best estimator xgboost's best error=1.7411
[flaml.automl: 09-17 16:41:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:41:24] {3072} INFO -  at 15.1s,	estimator xgboost's best error=1.0597,	best estimator xgboost's best error=1.0597
[flaml.automl: 09-17 16:41:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:41:25] {3072} INFO -  at 16.7s,	estimator xgboost's best error=1.0597,	best estimator xgboost's best error=1.0597
[flaml.automl: 09-17 16:41:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:41:27] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.8796,	best estimator xgboost's best error=0.8796
[flaml.automl: 09-17 16:41:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:41:30] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.8796,	best estimator xgboost's best error=0.8796
[flaml.automl: 09-17 16:41:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:41:32] {3072} INFO -  at 23.9s,	estimator xgboost's best error=0.8796,	best estimator xgboost's best error=0.8796
[flaml.automl: 09-17 16:41:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:41:38] {3072} INFO -  at 29.4s,	estimator xgboost's best error=0.8796,	best estimator xgboost's best error=0.8796
[flaml.automl: 09-17 16:41:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:41:41] {3072} INFO -  at 32.6s,	estimator xgboost's best error=0.8496,	best estimator xgboost's best error=0.8496
[flaml.automl: 09-17 16:41:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:41:43] {3072} INFO -  at 34.7s,	estimator xgboost's best error=0.8496,	best estimator xgboost's best error=0.8496
[flaml.automl: 09-17 16:41:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:41:55] {3072} INFO -  at 46.2s,	estimator xgboost's best error=0.8418,	best estimator xgboost's best error=0.8418
[flaml.automl: 09-17 16:41:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:42:08] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.8361,	best estimator xgboost's best error=0.8361
[flaml.automl: 09-17 16:42:28] {3335} INFO - retrain xgboost for 20.3s
[flaml.automl: 09-17 16:42:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 16:42:28] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:42:28] {2637} INFO - Time taken to find the best model: 59.274197578430176
[flaml.automl: 09-17 16:42:28] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
SO2(0)最佳损失：0.16388877021198411
SO2(0)最好结果：{'pred_time': 2.9345209123243527e-05, 'wall_clock_time': 59.274197578430176, 'metric_for_logging': {'pred_time': 2.9345209123243527e-05}, 'val_loss': 0.8361112297880159, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 13.02956223487854}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7166926762402266
SO2(0)的mse=1.8003161199778668
SO2(0)的mae=0.8240061777962194
SO2(0)的mar=0.13925925764803904
总共花费的时间为：80.15
南充市
1905A
1906A
1907A
1908A
1909A
[flaml.automl: 09-17 16:57:31] {2390} INFO - task = regression
[flaml.automl: 09-17 16:57:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 16:57:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 16:57:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 16:57:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 16:57:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 16:57:32] {3025} INFO - Estimated sufficient time budget=60262s. Estimated necessary time budget=60s.
[flaml.automl: 09-17 16:57:32] {3072} INFO -  at 1.4s,	estimator xgboost's best error=2.4397,	best estimator xgboost's best error=2.4397
[flaml.automl: 09-17 16:57:32] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 16:57:34] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.5247,	best estimator xgboost's best error=1.5247
[flaml.automl: 09-17 16:57:34] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 16:57:35] {3072} INFO -  at 4.6s,	estimator xgboost's best error=1.5247,	best estimator xgboost's best error=1.5247
[flaml.automl: 09-17 16:57:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 16:57:41] {3072} INFO -  at 9.8s,	estimator xgboost's best error=1.5247,	best estimator xgboost's best error=1.5247
[flaml.automl: 09-17 16:57:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 16:57:42] {3072} INFO -  at 10.9s,	estimator xgboost's best error=1.3377,	best estimator xgboost's best error=1.3377
[flaml.automl: 09-17 16:57:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 16:57:43] {3072} INFO -  at 12.5s,	estimator xgboost's best error=1.3377,	best estimator xgboost's best error=1.3377
[flaml.automl: 09-17 16:57:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 16:57:45] {3072} INFO -  at 14.1s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:57:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 16:57:48] {3072} INFO -  at 16.8s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:57:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 16:57:49] {3072} INFO -  at 18.4s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:57:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 16:57:52] {3072} INFO -  at 21.4s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:57:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 16:57:54] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:57:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 16:57:55] {3072} INFO -  at 24.5s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:57:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 16:57:56] {3072} INFO -  at 25.6s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:57:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 16:58:03] {3072} INFO -  at 32.6s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:58:03] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 16:58:07] {3072} INFO -  at 36.2s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:58:07] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 16:58:08] {3072} INFO -  at 37.6s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:58:08] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 16:58:11] {3072} INFO -  at 40.3s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:58:11] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 16:58:13] {3072} INFO -  at 41.9s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:58:13] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 16:58:14] {3072} INFO -  at 43.3s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:58:14] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 16:58:19] {3072} INFO -  at 48.3s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:58:19] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 16:58:21] {3072} INFO -  at 50.5s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:58:21] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-17 16:58:30] {3072} INFO -  at 59.5s,	estimator xgboost's best error=1.2452,	best estimator xgboost's best error=1.2452
[flaml.automl: 09-17 16:58:33] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-17 16:58:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 16:58:33] {2636} INFO - fit succeeded
[flaml.automl: 09-17 16:58:33] {2637} INFO - Time taken to find the best model: 14.132917642593384
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：-0.24524000427289971
SO2(0)最好结果：{'pred_time': 7.0668795915712e-06, 'wall_clock_time': 14.132917642593384, 'metric_for_logging': {'pred_time': 7.0668795915712e-06}, 'val_loss': 1.2452400042728997, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557, 'FLAML_sample_size': 10000}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 1.6435081958770752}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=-0.28373613786117846
SO2(0)的mse=4.560201950376653
SO2(0)的mae=1.2931990753602203
SO2(0)的mar=0.4316726947022835
总共花费的时间为：63.35
遵义市
1911A
1912A
1913A
1914A
3536A
[flaml.automl: 09-17 17:14:07] {2390} INFO - task = regression
[flaml.automl: 09-17 17:14:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:14:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:14:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:14:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:14:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:14:08] {3025} INFO - Estimated sufficient time budget=65503s. Estimated necessary time budget=66s.
[flaml.automl: 09-17 17:14:08] {3072} INFO -  at 1.5s,	estimator xgboost's best error=5.7979,	best estimator xgboost's best error=5.7979
[flaml.automl: 09-17 17:14:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:14:10] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.8496,	best estimator xgboost's best error=2.8496
[flaml.automl: 09-17 17:14:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:14:11] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.8496,	best estimator xgboost's best error=2.8496
[flaml.automl: 09-17 17:14:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:14:16] {3072} INFO -  at 9.6s,	estimator xgboost's best error=2.8496,	best estimator xgboost's best error=2.8496
[flaml.automl: 09-17 17:14:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:14:17] {3072} INFO -  at 10.7s,	estimator xgboost's best error=2.1031,	best estimator xgboost's best error=2.1031
[flaml.automl: 09-17 17:14:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:14:19] {3072} INFO -  at 12.3s,	estimator xgboost's best error=2.1031,	best estimator xgboost's best error=2.1031
[flaml.automl: 09-17 17:14:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:14:20] {3072} INFO -  at 14.0s,	estimator xgboost's best error=1.7270,	best estimator xgboost's best error=1.7270
[flaml.automl: 09-17 17:14:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:14:23] {3072} INFO -  at 16.7s,	estimator xgboost's best error=1.7270,	best estimator xgboost's best error=1.7270
[flaml.automl: 09-17 17:14:23] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:14:25] {3072} INFO -  at 18.3s,	estimator xgboost's best error=1.7270,	best estimator xgboost's best error=1.7270
[flaml.automl: 09-17 17:14:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:14:28] {3072} INFO -  at 21.3s,	estimator xgboost's best error=1.7270,	best estimator xgboost's best error=1.7270
[flaml.automl: 09-17 17:14:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:14:29] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.7270,	best estimator xgboost's best error=1.7270
[flaml.automl: 09-17 17:14:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:14:31] {3072} INFO -  at 24.5s,	estimator xgboost's best error=1.6899,	best estimator xgboost's best error=1.6899
[flaml.automl: 09-17 17:14:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:14:32] {3072} INFO -  at 25.7s,	estimator xgboost's best error=1.6899,	best estimator xgboost's best error=1.6899
[flaml.automl: 09-17 17:14:32] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:14:39] {3072} INFO -  at 32.7s,	estimator xgboost's best error=1.6102,	best estimator xgboost's best error=1.6102
[flaml.automl: 09-17 17:14:39] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 17:14:52] {3072} INFO -  at 45.5s,	estimator xgboost's best error=1.5957,	best estimator xgboost's best error=1.5957
[flaml.automl: 09-17 17:14:52] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 17:14:59] {3072} INFO -  at 52.6s,	estimator xgboost's best error=1.5957,	best estimator xgboost's best error=1.5957
[flaml.automl: 09-17 17:15:12] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-17 17:15:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:15:12] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:15:12] {2637} INFO - Time taken to find the best model: 45.53703546524048
[flaml.automl: 09-17 17:15:12] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54882}
SO2(0)最佳损失：-0.5957358322840947
SO2(0)最好结果：{'pred_time': 6.546992164182749e-06, 'wall_clock_time': 45.53703546524048, 'metric_for_logging': {'pred_time': 6.546992164182749e-06}, 'val_loss': 1.5957358322840947, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54882}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 54882, 'experiment_tag': 'exp', 'time_total_s': 12.799761056900024}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.803147796987033
SO2(0)的mse=10.076649161813236
SO2(0)的mae=1.6465833463311337
SO2(0)的mar=0.19036923432648495
总共花费的时间为：66.14
曲靖市
1917A
3376A
3377A
[flaml.automl: 09-17 17:24:39] {2390} INFO - task = regression
[flaml.automl: 09-17 17:24:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:24:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:24:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:24:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:24:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:24:40] {3025} INFO - Estimated sufficient time budget=12028s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 17:24:40] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.9013,	best estimator xgboost's best error=3.9013
[flaml.automl: 09-17 17:24:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:24:42] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.0070,	best estimator xgboost's best error=2.0070
[flaml.automl: 09-17 17:24:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:24:44] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.0070,	best estimator xgboost's best error=2.0070
[flaml.automl: 09-17 17:24:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:24:54] {3072} INFO -  at 14.7s,	estimator xgboost's best error=2.0070,	best estimator xgboost's best error=2.0070
[flaml.automl: 09-17 17:24:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:24:55] {3072} INFO -  at 15.9s,	estimator xgboost's best error=1.6352,	best estimator xgboost's best error=1.6352
[flaml.automl: 09-17 17:24:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:24:56] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.6352,	best estimator xgboost's best error=1.6352
[flaml.automl: 09-17 17:24:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:24:58] {3072} INFO -  at 19.2s,	estimator xgboost's best error=1.5016,	best estimator xgboost's best error=1.5016
[flaml.automl: 09-17 17:24:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:25:01] {3072} INFO -  at 21.9s,	estimator xgboost's best error=1.5016,	best estimator xgboost's best error=1.5016
[flaml.automl: 09-17 17:25:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:25:02] {3072} INFO -  at 23.5s,	estimator xgboost's best error=1.5016,	best estimator xgboost's best error=1.5016
[flaml.automl: 09-17 17:25:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:25:05] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.4841,	best estimator xgboost's best error=1.4841
[flaml.automl: 09-17 17:25:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:25:07] {3072} INFO -  at 28.2s,	estimator xgboost's best error=1.4841,	best estimator xgboost's best error=1.4841
[flaml.automl: 09-17 17:25:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:25:08] {3072} INFO -  at 29.4s,	estimator xgboost's best error=1.4841,	best estimator xgboost's best error=1.4841
[flaml.automl: 09-17 17:25:08] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:25:22] {3072} INFO -  at 43.0s,	estimator xgboost's best error=1.4841,	best estimator xgboost's best error=1.4841
[flaml.automl: 09-17 17:25:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:25:27] {3072} INFO -  at 48.2s,	estimator xgboost's best error=1.4764,	best estimator xgboost's best error=1.4764
[flaml.automl: 09-17 17:25:27] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 17:25:30] {3072} INFO -  at 50.9s,	estimator xgboost's best error=1.4764,	best estimator xgboost's best error=1.4764
[flaml.automl: 09-17 17:25:30] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 17:25:39] {3072} INFO -  at 59.8s,	estimator xgboost's best error=1.4755,	best estimator xgboost's best error=1.4755
[flaml.automl: 09-17 17:25:48] {3335} INFO - retrain xgboost for 8.9s
[flaml.automl: 09-17 17:25:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7044297373129306, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.14630823584071445,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=0.5617648129523786, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04940360069037374, scale_pos_weight=1,
             subsample=0.8558352664247904, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:25:48] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:25:48] {2637} INFO - Time taken to find the best model: 59.83592486381531
[flaml.automl: 09-17 17:25:48] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 17, 'max_leaves': 9, 'min_child_weight': 0.5617648129523786, 'learning_rate': 0.14630823584071445, 'subsample': 0.8558352664247904, 'colsample_bylevel': 0.7044297373129306, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04940360069037374}
SO2(0)最佳损失：-0.4754975350741144
SO2(0)最好结果：{'pred_time': 1.0699862030611376e-05, 'wall_clock_time': 59.83592486381531, 'metric_for_logging': {'pred_time': 1.0699862030611376e-05}, 'val_loss': 1.4754975350741144, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 9, 'min_child_weight': 0.5617648129523786, 'learning_rate': 0.14630823584071445, 'subsample': 0.8558352664247904, 'colsample_bylevel': 0.7044297373129306, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.04940360069037374}, 'config/n_estimators': 17, 'config/max_leaves': 9, 'config/min_child_weight': 0.5617648129523786, 'config/learning_rate': 0.14630823584071445, 'config/subsample': 0.8558352664247904, 'config/colsample_bylevel': 0.7044297373129306, 'config/colsample_bytree': 0.92006762344246, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.04940360069037374, 'experiment_tag': 'exp', 'time_total_s': 8.947884559631348}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7044297373129306, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.14630823584071445,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=0.5617648129523786, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.04940360069037374, scale_pos_weight=1,
             subsample=0.8558352664247904, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.10407125720451504
SO2(0)的mse=7.831701856056985
SO2(0)的mae=1.5439063940871398
SO2(0)的mar=0.21898487254098578
总共花费的时间为：69.26
咸阳市
1918A
1919A
1920A
3525A
[flaml.automl: 09-17 17:38:04] {2390} INFO - task = regression
[flaml.automl: 09-17 17:38:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:38:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:38:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:38:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:38:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:38:06] {3025} INFO - Estimated sufficient time budget=83914s. Estimated necessary time budget=84s.
[flaml.automl: 09-17 17:38:06] {3072} INFO -  at 2.2s,	estimator xgboost's best error=5.1176,	best estimator xgboost's best error=5.1176
[flaml.automl: 09-17 17:38:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:38:10] {3072} INFO -  at 5.9s,	estimator xgboost's best error=2.3153,	best estimator xgboost's best error=2.3153
[flaml.automl: 09-17 17:38:10] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:38:12] {3072} INFO -  at 8.1s,	estimator xgboost's best error=2.3153,	best estimator xgboost's best error=2.3153
[flaml.automl: 09-17 17:38:12] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:38:18] {3072} INFO -  at 13.7s,	estimator xgboost's best error=2.3153,	best estimator xgboost's best error=2.3153
[flaml.automl: 09-17 17:38:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:38:20] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.1733,	best estimator xgboost's best error=1.1733
[flaml.automl: 09-17 17:38:20] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:38:23] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.9853,	best estimator xgboost's best error=0.9853
[flaml.automl: 09-17 17:38:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:38:25] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.8978,	best estimator xgboost's best error=0.8978
[flaml.automl: 09-17 17:38:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:38:29] {3072} INFO -  at 25.0s,	estimator xgboost's best error=0.8978,	best estimator xgboost's best error=0.8978
[flaml.automl: 09-17 17:38:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:38:31] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.8978,	best estimator xgboost's best error=0.8978
[flaml.automl: 09-17 17:38:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:38:35] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.8978,	best estimator xgboost's best error=0.8978
[flaml.automl: 09-17 17:38:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:38:37] {3072} INFO -  at 32.7s,	estimator xgboost's best error=0.8978,	best estimator xgboost's best error=0.8978
[flaml.automl: 09-17 17:38:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:38:40] {3072} INFO -  at 35.6s,	estimator xgboost's best error=0.8959,	best estimator xgboost's best error=0.8959
[flaml.automl: 09-17 17:38:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:38:42] {3072} INFO -  at 37.8s,	estimator xgboost's best error=0.8959,	best estimator xgboost's best error=0.8959
[flaml.automl: 09-17 17:38:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:38:51] {3072} INFO -  at 46.6s,	estimator xgboost's best error=0.7075,	best estimator xgboost's best error=0.7075
[flaml.automl: 09-17 17:38:51] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 17:39:04] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.6920,	best estimator xgboost's best error=0.6920
[flaml.automl: 09-17 17:39:16] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-17 17:39:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 17:39:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:39:16] {2637} INFO - Time taken to find the best model: 59.438380002975464
[flaml.automl: 09-17 17:39:16] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 42418}
SO2(0)最佳损失：0.3079540533790863
SO2(0)最好结果：{'pred_time': 8.588177094684095e-06, 'wall_clock_time': 59.438380002975464, 'metric_for_logging': {'pred_time': 8.588177094684095e-06}, 'val_loss': 0.6920459466209137, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 42418}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 42418, 'experiment_tag': 'exp', 'time_total_s': 12.794394969940186}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8928156468443162
SO2(0)的mse=1.152298827263508
SO2(0)的mae=0.6771393240875294
SO2(0)的mar=0.0825743663935789
总共花费的时间为：73.06
铜川市
1922A
1923A
[flaml.automl: 09-17 17:45:54] {2390} INFO - task = regression
[flaml.automl: 09-17 17:45:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:45:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:45:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:45:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:45:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:45:55] {3025} INFO - Estimated sufficient time budget=12141s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 17:45:55] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.9223,	best estimator xgboost's best error=4.9223
[flaml.automl: 09-17 17:45:55] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:45:57] {3072} INFO -  at 3.2s,	estimator xgboost's best error=2.7058,	best estimator xgboost's best error=2.7058
[flaml.automl: 09-17 17:45:57] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:45:59] {3072} INFO -  at 4.4s,	estimator xgboost's best error=2.7058,	best estimator xgboost's best error=2.7058
[flaml.automl: 09-17 17:45:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:46:12] {3072} INFO -  at 18.0s,	estimator xgboost's best error=2.7058,	best estimator xgboost's best error=2.7058
[flaml.automl: 09-17 17:46:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:46:14] {3072} INFO -  at 19.9s,	estimator xgboost's best error=1.7216,	best estimator xgboost's best error=1.7216
[flaml.automl: 09-17 17:46:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:46:17] {3072} INFO -  at 22.4s,	estimator xgboost's best error=1.5544,	best estimator xgboost's best error=1.5544
[flaml.automl: 09-17 17:46:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:46:19] {3072} INFO -  at 25.0s,	estimator xgboost's best error=1.5003,	best estimator xgboost's best error=1.5003
[flaml.automl: 09-17 17:46:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:46:22] {3072} INFO -  at 28.2s,	estimator xgboost's best error=1.5003,	best estimator xgboost's best error=1.5003
[flaml.automl: 09-17 17:46:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:46:25] {3072} INFO -  at 30.9s,	estimator xgboost's best error=1.5003,	best estimator xgboost's best error=1.5003
[flaml.automl: 09-17 17:46:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:46:30] {3072} INFO -  at 35.8s,	estimator xgboost's best error=1.4065,	best estimator xgboost's best error=1.4065
[flaml.automl: 09-17 17:46:30] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:46:32] {3072} INFO -  at 38.0s,	estimator xgboost's best error=1.4065,	best estimator xgboost's best error=1.4065
[flaml.automl: 09-17 17:46:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:46:34] {3072} INFO -  at 39.9s,	estimator xgboost's best error=1.4065,	best estimator xgboost's best error=1.4065
[flaml.automl: 09-17 17:46:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:46:54] {3072} INFO -  at 59.5s,	estimator xgboost's best error=1.4065,	best estimator xgboost's best error=1.4065
[flaml.automl: 09-17 17:46:59] {3335} INFO - retrain xgboost for 5.5s
[flaml.automl: 09-17 17:46:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 17:46:59] {2636} INFO - fit succeeded
[flaml.automl: 09-17 17:46:59] {2637} INFO - Time taken to find the best model: 35.832223653793335
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-0.4065138233551746
SO2(0)最好结果：{'pred_time': 3.019802799674863e-05, 'wall_clock_time': 35.832223653793335, 'metric_for_logging': {'pred_time': 3.019802799674863e-05}, 'val_loss': 1.4065138233551746, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 4.946456432342529}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6865575570106379
SO2(0)的mse=5.402083067660343
SO2(0)的mae=1.341939692401847
SO2(0)的mar=0.18901665602475728
总共花费的时间为：65.51
延安市
1926A
1927A
1929A
3652A
[flaml.automl: 09-17 17:59:04] {2390} INFO - task = regression
[flaml.automl: 09-17 17:59:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 17:59:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 17:59:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 17:59:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 17:59:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 17:59:05] {3025} INFO - Estimated sufficient time budget=49806s. Estimated necessary time budget=50s.
[flaml.automl: 09-17 17:59:05] {3072} INFO -  at 1.4s,	estimator xgboost's best error=2.6649,	best estimator xgboost's best error=2.6649
[flaml.automl: 09-17 17:59:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 17:59:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.2585,	best estimator xgboost's best error=1.2585
[flaml.automl: 09-17 17:59:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 17:59:09] {3072} INFO -  at 4.6s,	estimator xgboost's best error=1.2585,	best estimator xgboost's best error=1.2585
[flaml.automl: 09-17 17:59:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 17:59:15] {3072} INFO -  at 10.9s,	estimator xgboost's best error=1.2585,	best estimator xgboost's best error=1.2585
[flaml.automl: 09-17 17:59:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 17:59:16] {3072} INFO -  at 12.1s,	estimator xgboost's best error=0.8649,	best estimator xgboost's best error=0.8649
[flaml.automl: 09-17 17:59:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 17:59:18] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.8649,	best estimator xgboost's best error=0.8649
[flaml.automl: 09-17 17:59:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 17:59:19] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.7325,	best estimator xgboost's best error=0.7325
[flaml.automl: 09-17 17:59:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 17:59:22] {3072} INFO -  at 18.0s,	estimator xgboost's best error=0.7325,	best estimator xgboost's best error=0.7325
[flaml.automl: 09-17 17:59:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 17:59:24] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.7325,	best estimator xgboost's best error=0.7325
[flaml.automl: 09-17 17:59:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 17:59:27] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.7325,	best estimator xgboost's best error=0.7325
[flaml.automl: 09-17 17:59:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 17:59:28] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.7325,	best estimator xgboost's best error=0.7325
[flaml.automl: 09-17 17:59:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 17:59:30] {3072} INFO -  at 25.8s,	estimator xgboost's best error=0.7325,	best estimator xgboost's best error=0.7325
[flaml.automl: 09-17 17:59:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 17:59:31] {3072} INFO -  at 27.0s,	estimator xgboost's best error=0.7325,	best estimator xgboost's best error=0.7325
[flaml.automl: 09-17 17:59:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 17:59:38] {3072} INFO -  at 33.9s,	estimator xgboost's best error=0.6704,	best estimator xgboost's best error=0.6704
[flaml.automl: 09-17 17:59:38] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 17:59:51] {3072} INFO -  at 46.7s,	estimator xgboost's best error=0.6704,	best estimator xgboost's best error=0.6704
[flaml.automl: 09-17 17:59:51] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 17:59:55] {3072} INFO -  at 51.1s,	estimator xgboost's best error=0.6704,	best estimator xgboost's best error=0.6704
[flaml.automl: 09-17 17:59:55] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 18:00:04] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.6704,	best estimator xgboost's best error=0.6704
[flaml.automl: 09-17 18:00:11] {3335} INFO - retrain xgboost for 7.0s
[flaml.automl: 09-17 18:00:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:00:11] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:00:11] {2637} INFO - Time taken to find the best model: 33.93766379356384
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41769}
SO2(0)最佳损失：0.3295581767472765
SO2(0)最好结果：{'pred_time': 8.566016937216414e-06, 'wall_clock_time': 33.93766379356384, 'metric_for_logging': {'pred_time': 8.566016937216414e-06}, 'val_loss': 0.6704418232527235, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41769}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 41769, 'experiment_tag': 'exp', 'time_total_s': 6.964205741882324}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7921666861171186
SO2(0)的mse=1.3493696332616603
SO2(0)的mae=0.6649208544344085
SO2(0)的mar=0.15266254564821855
总共花费的时间为：67.19
宝鸡市
1930A
1931A
1932A
1933A
1934A
1935A
1937A
[flaml.automl: 09-17 18:22:05] {2390} INFO - task = regression
[flaml.automl: 09-17 18:22:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:22:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:22:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:22:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:22:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:22:08] {3025} INFO - Estimated sufficient time budget=208406s. Estimated necessary time budget=208s.
[flaml.automl: 09-17 18:22:08] {3072} INFO -  at 3.2s,	estimator xgboost's best error=3.6597,	best estimator xgboost's best error=3.6597
[flaml.automl: 09-17 18:22:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:22:11] {3072} INFO -  at 6.4s,	estimator xgboost's best error=2.6389,	best estimator xgboost's best error=2.6389
[flaml.automl: 09-17 18:22:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:22:14] {3072} INFO -  at 9.5s,	estimator xgboost's best error=2.6389,	best estimator xgboost's best error=2.6389
[flaml.automl: 09-17 18:22:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:22:17] {3072} INFO -  at 12.2s,	estimator xgboost's best error=2.6389,	best estimator xgboost's best error=2.6389
[flaml.automl: 09-17 18:22:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:22:19] {3072} INFO -  at 15.0s,	estimator xgboost's best error=1.1538,	best estimator xgboost's best error=1.1538
[flaml.automl: 09-17 18:22:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:22:22] {3072} INFO -  at 17.7s,	estimator xgboost's best error=1.1538,	best estimator xgboost's best error=1.1538
[flaml.automl: 09-17 18:22:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:22:25] {3072} INFO -  at 20.3s,	estimator xgboost's best error=1.1538,	best estimator xgboost's best error=1.1538
[flaml.automl: 09-17 18:22:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:22:26] {3072} INFO -  at 21.8s,	estimator xgboost's best error=1.1538,	best estimator xgboost's best error=1.1538
[flaml.automl: 09-17 18:22:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:22:28] {3072} INFO -  at 24.0s,	estimator xgboost's best error=1.1538,	best estimator xgboost's best error=1.1538
[flaml.automl: 09-17 18:22:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:22:31] {3072} INFO -  at 26.9s,	estimator xgboost's best error=1.1538,	best estimator xgboost's best error=1.1538
[flaml.automl: 09-17 18:22:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:22:35] {3072} INFO -  at 31.1s,	estimator xgboost's best error=1.1538,	best estimator xgboost's best error=1.1538
[flaml.automl: 09-17 18:22:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:22:38] {3072} INFO -  at 34.1s,	estimator xgboost's best error=1.0817,	best estimator xgboost's best error=1.0817
[flaml.automl: 09-17 18:22:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:22:41] {3072} INFO -  at 36.7s,	estimator xgboost's best error=1.0817,	best estimator xgboost's best error=1.0817
[flaml.automl: 09-17 18:22:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:22:50] {3072} INFO -  at 45.3s,	estimator xgboost's best error=0.9107,	best estimator xgboost's best error=0.9107
[flaml.automl: 09-17 18:22:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 18:23:01] {3072} INFO -  at 57.1s,	estimator xgboost's best error=0.8994,	best estimator xgboost's best error=0.8994
[flaml.automl: 09-17 18:23:09] {3335} INFO - retrain xgboost for 7.9s
[flaml.automl: 09-17 18:23:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628243, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609471, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:23:09] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:23:09] {2637} INFO - Time taken to find the best model: 57.14864230155945
[flaml.automl: 09-17 18:23:09] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628243, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609471, 'FLAML_sample_size': 74285}
SO2(0)最佳损失：0.10057021693741985
SO2(0)最好结果：{'pred_time': 4.8665524337838806e-06, 'wall_clock_time': 57.14864230155945, 'metric_for_logging': {'pred_time': 4.8665524337838806e-06}, 'val_loss': 0.8994297830625801, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628243, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609471, 'FLAML_sample_size': 74285}, 'config/n_estimators': 19, 'config/max_leaves': 7, 'config/min_child_weight': 6.532042959250319, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8835265175628243, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.10065328594609471, 'config/FLAML_sample_size': 74285, 'experiment_tag': 'exp', 'time_total_s': 11.848358631134033}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628243, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609471, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.48360915137811733
SO2(0)的mse=7.5830593112114215
SO2(0)的mae=0.9419417890178092
SO2(0)的mar=0.13258257828638184
总共花费的时间为：66.50
渭南市
1938A
1939A
1941A
[flaml.automl: 09-17 18:33:08] {2390} INFO - task = regression
[flaml.automl: 09-17 18:33:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:33:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:33:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:33:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:33:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:33:09] {3025} INFO - Estimated sufficient time budget=12010s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 18:33:09] {3072} INFO -  at 1.3s,	estimator xgboost's best error=5.8260,	best estimator xgboost's best error=5.8260
[flaml.automl: 09-17 18:33:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:33:11] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.6847,	best estimator xgboost's best error=2.6847
[flaml.automl: 09-17 18:33:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:33:13] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.6847,	best estimator xgboost's best error=2.6847
[flaml.automl: 09-17 18:33:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:33:23] {3072} INFO -  at 14.7s,	estimator xgboost's best error=2.6847,	best estimator xgboost's best error=2.6847
[flaml.automl: 09-17 18:33:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:33:24] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.7297,	best estimator xgboost's best error=1.7297
[flaml.automl: 09-17 18:33:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:33:25] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.7297,	best estimator xgboost's best error=1.7297
[flaml.automl: 09-17 18:33:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:33:27] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.4156,	best estimator xgboost's best error=1.4156
[flaml.automl: 09-17 18:33:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:33:30] {3072} INFO -  at 21.8s,	estimator xgboost's best error=1.4156,	best estimator xgboost's best error=1.4156
[flaml.automl: 09-17 18:33:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:33:31] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.4156,	best estimator xgboost's best error=1.4156
[flaml.automl: 09-17 18:33:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:33:34] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.4156,	best estimator xgboost's best error=1.4156
[flaml.automl: 09-17 18:33:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:33:36] {3072} INFO -  at 27.9s,	estimator xgboost's best error=1.4156,	best estimator xgboost's best error=1.4156
[flaml.automl: 09-17 18:33:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:33:37] {3072} INFO -  at 29.1s,	estimator xgboost's best error=1.4156,	best estimator xgboost's best error=1.4156
[flaml.automl: 09-17 18:33:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:33:44] {3072} INFO -  at 36.1s,	estimator xgboost's best error=1.4029,	best estimator xgboost's best error=1.4029
[flaml.automl: 09-17 18:33:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:33:56] {3072} INFO -  at 48.5s,	estimator xgboost's best error=1.3662,	best estimator xgboost's best error=1.3662
[flaml.automl: 09-17 18:34:09] {3335} INFO - retrain xgboost for 12.5s
[flaml.automl: 09-17 18:34:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:34:09] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:34:09] {2637} INFO - Time taken to find the best model: 48.488035440444946
[flaml.automl: 09-17 18:34:09] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
SO2(0)最佳损失：-0.36624553664164106
SO2(0)最好结果：{'pred_time': 1.3689604102139267e-05, 'wall_clock_time': 48.488035440444946, 'metric_for_logging': {'pred_time': 1.3689604102139267e-05}, 'val_loss': 1.366245536641641, 'training_iteration': 1, 'config': {'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 30, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 12.36966061592102}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6985804631957506
SO2(0)的mse=5.671871576449228
SO2(0)的mae=1.3804263312798655
SO2(0)的mar=0.14036223247757249
总共花费的时间为：61.58
金昌市
金昌市没有数据
嘉峪关市
3248A
[flaml.automl: 09-17 18:37:32] {2390} INFO - task = regression
[flaml.automl: 09-17 18:37:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:37:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:37:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:37:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:37:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:37:35] {3025} INFO - Estimated sufficient time budget=21824s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 18:37:35] {3072} INFO -  at 2.2s,	estimator xgboost's best error=10.2382,	best estimator xgboost's best error=10.2382
[flaml.automl: 09-17 18:37:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:37:38] {3072} INFO -  at 5.7s,	estimator xgboost's best error=6.1523,	best estimator xgboost's best error=6.1523
[flaml.automl: 09-17 18:37:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:37:40] {3072} INFO -  at 7.9s,	estimator xgboost's best error=6.1523,	best estimator xgboost's best error=6.1523
[flaml.automl: 09-17 18:37:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:37:54] {3072} INFO -  at 21.2s,	estimator xgboost's best error=6.1523,	best estimator xgboost's best error=6.1523
[flaml.automl: 09-17 18:37:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:37:56] {3072} INFO -  at 23.3s,	estimator xgboost's best error=4.4882,	best estimator xgboost's best error=4.4882
[flaml.automl: 09-17 18:37:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:37:59] {3072} INFO -  at 26.1s,	estimator xgboost's best error=4.3891,	best estimator xgboost's best error=4.3891
[flaml.automl: 09-17 18:37:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:38:02] {3072} INFO -  at 29.2s,	estimator xgboost's best error=4.3598,	best estimator xgboost's best error=4.3598
[flaml.automl: 09-17 18:38:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:38:06] {3072} INFO -  at 33.7s,	estimator xgboost's best error=4.3598,	best estimator xgboost's best error=4.3598
[flaml.automl: 09-17 18:38:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:38:09] {3072} INFO -  at 36.7s,	estimator xgboost's best error=4.3598,	best estimator xgboost's best error=4.3598
[flaml.automl: 09-17 18:38:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:38:14] {3072} INFO -  at 41.6s,	estimator xgboost's best error=4.3598,	best estimator xgboost's best error=4.3598
[flaml.automl: 09-17 18:38:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:38:16] {3072} INFO -  at 43.8s,	estimator xgboost's best error=4.2284,	best estimator xgboost's best error=4.2284
[flaml.automl: 09-17 18:38:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:38:17] {3072} INFO -  at 44.9s,	estimator xgboost's best error=4.2284,	best estimator xgboost's best error=4.2284
[flaml.automl: 09-17 18:38:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:38:23] {3072} INFO -  at 50.7s,	estimator xgboost's best error=4.2284,	best estimator xgboost's best error=4.2284
[flaml.automl: 09-17 18:38:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:38:26] {3072} INFO -  at 54.0s,	estimator xgboost's best error=4.2209,	best estimator xgboost's best error=4.2209
[flaml.automl: 09-17 18:38:26] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 18:38:28] {3072} INFO -  at 56.0s,	estimator xgboost's best error=4.2209,	best estimator xgboost's best error=4.2209
[flaml.automl: 09-17 18:38:28] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 18:38:32] {3072} INFO -  at 59.7s,	estimator xgboost's best error=4.2209,	best estimator xgboost's best error=4.2209
[flaml.automl: 09-17 18:38:35] {3335} INFO - retrain xgboost for 3.1s
[flaml.automl: 09-17 18:38:35] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9821088952594532, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.319252434838056,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=30.72218267001315, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35085403513777375, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 18:38:35] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:38:35] {2637} INFO - Time taken to find the best model: 53.99311351776123
[flaml.automl: 09-17 18:38:35] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 4, 'min_child_weight': 30.72218267001315, 'learning_rate': 0.319252434838056, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9821088952594532, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35085403513777375}
SO2(0)最佳损失：-3.2209486103456886
SO2(0)最好结果：{'pred_time': 3.2887398947232934e-05, 'wall_clock_time': 53.99311351776123, 'metric_for_logging': {'pred_time': 3.2887398947232934e-05}, 'val_loss': 4.220948610345689, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 4, 'min_child_weight': 30.72218267001315, 'learning_rate': 0.319252434838056, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9821088952594532, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35085403513777375}, 'config/n_estimators': 13, 'config/max_leaves': 4, 'config/min_child_weight': 30.72218267001315, 'config/learning_rate': 0.319252434838056, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9821088952594532, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35085403513777375, 'experiment_tag': 'exp', 'time_total_s': 3.2645139694213867}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9821088952594532, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.319252434838056,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=30.72218267001315, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35085403513777375, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.18792821658029635
SO2(0)的mse=42.34288340649045
SO2(0)的mae=4.105321556082685
SO2(0)的mar=0.3409606226091176
总共花费的时间为：63.02
石嘴山市
1947A
1949A
1950A
3520A
3521A
[flaml.automl: 09-17 18:54:19] {2390} INFO - task = regression
[flaml.automl: 09-17 18:54:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 18:54:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 18:54:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 18:54:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 18:54:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 18:54:22] {3025} INFO - Estimated sufficient time budget=122977s. Estimated necessary time budget=123s.
[flaml.automl: 09-17 18:54:22] {3072} INFO -  at 2.6s,	estimator xgboost's best error=11.9729,	best estimator xgboost's best error=11.9729
[flaml.automl: 09-17 18:54:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 18:54:25] {3072} INFO -  at 6.5s,	estimator xgboost's best error=6.8770,	best estimator xgboost's best error=6.8770
[flaml.automl: 09-17 18:54:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 18:54:28] {3072} INFO -  at 8.8s,	estimator xgboost's best error=6.8770,	best estimator xgboost's best error=6.8770
[flaml.automl: 09-17 18:54:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 18:54:32] {3072} INFO -  at 12.9s,	estimator xgboost's best error=6.8770,	best estimator xgboost's best error=6.8770
[flaml.automl: 09-17 18:54:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 18:54:34] {3072} INFO -  at 15.1s,	estimator xgboost's best error=6.1211,	best estimator xgboost's best error=6.1211
[flaml.automl: 09-17 18:54:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 18:54:37] {3072} INFO -  at 17.9s,	estimator xgboost's best error=6.1211,	best estimator xgboost's best error=6.1211
[flaml.automl: 09-17 18:54:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 18:54:40] {3072} INFO -  at 20.7s,	estimator xgboost's best error=5.7648,	best estimator xgboost's best error=5.7648
[flaml.automl: 09-17 18:54:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 18:54:43] {3072} INFO -  at 24.1s,	estimator xgboost's best error=5.7648,	best estimator xgboost's best error=5.7648
[flaml.automl: 09-17 18:54:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 18:54:46] {3072} INFO -  at 27.0s,	estimator xgboost's best error=5.7648,	best estimator xgboost's best error=5.7648
[flaml.automl: 09-17 18:54:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 18:54:48] {3072} INFO -  at 29.5s,	estimator xgboost's best error=5.7648,	best estimator xgboost's best error=5.7648
[flaml.automl: 09-17 18:54:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 18:54:51] {3072} INFO -  at 31.9s,	estimator xgboost's best error=5.7648,	best estimator xgboost's best error=5.7648
[flaml.automl: 09-17 18:54:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 18:54:55] {3072} INFO -  at 36.0s,	estimator xgboost's best error=5.7648,	best estimator xgboost's best error=5.7648
[flaml.automl: 09-17 18:54:55] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 18:54:58] {3072} INFO -  at 39.1s,	estimator xgboost's best error=5.7648,	best estimator xgboost's best error=5.7648
[flaml.automl: 09-17 18:54:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 18:55:16] {3072} INFO -  at 57.3s,	estimator xgboost's best error=5.7648,	best estimator xgboost's best error=5.7648
[flaml.automl: 09-17 18:55:19] {3335} INFO - retrain xgboost for 3.1s
[flaml.automl: 09-17 18:55:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 18:55:19] {2636} INFO - fit succeeded
[flaml.automl: 09-17 18:55:19] {2637} INFO - Time taken to find the best model: 20.664633750915527
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：-4.764787360754508
SO2(0)最好结果：{'pred_time': 1.1548353854998082e-05, 'wall_clock_time': 20.664633750915527, 'metric_for_logging': {'pred_time': 1.1548353854998082e-05}, 'val_loss': 5.764787360754508, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557, 'FLAML_sample_size': 10000}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 2.741692066192627}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.21294823060033174
SO2(0)的mse=100.26983242457875
SO2(0)的mae=5.720004927691812
SO2(0)的mar=0.38985151363231063
总共花费的时间为：61.37
克拉玛依市
1951A
1955A
3612A
[flaml.automl: 09-17 19:05:00] {2390} INFO - task = regression
[flaml.automl: 09-17 19:05:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:05:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:05:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:05:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:05:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:05:02] {3025} INFO - Estimated sufficient time budget=21770s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 19:05:02] {3072} INFO -  at 2.3s,	estimator xgboost's best error=4.1734,	best estimator xgboost's best error=4.1734
[flaml.automl: 09-17 19:05:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:05:06] {3072} INFO -  at 6.2s,	estimator xgboost's best error=1.8856,	best estimator xgboost's best error=1.8856
[flaml.automl: 09-17 19:05:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:05:08] {3072} INFO -  at 8.2s,	estimator xgboost's best error=1.8856,	best estimator xgboost's best error=1.8856
[flaml.automl: 09-17 19:05:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:05:26] {3072} INFO -  at 26.4s,	estimator xgboost's best error=1.8856,	best estimator xgboost's best error=1.8856
[flaml.automl: 09-17 19:05:26] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:05:30] {3072} INFO -  at 29.7s,	estimator xgboost's best error=0.8479,	best estimator xgboost's best error=0.8479
[flaml.automl: 09-17 19:05:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:05:34] {3072} INFO -  at 34.2s,	estimator xgboost's best error=0.6640,	best estimator xgboost's best error=0.6640
[flaml.automl: 09-17 19:05:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:05:39] {3072} INFO -  at 38.6s,	estimator xgboost's best error=0.6233,	best estimator xgboost's best error=0.6233
[flaml.automl: 09-17 19:05:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:05:46] {3072} INFO -  at 46.3s,	estimator xgboost's best error=0.6233,	best estimator xgboost's best error=0.6233
[flaml.automl: 09-17 19:05:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:05:50] {3072} INFO -  at 49.6s,	estimator xgboost's best error=0.6233,	best estimator xgboost's best error=0.6233
[flaml.automl: 09-17 19:05:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:05:55] {3072} INFO -  at 55.1s,	estimator xgboost's best error=0.6233,	best estimator xgboost's best error=0.6233
[flaml.automl: 09-17 19:05:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:05:58] {3072} INFO -  at 57.8s,	estimator xgboost's best error=0.6233,	best estimator xgboost's best error=0.6233
[flaml.automl: 09-17 19:06:01] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-17 19:06:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:06:01] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:06:01] {2637} INFO - Time taken to find the best model: 38.596027851104736
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：0.3766910929200996
SO2(0)最好结果：{'pred_time': 2.77937222749759e-05, 'wall_clock_time': 38.596027851104736, 'metric_for_logging': {'pred_time': 2.77937222749759e-05}, 'val_loss': 0.6233089070799004, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.406026601791382}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8612619264940709
SO2(0)的mse=0.8007866291769266
SO2(0)的mae=0.6204988875641273
SO2(0)的mar=0.11060365385995391
总共花费的时间为：61.33
巴音郭楞州
1957A
1958A
[flaml.automl: 09-17 19:12:27] {2390} INFO - task = regression
[flaml.automl: 09-17 19:12:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:12:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:12:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:12:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:12:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:12:28] {3025} INFO - Estimated sufficient time budget=12536s. Estimated necessary time budget=13s.
[flaml.automl: 09-17 19:12:28] {3072} INFO -  at 1.3s,	estimator xgboost's best error=2.3329,	best estimator xgboost's best error=2.3329
[flaml.automl: 09-17 19:12:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:12:31] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.0872,	best estimator xgboost's best error=1.0872
[flaml.automl: 09-17 19:12:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:12:32] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.0872,	best estimator xgboost's best error=1.0872
[flaml.automl: 09-17 19:12:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:12:41] {3072} INFO -  at 14.0s,	estimator xgboost's best error=1.0872,	best estimator xgboost's best error=1.0872
[flaml.automl: 09-17 19:12:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:12:42] {3072} INFO -  at 15.2s,	estimator xgboost's best error=0.5170,	best estimator xgboost's best error=0.5170
[flaml.automl: 09-17 19:12:42] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:12:44] {3072} INFO -  at 16.8s,	estimator xgboost's best error=0.4476,	best estimator xgboost's best error=0.4476
[flaml.automl: 09-17 19:12:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:12:46] {3072} INFO -  at 18.4s,	estimator xgboost's best error=0.3934,	best estimator xgboost's best error=0.3934
[flaml.automl: 09-17 19:12:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:12:48] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.3934,	best estimator xgboost's best error=0.3934
[flaml.automl: 09-17 19:12:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:12:50] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.3735,	best estimator xgboost's best error=0.3735
[flaml.automl: 09-17 19:12:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:12:54] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.3735,	best estimator xgboost's best error=0.3735
[flaml.automl: 09-17 19:12:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:12:57] {3072} INFO -  at 30.1s,	estimator xgboost's best error=0.3735,	best estimator xgboost's best error=0.3735
[flaml.automl: 09-17 19:12:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:12:59] {3072} INFO -  at 32.3s,	estimator xgboost's best error=0.3735,	best estimator xgboost's best error=0.3735
[flaml.automl: 09-17 19:12:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:13:11] {3072} INFO -  at 44.2s,	estimator xgboost's best error=0.3648,	best estimator xgboost's best error=0.3648
[flaml.automl: 09-17 19:13:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:13:27] {3072} INFO -  at 59.4s,	estimator xgboost's best error=0.3527,	best estimator xgboost's best error=0.3527
[flaml.automl: 09-17 19:13:47] {3335} INFO - retrain xgboost for 20.4s
[flaml.automl: 09-17 19:13:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 19:13:47] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:13:47] {2637} INFO - Time taken to find the best model: 59.37903714179993
[flaml.automl: 09-17 19:13:47] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}
SO2(0)最佳损失：0.6473132038593292
SO2(0)最好结果：{'pred_time': 3.064775466918945e-05, 'wall_clock_time': 59.37903714179993, 'metric_for_logging': {'pred_time': 3.064775466918945e-05}, 'val_loss': 0.3526867961406708, 'training_iteration': 1, 'config': {'n_estimators': 27, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}, 'config/n_estimators': 27, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'experiment_tag': 'exp', 'time_total_s': 15.217118978500366}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=27, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8047653143000926
SO2(0)的mse=0.3858490412166424
SO2(0)的mae=0.3588164035925512
SO2(0)的mar=0.10332266953600372
总共花费的时间为：80.23
信阳市
2054A
2064A
2065A
2066A
[flaml.automl: 09-17 19:26:11] {2390} INFO - task = regression
[flaml.automl: 09-17 19:26:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:26:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:26:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:26:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:26:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:26:13] {3025} INFO - Estimated sufficient time budget=89487s. Estimated necessary time budget=89s.
[flaml.automl: 09-17 19:26:13] {3072} INFO -  at 2.2s,	estimator xgboost's best error=4.1428,	best estimator xgboost's best error=4.1428
[flaml.automl: 09-17 19:26:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:26:16] {3072} INFO -  at 5.7s,	estimator xgboost's best error=1.8622,	best estimator xgboost's best error=1.8622
[flaml.automl: 09-17 19:26:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:26:19] {3072} INFO -  at 8.0s,	estimator xgboost's best error=1.8622,	best estimator xgboost's best error=1.8622
[flaml.automl: 09-17 19:26:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:26:24] {3072} INFO -  at 13.5s,	estimator xgboost's best error=1.8622,	best estimator xgboost's best error=1.8622
[flaml.automl: 09-17 19:26:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:26:27] {3072} INFO -  at 16.7s,	estimator xgboost's best error=0.8696,	best estimator xgboost's best error=0.8696
[flaml.automl: 09-17 19:26:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:26:32] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.6253,	best estimator xgboost's best error=0.6253
[flaml.automl: 09-17 19:26:32] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:26:35] {3072} INFO -  at 24.7s,	estimator xgboost's best error=0.6253,	best estimator xgboost's best error=0.6253
[flaml.automl: 09-17 19:26:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:26:39] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.6253,	best estimator xgboost's best error=0.6253
[flaml.automl: 09-17 19:26:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:26:42] {3072} INFO -  at 31.2s,	estimator xgboost's best error=0.6253,	best estimator xgboost's best error=0.6253
[flaml.automl: 09-17 19:26:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:26:44] {3072} INFO -  at 33.5s,	estimator xgboost's best error=0.6253,	best estimator xgboost's best error=0.6253
[flaml.automl: 09-17 19:26:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:26:49] {3072} INFO -  at 38.0s,	estimator xgboost's best error=0.6253,	best estimator xgboost's best error=0.6253
[flaml.automl: 09-17 19:26:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:26:52] {3072} INFO -  at 41.3s,	estimator xgboost's best error=0.6253,	best estimator xgboost's best error=0.6253
[flaml.automl: 09-17 19:26:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:27:04] {3072} INFO -  at 53.5s,	estimator xgboost's best error=0.5244,	best estimator xgboost's best error=0.5244
[flaml.automl: 09-17 19:27:16] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-17 19:27:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 19:27:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:27:16] {2637} INFO - Time taken to find the best model: 53.479193687438965
[flaml.automl: 09-17 19:27:16] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 44056}
SO2(0)最佳损失：0.4755742581305551
SO2(0)最好结果：{'pred_time': 1.2781300575904596e-05, 'wall_clock_time': 53.479193687438965, 'metric_for_logging': {'pred_time': 1.2781300575904596e-05}, 'val_loss': 0.5244257418694449, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 44056}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 44056, 'experiment_tag': 'exp', 'time_total_s': 12.134865999221802}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8315561198847415
SO2(0)的mse=0.6461048092404532
SO2(0)的mae=0.5142033804427175
SO2(0)的mar=0.07866080564382429
总共花费的时间为：66.33
周口市
2067A
2068A
2069A
2070A
[flaml.automl: 09-17 19:39:26] {2390} INFO - task = regression
[flaml.automl: 09-17 19:39:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:39:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:39:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:39:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:39:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:39:28] {3025} INFO - Estimated sufficient time budget=77276s. Estimated necessary time budget=77s.
[flaml.automl: 09-17 19:39:28] {3072} INFO -  at 1.9s,	estimator xgboost's best error=4.7428,	best estimator xgboost's best error=4.7428
[flaml.automl: 09-17 19:39:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:39:31] {3072} INFO -  at 5.2s,	estimator xgboost's best error=2.2402,	best estimator xgboost's best error=2.2402
[flaml.automl: 09-17 19:39:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:39:33] {3072} INFO -  at 7.4s,	estimator xgboost's best error=2.2402,	best estimator xgboost's best error=2.2402
[flaml.automl: 09-17 19:39:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:39:39] {3072} INFO -  at 12.8s,	estimator xgboost's best error=2.2402,	best estimator xgboost's best error=2.2402
[flaml.automl: 09-17 19:39:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:39:41] {3072} INFO -  at 15.6s,	estimator xgboost's best error=1.4019,	best estimator xgboost's best error=1.4019
[flaml.automl: 09-17 19:39:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:39:46] {3072} INFO -  at 19.8s,	estimator xgboost's best error=1.2543,	best estimator xgboost's best error=1.2543
[flaml.automl: 09-17 19:39:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:39:49] {3072} INFO -  at 23.6s,	estimator xgboost's best error=1.2543,	best estimator xgboost's best error=1.2543
[flaml.automl: 09-17 19:39:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:39:53] {3072} INFO -  at 27.2s,	estimator xgboost's best error=1.2543,	best estimator xgboost's best error=1.2543
[flaml.automl: 09-17 19:39:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:39:56] {3072} INFO -  at 29.9s,	estimator xgboost's best error=1.2543,	best estimator xgboost's best error=1.2543
[flaml.automl: 09-17 19:39:56] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:39:58] {3072} INFO -  at 32.2s,	estimator xgboost's best error=1.2543,	best estimator xgboost's best error=1.2543
[flaml.automl: 09-17 19:39:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:40:03] {3072} INFO -  at 36.7s,	estimator xgboost's best error=1.2543,	best estimator xgboost's best error=1.2543
[flaml.automl: 09-17 19:40:03] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:40:06] {3072} INFO -  at 39.9s,	estimator xgboost's best error=1.2543,	best estimator xgboost's best error=1.2543
[flaml.automl: 09-17 19:40:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:40:24] {3072} INFO -  at 58.1s,	estimator xgboost's best error=1.0742,	best estimator xgboost's best error=1.0742
[flaml.automl: 09-17 19:40:38] {3335} INFO - retrain xgboost for 13.7s
[flaml.automl: 09-17 19:40:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 19:40:38] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:40:38] {2637} INFO - Time taken to find the best model: 58.0720739364624
[flaml.automl: 09-17 19:40:38] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 44166}
SO2(0)最佳损失：-0.07423079497171003
SO2(0)最好结果：{'pred_time': 2.5401099872666834e-05, 'wall_clock_time': 58.0720739364624, 'metric_for_logging': {'pred_time': 2.5401099872666834e-05}, 'val_loss': 1.07423079497171, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 44166}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 44166, 'experiment_tag': 'exp', 'time_total_s': 18.131764888763428}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7521510875372598
SO2(0)的mse=3.101096001447205
SO2(0)的mae=1.057261483583585
SO2(0)的mar=0.1420283129688761
总共花费的时间为：72.54
漳州市
2075A
2920A
3216A
3530A
[flaml.automl: 09-17 19:53:08] {2390} INFO - task = regression
[flaml.automl: 09-17 19:53:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 19:53:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 19:53:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 19:53:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 19:53:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 19:53:10] {3025} INFO - Estimated sufficient time budget=86073s. Estimated necessary time budget=86s.
[flaml.automl: 09-17 19:53:10] {3072} INFO -  at 2.3s,	estimator xgboost's best error=3.1876,	best estimator xgboost's best error=3.1876
[flaml.automl: 09-17 19:53:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 19:53:14] {3072} INFO -  at 6.1s,	estimator xgboost's best error=1.6075,	best estimator xgboost's best error=1.6075
[flaml.automl: 09-17 19:53:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 19:53:16] {3072} INFO -  at 8.4s,	estimator xgboost's best error=1.6075,	best estimator xgboost's best error=1.6075
[flaml.automl: 09-17 19:53:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 19:53:21] {3072} INFO -  at 13.6s,	estimator xgboost's best error=1.6075,	best estimator xgboost's best error=1.6075
[flaml.automl: 09-17 19:53:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 19:53:23] {3072} INFO -  at 15.7s,	estimator xgboost's best error=1.1152,	best estimator xgboost's best error=1.1152
[flaml.automl: 09-17 19:53:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 19:53:26] {3072} INFO -  at 18.5s,	estimator xgboost's best error=1.1152,	best estimator xgboost's best error=1.1152
[flaml.automl: 09-17 19:53:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 19:53:29] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.9931,	best estimator xgboost's best error=0.9931
[flaml.automl: 09-17 19:53:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 19:53:32] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.9931,	best estimator xgboost's best error=0.9931
[flaml.automl: 09-17 19:53:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 19:53:35] {3072} INFO -  at 27.5s,	estimator xgboost's best error=0.9931,	best estimator xgboost's best error=0.9931
[flaml.automl: 09-17 19:53:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 19:53:38] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.9931,	best estimator xgboost's best error=0.9931
[flaml.automl: 09-17 19:53:38] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 19:53:41] {3072} INFO -  at 33.7s,	estimator xgboost's best error=0.9931,	best estimator xgboost's best error=0.9931
[flaml.automl: 09-17 19:53:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 19:53:46] {3072} INFO -  at 38.5s,	estimator xgboost's best error=0.9931,	best estimator xgboost's best error=0.9931
[flaml.automl: 09-17 19:53:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 19:53:49] {3072} INFO -  at 41.8s,	estimator xgboost's best error=0.9931,	best estimator xgboost's best error=0.9931
[flaml.automl: 09-17 19:53:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 19:54:03] {3072} INFO -  at 55.3s,	estimator xgboost's best error=0.9653,	best estimator xgboost's best error=0.9653
[flaml.automl: 09-17 19:54:16] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 19:54:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 19:54:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 19:54:16] {2637} INFO - Time taken to find the best model: 55.26125407218933
[flaml.automl: 09-17 19:54:16] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41458}
SO2(0)最佳损失：0.03470947363642107
SO2(0)最好结果：{'pred_time': 1.849412969842194e-05, 'wall_clock_time': 55.26125407218933, 'metric_for_logging': {'pred_time': 1.849412969842194e-05}, 'val_loss': 0.9652905263635789, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 41458}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 41458, 'experiment_tag': 'exp', 'time_total_s': 13.489529132843018}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6325089693027933
SO2(0)的mse=2.1948717964938735
SO2(0)的mae=0.9518505678812166
SO2(0)的mar=0.22907356623137262
总共花费的时间为：68.86
晋城市
2160A
2161A
2162A
2163A
3620A
[flaml.automl: 09-17 20:09:34] {2390} INFO - task = regression
[flaml.automl: 09-17 20:09:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:09:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:09:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:09:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:09:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:09:35] {3025} INFO - Estimated sufficient time budget=66257s. Estimated necessary time budget=66s.
[flaml.automl: 09-17 20:09:35] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.7534,	best estimator xgboost's best error=4.7534
[flaml.automl: 09-17 20:09:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:09:37] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.3470,	best estimator xgboost's best error=2.3470
[flaml.automl: 09-17 20:09:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:09:38] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.3470,	best estimator xgboost's best error=2.3470
[flaml.automl: 09-17 20:09:38] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:09:43] {3072} INFO -  at 9.5s,	estimator xgboost's best error=2.3470,	best estimator xgboost's best error=2.3470
[flaml.automl: 09-17 20:09:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:09:44] {3072} INFO -  at 10.6s,	estimator xgboost's best error=1.6978,	best estimator xgboost's best error=1.6978
[flaml.automl: 09-17 20:09:44] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:09:46] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.6978,	best estimator xgboost's best error=1.6978
[flaml.automl: 09-17 20:09:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:09:48] {3072} INFO -  at 13.9s,	estimator xgboost's best error=1.3490,	best estimator xgboost's best error=1.3490
[flaml.automl: 09-17 20:09:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:09:50] {3072} INFO -  at 16.6s,	estimator xgboost's best error=1.3490,	best estimator xgboost's best error=1.3490
[flaml.automl: 09-17 20:09:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:09:52] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.3490,	best estimator xgboost's best error=1.3490
[flaml.automl: 09-17 20:09:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:09:55] {3072} INFO -  at 21.2s,	estimator xgboost's best error=1.3490,	best estimator xgboost's best error=1.3490
[flaml.automl: 09-17 20:09:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:09:56] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.3490,	best estimator xgboost's best error=1.3490
[flaml.automl: 09-17 20:09:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:09:58] {3072} INFO -  at 24.4s,	estimator xgboost's best error=1.3490,	best estimator xgboost's best error=1.3490
[flaml.automl: 09-17 20:09:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:09:59] {3072} INFO -  at 25.6s,	estimator xgboost's best error=1.3490,	best estimator xgboost's best error=1.3490
[flaml.automl: 09-17 20:09:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:10:06] {3072} INFO -  at 32.7s,	estimator xgboost's best error=1.2494,	best estimator xgboost's best error=1.2494
[flaml.automl: 09-17 20:10:06] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:10:19] {3072} INFO -  at 45.5s,	estimator xgboost's best error=1.2245,	best estimator xgboost's best error=1.2245
[flaml.automl: 09-17 20:10:19] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 20:10:32] {3072} INFO -  at 58.4s,	estimator xgboost's best error=1.2245,	best estimator xgboost's best error=1.2245
[flaml.automl: 09-17 20:10:56] {3335} INFO - retrain xgboost for 23.4s
[flaml.automl: 09-17 20:10:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:10:56] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:10:56] {2637} INFO - Time taken to find the best model: 45.47079038619995
[flaml.automl: 09-17 20:10:56] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54763}
SO2(0)最佳损失：-0.22454119578873222
SO2(0)最好结果：{'pred_time': 6.734022079380007e-06, 'wall_clock_time': 45.47079038619995, 'metric_for_logging': {'pred_time': 6.734022079380007e-06}, 'val_loss': 1.2245411957887322, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 54763}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 54763, 'experiment_tag': 'exp', 'time_total_s': 12.81012773513794}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8329929256468429
SO2(0)的mse=3.956944542946367
SO2(0)的mae=1.2172238349103484
SO2(0)的mar=0.1970264592732217
总共花费的时间为：82.85
朔州市
2166A
2167A
2168A
2169A
2170A
3571A
[flaml.automl: 09-17 20:29:42] {2390} INFO - task = regression
[flaml.automl: 09-17 20:29:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:29:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:29:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:29:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:29:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:29:43] {3025} INFO - Estimated sufficient time budget=77886s. Estimated necessary time budget=78s.
[flaml.automl: 09-17 20:29:43] {3072} INFO -  at 1.5s,	estimator xgboost's best error=7.8409,	best estimator xgboost's best error=7.8409
[flaml.automl: 09-17 20:29:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:29:45] {3072} INFO -  at 3.6s,	estimator xgboost's best error=4.0025,	best estimator xgboost's best error=4.0025
[flaml.automl: 09-17 20:29:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:29:47] {3072} INFO -  at 4.8s,	estimator xgboost's best error=4.0025,	best estimator xgboost's best error=4.0025
[flaml.automl: 09-17 20:29:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:29:50] {3072} INFO -  at 8.6s,	estimator xgboost's best error=4.0025,	best estimator xgboost's best error=4.0025
[flaml.automl: 09-17 20:29:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:29:52] {3072} INFO -  at 9.8s,	estimator xgboost's best error=3.3111,	best estimator xgboost's best error=3.3111
[flaml.automl: 09-17 20:29:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:29:53] {3072} INFO -  at 11.3s,	estimator xgboost's best error=3.3111,	best estimator xgboost's best error=3.3111
[flaml.automl: 09-17 20:29:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:29:55] {3072} INFO -  at 13.0s,	estimator xgboost's best error=2.7940,	best estimator xgboost's best error=2.7940
[flaml.automl: 09-17 20:29:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:29:58] {3072} INFO -  at 15.7s,	estimator xgboost's best error=2.7940,	best estimator xgboost's best error=2.7940
[flaml.automl: 09-17 20:29:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:29:59] {3072} INFO -  at 17.3s,	estimator xgboost's best error=2.7940,	best estimator xgboost's best error=2.7940
[flaml.automl: 09-17 20:29:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:30:02] {3072} INFO -  at 20.3s,	estimator xgboost's best error=2.7910,	best estimator xgboost's best error=2.7910
[flaml.automl: 09-17 20:30:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:30:04] {3072} INFO -  at 22.0s,	estimator xgboost's best error=2.7910,	best estimator xgboost's best error=2.7910
[flaml.automl: 09-17 20:30:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:30:05] {3072} INFO -  at 23.2s,	estimator xgboost's best error=2.7910,	best estimator xgboost's best error=2.7910
[flaml.automl: 09-17 20:30:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:30:07] {3072} INFO -  at 25.2s,	estimator xgboost's best error=2.7910,	best estimator xgboost's best error=2.7910
[flaml.automl: 09-17 20:30:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:30:10] {3072} INFO -  at 27.7s,	estimator xgboost's best error=2.7910,	best estimator xgboost's best error=2.7910
[flaml.automl: 09-17 20:30:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:30:13] {3072} INFO -  at 30.8s,	estimator xgboost's best error=2.7644,	best estimator xgboost's best error=2.7644
[flaml.automl: 09-17 20:30:13] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 20:30:18] {3072} INFO -  at 35.7s,	estimator xgboost's best error=2.7644,	best estimator xgboost's best error=2.7644
[flaml.automl: 09-17 20:30:18] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 20:30:19] {3072} INFO -  at 37.6s,	estimator xgboost's best error=2.7644,	best estimator xgboost's best error=2.7644
[flaml.automl: 09-17 20:30:19] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 20:30:21] {3072} INFO -  at 39.3s,	estimator xgboost's best error=2.7644,	best estimator xgboost's best error=2.7644
[flaml.automl: 09-17 20:30:21] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 20:30:29] {3072} INFO -  at 46.7s,	estimator xgboost's best error=2.7402,	best estimator xgboost's best error=2.7402
[flaml.automl: 09-17 20:30:29] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-17 20:30:31] {3072} INFO -  at 49.2s,	estimator xgboost's best error=2.7402,	best estimator xgboost's best error=2.7402
[flaml.automl: 09-17 20:30:31] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-17 20:30:40] {3072} INFO -  at 58.3s,	estimator xgboost's best error=2.7402,	best estimator xgboost's best error=2.7402
[flaml.automl: 09-17 20:30:49] {3335} INFO - retrain xgboost for 9.2s
[flaml.automl: 09-17 20:30:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.607218895874472, colsample_bynode=1,
             colsample_bytree=0.9815981571650833, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.891244238278541,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.0846240478525794, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.09799917014136278, scale_pos_weight=1,
             subsample=0.9736686748482157, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:30:49] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:30:49] {2637} INFO - Time taken to find the best model: 46.70072317123413
[flaml.automl: 09-17 20:30:49] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.0846240478525794, 'learning_rate': 0.891244238278541, 'subsample': 0.9736686748482157, 'colsample_bylevel': 0.607218895874472, 'colsample_bytree': 0.9815981571650833, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.09799917014136278, 'FLAML_sample_size': 65145}
SO2(0)最佳损失：-1.740188139350165
SO2(0)最好结果：{'pred_time': 5.701856564414948e-06, 'wall_clock_time': 46.70072317123413, 'metric_for_logging': {'pred_time': 5.701856564414948e-06}, 'val_loss': 2.740188139350165, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 16, 'min_child_weight': 0.0846240478525794, 'learning_rate': 0.891244238278541, 'subsample': 0.9736686748482157, 'colsample_bylevel': 0.607218895874472, 'colsample_bytree': 0.9815981571650833, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.09799917014136278, 'FLAML_sample_size': 65145}, 'config/n_estimators': 8, 'config/max_leaves': 16, 'config/min_child_weight': 0.0846240478525794, 'config/learning_rate': 0.891244238278541, 'config/subsample': 0.9736686748482157, 'config/colsample_bylevel': 0.607218895874472, 'config/colsample_bytree': 0.9815981571650833, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.09799917014136278, 'config/FLAML_sample_size': 65145, 'experiment_tag': 'exp', 'time_total_s': 7.4070725440979}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.607218895874472, colsample_bynode=1,
             colsample_bytree=0.9815981571650833, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.891244238278541,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.0846240478525794, missing=nan,
             monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.09799917014136278, scale_pos_weight=1,
             subsample=0.9736686748482157, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6480386204512465
SO2(0)的mse=21.287022116681396
SO2(0)的mae=2.6960055190029095
SO2(0)的mar=0.2344082875014272
总共花费的时间为：68.60
晋中市
2171A
2174A
2865A
[flaml.automl: 09-17 20:40:13] {2390} INFO - task = regression
[flaml.automl: 09-17 20:40:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:40:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:40:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:40:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:40:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:40:15] {3025} INFO - Estimated sufficient time budget=12167s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 20:40:15] {3072} INFO -  at 1.4s,	estimator xgboost's best error=8.4816,	best estimator xgboost's best error=8.4816
[flaml.automl: 09-17 20:40:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:40:17] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.1577,	best estimator xgboost's best error=4.1577
[flaml.automl: 09-17 20:40:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:40:18] {3072} INFO -  at 4.7s,	estimator xgboost's best error=4.1577,	best estimator xgboost's best error=4.1577
[flaml.automl: 09-17 20:40:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:40:28] {3072} INFO -  at 14.7s,	estimator xgboost's best error=4.1577,	best estimator xgboost's best error=4.1577
[flaml.automl: 09-17 20:40:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:40:29] {3072} INFO -  at 15.9s,	estimator xgboost's best error=3.0319,	best estimator xgboost's best error=3.0319
[flaml.automl: 09-17 20:40:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:40:31] {3072} INFO -  at 17.5s,	estimator xgboost's best error=3.0319,	best estimator xgboost's best error=3.0319
[flaml.automl: 09-17 20:40:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:40:32] {3072} INFO -  at 19.2s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-17 20:40:32] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:40:35] {3072} INFO -  at 21.9s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-17 20:40:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:40:37] {3072} INFO -  at 23.5s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-17 20:40:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:40:40] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-17 20:40:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:40:41] {3072} INFO -  at 28.1s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-17 20:40:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:40:42] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.4749,	best estimator xgboost's best error=2.4749
[flaml.automl: 09-17 20:40:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:40:49] {3072} INFO -  at 36.3s,	estimator xgboost's best error=2.3596,	best estimator xgboost's best error=2.3596
[flaml.automl: 09-17 20:40:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:41:06] {3072} INFO -  at 52.4s,	estimator xgboost's best error=2.2632,	best estimator xgboost's best error=2.2632
[flaml.automl: 09-17 20:41:29] {3335} INFO - retrain xgboost for 23.3s
[flaml.automl: 09-17 20:41:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 20:41:29] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:41:29] {2637} INFO - Time taken to find the best model: 52.41899633407593
[flaml.automl: 09-17 20:41:29] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
SO2(0)最佳损失：-1.263165560101446
SO2(0)最好结果：{'pred_time': 2.5400393096999888e-05, 'wall_clock_time': 52.41899633407593, 'metric_for_logging': {'pred_time': 2.5400393096999888e-05}, 'val_loss': 2.263165560101446, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 16.143872022628784}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8408727728189505
SO2(0)的mse=16.588931804958612
SO2(0)的mae=2.254405749967305
SO2(0)的mar=0.17095620981970966
总共花费的时间为：76.36
运城市
2175A
2178A
2179A
3670A
[flaml.automl: 09-17 20:53:47] {2390} INFO - task = regression
[flaml.automl: 09-17 20:53:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 20:53:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 20:53:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 20:53:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 20:53:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 20:53:48] {3025} INFO - Estimated sufficient time budget=51645s. Estimated necessary time budget=52s.
[flaml.automl: 09-17 20:53:48] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.4062,	best estimator xgboost's best error=5.4062
[flaml.automl: 09-17 20:53:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 20:53:50] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.5238,	best estimator xgboost's best error=2.5238
[flaml.automl: 09-17 20:53:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 20:53:51] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.5238,	best estimator xgboost's best error=2.5238
[flaml.automl: 09-17 20:53:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 20:53:57] {3072} INFO -  at 10.5s,	estimator xgboost's best error=2.5238,	best estimator xgboost's best error=2.5238
[flaml.automl: 09-17 20:53:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 20:53:58] {3072} INFO -  at 11.6s,	estimator xgboost's best error=1.8255,	best estimator xgboost's best error=1.8255
[flaml.automl: 09-17 20:53:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 20:54:00] {3072} INFO -  at 13.2s,	estimator xgboost's best error=1.6552,	best estimator xgboost's best error=1.6552
[flaml.automl: 09-17 20:54:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 20:54:01] {3072} INFO -  at 14.8s,	estimator xgboost's best error=1.5087,	best estimator xgboost's best error=1.5087
[flaml.automl: 09-17 20:54:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 20:54:04] {3072} INFO -  at 17.5s,	estimator xgboost's best error=1.5087,	best estimator xgboost's best error=1.5087
[flaml.automl: 09-17 20:54:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 20:54:06] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.5087,	best estimator xgboost's best error=1.5087
[flaml.automl: 09-17 20:54:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 20:54:10] {3072} INFO -  at 23.6s,	estimator xgboost's best error=1.3388,	best estimator xgboost's best error=1.3388
[flaml.automl: 09-17 20:54:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 20:54:13] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.3388,	best estimator xgboost's best error=1.3388
[flaml.automl: 09-17 20:54:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 20:54:15] {3072} INFO -  at 28.5s,	estimator xgboost's best error=1.3388,	best estimator xgboost's best error=1.3388
[flaml.automl: 09-17 20:54:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 20:54:17] {3072} INFO -  at 30.6s,	estimator xgboost's best error=1.3388,	best estimator xgboost's best error=1.3388
[flaml.automl: 09-17 20:54:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 20:54:20] {3072} INFO -  at 33.6s,	estimator xgboost's best error=1.3388,	best estimator xgboost's best error=1.3388
[flaml.automl: 09-17 20:54:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 20:54:26] {3072} INFO -  at 39.3s,	estimator xgboost's best error=1.3360,	best estimator xgboost's best error=1.3360
[flaml.automl: 09-17 20:54:26] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 20:54:35] {3072} INFO -  at 48.4s,	estimator xgboost's best error=1.3360,	best estimator xgboost's best error=1.3360
[flaml.automl: 09-17 20:54:35] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-17 20:54:38] {3072} INFO -  at 51.9s,	estimator xgboost's best error=1.3172,	best estimator xgboost's best error=1.3172
[flaml.automl: 09-17 20:54:38] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-17 20:54:42] {3072} INFO -  at 55.0s,	estimator xgboost's best error=1.3172,	best estimator xgboost's best error=1.3172
[flaml.automl: 09-17 20:54:42] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-17 20:54:46] {3072} INFO -  at 59.1s,	estimator xgboost's best error=1.3172,	best estimator xgboost's best error=1.3172
[flaml.automl: 09-17 20:54:49] {3335} INFO - retrain xgboost for 3.4s
[flaml.automl: 09-17 20:54:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 20:54:49] {2636} INFO - fit succeeded
[flaml.automl: 09-17 20:54:49] {2637} INFO - Time taken to find the best model: 51.91525197029114
[flaml.automl: 09-17 20:54:49] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 43184}
SO2(0)最佳损失：-0.3172211808993384
SO2(0)最好结果：{'pred_time': 1.5653702437219384e-05, 'wall_clock_time': 51.91525197029114, 'metric_for_logging': {'pred_time': 1.5653702437219384e-05}, 'val_loss': 1.3172211808993384, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 43184}, 'config/n_estimators': 7, 'config/max_leaves': 4, 'config/min_child_weight': 0.031257867156220746, 'config/learning_rate': 0.3417584818146937, 'config/subsample': 0.8188658116452241, 'config/colsample_bylevel': 0.9718368347363348, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.001562580531679857, 'config/reg_lambda': 0.36612402315272025, 'config/FLAML_sample_size': 43184, 'experiment_tag': 'exp', 'time_total_s': 3.4759602546691895}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7139047518125938
SO2(0)的mse=5.830327155565197
SO2(0)的mae=1.310190104177279
SO2(0)的mar=0.14041418015169801
总共花费的时间为：63.27
忻州市
2182A
3208A
[flaml.automl: 09-17 21:01:23] {2390} INFO - task = regression
[flaml.automl: 09-17 21:01:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:01:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:01:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:01:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:01:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:01:26] {3025} INFO - Estimated sufficient time budget=26216s. Estimated necessary time budget=26s.
[flaml.automl: 09-17 21:01:26] {3072} INFO -  at 2.7s,	estimator xgboost's best error=8.9216,	best estimator xgboost's best error=8.9216
[flaml.automl: 09-17 21:01:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:01:31] {3072} INFO -  at 7.8s,	estimator xgboost's best error=4.2133,	best estimator xgboost's best error=4.2133
[flaml.automl: 09-17 21:01:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:01:34] {3072} INFO -  at 10.9s,	estimator xgboost's best error=4.2133,	best estimator xgboost's best error=4.2133
[flaml.automl: 09-17 21:01:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:01:56] {3072} INFO -  at 33.2s,	estimator xgboost's best error=4.2133,	best estimator xgboost's best error=4.2133
[flaml.automl: 09-17 21:01:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:01:59] {3072} INFO -  at 35.9s,	estimator xgboost's best error=2.7928,	best estimator xgboost's best error=2.7928
[flaml.automl: 09-17 21:01:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:02:03] {3072} INFO -  at 39.9s,	estimator xgboost's best error=2.6502,	best estimator xgboost's best error=2.6502
[flaml.automl: 09-17 21:02:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:02:07] {3072} INFO -  at 44.0s,	estimator xgboost's best error=2.4759,	best estimator xgboost's best error=2.4759
[flaml.automl: 09-17 21:02:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:02:14] {3072} INFO -  at 50.7s,	estimator xgboost's best error=2.4759,	best estimator xgboost's best error=2.4759
[flaml.automl: 09-17 21:02:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:02:21] {3072} INFO -  at 57.6s,	estimator xgboost's best error=2.4759,	best estimator xgboost's best error=2.4759
[flaml.automl: 09-17 21:02:26] {3335} INFO - retrain xgboost for 4.8s
[flaml.automl: 09-17 21:02:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:02:26] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:02:26] {2637} INFO - Time taken to find the best model: 43.95299983024597
[flaml.automl: 09-17 21:02:26] {2648} WARNING - Time taken to find the best model is 73% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：-1.4759408105057452
SO2(0)最好结果：{'pred_time': 4.613929324679904e-05, 'wall_clock_time': 43.95299983024597, 'metric_for_logging': {'pred_time': 4.613929324679904e-05}, 'val_loss': 2.475940810505745, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.036006212234497}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.4445931666612456
SO2(0)的mse=14.696625830274893
SO2(0)的mae=2.2596778670063724
SO2(0)的mar=0.15830433822558485
总共花费的时间为：62.96
吕梁市
2183A
2867A
[flaml.automl: 09-17 21:09:01] {2390} INFO - task = regression
[flaml.automl: 09-17 21:09:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:09:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:09:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:09:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:09:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:09:03] {3025} INFO - Estimated sufficient time budget=20483s. Estimated necessary time budget=20s.
[flaml.automl: 09-17 21:09:03] {3072} INFO -  at 2.2s,	estimator xgboost's best error=6.6342,	best estimator xgboost's best error=6.6342
[flaml.automl: 09-17 21:09:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:09:07] {3072} INFO -  at 5.9s,	estimator xgboost's best error=3.1976,	best estimator xgboost's best error=3.1976
[flaml.automl: 09-17 21:09:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:09:09] {3072} INFO -  at 7.7s,	estimator xgboost's best error=3.1976,	best estimator xgboost's best error=3.1976
[flaml.automl: 09-17 21:09:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:09:25] {3072} INFO -  at 24.1s,	estimator xgboost's best error=3.1976,	best estimator xgboost's best error=3.1976
[flaml.automl: 09-17 21:09:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:09:27] {3072} INFO -  at 26.2s,	estimator xgboost's best error=2.2151,	best estimator xgboost's best error=2.2151
[flaml.automl: 09-17 21:09:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:09:30] {3072} INFO -  at 28.6s,	estimator xgboost's best error=2.0014,	best estimator xgboost's best error=2.0014
[flaml.automl: 09-17 21:09:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:09:33] {3072} INFO -  at 31.6s,	estimator xgboost's best error=1.8804,	best estimator xgboost's best error=1.8804
[flaml.automl: 09-17 21:09:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:09:37] {3072} INFO -  at 36.4s,	estimator xgboost's best error=1.8804,	best estimator xgboost's best error=1.8804
[flaml.automl: 09-17 21:09:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:09:40] {3072} INFO -  at 39.2s,	estimator xgboost's best error=1.8804,	best estimator xgboost's best error=1.8804
[flaml.automl: 09-17 21:09:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:09:48] {3072} INFO -  at 47.1s,	estimator xgboost's best error=1.6660,	best estimator xgboost's best error=1.6660
[flaml.automl: 09-17 21:09:48] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:09:53] {3072} INFO -  at 51.7s,	estimator xgboost's best error=1.6660,	best estimator xgboost's best error=1.6660
[flaml.automl: 09-17 21:09:53] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:09:56] {3072} INFO -  at 55.0s,	estimator xgboost's best error=1.6660,	best estimator xgboost's best error=1.6660
[flaml.automl: 09-17 21:10:05] {3335} INFO - retrain xgboost for 8.7s
[flaml.automl: 09-17 21:10:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 21:10:05] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:10:05] {2637} INFO - Time taken to find the best model: 47.11067295074463
[flaml.automl: 09-17 21:10:05] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-0.6660040921624215
SO2(0)最好结果：{'pred_time': 4.5628751411532027e-05, 'wall_clock_time': 47.11067295074463, 'metric_for_logging': {'pred_time': 4.5628751411532027e-05}, 'val_loss': 1.6660040921624215, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 7.884171724319458}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7669110636739727
SO2(0)的mse=7.378399765783002
SO2(0)的mae=1.628462253172164
SO2(0)的mar=0.18792378846834762
总共花费的时间为：64.32
乌海市
2188A
3284A
3621A
[flaml.automl: 09-17 21:19:57] {2390} INFO - task = regression
[flaml.automl: 09-17 21:19:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:19:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:19:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:19:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:19:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:19:58] {3025} INFO - Estimated sufficient time budget=12034s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:19:58] {3072} INFO -  at 1.3s,	estimator xgboost's best error=12.6608,	best estimator xgboost's best error=12.6608
[flaml.automl: 09-17 21:19:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:20:01] {3072} INFO -  at 3.5s,	estimator xgboost's best error=6.4916,	best estimator xgboost's best error=6.4916
[flaml.automl: 09-17 21:20:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:20:02] {3072} INFO -  at 4.7s,	estimator xgboost's best error=6.4916,	best estimator xgboost's best error=6.4916
[flaml.automl: 09-17 21:20:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:20:13] {3072} INFO -  at 16.2s,	estimator xgboost's best error=6.4916,	best estimator xgboost's best error=6.4916
[flaml.automl: 09-17 21:20:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:20:15] {3072} INFO -  at 18.3s,	estimator xgboost's best error=5.8861,	best estimator xgboost's best error=5.8861
[flaml.automl: 09-17 21:20:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:20:18] {3072} INFO -  at 21.2s,	estimator xgboost's best error=5.3222,	best estimator xgboost's best error=5.3222
[flaml.automl: 09-17 21:20:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:20:21] {3072} INFO -  at 24.3s,	estimator xgboost's best error=5.2014,	best estimator xgboost's best error=5.2014
[flaml.automl: 09-17 21:20:21] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:20:26] {3072} INFO -  at 29.2s,	estimator xgboost's best error=5.2014,	best estimator xgboost's best error=5.2014
[flaml.automl: 09-17 21:20:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:20:29] {3072} INFO -  at 32.2s,	estimator xgboost's best error=5.2014,	best estimator xgboost's best error=5.2014
[flaml.automl: 09-17 21:20:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:20:35] {3072} INFO -  at 37.9s,	estimator xgboost's best error=5.0015,	best estimator xgboost's best error=5.0015
[flaml.automl: 09-17 21:20:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:20:38] {3072} INFO -  at 41.0s,	estimator xgboost's best error=5.0015,	best estimator xgboost's best error=5.0015
[flaml.automl: 09-17 21:20:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:20:40] {3072} INFO -  at 43.0s,	estimator xgboost's best error=5.0015,	best estimator xgboost's best error=5.0015
[flaml.automl: 09-17 21:20:40] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:20:55] {3072} INFO -  at 58.3s,	estimator xgboost's best error=4.6445,	best estimator xgboost's best error=4.6445
[flaml.automl: 09-17 21:21:16] {3335} INFO - retrain xgboost for 20.2s
[flaml.automl: 09-17 21:21:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 21:21:16] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:21:16] {2637} INFO - Time taken to find the best model: 58.29748582839966
[flaml.automl: 09-17 21:21:16] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
SO2(0)最佳损失：-3.6444962547367963
SO2(0)最好结果：{'pred_time': 1.822772639497978e-05, 'wall_clock_time': 58.29748582839966, 'metric_for_logging': {'pred_time': 1.822772639497978e-05}, 'val_loss': 4.644496254736796, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 15.246071338653564}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7372386507674045
SO2(0)的mse=71.57241862572803
SO2(0)的mae=4.844425531648701
SO2(0)的mar=0.23789165397001116
总共花费的时间为：79.04
通辽市
2191A
3706A
3708A
[flaml.automl: 09-17 21:31:37] {2390} INFO - task = regression
[flaml.automl: 09-17 21:31:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:31:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:31:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:31:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:31:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:31:39] {3025} INFO - Estimated sufficient time budget=21944s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 21:31:39] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.0772,	best estimator xgboost's best error=5.0772
[flaml.automl: 09-17 21:31:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:31:43] {3072} INFO -  at 6.3s,	estimator xgboost's best error=2.5000,	best estimator xgboost's best error=2.5000
[flaml.automl: 09-17 21:31:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:31:46] {3072} INFO -  at 8.6s,	estimator xgboost's best error=2.5000,	best estimator xgboost's best error=2.5000
[flaml.automl: 09-17 21:31:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:32:04] {3072} INFO -  at 26.9s,	estimator xgboost's best error=2.5000,	best estimator xgboost's best error=2.5000
[flaml.automl: 09-17 21:32:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:32:06] {3072} INFO -  at 29.0s,	estimator xgboost's best error=1.7981,	best estimator xgboost's best error=1.7981
[flaml.automl: 09-17 21:32:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:32:09] {3072} INFO -  at 31.9s,	estimator xgboost's best error=1.7981,	best estimator xgboost's best error=1.7981
[flaml.automl: 09-17 21:32:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:32:12] {3072} INFO -  at 34.9s,	estimator xgboost's best error=1.5420,	best estimator xgboost's best error=1.5420
[flaml.automl: 09-17 21:32:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:32:17] {3072} INFO -  at 39.7s,	estimator xgboost's best error=1.5420,	best estimator xgboost's best error=1.5420
[flaml.automl: 09-17 21:32:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:32:20] {3072} INFO -  at 42.5s,	estimator xgboost's best error=1.5420,	best estimator xgboost's best error=1.5420
[flaml.automl: 09-17 21:32:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:32:25] {3072} INFO -  at 48.1s,	estimator xgboost's best error=1.5420,	best estimator xgboost's best error=1.5420
[flaml.automl: 09-17 21:32:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:32:28] {3072} INFO -  at 50.7s,	estimator xgboost's best error=1.5420,	best estimator xgboost's best error=1.5420
[flaml.automl: 09-17 21:32:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:32:30] {3072} INFO -  at 52.9s,	estimator xgboost's best error=1.5420,	best estimator xgboost's best error=1.5420
[flaml.automl: 09-17 21:32:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:32:36] {3072} INFO -  at 59.1s,	estimator xgboost's best error=1.5420,	best estimator xgboost's best error=1.5420
[flaml.automl: 09-17 21:32:39] {3335} INFO - retrain xgboost for 3.1s
[flaml.automl: 09-17 21:32:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 21:32:39] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:32:39] {2637} INFO - Time taken to find the best model: 34.92937922477722
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-0.5419949798913204
SO2(0)最好结果：{'pred_time': 2.1267839954265773e-05, 'wall_clock_time': 34.92937922477722, 'metric_for_logging': {'pred_time': 2.1267839954265773e-05}, 'val_loss': 1.5419949798913204, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 2.9880471229553223}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.4667276217860227
SO2(0)的mse=7.847658458098321
SO2(0)的mae=1.5746951946725942
SO2(0)的mar=0.20467085898906065
总共花费的时间为：62.83
呼伦贝尔市
2192A
[flaml.automl: 09-17 21:36:26] {2390} INFO - task = regression
[flaml.automl: 09-17 21:36:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:36:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:36:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:36:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:36:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:36:27] {3025} INFO - Estimated sufficient time budget=11980s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:36:27] {3072} INFO -  at 1.2s,	estimator xgboost's best error=3.0091,	best estimator xgboost's best error=3.0091
[flaml.automl: 09-17 21:36:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:36:29] {3072} INFO -  at 3.1s,	estimator xgboost's best error=1.5572,	best estimator xgboost's best error=1.5572
[flaml.automl: 09-17 21:36:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:36:31] {3072} INFO -  at 4.3s,	estimator xgboost's best error=1.5572,	best estimator xgboost's best error=1.5572
[flaml.automl: 09-17 21:36:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:36:35] {3072} INFO -  at 8.4s,	estimator xgboost's best error=1.5572,	best estimator xgboost's best error=1.5572
[flaml.automl: 09-17 21:36:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:36:36] {3072} INFO -  at 9.7s,	estimator xgboost's best error=0.5838,	best estimator xgboost's best error=0.5838
[flaml.automl: 09-17 21:36:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:36:40] {3072} INFO -  at 14.1s,	estimator xgboost's best error=0.4649,	best estimator xgboost's best error=0.4649
[flaml.automl: 09-17 21:36:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:36:45] {3072} INFO -  at 18.6s,	estimator xgboost's best error=0.4527,	best estimator xgboost's best error=0.4527
[flaml.automl: 09-17 21:36:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:36:51] {3072} INFO -  at 25.2s,	estimator xgboost's best error=0.4527,	best estimator xgboost's best error=0.4527
[flaml.automl: 09-17 21:36:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:36:56] {3072} INFO -  at 29.8s,	estimator xgboost's best error=0.4247,	best estimator xgboost's best error=0.4247
[flaml.automl: 09-17 21:36:56] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:37:03] {3072} INFO -  at 36.6s,	estimator xgboost's best error=0.4186,	best estimator xgboost's best error=0.4186
[flaml.automl: 09-17 21:37:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:37:06] {3072} INFO -  at 39.3s,	estimator xgboost's best error=0.4186,	best estimator xgboost's best error=0.4186
[flaml.automl: 09-17 21:37:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:37:25] {3072} INFO -  at 58.6s,	estimator xgboost's best error=0.4186,	best estimator xgboost's best error=0.4186
[flaml.automl: 09-17 21:37:32] {3335} INFO - retrain xgboost for 6.7s
[flaml.automl: 09-17 21:37:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 21:37:32] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:37:32] {2637} INFO - Time taken to find the best model: 36.63465094566345
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}
SO2(0)最佳损失：0.5813561585635408
SO2(0)最好结果：{'pred_time': 9.634597765884461e-05, 'wall_clock_time': 36.63465094566345, 'metric_for_logging': {'pred_time': 9.634597765884461e-05}, 'val_loss': 0.41864384143645916, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}, 'config/n_estimators': 6, 'config/max_leaves': 7, 'config/min_child_weight': 0.056902942069697794, 'config/learning_rate': 0.6286550459839113, 'config/subsample': 0.8219353140149964, 'config/colsample_bylevel': 0.6552426578592088, 'config/colsample_bytree': 0.8391963407651736, 'config/reg_alpha': 0.007347191482940116, 'config/reg_lambda': 9.546883854467293, 'experiment_tag': 'exp', 'time_total_s': 6.807871103286743}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.2730061759860539
SO2(0)的mse=0.4006024964082327
SO2(0)的mae=0.3821191271700058
SO2(0)的mar=0.07247310342515374
总共花费的时间为：65.55
巴彦淖尔市
2196A
[flaml.automl: 09-17 21:41:11] {2390} INFO - task = regression
[flaml.automl: 09-17 21:41:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:41:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:41:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:41:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:41:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:41:12] {3025} INFO - Estimated sufficient time budget=11931s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:41:12] {3072} INFO -  at 1.2s,	estimator xgboost's best error=4.7067,	best estimator xgboost's best error=4.7067
[flaml.automl: 09-17 21:41:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:41:14] {3072} INFO -  at 3.1s,	estimator xgboost's best error=2.6700,	best estimator xgboost's best error=2.6700
[flaml.automl: 09-17 21:41:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:41:15] {3072} INFO -  at 4.3s,	estimator xgboost's best error=2.6700,	best estimator xgboost's best error=2.6700
[flaml.automl: 09-17 21:41:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:41:25] {3072} INFO -  at 14.0s,	estimator xgboost's best error=2.6700,	best estimator xgboost's best error=2.6700
[flaml.automl: 09-17 21:41:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:41:27] {3072} INFO -  at 16.1s,	estimator xgboost's best error=1.7549,	best estimator xgboost's best error=1.7549
[flaml.automl: 09-17 21:41:27] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:41:30] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.7040,	best estimator xgboost's best error=1.7040
[flaml.automl: 09-17 21:41:30] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:41:33] {3072} INFO -  at 22.0s,	estimator xgboost's best error=1.6142,	best estimator xgboost's best error=1.6142
[flaml.automl: 09-17 21:41:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:41:37] {3072} INFO -  at 26.4s,	estimator xgboost's best error=1.6142,	best estimator xgboost's best error=1.6142
[flaml.automl: 09-17 21:41:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:41:40] {3072} INFO -  at 29.4s,	estimator xgboost's best error=1.6142,	best estimator xgboost's best error=1.6142
[flaml.automl: 09-17 21:41:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:41:45] {3072} INFO -  at 34.2s,	estimator xgboost's best error=1.6142,	best estimator xgboost's best error=1.6142
[flaml.automl: 09-17 21:41:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:41:49] {3072} INFO -  at 37.9s,	estimator xgboost's best error=1.5960,	best estimator xgboost's best error=1.5960
[flaml.automl: 09-17 21:41:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:41:52] {3072} INFO -  at 41.1s,	estimator xgboost's best error=1.5960,	best estimator xgboost's best error=1.5960
[flaml.automl: 09-17 21:41:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:42:08] {3072} INFO -  at 57.4s,	estimator xgboost's best error=1.5960,	best estimator xgboost's best error=1.5960
[flaml.automl: 09-17 21:42:13] {3335} INFO - retrain xgboost for 4.7s
[flaml.automl: 09-17 21:42:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8660549326361876, colsample_bynode=1,
             colsample_bytree=0.9004061282219729, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.590319080116618,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=53.34352431682733, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.778332386065348, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 21:42:13] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:42:13] {2637} INFO - Time taken to find the best model: 37.93419337272644
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 53.34352431682733, 'learning_rate': 0.590319080116618, 'subsample': 1.0, 'colsample_bylevel': 0.8660549326361876, 'colsample_bytree': 0.9004061282219729, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.778332386065348}
SO2(0)最佳损失：-0.5960097244359865
SO2(0)最好结果：{'pred_time': 7.89422134737941e-05, 'wall_clock_time': 37.93419337272644, 'metric_for_logging': {'pred_time': 7.89422134737941e-05}, 'val_loss': 1.5960097244359865, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 53.34352431682733, 'learning_rate': 0.590319080116618, 'subsample': 1.0, 'colsample_bylevel': 0.8660549326361876, 'colsample_bytree': 0.9004061282219729, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.778332386065348}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 53.34352431682733, 'config/learning_rate': 0.590319080116618, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8660549326361876, 'config/colsample_bytree': 0.9004061282219729, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.778332386065348, 'experiment_tag': 'exp', 'time_total_s': 3.7074248790740967}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8660549326361876, colsample_bynode=1,
             colsample_bytree=0.9004061282219729, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.590319080116618,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=53.34352431682733, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.778332386065348, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6229838538326982
SO2(0)的mse=5.862546427239556
SO2(0)的mae=1.4358331554034758
SO2(0)的mar=0.18235266598273484
总共花费的时间为：62.46
乌兰察布市
2197A
3285A
3421A
[flaml.automl: 09-17 21:51:57] {2390} INFO - task = regression
[flaml.automl: 09-17 21:51:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 21:51:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 21:51:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 21:51:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 21:51:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 21:51:58] {3025} INFO - Estimated sufficient time budget=12215s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 21:51:58] {3072} INFO -  at 1.3s,	estimator xgboost's best error=10.0418,	best estimator xgboost's best error=10.0418
[flaml.automl: 09-17 21:51:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 21:52:01] {3072} INFO -  at 4.2s,	estimator xgboost's best error=4.8057,	best estimator xgboost's best error=4.8057
[flaml.automl: 09-17 21:52:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 21:52:04] {3072} INFO -  at 6.5s,	estimator xgboost's best error=4.8057,	best estimator xgboost's best error=4.8057
[flaml.automl: 09-17 21:52:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 21:52:22] {3072} INFO -  at 25.2s,	estimator xgboost's best error=4.8057,	best estimator xgboost's best error=4.8057
[flaml.automl: 09-17 21:52:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 21:52:24] {3072} INFO -  at 27.3s,	estimator xgboost's best error=3.0987,	best estimator xgboost's best error=3.0987
[flaml.automl: 09-17 21:52:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 21:52:27] {3072} INFO -  at 30.3s,	estimator xgboost's best error=3.0987,	best estimator xgboost's best error=3.0987
[flaml.automl: 09-17 21:52:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 21:52:31] {3072} INFO -  at 33.5s,	estimator xgboost's best error=2.7187,	best estimator xgboost's best error=2.7187
[flaml.automl: 09-17 21:52:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 21:52:36] {3072} INFO -  at 38.5s,	estimator xgboost's best error=2.7187,	best estimator xgboost's best error=2.7187
[flaml.automl: 09-17 21:52:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 21:52:39] {3072} INFO -  at 41.6s,	estimator xgboost's best error=2.7187,	best estimator xgboost's best error=2.7187
[flaml.automl: 09-17 21:52:39] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 21:52:44] {3072} INFO -  at 47.3s,	estimator xgboost's best error=2.7187,	best estimator xgboost's best error=2.7187
[flaml.automl: 09-17 21:52:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 21:52:47] {3072} INFO -  at 50.0s,	estimator xgboost's best error=2.6925,	best estimator xgboost's best error=2.6925
[flaml.automl: 09-17 21:52:47] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 21:52:49] {3072} INFO -  at 52.1s,	estimator xgboost's best error=2.6925,	best estimator xgboost's best error=2.6925
[flaml.automl: 09-17 21:52:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 21:52:56] {3072} INFO -  at 59.3s,	estimator xgboost's best error=2.6158,	best estimator xgboost's best error=2.6158
[flaml.automl: 09-17 21:53:07] {3335} INFO - retrain xgboost for 10.9s
[flaml.automl: 09-17 21:53:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 21:53:07] {2636} INFO - fit succeeded
[flaml.automl: 09-17 21:53:07] {2637} INFO - Time taken to find the best model: 59.300193071365356
[flaml.automl: 09-17 21:53:07] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
SO2(0)最佳损失：-1.6157722350912294
SO2(0)最好结果：{'pred_time': 2.108788674300929e-05, 'wall_clock_time': 59.300193071365356, 'metric_for_logging': {'pred_time': 2.108788674300929e-05}, 'val_loss': 2.6157722350912294, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 7.199887990951538}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.568005086515126
SO2(0)的mse=13.635047945321059
SO2(0)的mae=2.438222454310469
SO2(0)的mar=0.18028667044887553
总共花费的时间为：70.79
阜新市
2207A
2208A
2209A
2210A
2211A
[flaml.automl: 09-17 22:09:21] {2390} INFO - task = regression
[flaml.automl: 09-17 22:09:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:09:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:09:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:09:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:09:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:09:25] {3025} INFO - Estimated sufficient time budget=184307s. Estimated necessary time budget=184s.
[flaml.automl: 09-17 22:09:25] {3072} INFO -  at 3.6s,	estimator xgboost's best error=10.0368,	best estimator xgboost's best error=10.0368
[flaml.automl: 09-17 22:09:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:09:29] {3072} INFO -  at 8.2s,	estimator xgboost's best error=6.4372,	best estimator xgboost's best error=6.4372
[flaml.automl: 09-17 22:09:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:09:33] {3072} INFO -  at 11.7s,	estimator xgboost's best error=6.4372,	best estimator xgboost's best error=6.4372
[flaml.automl: 09-17 22:09:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:09:36] {3072} INFO -  at 15.2s,	estimator xgboost's best error=6.4372,	best estimator xgboost's best error=6.4372
[flaml.automl: 09-17 22:09:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:09:39] {3072} INFO -  at 18.4s,	estimator xgboost's best error=4.3603,	best estimator xgboost's best error=4.3603
[flaml.automl: 09-17 22:09:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:09:42] {3072} INFO -  at 20.9s,	estimator xgboost's best error=4.3603,	best estimator xgboost's best error=4.3603
[flaml.automl: 09-17 22:09:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:09:45] {3072} INFO -  at 23.6s,	estimator xgboost's best error=4.3603,	best estimator xgboost's best error=4.3603
[flaml.automl: 09-17 22:09:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:09:47] {3072} INFO -  at 25.9s,	estimator xgboost's best error=4.3603,	best estimator xgboost's best error=4.3603
[flaml.automl: 09-17 22:09:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:09:49] {3072} INFO -  at 28.0s,	estimator xgboost's best error=4.3603,	best estimator xgboost's best error=4.3603
[flaml.automl: 09-17 22:09:49] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:09:51] {3072} INFO -  at 30.1s,	estimator xgboost's best error=4.3603,	best estimator xgboost's best error=4.3603
[flaml.automl: 09-17 22:09:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:09:54] {3072} INFO -  at 33.4s,	estimator xgboost's best error=4.3603,	best estimator xgboost's best error=4.3603
[flaml.automl: 09-17 22:09:54] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:09:57] {3072} INFO -  at 35.6s,	estimator xgboost's best error=4.1891,	best estimator xgboost's best error=4.1891
[flaml.automl: 09-17 22:09:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:09:59] {3072} INFO -  at 37.7s,	estimator xgboost's best error=4.1891,	best estimator xgboost's best error=4.1891
[flaml.automl: 09-17 22:09:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:10:06] {3072} INFO -  at 45.0s,	estimator xgboost's best error=3.8540,	best estimator xgboost's best error=3.8540
[flaml.automl: 09-17 22:10:06] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:10:14] {3072} INFO -  at 53.0s,	estimator xgboost's best error=3.7231,	best estimator xgboost's best error=3.7231
[flaml.automl: 09-17 22:10:22] {3335} INFO - retrain xgboost for 7.9s
[flaml.automl: 09-17 22:10:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628243, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609471, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:10:22] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:10:22] {2637} INFO - Time taken to find the best model: 52.968549489974976
[flaml.automl: 09-17 22:10:22] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628243, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609471, 'FLAML_sample_size': 54766}
SO2(0)最佳损失：-2.7230813757902377
SO2(0)最好结果：{'pred_time': 7.361751459869475e-06, 'wall_clock_time': 52.968549489974976, 'metric_for_logging': {'pred_time': 7.361751459869475e-06}, 'val_loss': 3.7230813757902377, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 7, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628243, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609471, 'FLAML_sample_size': 54766}, 'config/n_estimators': 19, 'config/max_leaves': 7, 'config/min_child_weight': 6.532042959250319, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8835265175628243, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.10065328594609471, 'config/FLAML_sample_size': 54766, 'experiment_tag': 'exp', 'time_total_s': 7.925485849380493}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628243, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609471, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6261677261810636
SO2(0)的mse=50.56412722079536
SO2(0)的mae=3.896826804477553
SO2(0)的mar=0.24594728665934826
总共花费的时间为：61.89
辽阳市
2212A
2213A
2214A
2215A
[flaml.automl: 09-17 22:23:08] {2390} INFO - task = regression
[flaml.automl: 09-17 22:23:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:23:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:23:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:23:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:23:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:23:10] {3025} INFO - Estimated sufficient time budget=97516s. Estimated necessary time budget=98s.
[flaml.automl: 09-17 22:23:10] {3072} INFO -  at 2.4s,	estimator xgboost's best error=7.8767,	best estimator xgboost's best error=7.8767
[flaml.automl: 09-17 22:23:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:23:14] {3072} INFO -  at 6.3s,	estimator xgboost's best error=3.7407,	best estimator xgboost's best error=3.7407
[flaml.automl: 09-17 22:23:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:23:16] {3072} INFO -  at 8.6s,	estimator xgboost's best error=3.7407,	best estimator xgboost's best error=3.7407
[flaml.automl: 09-17 22:23:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:23:21] {3072} INFO -  at 13.7s,	estimator xgboost's best error=3.7407,	best estimator xgboost's best error=3.7407
[flaml.automl: 09-17 22:23:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:23:23] {3072} INFO -  at 15.9s,	estimator xgboost's best error=2.6531,	best estimator xgboost's best error=2.6531
[flaml.automl: 09-17 22:23:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:23:26] {3072} INFO -  at 18.8s,	estimator xgboost's best error=2.6531,	best estimator xgboost's best error=2.6531
[flaml.automl: 09-17 22:23:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:23:28] {3072} INFO -  at 20.5s,	estimator xgboost's best error=2.2150,	best estimator xgboost's best error=2.2150
[flaml.automl: 09-17 22:23:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:23:31] {3072} INFO -  at 23.1s,	estimator xgboost's best error=2.2150,	best estimator xgboost's best error=2.2150
[flaml.automl: 09-17 22:23:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:23:32] {3072} INFO -  at 24.7s,	estimator xgboost's best error=2.2150,	best estimator xgboost's best error=2.2150
[flaml.automl: 09-17 22:23:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:23:35] {3072} INFO -  at 27.7s,	estimator xgboost's best error=2.2150,	best estimator xgboost's best error=2.2150
[flaml.automl: 09-17 22:23:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:23:37] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.2150,	best estimator xgboost's best error=2.2150
[flaml.automl: 09-17 22:23:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:23:38] {3072} INFO -  at 30.9s,	estimator xgboost's best error=2.2150,	best estimator xgboost's best error=2.2150
[flaml.automl: 09-17 22:23:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:23:40] {3072} INFO -  at 32.0s,	estimator xgboost's best error=2.2150,	best estimator xgboost's best error=2.2150
[flaml.automl: 09-17 22:23:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:23:47] {3072} INFO -  at 39.1s,	estimator xgboost's best error=2.1916,	best estimator xgboost's best error=2.1916
[flaml.automl: 09-17 22:23:47] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:24:01] {3072} INFO -  at 53.2s,	estimator xgboost's best error=2.1514,	best estimator xgboost's best error=2.1514
[flaml.automl: 09-17 22:24:23] {3335} INFO - retrain xgboost for 21.7s
[flaml.automl: 09-17 22:24:23] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:24:23] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:24:23] {2637} INFO - Time taken to find the best model: 53.22896122932434
[flaml.automl: 09-17 22:24:23] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43735}
SO2(0)最佳损失：-1.151409770529947
SO2(0)最好结果：{'pred_time': 1.5966097513834636e-05, 'wall_clock_time': 53.22896122932434, 'metric_for_logging': {'pred_time': 1.5966097513834636e-05}, 'val_loss': 2.151409770529947, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43735}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 43735, 'experiment_tag': 'exp', 'time_total_s': 14.164772748947144}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6776347266489007
SO2(0)的mse=12.896920116457151
SO2(0)的mae=2.1132538423714813
SO2(0)的mar=0.15751244920155644
总共花费的时间为：75.66
铁岭市
2216A
2217A
2218A
2219A
[flaml.automl: 09-17 22:36:47] {2390} INFO - task = regression
[flaml.automl: 09-17 22:36:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:36:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:36:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:36:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:36:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:36:49] {3025} INFO - Estimated sufficient time budget=93798s. Estimated necessary time budget=94s.
[flaml.automl: 09-17 22:36:49] {3072} INFO -  at 2.4s,	estimator xgboost's best error=4.5120,	best estimator xgboost's best error=4.5120
[flaml.automl: 09-17 22:36:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:36:53] {3072} INFO -  at 6.2s,	estimator xgboost's best error=2.2293,	best estimator xgboost's best error=2.2293
[flaml.automl: 09-17 22:36:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:36:55] {3072} INFO -  at 8.4s,	estimator xgboost's best error=2.2293,	best estimator xgboost's best error=2.2293
[flaml.automl: 09-17 22:36:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:37:00] {3072} INFO -  at 13.4s,	estimator xgboost's best error=2.2293,	best estimator xgboost's best error=2.2293
[flaml.automl: 09-17 22:37:00] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:37:02] {3072} INFO -  at 15.5s,	estimator xgboost's best error=1.7986,	best estimator xgboost's best error=1.7986
[flaml.automl: 09-17 22:37:02] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:37:05] {3072} INFO -  at 18.4s,	estimator xgboost's best error=1.7986,	best estimator xgboost's best error=1.7986
[flaml.automl: 09-17 22:37:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:37:08] {3072} INFO -  at 20.8s,	estimator xgboost's best error=1.3773,	best estimator xgboost's best error=1.3773
[flaml.automl: 09-17 22:37:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:37:10] {3072} INFO -  at 23.5s,	estimator xgboost's best error=1.3773,	best estimator xgboost's best error=1.3773
[flaml.automl: 09-17 22:37:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:37:12] {3072} INFO -  at 25.1s,	estimator xgboost's best error=1.3773,	best estimator xgboost's best error=1.3773
[flaml.automl: 09-17 22:37:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:37:15] {3072} INFO -  at 28.1s,	estimator xgboost's best error=1.3773,	best estimator xgboost's best error=1.3773
[flaml.automl: 09-17 22:37:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:37:16] {3072} INFO -  at 29.5s,	estimator xgboost's best error=1.3773,	best estimator xgboost's best error=1.3773
[flaml.automl: 09-17 22:37:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:37:18] {3072} INFO -  at 31.2s,	estimator xgboost's best error=1.3773,	best estimator xgboost's best error=1.3773
[flaml.automl: 09-17 22:37:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:37:19] {3072} INFO -  at 32.4s,	estimator xgboost's best error=1.3773,	best estimator xgboost's best error=1.3773
[flaml.automl: 09-17 22:37:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:37:26] {3072} INFO -  at 39.4s,	estimator xgboost's best error=1.3706,	best estimator xgboost's best error=1.3706
[flaml.automl: 09-17 22:37:26] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:37:39] {3072} INFO -  at 52.2s,	estimator xgboost's best error=1.3376,	best estimator xgboost's best error=1.3376
[flaml.automl: 09-17 22:37:52] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 22:37:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:37:52] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:37:52] {2637} INFO - Time taken to find the best model: 52.23649024963379
[flaml.automl: 09-17 22:37:52] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43716}
SO2(0)最佳损失：-0.33756185085894197
SO2(0)最好结果：{'pred_time': 8.064221334634076e-06, 'wall_clock_time': 52.23649024963379, 'metric_for_logging': {'pred_time': 8.064221334634076e-06}, 'val_loss': 1.337561850858942, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43716}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 43716, 'experiment_tag': 'exp', 'time_total_s': 12.79077696800232}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6854044261873147
SO2(0)的mse=6.04502088009052
SO2(0)的mae=1.3372964823453592
SO2(0)的mar=0.1975745406017942
总共花费的时间为：65.69
朝阳市
2220A
2221A
2222A
2223A
[flaml.automl: 09-17 22:49:55] {2390} INFO - task = regression
[flaml.automl: 09-17 22:49:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 22:49:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 22:49:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 22:49:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 22:49:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 22:49:58] {3025} INFO - Estimated sufficient time budget=98131s. Estimated necessary time budget=98s.
[flaml.automl: 09-17 22:49:58] {3072} INFO -  at 2.4s,	estimator xgboost's best error=6.7477,	best estimator xgboost's best error=6.7477
[flaml.automl: 09-17 22:49:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 22:50:01] {3072} INFO -  at 6.3s,	estimator xgboost's best error=3.3389,	best estimator xgboost's best error=3.3389
[flaml.automl: 09-17 22:50:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 22:50:04] {3072} INFO -  at 8.6s,	estimator xgboost's best error=3.3389,	best estimator xgboost's best error=3.3389
[flaml.automl: 09-17 22:50:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 22:50:09] {3072} INFO -  at 13.6s,	estimator xgboost's best error=3.3389,	best estimator xgboost's best error=3.3389
[flaml.automl: 09-17 22:50:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 22:50:11] {3072} INFO -  at 15.7s,	estimator xgboost's best error=2.2626,	best estimator xgboost's best error=2.2626
[flaml.automl: 09-17 22:50:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 22:50:14] {3072} INFO -  at 18.7s,	estimator xgboost's best error=2.2413,	best estimator xgboost's best error=2.2413
[flaml.automl: 09-17 22:50:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 22:50:17] {3072} INFO -  at 21.6s,	estimator xgboost's best error=2.0816,	best estimator xgboost's best error=2.0816
[flaml.automl: 09-17 22:50:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 22:50:20] {3072} INFO -  at 25.1s,	estimator xgboost's best error=2.0816,	best estimator xgboost's best error=2.0816
[flaml.automl: 09-17 22:50:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 22:50:23] {3072} INFO -  at 28.2s,	estimator xgboost's best error=2.0816,	best estimator xgboost's best error=2.0816
[flaml.automl: 09-17 22:50:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 22:50:27] {3072} INFO -  at 31.6s,	estimator xgboost's best error=2.0816,	best estimator xgboost's best error=2.0816
[flaml.automl: 09-17 22:50:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 22:50:30] {3072} INFO -  at 34.4s,	estimator xgboost's best error=2.0816,	best estimator xgboost's best error=2.0816
[flaml.automl: 09-17 22:50:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 22:50:32] {3072} INFO -  at 36.6s,	estimator xgboost's best error=2.0816,	best estimator xgboost's best error=2.0816
[flaml.automl: 09-17 22:50:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 22:50:33] {3072} INFO -  at 37.8s,	estimator xgboost's best error=2.0816,	best estimator xgboost's best error=2.0816
[flaml.automl: 09-17 22:50:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 22:50:40] {3072} INFO -  at 44.7s,	estimator xgboost's best error=1.9419,	best estimator xgboost's best error=1.9419
[flaml.automl: 09-17 22:50:40] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 22:50:53] {3072} INFO -  at 57.5s,	estimator xgboost's best error=1.9361,	best estimator xgboost's best error=1.9361
[flaml.automl: 09-17 22:51:05] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-17 22:51:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 22:51:05] {2636} INFO - fit succeeded
[flaml.automl: 09-17 22:51:05] {2637} INFO - Time taken to find the best model: 57.47939968109131
[flaml.automl: 09-17 22:51:05] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 44244}
SO2(0)最佳损失：-0.9361288593580064
SO2(0)最好结果：{'pred_time': 9.187915981215919e-06, 'wall_clock_time': 57.47939968109131, 'metric_for_logging': {'pred_time': 9.187915981215919e-06}, 'val_loss': 1.9361288593580064, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 44244}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 44244, 'experiment_tag': 'exp', 'time_total_s': 12.785630702972412}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6780798781928521
SO2(0)的mse=10.269987395228139
SO2(0)的mae=1.9363859429499966
SO2(0)的mar=0.17781740407582264
总共花费的时间为：71.12
四平市
2226A
3486A
3713A
[flaml.automl: 09-17 23:00:31] {2390} INFO - task = regression
[flaml.automl: 09-17 23:00:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:00:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:00:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:00:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:00:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:00:35] {3025} INFO - Estimated sufficient time budget=33921s. Estimated necessary time budget=34s.
[flaml.automl: 09-17 23:00:35] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.1892,	best estimator xgboost's best error=5.1892
[flaml.automl: 09-17 23:00:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:00:41] {3072} INFO -  at 9.7s,	estimator xgboost's best error=2.5118,	best estimator xgboost's best error=2.5118
[flaml.automl: 09-17 23:00:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:00:44] {3072} INFO -  at 13.1s,	estimator xgboost's best error=2.5118,	best estimator xgboost's best error=2.5118
[flaml.automl: 09-17 23:00:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:00:57] {3072} INFO -  at 25.6s,	estimator xgboost's best error=2.5118,	best estimator xgboost's best error=2.5118
[flaml.automl: 09-17 23:00:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:00:58] {3072} INFO -  at 26.7s,	estimator xgboost's best error=1.7747,	best estimator xgboost's best error=1.7747
[flaml.automl: 09-17 23:00:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:00:59] {3072} INFO -  at 28.3s,	estimator xgboost's best error=1.7747,	best estimator xgboost's best error=1.7747
[flaml.automl: 09-17 23:00:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:01:01] {3072} INFO -  at 30.0s,	estimator xgboost's best error=1.4581,	best estimator xgboost's best error=1.4581
[flaml.automl: 09-17 23:01:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:01:04] {3072} INFO -  at 32.7s,	estimator xgboost's best error=1.4581,	best estimator xgboost's best error=1.4581
[flaml.automl: 09-17 23:01:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:01:05] {3072} INFO -  at 34.3s,	estimator xgboost's best error=1.4581,	best estimator xgboost's best error=1.4581
[flaml.automl: 09-17 23:01:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:01:08] {3072} INFO -  at 37.4s,	estimator xgboost's best error=1.4581,	best estimator xgboost's best error=1.4581
[flaml.automl: 09-17 23:01:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:01:10] {3072} INFO -  at 38.8s,	estimator xgboost's best error=1.4483,	best estimator xgboost's best error=1.4483
[flaml.automl: 09-17 23:01:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:01:11] {3072} INFO -  at 40.0s,	estimator xgboost's best error=1.4483,	best estimator xgboost's best error=1.4483
[flaml.automl: 09-17 23:01:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:01:17] {3072} INFO -  at 46.0s,	estimator xgboost's best error=1.3562,	best estimator xgboost's best error=1.3562
[flaml.automl: 09-17 23:01:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:01:28] {3072} INFO -  at 56.7s,	estimator xgboost's best error=1.3560,	best estimator xgboost's best error=1.3560
[flaml.automl: 09-17 23:01:38] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-17 23:01:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:01:38] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:01:38] {2637} INFO - Time taken to find the best model: 56.719428062438965
[flaml.automl: 09-17 23:01:38] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
SO2(0)最佳损失：-0.35598889825757873
SO2(0)最好结果：{'pred_time': 1.1702871191271737e-05, 'wall_clock_time': 56.719428062438965, 'metric_for_logging': {'pred_time': 1.1702871191271737e-05}, 'val_loss': 1.3559888982575787, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 10.739292621612549}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6864954305398465
SO2(0)的mse=5.209522280632585
SO2(0)的mae=1.405329139976142
SO2(0)的mar=0.1783656699355333
总共花费的时间为：68.09
辽源市
2227A
[flaml.automl: 09-17 23:04:58] {2390} INFO - task = regression
[flaml.automl: 09-17 23:04:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:04:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:04:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:04:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:04:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:05:00] {3025} INFO - Estimated sufficient time budget=22883s. Estimated necessary time budget=23s.
[flaml.automl: 09-17 23:05:00] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.7604,	best estimator xgboost's best error=5.7604
[flaml.automl: 09-17 23:05:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:05:03] {3072} INFO -  at 5.9s,	estimator xgboost's best error=3.2431,	best estimator xgboost's best error=3.2431
[flaml.automl: 09-17 23:05:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:05:06] {3072} INFO -  at 8.2s,	estimator xgboost's best error=3.2431,	best estimator xgboost's best error=3.2431
[flaml.automl: 09-17 23:05:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:05:18] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.2431,	best estimator xgboost's best error=3.2431
[flaml.automl: 09-17 23:05:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:05:21] {3072} INFO -  at 23.1s,	estimator xgboost's best error=2.2380,	best estimator xgboost's best error=2.2380
[flaml.automl: 09-17 23:05:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:05:24] {3072} INFO -  at 26.1s,	estimator xgboost's best error=2.2380,	best estimator xgboost's best error=2.2380
[flaml.automl: 09-17 23:05:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:05:27] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.0894,	best estimator xgboost's best error=2.0894
[flaml.automl: 09-17 23:05:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:05:29] {3072} INFO -  at 31.7s,	estimator xgboost's best error=2.0894,	best estimator xgboost's best error=2.0894
[flaml.automl: 09-17 23:05:29] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:05:31] {3072} INFO -  at 33.3s,	estimator xgboost's best error=2.0894,	best estimator xgboost's best error=2.0894
[flaml.automl: 09-17 23:05:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:05:33] {3072} INFO -  at 35.9s,	estimator xgboost's best error=2.0894,	best estimator xgboost's best error=2.0894
[flaml.automl: 09-17 23:05:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:05:35] {3072} INFO -  at 37.6s,	estimator xgboost's best error=2.0894,	best estimator xgboost's best error=2.0894
[flaml.automl: 09-17 23:05:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:05:36] {3072} INFO -  at 38.7s,	estimator xgboost's best error=2.0894,	best estimator xgboost's best error=2.0894
[flaml.automl: 09-17 23:05:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:05:42] {3072} INFO -  at 44.6s,	estimator xgboost's best error=2.0894,	best estimator xgboost's best error=2.0894
[flaml.automl: 09-17 23:05:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:05:45] {3072} INFO -  at 47.9s,	estimator xgboost's best error=2.0788,	best estimator xgboost's best error=2.0788
[flaml.automl: 09-17 23:05:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-17 23:05:47] {3072} INFO -  at 49.9s,	estimator xgboost's best error=2.0788,	best estimator xgboost's best error=2.0788
[flaml.automl: 09-17 23:05:47] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-17 23:05:57] {3072} INFO -  at 59.1s,	estimator xgboost's best error=2.0769,	best estimator xgboost's best error=2.0769
[flaml.automl: 09-17 23:06:12] {3335} INFO - retrain xgboost for 15.0s
[flaml.automl: 09-17 23:06:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.19405630899231047,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=11.615070090947864, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.07808876344933313, scale_pos_weight=1,
             subsample=0.9453052099956202, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:06:12] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:06:12] {2637} INFO - Time taken to find the best model: 59.05706810951233
[flaml.automl: 09-17 23:06:12] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 7, 'min_child_weight': 11.615070090947864, 'learning_rate': 0.19405630899231047, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.07808876344933313}
SO2(0)最佳损失：-1.0768890723124218
SO2(0)最好结果：{'pred_time': 6.757027562251876e-05, 'wall_clock_time': 59.05706810951233, 'metric_for_logging': {'pred_time': 6.757027562251876e-05}, 'val_loss': 2.076889072312422, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 7, 'min_child_weight': 11.615070090947864, 'learning_rate': 0.19405630899231047, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.07808876344933313}, 'config/n_estimators': 13, 'config/max_leaves': 7, 'config/min_child_weight': 11.615070090947864, 'config/learning_rate': 0.19405630899231047, 'config/subsample': 0.9453052099956202, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.92006762344246, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.07808876344933313, 'experiment_tag': 'exp', 'time_total_s': 9.145210981369019}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.19405630899231047,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=11.615070090947864, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.07808876344933313, scale_pos_weight=1,
             subsample=0.9453052099956202, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=-0.009294932529724909
SO2(0)的mse=15.849761927985751
SO2(0)的mae=2.0357287959380295
SO2(0)的mar=0.1746423168458608
总共花费的时间为：74.40
通化市
2229A
2230A
[flaml.automl: 09-17 23:12:35] {2390} INFO - task = regression
[flaml.automl: 09-17 23:12:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:12:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:12:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:12:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:12:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:12:38] {3025} INFO - Estimated sufficient time budget=21505s. Estimated necessary time budget=22s.
[flaml.automl: 09-17 23:12:38] {3072} INFO -  at 2.3s,	estimator xgboost's best error=8.9242,	best estimator xgboost's best error=8.9242
[flaml.automl: 09-17 23:12:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:12:41] {3072} INFO -  at 6.2s,	estimator xgboost's best error=4.3049,	best estimator xgboost's best error=4.3049
[flaml.automl: 09-17 23:12:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:12:44] {3072} INFO -  at 8.4s,	estimator xgboost's best error=4.3049,	best estimator xgboost's best error=4.3049
[flaml.automl: 09-17 23:12:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:13:01] {3072} INFO -  at 26.0s,	estimator xgboost's best error=4.3049,	best estimator xgboost's best error=4.3049
[flaml.automl: 09-17 23:13:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:13:03] {3072} INFO -  at 28.1s,	estimator xgboost's best error=3.0870,	best estimator xgboost's best error=3.0870
[flaml.automl: 09-17 23:13:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:13:06] {3072} INFO -  at 31.0s,	estimator xgboost's best error=3.0870,	best estimator xgboost's best error=3.0870
[flaml.automl: 09-17 23:13:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:13:09] {3072} INFO -  at 34.2s,	estimator xgboost's best error=2.6958,	best estimator xgboost's best error=2.6958
[flaml.automl: 09-17 23:13:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:13:14] {3072} INFO -  at 39.2s,	estimator xgboost's best error=2.6958,	best estimator xgboost's best error=2.6958
[flaml.automl: 09-17 23:13:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:13:17] {3072} INFO -  at 42.2s,	estimator xgboost's best error=2.6958,	best estimator xgboost's best error=2.6958
[flaml.automl: 09-17 23:13:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:13:23] {3072} INFO -  at 48.0s,	estimator xgboost's best error=2.6958,	best estimator xgboost's best error=2.6958
[flaml.automl: 09-17 23:13:23] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:13:26] {3072} INFO -  at 51.1s,	estimator xgboost's best error=2.6958,	best estimator xgboost's best error=2.6958
[flaml.automl: 09-17 23:13:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:13:28] {3072} INFO -  at 53.1s,	estimator xgboost's best error=2.6958,	best estimator xgboost's best error=2.6958
[flaml.automl: 09-17 23:13:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:13:35] {3072} INFO -  at 59.3s,	estimator xgboost's best error=2.6823,	best estimator xgboost's best error=2.6823
[flaml.automl: 09-17 23:13:45] {3335} INFO - retrain xgboost for 10.4s
[flaml.automl: 09-17 23:13:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:13:45] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:13:45] {2637} INFO - Time taken to find the best model: 59.300615549087524
[flaml.automl: 09-17 23:13:45] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
SO2(0)最佳损失：-1.6823477788693495
SO2(0)最好结果：{'pred_time': 2.4903534652786127e-05, 'wall_clock_time': 59.300615549087524, 'metric_for_logging': {'pred_time': 2.4903534652786127e-05}, 'val_loss': 2.6823477788693495, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 6.165740489959717}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5861795141230631
SO2(0)的mse=24.39788082596589
SO2(0)的mae=2.8020250642848614
SO2(0)的mar=0.18823126318066746
总共花费的时间为：70.10
白山市
2231A
2232A
[flaml.automl: 09-17 23:21:02] {2390} INFO - task = regression
[flaml.automl: 09-17 23:21:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:21:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:21:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:21:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:21:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:21:06] {3025} INFO - Estimated sufficient time budget=34415s. Estimated necessary time budget=34s.
[flaml.automl: 09-17 23:21:06] {3072} INFO -  at 3.6s,	estimator xgboost's best error=8.8325,	best estimator xgboost's best error=8.8325
[flaml.automl: 09-17 23:21:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:21:12] {3072} INFO -  at 9.5s,	estimator xgboost's best error=4.3122,	best estimator xgboost's best error=4.3122
[flaml.automl: 09-17 23:21:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:21:15] {3072} INFO -  at 13.0s,	estimator xgboost's best error=4.3122,	best estimator xgboost's best error=4.3122
[flaml.automl: 09-17 23:21:15] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:21:37] {3072} INFO -  at 35.3s,	estimator xgboost's best error=4.3122,	best estimator xgboost's best error=4.3122
[flaml.automl: 09-17 23:21:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:21:41] {3072} INFO -  at 38.4s,	estimator xgboost's best error=3.1451,	best estimator xgboost's best error=3.1451
[flaml.automl: 09-17 23:21:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:21:45] {3072} INFO -  at 42.4s,	estimator xgboost's best error=3.1451,	best estimator xgboost's best error=3.1451
[flaml.automl: 09-17 23:21:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:21:49] {3072} INFO -  at 47.0s,	estimator xgboost's best error=2.7824,	best estimator xgboost's best error=2.7824
[flaml.automl: 09-17 23:21:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:21:56] {3072} INFO -  at 54.2s,	estimator xgboost's best error=2.7824,	best estimator xgboost's best error=2.7824
[flaml.automl: 09-17 23:21:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:22:01] {3072} INFO -  at 58.5s,	estimator xgboost's best error=2.7824,	best estimator xgboost's best error=2.7824
[flaml.automl: 09-17 23:22:05] {3335} INFO - retrain xgboost for 4.4s
[flaml.automl: 09-17 23:22:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 23:22:05] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:22:05] {2637} INFO - Time taken to find the best model: 46.985273599624634
[flaml.automl: 09-17 23:22:05] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-1.7824117394111418
SO2(0)最好结果：{'pred_time': 4.377838088433269e-05, 'wall_clock_time': 46.985273599624634, 'metric_for_logging': {'pred_time': 4.377838088433269e-05}, 'val_loss': 2.782411739411142, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.563537120819092}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.4424506652421747
SO2(0)的mse=23.285978091409007
SO2(0)的mae=2.649656198845535
SO2(0)的mar=0.17649631129263704
总共花费的时间为：63.51
松原市
2233A
2234A
[flaml.automl: 09-17 23:29:05] {2390} INFO - task = regression
[flaml.automl: 09-17 23:29:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:29:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:29:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:29:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:29:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:29:08] {3025} INFO - Estimated sufficient time budget=34347s. Estimated necessary time budget=34s.
[flaml.automl: 09-17 23:29:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.7685,	best estimator xgboost's best error=2.7685
[flaml.automl: 09-17 23:29:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:29:15] {3072} INFO -  at 9.7s,	estimator xgboost's best error=1.4755,	best estimator xgboost's best error=1.4755
[flaml.automl: 09-17 23:29:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:29:18] {3072} INFO -  at 13.1s,	estimator xgboost's best error=1.4755,	best estimator xgboost's best error=1.4755
[flaml.automl: 09-17 23:29:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:29:53] {3072} INFO -  at 47.7s,	estimator xgboost's best error=1.4755,	best estimator xgboost's best error=1.4755
[flaml.automl: 09-17 23:29:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:29:56] {3072} INFO -  at 50.7s,	estimator xgboost's best error=1.1696,	best estimator xgboost's best error=1.1696
[flaml.automl: 09-17 23:29:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:30:00] {3072} INFO -  at 54.9s,	estimator xgboost's best error=1.0876,	best estimator xgboost's best error=1.0876
[flaml.automl: 09-17 23:30:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:30:04] {3072} INFO -  at 59.2s,	estimator xgboost's best error=1.0524,	best estimator xgboost's best error=1.0524
[flaml.automl: 09-17 23:30:08] {3335} INFO - retrain xgboost for 3.8s
[flaml.automl: 09-17 23:30:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:30:08] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:30:08] {2637} INFO - Time taken to find the best model: 59.215747356414795
[flaml.automl: 09-17 23:30:08] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：-0.05237421450947233
SO2(0)最好结果：{'pred_time': 6.700452889443245e-05, 'wall_clock_time': 59.215747356414795, 'metric_for_logging': {'pred_time': 6.700452889443245e-05}, 'val_loss': 1.0523742145094723, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.333580017089844}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6002496987623598
SO2(0)的mse=3.13596464633494
SO2(0)的mae=1.1051908350909572
SO2(0)的mar=0.3140557623639628
总共花费的时间为：63.51
白城市
2235A
2236A
[flaml.automl: 09-17 23:36:38] {2390} INFO - task = regression
[flaml.automl: 09-17 23:36:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:36:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:36:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:36:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:36:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:36:39] {3025} INFO - Estimated sufficient time budget=12071s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 23:36:39] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.7093,	best estimator xgboost's best error=4.7093
[flaml.automl: 09-17 23:36:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:36:42] {3072} INFO -  at 4.4s,	estimator xgboost's best error=2.1388,	best estimator xgboost's best error=2.1388
[flaml.automl: 09-17 23:36:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:36:44] {3072} INFO -  at 6.6s,	estimator xgboost's best error=2.1388,	best estimator xgboost's best error=2.1388
[flaml.automl: 09-17 23:36:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:37:06] {3072} INFO -  at 27.8s,	estimator xgboost's best error=2.1388,	best estimator xgboost's best error=2.1388
[flaml.automl: 09-17 23:37:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:37:09] {3072} INFO -  at 30.9s,	estimator xgboost's best error=1.1184,	best estimator xgboost's best error=1.1184
[flaml.automl: 09-17 23:37:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:37:13] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.9308,	best estimator xgboost's best error=0.9308
[flaml.automl: 09-17 23:37:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:37:17] {3072} INFO -  at 39.6s,	estimator xgboost's best error=0.8638,	best estimator xgboost's best error=0.8638
[flaml.automl: 09-17 23:37:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:37:25] {3072} INFO -  at 46.8s,	estimator xgboost's best error=0.8638,	best estimator xgboost's best error=0.8638
[flaml.automl: 09-17 23:37:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:37:29] {3072} INFO -  at 51.1s,	estimator xgboost's best error=0.8638,	best estimator xgboost's best error=0.8638
[flaml.automl: 09-17 23:37:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:37:37] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.8478,	best estimator xgboost's best error=0.8478
[flaml.automl: 09-17 23:37:45] {3335} INFO - retrain xgboost for 7.9s
[flaml.automl: 09-17 23:37:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-17 23:37:45] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:37:45] {2637} INFO - Time taken to find the best model: 59.25769257545471
[flaml.automl: 09-17 23:37:45] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：0.15215605220129325
SO2(0)最好结果：{'pred_time': 4.2317911635997684e-05, 'wall_clock_time': 59.25769257545471, 'metric_for_logging': {'pred_time': 4.2317911635997684e-05}, 'val_loss': 0.8478439477987068, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 8.20405626296997}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7180294210459655
SO2(0)的mse=1.5509121354036535
SO2(0)的mae=0.851876233814365
SO2(0)的mar=0.12131816978047304
总共花费的时间为：67.69
延边朝鲜族自治州
2237A
2238A
2239A
[flaml.automl: 09-17 23:47:31] {2390} INFO - task = regression
[flaml.automl: 09-17 23:47:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-17 23:47:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-17 23:47:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-17 23:47:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-17 23:47:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-17 23:47:33] {3025} INFO - Estimated sufficient time budget=12083s. Estimated necessary time budget=12s.
[flaml.automl: 09-17 23:47:33] {3072} INFO -  at 1.3s,	estimator xgboost's best error=5.2705,	best estimator xgboost's best error=5.2705
[flaml.automl: 09-17 23:47:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-17 23:47:35] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.5197,	best estimator xgboost's best error=2.5197
[flaml.automl: 09-17 23:47:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-17 23:47:36] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.5197,	best estimator xgboost's best error=2.5197
[flaml.automl: 09-17 23:47:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-17 23:47:46] {3072} INFO -  at 14.7s,	estimator xgboost's best error=2.5197,	best estimator xgboost's best error=2.5197
[flaml.automl: 09-17 23:47:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-17 23:47:47] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.6108,	best estimator xgboost's best error=1.6108
[flaml.automl: 09-17 23:47:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-17 23:47:49] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.5884,	best estimator xgboost's best error=1.5884
[flaml.automl: 09-17 23:47:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-17 23:47:50] {3072} INFO -  at 19.0s,	estimator xgboost's best error=1.4433,	best estimator xgboost's best error=1.4433
[flaml.automl: 09-17 23:47:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-17 23:47:53] {3072} INFO -  at 21.7s,	estimator xgboost's best error=1.4433,	best estimator xgboost's best error=1.4433
[flaml.automl: 09-17 23:47:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-17 23:47:55] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.4433,	best estimator xgboost's best error=1.4433
[flaml.automl: 09-17 23:47:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-17 23:47:58] {3072} INFO -  at 26.4s,	estimator xgboost's best error=1.3863,	best estimator xgboost's best error=1.3863
[flaml.automl: 09-17 23:47:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-17 23:47:59] {3072} INFO -  at 28.0s,	estimator xgboost's best error=1.3863,	best estimator xgboost's best error=1.3863
[flaml.automl: 09-17 23:47:59] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-17 23:48:01] {3072} INFO -  at 29.2s,	estimator xgboost's best error=1.3863,	best estimator xgboost's best error=1.3863
[flaml.automl: 09-17 23:48:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-17 23:48:17] {3072} INFO -  at 45.6s,	estimator xgboost's best error=1.3863,	best estimator xgboost's best error=1.3863
[flaml.automl: 09-17 23:48:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-17 23:48:26] {3072} INFO -  at 54.9s,	estimator xgboost's best error=1.3560,	best estimator xgboost's best error=1.3560
[flaml.automl: 09-17 23:48:36] {3335} INFO - retrain xgboost for 9.7s
[flaml.automl: 09-17 23:48:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-17 23:48:36] {2636} INFO - fit succeeded
[flaml.automl: 09-17 23:48:36] {2637} INFO - Time taken to find the best model: 54.932844161987305
[flaml.automl: 09-17 23:48:36] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 16, 'max_leaves': 5, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}
SO2(0)最佳损失：-0.35595209835956676
SO2(0)最好结果：{'pred_time': 2.1476665352370597e-05, 'wall_clock_time': 54.932844161987305, 'metric_for_logging': {'pred_time': 2.1476665352370597e-05}, 'val_loss': 1.3559520983595668, 'training_iteration': 1, 'config': {'n_estimators': 16, 'max_leaves': 5, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}, 'config/n_estimators': 16, 'config/max_leaves': 5, 'config/min_child_weight': 0.07186500551974488, 'config/learning_rate': 0.18147476059072173, 'config/subsample': 0.7641710216408443, 'config/colsample_bylevel': 0.8883913141026579, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.14043216453256416, 'experiment_tag': 'exp', 'time_total_s': 9.29721999168396}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.30542621683556104
SO2(0)的mse=7.167073254623164
SO2(0)的mae=1.2890581839074393
SO2(0)的mar=0.12766676205113411
总共花费的时间为：65.23
鸡西市
2240A
2243A
3707A
3709A
[flaml.automl: 09-18 00:00:56] {2390} INFO - task = regression
[flaml.automl: 09-18 00:00:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:00:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:00:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:00:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:00:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:00:57] {3025} INFO - Estimated sufficient time budget=51484s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 00:00:57] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.5524,	best estimator xgboost's best error=4.5524
[flaml.automl: 09-18 00:00:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:00:59] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.3100,	best estimator xgboost's best error=2.3100
[flaml.automl: 09-18 00:00:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:01:01] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.3100,	best estimator xgboost's best error=2.3100
[flaml.automl: 09-18 00:01:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:01:07] {3072} INFO -  at 11.0s,	estimator xgboost's best error=2.3100,	best estimator xgboost's best error=2.3100
[flaml.automl: 09-18 00:01:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:01:08] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.6453,	best estimator xgboost's best error=1.6453
[flaml.automl: 09-18 00:01:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:01:10] {3072} INFO -  at 13.7s,	estimator xgboost's best error=1.6183,	best estimator xgboost's best error=1.6183
[flaml.automl: 09-18 00:01:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:01:11] {3072} INFO -  at 15.3s,	estimator xgboost's best error=1.4943,	best estimator xgboost's best error=1.4943
[flaml.automl: 09-18 00:01:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:01:14] {3072} INFO -  at 18.0s,	estimator xgboost's best error=1.4943,	best estimator xgboost's best error=1.4943
[flaml.automl: 09-18 00:01:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:01:16] {3072} INFO -  at 19.7s,	estimator xgboost's best error=1.4943,	best estimator xgboost's best error=1.4943
[flaml.automl: 09-18 00:01:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:01:19] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.4037,	best estimator xgboost's best error=1.4037
[flaml.automl: 09-18 00:01:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:01:20] {3072} INFO -  at 24.3s,	estimator xgboost's best error=1.4037,	best estimator xgboost's best error=1.4037
[flaml.automl: 09-18 00:01:20] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:01:21] {3072} INFO -  at 25.4s,	estimator xgboost's best error=1.4037,	best estimator xgboost's best error=1.4037
[flaml.automl: 09-18 00:01:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:01:25] {3072} INFO -  at 29.2s,	estimator xgboost's best error=1.4037,	best estimator xgboost's best error=1.4037
[flaml.automl: 09-18 00:01:25] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:01:29] {3072} INFO -  at 32.6s,	estimator xgboost's best error=1.4037,	best estimator xgboost's best error=1.4037
[flaml.automl: 09-18 00:01:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 00:01:32] {3072} INFO -  at 35.7s,	estimator xgboost's best error=1.3732,	best estimator xgboost's best error=1.3732
[flaml.automl: 09-18 00:01:32] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 00:01:37] {3072} INFO -  at 40.6s,	estimator xgboost's best error=1.3732,	best estimator xgboost's best error=1.3732
[flaml.automl: 09-18 00:01:37] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 00:01:38] {3072} INFO -  at 42.5s,	estimator xgboost's best error=1.3732,	best estimator xgboost's best error=1.3732
[flaml.automl: 09-18 00:01:38] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 00:01:40] {3072} INFO -  at 44.2s,	estimator xgboost's best error=1.3732,	best estimator xgboost's best error=1.3732
[flaml.automl: 09-18 00:01:40] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 00:01:47] {3072} INFO -  at 51.6s,	estimator xgboost's best error=1.3732,	best estimator xgboost's best error=1.3732
[flaml.automl: 09-18 00:01:47] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 00:01:49] {3072} INFO -  at 52.7s,	estimator xgboost's best error=1.3732,	best estimator xgboost's best error=1.3732
[flaml.automl: 09-18 00:01:49] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 00:01:55] {3072} INFO -  at 59.2s,	estimator xgboost's best error=1.3732,	best estimator xgboost's best error=1.3732
[flaml.automl: 09-18 00:01:58] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-18 00:01:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 00:01:58] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:01:58] {2637} INFO - Time taken to find the best model: 35.67739820480347
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978, 'FLAML_sample_size': 42994}
SO2(0)最佳损失：-0.37318879917307646
SO2(0)最好结果：{'pred_time': 9.420727426010777e-06, 'wall_clock_time': 35.67739820480347, 'metric_for_logging': {'pred_time': 9.420727426010777e-06}, 'val_loss': 1.3731887991730765, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978, 'FLAML_sample_size': 42994}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'config/FLAML_sample_size': 42994, 'experiment_tag': 'exp', 'time_total_s': 3.03124737739563}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6510034681844492
SO2(0)的mse=7.699705335590655
SO2(0)的mae=1.330298554809737
SO2(0)的mar=0.21714468157585956
总共花费的时间为：62.87
鹤岗市
2244A
2245A
2246A
2247A
[flaml.automl: 09-18 00:14:25] {2390} INFO - task = regression
[flaml.automl: 09-18 00:14:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:14:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:14:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:14:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:14:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:14:27] {3025} INFO - Estimated sufficient time budget=85941s. Estimated necessary time budget=86s.
[flaml.automl: 09-18 00:14:27] {3072} INFO -  at 2.4s,	estimator xgboost's best error=4.8409,	best estimator xgboost's best error=4.8409
[flaml.automl: 09-18 00:14:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:14:31] {3072} INFO -  at 6.1s,	estimator xgboost's best error=2.1970,	best estimator xgboost's best error=2.1970
[flaml.automl: 09-18 00:14:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:14:32] {3072} INFO -  at 7.7s,	estimator xgboost's best error=2.1970,	best estimator xgboost's best error=2.1970
[flaml.automl: 09-18 00:14:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:14:38] {3072} INFO -  at 13.6s,	estimator xgboost's best error=2.1970,	best estimator xgboost's best error=2.1970
[flaml.automl: 09-18 00:14:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:14:40] {3072} INFO -  at 15.7s,	estimator xgboost's best error=1.2302,	best estimator xgboost's best error=1.2302
[flaml.automl: 09-18 00:14:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:14:43] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.0069,	best estimator xgboost's best error=1.0069
[flaml.automl: 09-18 00:14:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:14:46] {3072} INFO -  at 21.0s,	estimator xgboost's best error=1.0066,	best estimator xgboost's best error=1.0066
[flaml.automl: 09-18 00:14:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:14:50] {3072} INFO -  at 25.1s,	estimator xgboost's best error=1.0066,	best estimator xgboost's best error=1.0066
[flaml.automl: 09-18 00:14:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:14:52] {3072} INFO -  at 27.7s,	estimator xgboost's best error=1.0066,	best estimator xgboost's best error=1.0066
[flaml.automl: 09-18 00:14:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:14:55] {3072} INFO -  at 30.9s,	estimator xgboost's best error=1.0066,	best estimator xgboost's best error=1.0066
[flaml.automl: 09-18 00:14:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:14:58] {3072} INFO -  at 33.4s,	estimator xgboost's best error=1.0066,	best estimator xgboost's best error=1.0066
[flaml.automl: 09-18 00:14:58] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:15:01] {3072} INFO -  at 36.3s,	estimator xgboost's best error=1.0066,	best estimator xgboost's best error=1.0066
[flaml.automl: 09-18 00:15:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:15:03] {3072} INFO -  at 38.5s,	estimator xgboost's best error=1.0066,	best estimator xgboost's best error=1.0066
[flaml.automl: 09-18 00:15:03] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:15:14] {3072} INFO -  at 49.7s,	estimator xgboost's best error=0.9302,	best estimator xgboost's best error=0.9302
[flaml.automl: 09-18 00:15:26] {3335} INFO - retrain xgboost for 11.7s
[flaml.automl: 09-18 00:15:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 00:15:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:15:26] {2637} INFO - Time taken to find the best model: 49.67506957054138
[flaml.automl: 09-18 00:15:26] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 40319}
SO2(0)最佳损失：0.0698479362364326
SO2(0)最好结果：{'pred_time': 1.1335153664861406e-05, 'wall_clock_time': 49.67506957054138, 'metric_for_logging': {'pred_time': 1.1335153664861406e-05}, 'val_loss': 0.9301520637635674, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 40319}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 40319, 'experiment_tag': 'exp', 'time_total_s': 11.216134548187256}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7128891014690237
SO2(0)的mse=2.432105326443008
SO2(0)的mae=0.9227038384345987
SO2(0)的mar=0.11266705886679816
总共花费的时间为：62.14
双鸭山市
2248A
2249A
2250A
2251A
[flaml.automl: 09-18 00:27:55] {2390} INFO - task = regression
[flaml.automl: 09-18 00:27:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:27:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:27:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:27:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:27:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:27:57] {3025} INFO - Estimated sufficient time budget=49938s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 00:27:57] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.0847,	best estimator xgboost's best error=4.0847
[flaml.automl: 09-18 00:27:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:27:59] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.0482,	best estimator xgboost's best error=2.0482
[flaml.automl: 09-18 00:27:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:28:00] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.0482,	best estimator xgboost's best error=2.0482
[flaml.automl: 09-18 00:28:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:28:06] {3072} INFO -  at 11.1s,	estimator xgboost's best error=2.0482,	best estimator xgboost's best error=2.0482
[flaml.automl: 09-18 00:28:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:28:07] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.4986,	best estimator xgboost's best error=1.4986
[flaml.automl: 09-18 00:28:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:28:09] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.4642,	best estimator xgboost's best error=1.4642
[flaml.automl: 09-18 00:28:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:28:11] {3072} INFO -  at 15.4s,	estimator xgboost's best error=1.3567,	best estimator xgboost's best error=1.3567
[flaml.automl: 09-18 00:28:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:28:13] {3072} INFO -  at 18.0s,	estimator xgboost's best error=1.3567,	best estimator xgboost's best error=1.3567
[flaml.automl: 09-18 00:28:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:28:15] {3072} INFO -  at 19.6s,	estimator xgboost's best error=1.3567,	best estimator xgboost's best error=1.3567
[flaml.automl: 09-18 00:28:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:28:18] {3072} INFO -  at 22.6s,	estimator xgboost's best error=1.2615,	best estimator xgboost's best error=1.2615
[flaml.automl: 09-18 00:28:18] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:28:19] {3072} INFO -  at 24.2s,	estimator xgboost's best error=1.2615,	best estimator xgboost's best error=1.2615
[flaml.automl: 09-18 00:28:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:28:21] {3072} INFO -  at 25.3s,	estimator xgboost's best error=1.2615,	best estimator xgboost's best error=1.2615
[flaml.automl: 09-18 00:28:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:28:24] {3072} INFO -  at 29.1s,	estimator xgboost's best error=1.2462,	best estimator xgboost's best error=1.2462
[flaml.automl: 09-18 00:28:24] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:28:28] {3072} INFO -  at 32.7s,	estimator xgboost's best error=1.1551,	best estimator xgboost's best error=1.1551
[flaml.automl: 09-18 00:28:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 00:28:31] {3072} INFO -  at 35.4s,	estimator xgboost's best error=1.1551,	best estimator xgboost's best error=1.1551
[flaml.automl: 09-18 00:28:31] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 00:28:33] {3072} INFO -  at 37.9s,	estimator xgboost's best error=1.1551,	best estimator xgboost's best error=1.1551
[flaml.automl: 09-18 00:28:33] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 00:28:35] {3072} INFO -  at 40.1s,	estimator xgboost's best error=1.1551,	best estimator xgboost's best error=1.1551
[flaml.automl: 09-18 00:28:35] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 00:28:38] {3072} INFO -  at 42.3s,	estimator xgboost's best error=1.1551,	best estimator xgboost's best error=1.1551
[flaml.automl: 09-18 00:28:38] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 00:28:55] {3072} INFO -  at 59.5s,	estimator xgboost's best error=1.1551,	best estimator xgboost's best error=1.1551
[flaml.automl: 09-18 00:29:19] {3335} INFO - retrain xgboost for 24.0s
[flaml.automl: 09-18 00:29:19] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:29:19] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:29:19] {2637} INFO - Time taken to find the best model: 32.656851053237915
SO2(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：-0.15509708058590266
SO2(0)最好结果：{'pred_time': 8.38781712001312e-06, 'wall_clock_time': 32.656851053237915, 'metric_for_logging': {'pred_time': 8.38781712001312e-06}, 'val_loss': 1.1550970805859027, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306, 'FLAML_sample_size': 10000}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.5938644409179688}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7180602567306404
SO2(0)的mse=5.421373518730101
SO2(0)的mae=1.1872169897321165
SO2(0)的mar=0.18907658698526028
总共花费的时间为：84.23
伊春市
2252A
2253A
2254A
3342A
3343A
3344A
3480A
[flaml.automl: 09-18 00:50:32] {2390} INFO - task = regression
[flaml.automl: 09-18 00:50:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 00:50:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 00:50:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 00:50:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 00:50:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 00:50:34] {3025} INFO - Estimated sufficient time budget=88351s. Estimated necessary time budget=88s.
[flaml.automl: 09-18 00:50:34] {3072} INFO -  at 1.5s,	estimator xgboost's best error=4.0623,	best estimator xgboost's best error=4.0623
[flaml.automl: 09-18 00:50:34] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 00:50:36] {3072} INFO -  at 3.7s,	estimator xgboost's best error=2.0686,	best estimator xgboost's best error=2.0686
[flaml.automl: 09-18 00:50:36] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 00:50:37] {3072} INFO -  at 4.9s,	estimator xgboost's best error=2.0686,	best estimator xgboost's best error=2.0686
[flaml.automl: 09-18 00:50:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 00:50:40] {3072} INFO -  at 8.1s,	estimator xgboost's best error=2.0686,	best estimator xgboost's best error=2.0686
[flaml.automl: 09-18 00:50:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 00:50:41] {3072} INFO -  at 9.3s,	estimator xgboost's best error=1.5297,	best estimator xgboost's best error=1.5297
[flaml.automl: 09-18 00:50:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 00:50:43] {3072} INFO -  at 10.9s,	estimator xgboost's best error=1.3227,	best estimator xgboost's best error=1.3227
[flaml.automl: 09-18 00:50:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 00:50:45] {3072} INFO -  at 12.5s,	estimator xgboost's best error=1.3227,	best estimator xgboost's best error=1.3227
[flaml.automl: 09-18 00:50:45] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 00:50:47] {3072} INFO -  at 14.9s,	estimator xgboost's best error=1.3227,	best estimator xgboost's best error=1.3227
[flaml.automl: 09-18 00:50:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 00:50:48] {3072} INFO -  at 15.9s,	estimator xgboost's best error=1.3227,	best estimator xgboost's best error=1.3227
[flaml.automl: 09-18 00:50:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 00:50:51] {3072} INFO -  at 18.6s,	estimator xgboost's best error=1.3227,	best estimator xgboost's best error=1.3227
[flaml.automl: 09-18 00:50:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 00:50:52] {3072} INFO -  at 20.2s,	estimator xgboost's best error=1.3227,	best estimator xgboost's best error=1.3227
[flaml.automl: 09-18 00:50:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 00:50:53] {3072} INFO -  at 21.4s,	estimator xgboost's best error=1.3227,	best estimator xgboost's best error=1.3227
[flaml.automl: 09-18 00:50:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 00:51:00] {3072} INFO -  at 27.9s,	estimator xgboost's best error=1.1997,	best estimator xgboost's best error=1.1997
[flaml.automl: 09-18 00:51:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 00:51:12] {3072} INFO -  at 40.0s,	estimator xgboost's best error=1.1469,	best estimator xgboost's best error=1.1469
[flaml.automl: 09-18 00:51:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 00:51:19] {3072} INFO -  at 46.6s,	estimator xgboost's best error=1.1469,	best estimator xgboost's best error=1.1469
[flaml.automl: 09-18 00:51:19] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 00:51:31] {3072} INFO -  at 59.1s,	estimator xgboost's best error=1.1469,	best estimator xgboost's best error=1.1469
[flaml.automl: 09-18 00:51:43] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 00:51:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 00:51:43] {2636} INFO - fit succeeded
[flaml.automl: 09-18 00:51:43] {2637} INFO - Time taken to find the best model: 40.04439902305603
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 74376}
SO2(0)最佳损失：-0.14692416827172416
SO2(0)最好结果：{'pred_time': 4.9544044213084405e-06, 'wall_clock_time': 40.04439902305603, 'metric_for_logging': {'pred_time': 4.9544044213084405e-06}, 'val_loss': 1.1469241682717242, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 74376}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 74376, 'experiment_tag': 'exp', 'time_total_s': 12.128290891647339}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6370051722390718
SO2(0)的mse=5.7648933292241225
SO2(0)的mae=1.07792610182529
SO2(0)的mar=0.18442954755182653
总共花费的时间为：72.27
佳木斯市
2255A
2256A
2257A
2258A
2259A
[flaml.automl: 09-18 01:07:24] {2390} INFO - task = regression
[flaml.automl: 09-18 01:07:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:07:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:07:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:07:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:07:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:07:25] {3025} INFO - Estimated sufficient time budget=62773s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 01:07:25] {3072} INFO -  at 1.5s,	estimator xgboost's best error=3.7386,	best estimator xgboost's best error=3.7386
[flaml.automl: 09-18 01:07:25] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:07:27] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.8866,	best estimator xgboost's best error=1.8866
[flaml.automl: 09-18 01:07:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:07:28] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.8866,	best estimator xgboost's best error=1.8866
[flaml.automl: 09-18 01:07:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:07:33] {3072} INFO -  at 9.5s,	estimator xgboost's best error=1.8866,	best estimator xgboost's best error=1.8866
[flaml.automl: 09-18 01:07:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:07:34] {3072} INFO -  at 10.6s,	estimator xgboost's best error=1.4037,	best estimator xgboost's best error=1.4037
[flaml.automl: 09-18 01:07:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:07:36] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.2432,	best estimator xgboost's best error=1.2432
[flaml.automl: 09-18 01:07:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:07:37] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.2238,	best estimator xgboost's best error=1.2238
[flaml.automl: 09-18 01:07:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:07:40] {3072} INFO -  at 16.5s,	estimator xgboost's best error=1.2238,	best estimator xgboost's best error=1.2238
[flaml.automl: 09-18 01:07:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:07:42] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.2238,	best estimator xgboost's best error=1.2238
[flaml.automl: 09-18 01:07:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:07:45] {3072} INFO -  at 21.2s,	estimator xgboost's best error=1.1688,	best estimator xgboost's best error=1.1688
[flaml.automl: 09-18 01:07:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:07:46] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.1688,	best estimator xgboost's best error=1.1688
[flaml.automl: 09-18 01:07:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:07:47] {3072} INFO -  at 23.9s,	estimator xgboost's best error=1.1688,	best estimator xgboost's best error=1.1688
[flaml.automl: 09-18 01:07:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:07:50] {3072} INFO -  at 26.8s,	estimator xgboost's best error=1.1688,	best estimator xgboost's best error=1.1688
[flaml.automl: 09-18 01:07:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:07:53] {3072} INFO -  at 29.6s,	estimator xgboost's best error=1.1688,	best estimator xgboost's best error=1.1688
[flaml.automl: 09-18 01:07:53] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:07:56] {3072} INFO -  at 32.7s,	estimator xgboost's best error=1.1664,	best estimator xgboost's best error=1.1664
[flaml.automl: 09-18 01:07:56] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 01:08:01] {3072} INFO -  at 37.6s,	estimator xgboost's best error=1.1664,	best estimator xgboost's best error=1.1664
[flaml.automl: 09-18 01:08:01] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 01:08:03] {3072} INFO -  at 39.5s,	estimator xgboost's best error=1.1509,	best estimator xgboost's best error=1.1509
[flaml.automl: 09-18 01:08:03] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 01:08:05] {3072} INFO -  at 41.2s,	estimator xgboost's best error=1.1509,	best estimator xgboost's best error=1.1509
[flaml.automl: 09-18 01:08:05] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 01:08:09] {3072} INFO -  at 45.5s,	estimator xgboost's best error=1.1509,	best estimator xgboost's best error=1.1509
[flaml.automl: 09-18 01:08:09] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 01:08:10] {3072} INFO -  at 46.7s,	estimator xgboost's best error=1.1509,	best estimator xgboost's best error=1.1509
[flaml.automl: 09-18 01:08:10] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 01:08:16] {3072} INFO -  at 52.0s,	estimator xgboost's best error=1.1469,	best estimator xgboost's best error=1.1469
[flaml.automl: 09-18 01:08:16] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 01:08:23] {3072} INFO -  at 59.9s,	estimator xgboost's best error=1.1255,	best estimator xgboost's best error=1.1255
[flaml.automl: 09-18 01:08:51] {3335} INFO - retrain xgboost for 27.2s
[flaml.automl: 09-18 01:08:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.008381997180108987, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003886437262068607, scale_pos_weight=1,
             subsample=0.9390180412130811, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:08:51] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:08:51] {2637} INFO - Time taken to find the best model: 59.86107277870178
[flaml.automl: 09-18 01:08:51] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 28, 'max_leaves': 15, 'min_child_weight': 0.008381997180108987, 'learning_rate': 0.6023269513573992, 'subsample': 0.9390180412130811, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003886437262068607, 'FLAML_sample_size': 53284}
SO2(0)最佳损失：-0.12546921603845962
SO2(0)最好结果：{'pred_time': 6.768091810935455e-06, 'wall_clock_time': 59.86107277870178, 'metric_for_logging': {'pred_time': 6.768091810935455e-06}, 'val_loss': 1.1254692160384596, 'training_iteration': 1, 'config': {'n_estimators': 28, 'max_leaves': 15, 'min_child_weight': 0.008381997180108987, 'learning_rate': 0.6023269513573992, 'subsample': 0.9390180412130811, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003886437262068607, 'FLAML_sample_size': 53284}, 'config/n_estimators': 28, 'config/max_leaves': 15, 'config/min_child_weight': 0.008381997180108987, 'config/learning_rate': 0.6023269513573992, 'config/subsample': 0.9390180412130811, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.936172861729374, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.003886437262068607, 'config/FLAML_sample_size': 53284, 'experiment_tag': 'exp', 'time_total_s': 7.838037490844727}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.008381997180108987, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003886437262068607, scale_pos_weight=1,
             subsample=0.9390180412130811, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.721505290180984
SO2(0)的mse=2.9831855434625454
SO2(0)的mae=1.10081330790203
SO2(0)的mar=0.2255775705138741
总共花费的时间为：87.87
七台河市
2262A
3345A
3637A
3684A
[flaml.automl: 09-18 01:21:09] {2390} INFO - task = regression
[flaml.automl: 09-18 01:21:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:21:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:21:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:21:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:21:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:21:10] {3025} INFO - Estimated sufficient time budget=50437s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 01:21:10] {3072} INFO -  at 1.4s,	estimator xgboost's best error=6.0820,	best estimator xgboost's best error=6.0820
[flaml.automl: 09-18 01:21:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:21:12] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.9707,	best estimator xgboost's best error=2.9707
[flaml.automl: 09-18 01:21:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:21:13] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.9707,	best estimator xgboost's best error=2.9707
[flaml.automl: 09-18 01:21:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:21:20] {3072} INFO -  at 11.0s,	estimator xgboost's best error=2.9707,	best estimator xgboost's best error=2.9707
[flaml.automl: 09-18 01:21:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:21:21] {3072} INFO -  at 12.1s,	estimator xgboost's best error=2.1346,	best estimator xgboost's best error=2.1346
[flaml.automl: 09-18 01:21:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:21:22] {3072} INFO -  at 13.7s,	estimator xgboost's best error=2.1346,	best estimator xgboost's best error=2.1346
[flaml.automl: 09-18 01:21:22] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:21:24] {3072} INFO -  at 15.4s,	estimator xgboost's best error=1.7717,	best estimator xgboost's best error=1.7717
[flaml.automl: 09-18 01:21:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:21:27] {3072} INFO -  at 18.1s,	estimator xgboost's best error=1.7717,	best estimator xgboost's best error=1.7717
[flaml.automl: 09-18 01:21:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:21:28] {3072} INFO -  at 19.8s,	estimator xgboost's best error=1.7717,	best estimator xgboost's best error=1.7717
[flaml.automl: 09-18 01:21:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:21:31] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.7717,	best estimator xgboost's best error=1.7717
[flaml.automl: 09-18 01:21:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:21:33] {3072} INFO -  at 24.2s,	estimator xgboost's best error=1.7717,	best estimator xgboost's best error=1.7717
[flaml.automl: 09-18 01:21:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:21:34] {3072} INFO -  at 25.9s,	estimator xgboost's best error=1.7717,	best estimator xgboost's best error=1.7717
[flaml.automl: 09-18 01:21:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:21:36] {3072} INFO -  at 27.1s,	estimator xgboost's best error=1.7717,	best estimator xgboost's best error=1.7717
[flaml.automl: 09-18 01:21:36] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:21:43] {3072} INFO -  at 34.1s,	estimator xgboost's best error=1.7271,	best estimator xgboost's best error=1.7271
[flaml.automl: 09-18 01:21:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:21:55] {3072} INFO -  at 46.9s,	estimator xgboost's best error=1.7247,	best estimator xgboost's best error=1.7247
[flaml.automl: 09-18 01:21:55] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 01:22:02] {3072} INFO -  at 54.0s,	estimator xgboost's best error=1.7247,	best estimator xgboost's best error=1.7247
[flaml.automl: 09-18 01:22:15] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-18 01:22:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:22:15] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:22:15] {2637} INFO - Time taken to find the best model: 46.88903832435608
[flaml.automl: 09-18 01:22:15] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 41879}
SO2(0)最佳损失：-0.724670386155583
SO2(0)最好结果：{'pred_time': 9.29110133191983e-06, 'wall_clock_time': 46.88903832435608, 'metric_for_logging': {'pred_time': 9.29110133191983e-06}, 'val_loss': 1.724670386155583, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 41879}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 41879, 'experiment_tag': 'exp', 'time_total_s': 12.812310695648193}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7178592213344119
SO2(0)的mse=7.952142990978371
SO2(0)的mae=1.6920994483993492
SO2(0)的mar=0.19796852973877677
总共花费的时间为：67.43
黑河市
2263A
2264A
2265A
[flaml.automl: 09-18 01:31:38] {2390} INFO - task = regression
[flaml.automl: 09-18 01:31:38] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:31:38] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:31:38] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:31:38] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:31:38] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:31:39] {3025} INFO - Estimated sufficient time budget=12163s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 01:31:39] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.2810,	best estimator xgboost's best error=3.2810
[flaml.automl: 09-18 01:31:39] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:31:41] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.6633,	best estimator xgboost's best error=1.6633
[flaml.automl: 09-18 01:31:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:31:42] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.6633,	best estimator xgboost's best error=1.6633
[flaml.automl: 09-18 01:31:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:31:52] {3072} INFO -  at 14.6s,	estimator xgboost's best error=1.6633,	best estimator xgboost's best error=1.6633
[flaml.automl: 09-18 01:31:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:31:53] {3072} INFO -  at 15.7s,	estimator xgboost's best error=1.3086,	best estimator xgboost's best error=1.3086
[flaml.automl: 09-18 01:31:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:31:55] {3072} INFO -  at 17.3s,	estimator xgboost's best error=1.3010,	best estimator xgboost's best error=1.3010
[flaml.automl: 09-18 01:31:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:31:56] {3072} INFO -  at 18.9s,	estimator xgboost's best error=1.2494,	best estimator xgboost's best error=1.2494
[flaml.automl: 09-18 01:31:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:31:59] {3072} INFO -  at 21.6s,	estimator xgboost's best error=1.2494,	best estimator xgboost's best error=1.2494
[flaml.automl: 09-18 01:31:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:32:01] {3072} INFO -  at 23.3s,	estimator xgboost's best error=1.2494,	best estimator xgboost's best error=1.2494
[flaml.automl: 09-18 01:32:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:32:04] {3072} INFO -  at 26.3s,	estimator xgboost's best error=1.1822,	best estimator xgboost's best error=1.1822
[flaml.automl: 09-18 01:32:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:32:05] {3072} INFO -  at 27.9s,	estimator xgboost's best error=1.1822,	best estimator xgboost's best error=1.1822
[flaml.automl: 09-18 01:32:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:32:07] {3072} INFO -  at 29.1s,	estimator xgboost's best error=1.1822,	best estimator xgboost's best error=1.1822
[flaml.automl: 09-18 01:32:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:32:20] {3072} INFO -  at 42.7s,	estimator xgboost's best error=1.1822,	best estimator xgboost's best error=1.1822
[flaml.automl: 09-18 01:32:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:32:25] {3072} INFO -  at 47.6s,	estimator xgboost's best error=1.1822,	best estimator xgboost's best error=1.1822
[flaml.automl: 09-18 01:32:25] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:32:27] {3072} INFO -  at 49.8s,	estimator xgboost's best error=1.1822,	best estimator xgboost's best error=1.1822
[flaml.automl: 09-18 01:32:27] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 01:32:32] {3072} INFO -  at 54.8s,	estimator xgboost's best error=1.1822,	best estimator xgboost's best error=1.1822
[flaml.automl: 09-18 01:32:32] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 01:32:34] {3072} INFO -  at 56.7s,	estimator xgboost's best error=1.1822,	best estimator xgboost's best error=1.1822
[flaml.automl: 09-18 01:32:34] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 01:32:36] {3072} INFO -  at 58.3s,	estimator xgboost's best error=1.1822,	best estimator xgboost's best error=1.1822
[flaml.automl: 09-18 01:32:39] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-18 01:32:39] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 01:32:39] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:32:39] {2637} INFO - Time taken to find the best model: 26.288724660873413
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-0.182206144037935
SO2(0)最好结果：{'pred_time': 1.2195373669990911e-05, 'wall_clock_time': 26.288724660873413, 'metric_for_logging': {'pred_time': 1.2195373669990911e-05}, 'val_loss': 1.182206144037935, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 3.027122735977173}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.21972318638857058
SO2(0)的mse=4.828967389033087
SO2(0)的mae=1.1511392602232835
SO2(0)的mar=0.20019347301384952
总共花费的时间为：61.81
绥化市
2266A
2267A
[flaml.automl: 09-18 01:39:04] {2390} INFO - task = regression
[flaml.automl: 09-18 01:39:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:39:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:39:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:39:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:39:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:39:05] {3025} INFO - Estimated sufficient time budget=11268s. Estimated necessary time budget=11s.
[flaml.automl: 09-18 01:39:05] {3072} INFO -  at 1.2s,	estimator xgboost's best error=4.4729,	best estimator xgboost's best error=4.4729
[flaml.automl: 09-18 01:39:05] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:39:07] {3072} INFO -  at 3.2s,	estimator xgboost's best error=2.2154,	best estimator xgboost's best error=2.2154
[flaml.automl: 09-18 01:39:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:39:08] {3072} INFO -  at 4.4s,	estimator xgboost's best error=2.2154,	best estimator xgboost's best error=2.2154
[flaml.automl: 09-18 01:39:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:39:17] {3072} INFO -  at 13.6s,	estimator xgboost's best error=2.2154,	best estimator xgboost's best error=2.2154
[flaml.automl: 09-18 01:39:17] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:39:19] {3072} INFO -  at 14.8s,	estimator xgboost's best error=1.6631,	best estimator xgboost's best error=1.6631
[flaml.automl: 09-18 01:39:19] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:39:20] {3072} INFO -  at 16.3s,	estimator xgboost's best error=1.5740,	best estimator xgboost's best error=1.5740
[flaml.automl: 09-18 01:39:20] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:39:22] {3072} INFO -  at 17.9s,	estimator xgboost's best error=1.5189,	best estimator xgboost's best error=1.5189
[flaml.automl: 09-18 01:39:22] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:39:24] {3072} INFO -  at 20.5s,	estimator xgboost's best error=1.5189,	best estimator xgboost's best error=1.5189
[flaml.automl: 09-18 01:39:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:39:26] {3072} INFO -  at 22.2s,	estimator xgboost's best error=1.5189,	best estimator xgboost's best error=1.5189
[flaml.automl: 09-18 01:39:26] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:39:29] {3072} INFO -  at 25.2s,	estimator xgboost's best error=1.3792,	best estimator xgboost's best error=1.3792
[flaml.automl: 09-18 01:39:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:39:31] {3072} INFO -  at 26.8s,	estimator xgboost's best error=1.3792,	best estimator xgboost's best error=1.3792
[flaml.automl: 09-18 01:39:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:39:32] {3072} INFO -  at 27.9s,	estimator xgboost's best error=1.3792,	best estimator xgboost's best error=1.3792
[flaml.automl: 09-18 01:39:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:39:44] {3072} INFO -  at 39.9s,	estimator xgboost's best error=1.3792,	best estimator xgboost's best error=1.3792
[flaml.automl: 09-18 01:39:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:39:50] {3072} INFO -  at 45.7s,	estimator xgboost's best error=1.3493,	best estimator xgboost's best error=1.3493
[flaml.automl: 09-18 01:39:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 01:39:53] {3072} INFO -  at 49.1s,	estimator xgboost's best error=1.3493,	best estimator xgboost's best error=1.3493
[flaml.automl: 09-18 01:39:53] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 01:40:02] {3072} INFO -  at 58.4s,	estimator xgboost's best error=1.3493,	best estimator xgboost's best error=1.3493
[flaml.automl: 09-18 01:40:08] {3335} INFO - retrain xgboost for 5.8s
[flaml.automl: 09-18 01:40:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 01:40:08] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:40:08] {2637} INFO - Time taken to find the best model: 45.71068525314331
[flaml.automl: 09-18 01:40:08] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 16, 'max_leaves': 6, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}
SO2(0)最佳损失：-0.3492560633365682
SO2(0)最好结果：{'pred_time': 1.674999069693862e-05, 'wall_clock_time': 45.71068525314331, 'metric_for_logging': {'pred_time': 1.674999069693862e-05}, 'val_loss': 1.3492560633365682, 'training_iteration': 1, 'config': {'n_estimators': 16, 'max_leaves': 6, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}, 'config/n_estimators': 16, 'config/max_leaves': 6, 'config/min_child_weight': 0.07186500551974488, 'config/learning_rate': 0.18147476059072173, 'config/subsample': 0.7641710216408443, 'config/colsample_bylevel': 0.8883913141026579, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.14043216453256416, 'experiment_tag': 'exp', 'time_total_s': 5.833724021911621}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=6, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.36526232625056454
SO2(0)的mse=7.373248334741039
SO2(0)的mae=1.4164776826038041
SO2(0)的mar=0.19271860896685192
总共花费的时间为：64.54
大兴安岭地区
3663A
[flaml.automl: 09-18 01:43:27] {2390} INFO - task = regression
[flaml.automl: 09-18 01:43:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:43:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:43:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:43:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:43:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:43:29] {3025} INFO - Estimated sufficient time budget=21709s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 01:43:29] {3072} INFO -  at 2.2s,	estimator xgboost's best error=4.3016,	best estimator xgboost's best error=4.3016
[flaml.automl: 09-18 01:43:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:43:33] {3072} INFO -  at 5.7s,	estimator xgboost's best error=2.5633,	best estimator xgboost's best error=2.5633
[flaml.automl: 09-18 01:43:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:43:34] {3072} INFO -  at 7.1s,	estimator xgboost's best error=2.5633,	best estimator xgboost's best error=2.5633
[flaml.automl: 09-18 01:43:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:43:44] {3072} INFO -  at 17.1s,	estimator xgboost's best error=2.5633,	best estimator xgboost's best error=2.5633
[flaml.automl: 09-18 01:43:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 01:43:46] {3072} INFO -  at 18.9s,	estimator xgboost's best error=1.8333,	best estimator xgboost's best error=1.8333
[flaml.automl: 09-18 01:43:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 01:43:48] {3072} INFO -  at 21.6s,	estimator xgboost's best error=1.7943,	best estimator xgboost's best error=1.7943
[flaml.automl: 09-18 01:43:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 01:43:51] {3072} INFO -  at 24.5s,	estimator xgboost's best error=1.7796,	best estimator xgboost's best error=1.7796
[flaml.automl: 09-18 01:43:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 01:43:55] {3072} INFO -  at 28.1s,	estimator xgboost's best error=1.7796,	best estimator xgboost's best error=1.7796
[flaml.automl: 09-18 01:43:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 01:43:57] {3072} INFO -  at 30.6s,	estimator xgboost's best error=1.7796,	best estimator xgboost's best error=1.7796
[flaml.automl: 09-18 01:43:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 01:44:02] {3072} INFO -  at 35.2s,	estimator xgboost's best error=1.6973,	best estimator xgboost's best error=1.6973
[flaml.automl: 09-18 01:44:02] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 01:44:05] {3072} INFO -  at 37.9s,	estimator xgboost's best error=1.6973,	best estimator xgboost's best error=1.6973
[flaml.automl: 09-18 01:44:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 01:44:07] {3072} INFO -  at 39.8s,	estimator xgboost's best error=1.6973,	best estimator xgboost's best error=1.6973
[flaml.automl: 09-18 01:44:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 01:44:22] {3072} INFO -  at 55.3s,	estimator xgboost's best error=1.6973,	best estimator xgboost's best error=1.6973
[flaml.automl: 09-18 01:44:22] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 01:44:26] {3072} INFO -  at 59.6s,	estimator xgboost's best error=1.6973,	best estimator xgboost's best error=1.6973
[flaml.automl: 09-18 01:44:31] {3335} INFO - retrain xgboost for 4.3s
[flaml.automl: 09-18 01:44:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 01:44:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 01:44:31] {2637} INFO - Time taken to find the best model: 35.17690348625183
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-0.6972889285828803
SO2(0)最好结果：{'pred_time': 6.801608395596296e-05, 'wall_clock_time': 35.17690348625183, 'metric_for_logging': {'pred_time': 6.801608395596296e-05}, 'val_loss': 1.6972889285828803, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 6, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 4.581723690032959}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.24975948375675838
SO2(0)的mse=13.513650479788001
SO2(0)的mae=1.889065823451565
SO2(0)的mar=0.24637757460484222
总共花费的时间为：64.17
蚌埠市
2270A
2271A
2274A
2275A
3715A
[flaml.automl: 09-18 01:59:50] {2390} INFO - task = regression
[flaml.automl: 09-18 01:59:50] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 01:59:50] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 01:59:50] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 01:59:50] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 01:59:50] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 01:59:51] {3025} INFO - Estimated sufficient time budget=66455s. Estimated necessary time budget=66s.
[flaml.automl: 09-18 01:59:51] {3072} INFO -  at 1.5s,	estimator xgboost's best error=6.3985,	best estimator xgboost's best error=6.3985
[flaml.automl: 09-18 01:59:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 01:59:53] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.9657,	best estimator xgboost's best error=2.9657
[flaml.automl: 09-18 01:59:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 01:59:55] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.9657,	best estimator xgboost's best error=2.9657
[flaml.automl: 09-18 01:59:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 01:59:59] {3072} INFO -  at 9.6s,	estimator xgboost's best error=2.9657,	best estimator xgboost's best error=2.9657
[flaml.automl: 09-18 01:59:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:00:01] {3072} INFO -  at 10.8s,	estimator xgboost's best error=1.7594,	best estimator xgboost's best error=1.7594
[flaml.automl: 09-18 02:00:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:00:02] {3072} INFO -  at 12.3s,	estimator xgboost's best error=1.7594,	best estimator xgboost's best error=1.7594
[flaml.automl: 09-18 02:00:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:00:04] {3072} INFO -  at 14.1s,	estimator xgboost's best error=1.4086,	best estimator xgboost's best error=1.4086
[flaml.automl: 09-18 02:00:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:00:07] {3072} INFO -  at 16.7s,	estimator xgboost's best error=1.4086,	best estimator xgboost's best error=1.4086
[flaml.automl: 09-18 02:00:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:00:08] {3072} INFO -  at 18.4s,	estimator xgboost's best error=1.4086,	best estimator xgboost's best error=1.4086
[flaml.automl: 09-18 02:00:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:00:11] {3072} INFO -  at 21.4s,	estimator xgboost's best error=1.4086,	best estimator xgboost's best error=1.4086
[flaml.automl: 09-18 02:00:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:00:13] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.3907,	best estimator xgboost's best error=1.3907
[flaml.automl: 09-18 02:00:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:00:14] {3072} INFO -  at 24.0s,	estimator xgboost's best error=1.3907,	best estimator xgboost's best error=1.3907
[flaml.automl: 09-18 02:00:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:00:17] {3072} INFO -  at 26.8s,	estimator xgboost's best error=1.3524,	best estimator xgboost's best error=1.3524
[flaml.automl: 09-18 02:00:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:00:19] {3072} INFO -  at 29.4s,	estimator xgboost's best error=1.3272,	best estimator xgboost's best error=1.3272
[flaml.automl: 09-18 02:00:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:00:22] {3072} INFO -  at 31.7s,	estimator xgboost's best error=1.3272,	best estimator xgboost's best error=1.3272
[flaml.automl: 09-18 02:00:22] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 02:00:24] {3072} INFO -  at 33.9s,	estimator xgboost's best error=1.3272,	best estimator xgboost's best error=1.3272
[flaml.automl: 09-18 02:00:24] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 02:00:26] {3072} INFO -  at 36.0s,	estimator xgboost's best error=1.3272,	best estimator xgboost's best error=1.3272
[flaml.automl: 09-18 02:00:26] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 02:00:28] {3072} INFO -  at 37.9s,	estimator xgboost's best error=1.3272,	best estimator xgboost's best error=1.3272
[flaml.automl: 09-18 02:00:28] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 02:00:38] {3072} INFO -  at 48.7s,	estimator xgboost's best error=1.3144,	best estimator xgboost's best error=1.3144
[flaml.automl: 09-18 02:00:38] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 02:00:43] {3072} INFO -  at 52.8s,	estimator xgboost's best error=1.3144,	best estimator xgboost's best error=1.3144
[flaml.automl: 09-18 02:00:53] {3335} INFO - retrain xgboost for 10.8s
[flaml.automl: 09-18 02:00:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:00:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:00:53] {2637} INFO - Time taken to find the best model: 48.68450450897217
[flaml.automl: 09-18 02:00:53] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 55161}
SO2(0)最佳损失：-0.3143715970278953
SO2(0)最好结果：{'pred_time': 6.516489274739052e-06, 'wall_clock_time': 48.68450450897217, 'metric_for_logging': {'pred_time': 6.516489274739052e-06}, 'val_loss': 1.3143715970278953, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 55161}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 55161, 'experiment_tag': 'exp', 'time_total_s': 10.78786039352417}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6551763402805537
SO2(0)的mse=5.411525381250738
SO2(0)的mae=1.325275695563728
SO2(0)的mar=0.13437207974849016
总共花费的时间为：64.43
淮南市
2278A
2279A
2280A
2281A
[flaml.automl: 09-18 02:13:25] {2390} INFO - task = regression
[flaml.automl: 09-18 02:13:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:13:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:13:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:13:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:13:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:13:26] {3025} INFO - Estimated sufficient time budget=48625s. Estimated necessary time budget=49s.
[flaml.automl: 09-18 02:13:26] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.6094,	best estimator xgboost's best error=4.6094
[flaml.automl: 09-18 02:13:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:13:28] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.1975,	best estimator xgboost's best error=2.1975
[flaml.automl: 09-18 02:13:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:13:30] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.1975,	best estimator xgboost's best error=2.1975
[flaml.automl: 09-18 02:13:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:13:36] {3072} INFO -  at 10.8s,	estimator xgboost's best error=2.1975,	best estimator xgboost's best error=2.1975
[flaml.automl: 09-18 02:13:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:13:37] {3072} INFO -  at 12.0s,	estimator xgboost's best error=1.4927,	best estimator xgboost's best error=1.4927
[flaml.automl: 09-18 02:13:37] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:13:38] {3072} INFO -  at 13.5s,	estimator xgboost's best error=1.4927,	best estimator xgboost's best error=1.4927
[flaml.automl: 09-18 02:13:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:13:40] {3072} INFO -  at 15.2s,	estimator xgboost's best error=1.2330,	best estimator xgboost's best error=1.2330
[flaml.automl: 09-18 02:13:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:13:43] {3072} INFO -  at 17.9s,	estimator xgboost's best error=1.2330,	best estimator xgboost's best error=1.2330
[flaml.automl: 09-18 02:13:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:13:44] {3072} INFO -  at 19.5s,	estimator xgboost's best error=1.2330,	best estimator xgboost's best error=1.2330
[flaml.automl: 09-18 02:13:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:13:47] {3072} INFO -  at 22.5s,	estimator xgboost's best error=1.2330,	best estimator xgboost's best error=1.2330
[flaml.automl: 09-18 02:13:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:13:49] {3072} INFO -  at 24.0s,	estimator xgboost's best error=1.2330,	best estimator xgboost's best error=1.2330
[flaml.automl: 09-18 02:13:49] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:13:51] {3072} INFO -  at 25.7s,	estimator xgboost's best error=1.2330,	best estimator xgboost's best error=1.2330
[flaml.automl: 09-18 02:13:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:13:52] {3072} INFO -  at 26.9s,	estimator xgboost's best error=1.2330,	best estimator xgboost's best error=1.2330
[flaml.automl: 09-18 02:13:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:13:59] {3072} INFO -  at 33.9s,	estimator xgboost's best error=1.2330,	best estimator xgboost's best error=1.2330
[flaml.automl: 09-18 02:13:59] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:14:02] {3072} INFO -  at 37.5s,	estimator xgboost's best error=1.2078,	best estimator xgboost's best error=1.2078
[flaml.automl: 09-18 02:14:02] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 02:14:04] {3072} INFO -  at 39.6s,	estimator xgboost's best error=1.2078,	best estimator xgboost's best error=1.2078
[flaml.automl: 09-18 02:14:04] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 02:14:10] {3072} INFO -  at 45.5s,	estimator xgboost's best error=1.2008,	best estimator xgboost's best error=1.2008
[flaml.automl: 09-18 02:14:10] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 02:14:14] {3072} INFO -  at 49.1s,	estimator xgboost's best error=1.2008,	best estimator xgboost's best error=1.2008
[flaml.automl: 09-18 02:14:14] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 02:14:17] {3072} INFO -  at 52.2s,	estimator xgboost's best error=1.2008,	best estimator xgboost's best error=1.2008
[flaml.automl: 09-18 02:14:17] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 02:14:25] {3072} INFO -  at 59.7s,	estimator xgboost's best error=1.2008,	best estimator xgboost's best error=1.2008
[flaml.automl: 09-18 02:14:30] {3335} INFO - retrain xgboost for 5.9s
[flaml.automl: 09-18 02:14:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.19405630899231047,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=11.615070090947864, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.07808876344933313, scale_pos_weight=1,
             subsample=0.9453052099956202, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:14:30] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:14:30] {2637} INFO - Time taken to find the best model: 45.47073197364807
[flaml.automl: 09-18 02:14:30] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 7, 'min_child_weight': 11.615070090947864, 'learning_rate': 0.19405630899231047, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.07808876344933313, 'FLAML_sample_size': 41715}
SO2(0)最佳损失：-0.2007827719189772
SO2(0)最好结果：{'pred_time': 9.698431691391084e-06, 'wall_clock_time': 45.47073197364807, 'metric_for_logging': {'pred_time': 9.698431691391084e-06}, 'val_loss': 1.2007827719189772, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 7, 'min_child_weight': 11.615070090947864, 'learning_rate': 0.19405630899231047, 'subsample': 0.9453052099956202, 'colsample_bylevel': 0.7581172583755363, 'colsample_bytree': 0.92006762344246, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.07808876344933313, 'FLAML_sample_size': 41715}, 'config/n_estimators': 14, 'config/max_leaves': 7, 'config/min_child_weight': 11.615070090947864, 'config/learning_rate': 0.19405630899231047, 'config/subsample': 0.9453052099956202, 'config/colsample_bylevel': 0.7581172583755363, 'config/colsample_bytree': 0.92006762344246, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.07808876344933313, 'config/FLAML_sample_size': 41715, 'experiment_tag': 'exp', 'time_total_s': 5.900852203369141}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7581172583755363, colsample_bynode=1,
             colsample_bytree=0.92006762344246, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.19405630899231047,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=11.615070090947864, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.07808876344933313, scale_pos_weight=1,
             subsample=0.9453052099956202, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.4780973045672924
SO2(0)的mse=4.200624353052669
SO2(0)的mae=1.1603545274717835
SO2(0)的mar=0.16220111092817563
总共花费的时间为：66.22
淮北市
2282A
2283A
2284A
3330A
[flaml.automl: 09-18 02:27:07] {2390} INFO - task = regression
[flaml.automl: 09-18 02:27:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:27:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:27:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:27:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:27:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:27:09] {3025} INFO - Estimated sufficient time budget=90166s. Estimated necessary time budget=90s.
[flaml.automl: 09-18 02:27:09] {3072} INFO -  at 2.2s,	estimator xgboost's best error=3.8205,	best estimator xgboost's best error=3.8205
[flaml.automl: 09-18 02:27:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:27:12] {3072} INFO -  at 5.6s,	estimator xgboost's best error=1.7946,	best estimator xgboost's best error=1.7946
[flaml.automl: 09-18 02:27:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:27:14] {3072} INFO -  at 7.6s,	estimator xgboost's best error=1.7946,	best estimator xgboost's best error=1.7946
[flaml.automl: 09-18 02:27:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:27:20] {3072} INFO -  at 13.2s,	estimator xgboost's best error=1.7946,	best estimator xgboost's best error=1.7946
[flaml.automl: 09-18 02:27:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:27:22] {3072} INFO -  at 15.0s,	estimator xgboost's best error=1.1741,	best estimator xgboost's best error=1.1741
[flaml.automl: 09-18 02:27:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:27:24] {3072} INFO -  at 17.5s,	estimator xgboost's best error=1.0473,	best estimator xgboost's best error=1.0473
[flaml.automl: 09-18 02:27:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:27:27] {3072} INFO -  at 20.6s,	estimator xgboost's best error=0.9839,	best estimator xgboost's best error=0.9839
[flaml.automl: 09-18 02:27:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:27:31] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.9839,	best estimator xgboost's best error=0.9839
[flaml.automl: 09-18 02:27:31] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:27:33] {3072} INFO -  at 25.9s,	estimator xgboost's best error=0.9839,	best estimator xgboost's best error=0.9839
[flaml.automl: 09-18 02:27:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:27:36] {3072} INFO -  at 28.9s,	estimator xgboost's best error=0.9458,	best estimator xgboost's best error=0.9458
[flaml.automl: 09-18 02:27:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:27:37] {3072} INFO -  at 30.5s,	estimator xgboost's best error=0.9458,	best estimator xgboost's best error=0.9458
[flaml.automl: 09-18 02:27:37] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:27:38] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.9458,	best estimator xgboost's best error=0.9458
[flaml.automl: 09-18 02:27:38] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:27:41] {3072} INFO -  at 34.5s,	estimator xgboost's best error=0.8961,	best estimator xgboost's best error=0.8961
[flaml.automl: 09-18 02:27:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:27:43] {3072} INFO -  at 36.7s,	estimator xgboost's best error=0.8961,	best estimator xgboost's best error=0.8961
[flaml.automl: 09-18 02:27:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:27:46] {3072} INFO -  at 39.3s,	estimator xgboost's best error=0.8961,	best estimator xgboost's best error=0.8961
[flaml.automl: 09-18 02:27:46] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 02:27:48] {3072} INFO -  at 41.1s,	estimator xgboost's best error=0.8961,	best estimator xgboost's best error=0.8961
[flaml.automl: 09-18 02:27:48] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 02:27:50] {3072} INFO -  at 42.8s,	estimator xgboost's best error=0.8961,	best estimator xgboost's best error=0.8961
[flaml.automl: 09-18 02:27:50] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 02:28:03] {3072} INFO -  at 56.5s,	estimator xgboost's best error=0.8802,	best estimator xgboost's best error=0.8802
[flaml.automl: 09-18 02:28:17] {3335} INFO - retrain xgboost for 13.7s
[flaml.automl: 09-18 02:28:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 02:28:17] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:28:17] {2637} INFO - Time taken to find the best model: 56.49373126029968
[flaml.automl: 09-18 02:28:17] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007, 'FLAML_sample_size': 44117}
SO2(0)最佳损失：0.11981762557771924
SO2(0)最好结果：{'pred_time': 8.218388613853003e-06, 'wall_clock_time': 56.49373126029968, 'metric_for_logging': {'pred_time': 8.218388613853003e-06}, 'val_loss': 0.8801823744222808, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007, 'FLAML_sample_size': 44117}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'config/FLAML_sample_size': 44117, 'experiment_tag': 'exp', 'time_total_s': 13.72320556640625}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7285412951714778
SO2(0)的mse=2.0673660096839943
SO2(0)的mae=0.916036191699219
SO2(0)的mar=0.1645776017011481
总共花费的时间为：71.29
铜陵市
2285A
2286A
2287A
2288A
2289A
2290A
[flaml.automl: 09-18 02:46:32] {2390} INFO - task = regression
[flaml.automl: 09-18 02:46:32] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:46:32] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:46:32] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:46:32] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:46:32] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:46:33] {3025} INFO - Estimated sufficient time budget=71660s. Estimated necessary time budget=72s.
[flaml.automl: 09-18 02:46:33] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.7142,	best estimator xgboost's best error=5.7142
[flaml.automl: 09-18 02:46:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:46:35] {3072} INFO -  at 3.3s,	estimator xgboost's best error=2.9727,	best estimator xgboost's best error=2.9727
[flaml.automl: 09-18 02:46:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:46:36] {3072} INFO -  at 4.5s,	estimator xgboost's best error=2.9727,	best estimator xgboost's best error=2.9727
[flaml.automl: 09-18 02:46:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:46:40] {3072} INFO -  at 8.7s,	estimator xgboost's best error=2.9727,	best estimator xgboost's best error=2.9727
[flaml.automl: 09-18 02:46:40] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:46:41] {3072} INFO -  at 9.8s,	estimator xgboost's best error=2.4424,	best estimator xgboost's best error=2.4424
[flaml.automl: 09-18 02:46:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:46:43] {3072} INFO -  at 11.5s,	estimator xgboost's best error=2.4354,	best estimator xgboost's best error=2.4354
[flaml.automl: 09-18 02:46:43] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:46:46] {3072} INFO -  at 14.1s,	estimator xgboost's best error=2.2695,	best estimator xgboost's best error=2.2695
[flaml.automl: 09-18 02:46:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:46:49] {3072} INFO -  at 17.1s,	estimator xgboost's best error=2.2695,	best estimator xgboost's best error=2.2695
[flaml.automl: 09-18 02:46:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:46:51] {3072} INFO -  at 19.4s,	estimator xgboost's best error=2.2695,	best estimator xgboost's best error=2.2695
[flaml.automl: 09-18 02:46:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:46:54] {3072} INFO -  at 22.2s,	estimator xgboost's best error=2.2695,	best estimator xgboost's best error=2.2695
[flaml.automl: 09-18 02:46:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:46:57] {3072} INFO -  at 24.8s,	estimator xgboost's best error=2.2695,	best estimator xgboost's best error=2.2695
[flaml.automl: 09-18 02:46:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:46:59] {3072} INFO -  at 27.7s,	estimator xgboost's best error=2.2667,	best estimator xgboost's best error=2.2667
[flaml.automl: 09-18 02:46:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:47:01] {3072} INFO -  at 29.6s,	estimator xgboost's best error=2.2667,	best estimator xgboost's best error=2.2667
[flaml.automl: 09-18 02:47:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:47:12] {3072} INFO -  at 40.6s,	estimator xgboost's best error=2.1716,	best estimator xgboost's best error=2.1716
[flaml.automl: 09-18 02:47:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:47:31] {3072} INFO -  at 59.4s,	estimator xgboost's best error=2.1524,	best estimator xgboost's best error=2.1524
[flaml.automl: 09-18 02:47:53] {3335} INFO - retrain xgboost for 21.7s
[flaml.automl: 09-18 02:47:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:47:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:47:53] {2637} INFO - Time taken to find the best model: 59.41193103790283
[flaml.automl: 09-18 02:47:53] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 63439}
SO2(0)最佳损失：-1.1524032736216934
SO2(0)最好结果：{'pred_time': 1.2048746207230884e-05, 'wall_clock_time': 59.41193103790283, 'metric_for_logging': {'pred_time': 1.2048746207230884e-05}, 'val_loss': 2.1524032736216934, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 63439}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 63439, 'experiment_tag': 'exp', 'time_total_s': 18.846962928771973}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.43227877048831265
SO2(0)的mse=13.677134234048665
SO2(0)的mae=2.1712158592351245
SO2(0)的mar=0.26545316358210186
总共花费的时间为：82.11
安庆市
2291A
2292A
3173A
[flaml.automl: 09-18 02:57:37] {2390} INFO - task = regression
[flaml.automl: 09-18 02:57:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 02:57:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 02:57:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 02:57:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 02:57:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 02:57:38] {3025} INFO - Estimated sufficient time budget=12028s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 02:57:38] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.1366,	best estimator xgboost's best error=4.1366
[flaml.automl: 09-18 02:57:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 02:57:40] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.9806,	best estimator xgboost's best error=1.9806
[flaml.automl: 09-18 02:57:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 02:57:41] {3072} INFO -  at 4.6s,	estimator xgboost's best error=1.9806,	best estimator xgboost's best error=1.9806
[flaml.automl: 09-18 02:57:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 02:57:51] {3072} INFO -  at 14.4s,	estimator xgboost's best error=1.9806,	best estimator xgboost's best error=1.9806
[flaml.automl: 09-18 02:57:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 02:57:52] {3072} INFO -  at 15.6s,	estimator xgboost's best error=1.2642,	best estimator xgboost's best error=1.2642
[flaml.automl: 09-18 02:57:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 02:57:54] {3072} INFO -  at 17.1s,	estimator xgboost's best error=1.2642,	best estimator xgboost's best error=1.2642
[flaml.automl: 09-18 02:57:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 02:57:55] {3072} INFO -  at 18.8s,	estimator xgboost's best error=1.0681,	best estimator xgboost's best error=1.0681
[flaml.automl: 09-18 02:57:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 02:57:58] {3072} INFO -  at 21.5s,	estimator xgboost's best error=1.0681,	best estimator xgboost's best error=1.0681
[flaml.automl: 09-18 02:57:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 02:57:59] {3072} INFO -  at 23.1s,	estimator xgboost's best error=1.0681,	best estimator xgboost's best error=1.0681
[flaml.automl: 09-18 02:57:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 02:58:03] {3072} INFO -  at 26.1s,	estimator xgboost's best error=1.0681,	best estimator xgboost's best error=1.0681
[flaml.automl: 09-18 02:58:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 02:58:04] {3072} INFO -  at 27.5s,	estimator xgboost's best error=1.0421,	best estimator xgboost's best error=1.0421
[flaml.automl: 09-18 02:58:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 02:58:05] {3072} INFO -  at 28.7s,	estimator xgboost's best error=1.0421,	best estimator xgboost's best error=1.0421
[flaml.automl: 09-18 02:58:05] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 02:58:11] {3072} INFO -  at 34.7s,	estimator xgboost's best error=1.0277,	best estimator xgboost's best error=1.0277
[flaml.automl: 09-18 02:58:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 02:58:22] {3072} INFO -  at 45.3s,	estimator xgboost's best error=1.0034,	best estimator xgboost's best error=1.0034
[flaml.automl: 09-18 02:58:22] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 02:58:28] {3072} INFO -  at 51.3s,	estimator xgboost's best error=1.0034,	best estimator xgboost's best error=1.0034
[flaml.automl: 09-18 02:58:38] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-18 02:58:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 02:58:38] {2636} INFO - fit succeeded
[flaml.automl: 09-18 02:58:38] {2637} INFO - Time taken to find the best model: 45.317033529281616
[flaml.automl: 09-18 02:58:38] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
SO2(0)最佳损失：-0.003387679312662595
SO2(0)最好结果：{'pred_time': 1.0952590783175239e-05, 'wall_clock_time': 45.317033529281616, 'metric_for_logging': {'pred_time': 1.0952590783175239e-05}, 'val_loss': 1.0033876793126626, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 10.663387537002563}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6599624296108701
SO2(0)的mse=3.1856609224246526
SO2(0)的mae=1.0327653386954676
SO2(0)的mar=0.1663903347901812
总共花费的时间为：62.53
黄山市
2295A
2296A
2297A
[flaml.automl: 09-18 03:08:30] {2390} INFO - task = regression
[flaml.automl: 09-18 03:08:30] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:08:30] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:08:30] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:08:30] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:08:30] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:08:31] {3025} INFO - Estimated sufficient time budget=12068s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 03:08:31] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.8145,	best estimator xgboost's best error=3.8145
[flaml.automl: 09-18 03:08:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:08:33] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.7292,	best estimator xgboost's best error=1.7292
[flaml.automl: 09-18 03:08:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:08:34] {3072} INFO -  at 4.6s,	estimator xgboost's best error=1.7292,	best estimator xgboost's best error=1.7292
[flaml.automl: 09-18 03:08:34] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:08:44] {3072} INFO -  at 14.2s,	estimator xgboost's best error=1.7292,	best estimator xgboost's best error=1.7292
[flaml.automl: 09-18 03:08:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:08:45] {3072} INFO -  at 15.4s,	estimator xgboost's best error=0.7863,	best estimator xgboost's best error=0.7863
[flaml.automl: 09-18 03:08:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:08:47] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.6835,	best estimator xgboost's best error=0.6835
[flaml.automl: 09-18 03:08:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:08:48] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.5351,	best estimator xgboost's best error=0.5351
[flaml.automl: 09-18 03:08:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:08:51] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.5351,	best estimator xgboost's best error=0.5351
[flaml.automl: 09-18 03:08:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:08:52] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.4746,	best estimator xgboost's best error=0.4746
[flaml.automl: 09-18 03:08:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:08:55] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.4746,	best estimator xgboost's best error=0.4746
[flaml.automl: 09-18 03:08:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:08:57] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.4746,	best estimator xgboost's best error=0.4746
[flaml.automl: 09-18 03:08:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:08:58] {3072} INFO -  at 28.2s,	estimator xgboost's best error=0.4746,	best estimator xgboost's best error=0.4746
[flaml.automl: 09-18 03:08:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:09:05] {3072} INFO -  at 35.0s,	estimator xgboost's best error=0.4186,	best estimator xgboost's best error=0.4186
[flaml.automl: 09-18 03:09:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:09:17] {3072} INFO -  at 47.7s,	estimator xgboost's best error=0.4186,	best estimator xgboost's best error=0.4186
[flaml.automl: 09-18 03:09:36] {3335} INFO - retrain xgboost for 18.3s
[flaml.automl: 09-18 03:09:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 03:09:36] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:09:36] {2637} INFO - Time taken to find the best model: 47.6693012714386
[flaml.automl: 09-18 03:09:36] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}
SO2(0)最佳损失：0.5814119963677937
SO2(0)最好结果：{'pred_time': 1.1330994983647494e-05, 'wall_clock_time': 47.6693012714386, 'metric_for_logging': {'pred_time': 1.1330994983647494e-05}, 'val_loss': 0.4185880036322062, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'experiment_tag': 'exp', 'time_total_s': 12.657583236694336}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8581749738693678
SO2(0)的mse=0.48897748206021074
SO2(0)的mae=0.4058810165176705
SO2(0)的mar=0.06985103306305186
总共花费的时间为：66.50
滁州市
2298A
2299A
3331A
[flaml.automl: 09-18 03:19:52] {2390} INFO - task = regression
[flaml.automl: 09-18 03:19:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:19:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:19:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:19:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:19:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:19:53] {3025} INFO - Estimated sufficient time budget=11728s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 03:19:53] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.7711,	best estimator xgboost's best error=4.7711
[flaml.automl: 09-18 03:19:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:19:55] {3072} INFO -  at 3.3s,	estimator xgboost's best error=2.1724,	best estimator xgboost's best error=2.1724
[flaml.automl: 09-18 03:19:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:19:56] {3072} INFO -  at 4.5s,	estimator xgboost's best error=2.1724,	best estimator xgboost's best error=2.1724
[flaml.automl: 09-18 03:19:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:20:06] {3072} INFO -  at 14.2s,	estimator xgboost's best error=2.1724,	best estimator xgboost's best error=2.1724
[flaml.automl: 09-18 03:20:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:20:07] {3072} INFO -  at 15.4s,	estimator xgboost's best error=1.1352,	best estimator xgboost's best error=1.1352
[flaml.automl: 09-18 03:20:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:20:08] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.9533,	best estimator xgboost's best error=0.9533
[flaml.automl: 09-18 03:20:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:20:10] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.8959,	best estimator xgboost's best error=0.8959
[flaml.automl: 09-18 03:20:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:20:13] {3072} INFO -  at 21.1s,	estimator xgboost's best error=0.8959,	best estimator xgboost's best error=0.8959
[flaml.automl: 09-18 03:20:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:20:14] {3072} INFO -  at 22.7s,	estimator xgboost's best error=0.8959,	best estimator xgboost's best error=0.8959
[flaml.automl: 09-18 03:20:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:20:17] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.8925,	best estimator xgboost's best error=0.8925
[flaml.automl: 09-18 03:20:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:20:19] {3072} INFO -  at 27.1s,	estimator xgboost's best error=0.8925,	best estimator xgboost's best error=0.8925
[flaml.automl: 09-18 03:20:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:20:20] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.8925,	best estimator xgboost's best error=0.8925
[flaml.automl: 09-18 03:20:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:20:33] {3072} INFO -  at 41.4s,	estimator xgboost's best error=0.7951,	best estimator xgboost's best error=0.7951
[flaml.automl: 09-18 03:20:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:20:51] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.7735,	best estimator xgboost's best error=0.7735
[flaml.automl: 09-18 03:21:15] {3335} INFO - retrain xgboost for 24.0s
[flaml.automl: 09-18 03:21:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:21:15] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:21:15] {2637} INFO - Time taken to find the best model: 59.81757378578186
[flaml.automl: 09-18 03:21:15] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
SO2(0)最佳损失：0.22650697255244634
SO2(0)最好结果：{'pred_time': 1.0740812013177097e-05, 'wall_clock_time': 59.81757378578186, 'metric_for_logging': {'pred_time': 1.0740812013177097e-05}, 'val_loss': 0.7734930274475537, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 18.397658348083496}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7085416771099682
SO2(0)的mse=1.5405482217001623
SO2(0)的mae=0.7835144399285986
SO2(0)的mar=0.10301966625955623
总共花费的时间为：84.29
阜阳市
2301A
2875A
3468A
3469A
[flaml.automl: 09-18 03:33:14] {2390} INFO - task = regression
[flaml.automl: 09-18 03:33:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:33:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:33:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:33:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:33:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:33:15] {3025} INFO - Estimated sufficient time budget=52945s. Estimated necessary time budget=53s.
[flaml.automl: 09-18 03:33:15] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.9898,	best estimator xgboost's best error=3.9898
[flaml.automl: 09-18 03:33:15] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:33:17] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.8922,	best estimator xgboost's best error=1.8922
[flaml.automl: 09-18 03:33:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:33:19] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.8922,	best estimator xgboost's best error=1.8922
[flaml.automl: 09-18 03:33:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:33:24] {3072} INFO -  at 10.6s,	estimator xgboost's best error=1.8922,	best estimator xgboost's best error=1.8922
[flaml.automl: 09-18 03:33:24] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:33:26] {3072} INFO -  at 11.7s,	estimator xgboost's best error=1.1413,	best estimator xgboost's best error=1.1413
[flaml.automl: 09-18 03:33:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:33:27] {3072} INFO -  at 13.3s,	estimator xgboost's best error=1.1413,	best estimator xgboost's best error=1.1413
[flaml.automl: 09-18 03:33:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:33:29] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.9297,	best estimator xgboost's best error=0.9297
[flaml.automl: 09-18 03:33:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:33:32] {3072} INFO -  at 17.7s,	estimator xgboost's best error=0.9297,	best estimator xgboost's best error=0.9297
[flaml.automl: 09-18 03:33:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:33:33] {3072} INFO -  at 19.3s,	estimator xgboost's best error=0.9297,	best estimator xgboost's best error=0.9297
[flaml.automl: 09-18 03:33:33] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:33:36] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.9297,	best estimator xgboost's best error=0.9297
[flaml.automl: 09-18 03:33:36] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:33:38] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.9122,	best estimator xgboost's best error=0.9122
[flaml.automl: 09-18 03:33:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:33:39] {3072} INFO -  at 24.9s,	estimator xgboost's best error=0.9122,	best estimator xgboost's best error=0.9122
[flaml.automl: 09-18 03:33:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:33:43] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.8751,	best estimator xgboost's best error=0.8751
[flaml.automl: 09-18 03:33:43] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:33:46] {3072} INFO -  at 32.2s,	estimator xgboost's best error=0.8592,	best estimator xgboost's best error=0.8592
[flaml.automl: 09-18 03:33:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 03:33:49] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.8592,	best estimator xgboost's best error=0.8592
[flaml.automl: 09-18 03:33:49] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 03:33:51] {3072} INFO -  at 37.3s,	estimator xgboost's best error=0.8592,	best estimator xgboost's best error=0.8592
[flaml.automl: 09-18 03:33:51] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 03:33:53] {3072} INFO -  at 39.6s,	estimator xgboost's best error=0.8592,	best estimator xgboost's best error=0.8592
[flaml.automl: 09-18 03:33:53] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 03:33:56] {3072} INFO -  at 41.7s,	estimator xgboost's best error=0.8592,	best estimator xgboost's best error=0.8592
[flaml.automl: 09-18 03:33:56] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 03:34:06] {3072} INFO -  at 52.5s,	estimator xgboost's best error=0.8459,	best estimator xgboost's best error=0.8459
[flaml.automl: 09-18 03:34:17] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-18 03:34:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:34:17] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:34:17] {2637} INFO - Time taken to find the best model: 52.452146768569946
[flaml.automl: 09-18 03:34:17] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 44181}
SO2(0)最佳损失：0.15411681072775074
SO2(0)最好结果：{'pred_time': 8.235201087842892e-06, 'wall_clock_time': 52.452146768569946, 'metric_for_logging': {'pred_time': 8.235201087842892e-06}, 'val_loss': 0.8458831892722493, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 44181}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 44181, 'experiment_tag': 'exp', 'time_total_s': 10.74321699142456}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6918044744641884
SO2(0)的mse=1.9099752737325333
SO2(0)的mae=0.8632545563849039
SO2(0)的mar=0.14849062669540064
总共花费的时间为：63.87
宿州市
3463A
3634A
3701A
[flaml.automl: 09-18 03:43:59] {2390} INFO - task = regression
[flaml.automl: 09-18 03:43:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:43:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:43:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:43:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:43:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:44:01] {3025} INFO - Estimated sufficient time budget=12082s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 03:44:01] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.0975,	best estimator xgboost's best error=3.0975
[flaml.automl: 09-18 03:44:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:44:03] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.6470,	best estimator xgboost's best error=1.6470
[flaml.automl: 09-18 03:44:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:44:04] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.6470,	best estimator xgboost's best error=1.6470
[flaml.automl: 09-18 03:44:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:44:14] {3072} INFO -  at 14.7s,	estimator xgboost's best error=1.6470,	best estimator xgboost's best error=1.6470
[flaml.automl: 09-18 03:44:14] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:44:15] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.3138,	best estimator xgboost's best error=1.3138
[flaml.automl: 09-18 03:44:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:44:17] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.3138,	best estimator xgboost's best error=1.3138
[flaml.automl: 09-18 03:44:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:44:18] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.1088,	best estimator xgboost's best error=1.1088
[flaml.automl: 09-18 03:44:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:44:21] {3072} INFO -  at 21.8s,	estimator xgboost's best error=1.1088,	best estimator xgboost's best error=1.1088
[flaml.automl: 09-18 03:44:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:44:23] {3072} INFO -  at 23.5s,	estimator xgboost's best error=1.1088,	best estimator xgboost's best error=1.1088
[flaml.automl: 09-18 03:44:23] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:44:26] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.1088,	best estimator xgboost's best error=1.1088
[flaml.automl: 09-18 03:44:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:44:27] {3072} INFO -  at 28.0s,	estimator xgboost's best error=1.1088,	best estimator xgboost's best error=1.1088
[flaml.automl: 09-18 03:44:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:44:28] {3072} INFO -  at 29.1s,	estimator xgboost's best error=1.1088,	best estimator xgboost's best error=1.1088
[flaml.automl: 09-18 03:44:28] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:44:35] {3072} INFO -  at 36.2s,	estimator xgboost's best error=1.1025,	best estimator xgboost's best error=1.1025
[flaml.automl: 09-18 03:44:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:44:48] {3072} INFO -  at 48.9s,	estimator xgboost's best error=1.0890,	best estimator xgboost's best error=1.0890
[flaml.automl: 09-18 03:45:01] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-18 03:45:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 03:45:01] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:45:01] {2637} INFO - Time taken to find the best model: 48.94580674171448
[flaml.automl: 09-18 03:45:01] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}
SO2(0)最佳损失：-0.08899743992816078
SO2(0)最好结果：{'pred_time': 1.0569179028975857e-05, 'wall_clock_time': 48.94580674171448, 'metric_for_logging': {'pred_time': 1.0569179028975857e-05}, 'val_loss': 1.0889974399281608, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'experiment_tag': 'exp', 'time_total_s': 12.76050615310669}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5821682267979615
SO2(0)的mse=3.2289178463203188
SO2(0)的mae=1.063778343028389
SO2(0)的mar=0.25369083689980376
总共花费的时间为：62.18
六安市
2307A
2308A
2309A
2310A
[flaml.automl: 09-18 03:57:48] {2390} INFO - task = regression
[flaml.automl: 09-18 03:57:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 03:57:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 03:57:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 03:57:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 03:57:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 03:57:50] {3025} INFO - Estimated sufficient time budget=52521s. Estimated necessary time budget=53s.
[flaml.automl: 09-18 03:57:50] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.9097,	best estimator xgboost's best error=3.9097
[flaml.automl: 09-18 03:57:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 03:57:52] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.7821,	best estimator xgboost's best error=1.7821
[flaml.automl: 09-18 03:57:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 03:57:53] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.7821,	best estimator xgboost's best error=1.7821
[flaml.automl: 09-18 03:57:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 03:57:59] {3072} INFO -  at 10.8s,	estimator xgboost's best error=1.7821,	best estimator xgboost's best error=1.7821
[flaml.automl: 09-18 03:57:59] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 03:58:00] {3072} INFO -  at 11.9s,	estimator xgboost's best error=0.9726,	best estimator xgboost's best error=0.9726
[flaml.automl: 09-18 03:58:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 03:58:02] {3072} INFO -  at 13.5s,	estimator xgboost's best error=0.9067,	best estimator xgboost's best error=0.9067
[flaml.automl: 09-18 03:58:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 03:58:03] {3072} INFO -  at 15.1s,	estimator xgboost's best error=0.7811,	best estimator xgboost's best error=0.7811
[flaml.automl: 09-18 03:58:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 03:58:06] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.7811,	best estimator xgboost's best error=0.7811
[flaml.automl: 09-18 03:58:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 03:58:08] {3072} INFO -  at 19.3s,	estimator xgboost's best error=0.7421,	best estimator xgboost's best error=0.7421
[flaml.automl: 09-18 03:58:08] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 03:58:10] {3072} INFO -  at 22.2s,	estimator xgboost's best error=0.7421,	best estimator xgboost's best error=0.7421
[flaml.automl: 09-18 03:58:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 03:58:12] {3072} INFO -  at 23.5s,	estimator xgboost's best error=0.7421,	best estimator xgboost's best error=0.7421
[flaml.automl: 09-18 03:58:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 03:58:13] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.7421,	best estimator xgboost's best error=0.7421
[flaml.automl: 09-18 03:58:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 03:58:17] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.7351,	best estimator xgboost's best error=0.7351
[flaml.automl: 09-18 03:58:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 03:58:20] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.7180,	best estimator xgboost's best error=0.7180
[flaml.automl: 09-18 03:58:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 03:58:23] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.7180,	best estimator xgboost's best error=0.7180
[flaml.automl: 09-18 03:58:23] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 03:58:25] {3072} INFO -  at 36.8s,	estimator xgboost's best error=0.7180,	best estimator xgboost's best error=0.7180
[flaml.automl: 09-18 03:58:25] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 03:58:27] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.7180,	best estimator xgboost's best error=0.7180
[flaml.automl: 09-18 03:58:27] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 03:58:30] {3072} INFO -  at 41.3s,	estimator xgboost's best error=0.7180,	best estimator xgboost's best error=0.7180
[flaml.automl: 09-18 03:58:30] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 03:58:42] {3072} INFO -  at 54.0s,	estimator xgboost's best error=0.7131,	best estimator xgboost's best error=0.7131
[flaml.automl: 09-18 03:58:55] {3335} INFO - retrain xgboost for 12.7s
[flaml.automl: 09-18 03:58:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 03:58:55] {2636} INFO - fit succeeded
[flaml.automl: 09-18 03:58:55] {2637} INFO - Time taken to find the best model: 54.02128791809082
[flaml.automl: 09-18 03:58:55] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 44251}
SO2(0)最佳损失：0.28691315529718375
SO2(0)最好结果：{'pred_time': 8.210191305803287e-06, 'wall_clock_time': 54.02128791809082, 'metric_for_logging': {'pred_time': 8.210191305803287e-06}, 'val_loss': 0.7130868447028162, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 44251}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'config/FLAML_sample_size': 44251, 'experiment_tag': 'exp', 'time_total_s': 12.764670610427856}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6624386678330183
SO2(0)的mse=1.1746736807280715
SO2(0)的mae=0.6918628090995297
SO2(0)的mar=0.1158572941499775
总共花费的时间为：67.41
亳州市
2311A
2312A
3332A
[flaml.automl: 09-18 04:08:29] {2390} INFO - task = regression
[flaml.automl: 09-18 04:08:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:08:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:08:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:08:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:08:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:08:31] {3025} INFO - Estimated sufficient time budget=19359s. Estimated necessary time budget=19s.
[flaml.automl: 09-18 04:08:31] {3072} INFO -  at 2.1s,	estimator xgboost's best error=3.5193,	best estimator xgboost's best error=3.5193
[flaml.automl: 09-18 04:08:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:08:35] {3072} INFO -  at 5.3s,	estimator xgboost's best error=1.6558,	best estimator xgboost's best error=1.6558
[flaml.automl: 09-18 04:08:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:08:36] {3072} INFO -  at 6.9s,	estimator xgboost's best error=1.6558,	best estimator xgboost's best error=1.6558
[flaml.automl: 09-18 04:08:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:08:49] {3072} INFO -  at 19.5s,	estimator xgboost's best error=1.6558,	best estimator xgboost's best error=1.6558
[flaml.automl: 09-18 04:08:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:08:50] {3072} INFO -  at 20.7s,	estimator xgboost's best error=1.0702,	best estimator xgboost's best error=1.0702
[flaml.automl: 09-18 04:08:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:08:52] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.9522,	best estimator xgboost's best error=0.9522
[flaml.automl: 09-18 04:08:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:08:53] {3072} INFO -  at 23.9s,	estimator xgboost's best error=0.8923,	best estimator xgboost's best error=0.8923
[flaml.automl: 09-18 04:08:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:08:56] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.8923,	best estimator xgboost's best error=0.8923
[flaml.automl: 09-18 04:08:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:08:58] {3072} INFO -  at 28.2s,	estimator xgboost's best error=0.8923,	best estimator xgboost's best error=0.8923
[flaml.automl: 09-18 04:08:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:09:01] {3072} INFO -  at 31.3s,	estimator xgboost's best error=0.8390,	best estimator xgboost's best error=0.8390
[flaml.automl: 09-18 04:09:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:09:02] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.8390,	best estimator xgboost's best error=0.8390
[flaml.automl: 09-18 04:09:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:09:03] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.8390,	best estimator xgboost's best error=0.8390
[flaml.automl: 09-18 04:09:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:09:17] {3072} INFO -  at 47.7s,	estimator xgboost's best error=0.8098,	best estimator xgboost's best error=0.8098
[flaml.automl: 09-18 04:09:31] {3335} INFO - retrain xgboost for 13.6s
[flaml.automl: 09-18 04:09:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 04:09:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:09:31] {2637} INFO - Time taken to find the best model: 47.693217277526855
[flaml.automl: 09-18 04:09:31] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
SO2(0)最佳损失：0.19018896150458708
SO2(0)最好结果：{'pred_time': 1.2969840419748441e-05, 'wall_clock_time': 47.693217277526855, 'metric_for_logging': {'pred_time': 1.2969840419748441e-05}, 'val_loss': 0.8098110384954129, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 13.633490324020386}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7705191253438973
SO2(0)的mse=1.679044140589444
SO2(0)的mae=0.7949821356622598
SO2(0)的mar=0.15061336807325873
总共花费的时间为：61.93
池州市
3237A
3333A
3334A
[flaml.automl: 09-18 04:18:37] {2390} INFO - task = regression
[flaml.automl: 09-18 04:18:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:18:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:18:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:18:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:18:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:18:40] {3025} INFO - Estimated sufficient time budget=31603s. Estimated necessary time budget=32s.
[flaml.automl: 09-18 04:18:40] {3072} INFO -  at 3.4s,	estimator xgboost's best error=3.5228,	best estimator xgboost's best error=3.5228
[flaml.automl: 09-18 04:18:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:18:46] {3072} INFO -  at 9.2s,	estimator xgboost's best error=1.6697,	best estimator xgboost's best error=1.6697
[flaml.automl: 09-18 04:18:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:18:49] {3072} INFO -  at 12.5s,	estimator xgboost's best error=1.6697,	best estimator xgboost's best error=1.6697
[flaml.automl: 09-18 04:18:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:19:16] {3072} INFO -  at 39.2s,	estimator xgboost's best error=1.6697,	best estimator xgboost's best error=1.6697
[flaml.automl: 09-18 04:19:16] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:19:18] {3072} INFO -  at 41.3s,	estimator xgboost's best error=1.0967,	best estimator xgboost's best error=1.0967
[flaml.automl: 09-18 04:19:18] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:19:21] {3072} INFO -  at 44.2s,	estimator xgboost's best error=1.0967,	best estimator xgboost's best error=1.0967
[flaml.automl: 09-18 04:19:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:19:24] {3072} INFO -  at 47.4s,	estimator xgboost's best error=0.9161,	best estimator xgboost's best error=0.9161
[flaml.automl: 09-18 04:19:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:19:28] {3072} INFO -  at 50.9s,	estimator xgboost's best error=0.9161,	best estimator xgboost's best error=0.9161
[flaml.automl: 09-18 04:19:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:19:29] {3072} INFO -  at 52.6s,	estimator xgboost's best error=0.9161,	best estimator xgboost's best error=0.9161
[flaml.automl: 09-18 04:19:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:19:32] {3072} INFO -  at 55.5s,	estimator xgboost's best error=0.9161,	best estimator xgboost's best error=0.9161
[flaml.automl: 09-18 04:19:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:19:34] {3072} INFO -  at 57.0s,	estimator xgboost's best error=0.9100,	best estimator xgboost's best error=0.9100
[flaml.automl: 09-18 04:19:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:19:35] {3072} INFO -  at 58.1s,	estimator xgboost's best error=0.9100,	best estimator xgboost's best error=0.9100
[flaml.automl: 09-18 04:19:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:19:36] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.9100,	best estimator xgboost's best error=0.9100
[flaml.automl: 09-18 04:19:38] {3335} INFO - retrain xgboost for 1.4s
[flaml.automl: 09-18 04:19:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.923976097470924, colsample_bynode=1,
             colsample_bytree=0.9654640505640502, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4847187609776744,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=104.45542841808965, missing=nan,
             monotone_constraints='()', n_estimators=5, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3217956171961422, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 04:19:38] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:19:38] {2637} INFO - Time taken to find the best model: 56.977540731430054
[flaml.automl: 09-18 04:19:38] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 5, 'max_leaves': 4, 'min_child_weight': 104.45542841808965, 'learning_rate': 0.4847187609776744, 'subsample': 1.0, 'colsample_bylevel': 0.923976097470924, 'colsample_bytree': 0.9654640505640502, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3217956171961422}
SO2(0)最佳损失：0.08995867167756044
SO2(0)最好结果：{'pred_time': 1.2435552222061261e-05, 'wall_clock_time': 56.977540731430054, 'metric_for_logging': {'pred_time': 1.2435552222061261e-05}, 'val_loss': 0.9100413283224396, 'training_iteration': 1, 'config': {'n_estimators': 5, 'max_leaves': 4, 'min_child_weight': 104.45542841808965, 'learning_rate': 0.4847187609776744, 'subsample': 1.0, 'colsample_bylevel': 0.923976097470924, 'colsample_bytree': 0.9654640505640502, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.3217956171961422}, 'config/n_estimators': 5, 'config/max_leaves': 4, 'config/min_child_weight': 104.45542841808965, 'config/learning_rate': 0.4847187609776744, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.923976097470924, 'config/colsample_bytree': 0.9654640505640502, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.3217956171961422, 'experiment_tag': 'exp', 'time_total_s': 1.429814100265503}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.923976097470924, colsample_bynode=1,
             colsample_bytree=0.9654640505640502, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4847187609776744,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=104.45542841808965, missing=nan,
             monotone_constraints='()', n_estimators=5, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.3217956171961422, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.45554703028805166
SO2(0)的mse=2.449571877957012
SO2(0)的mae=0.8994758968174399
SO2(0)的mar=0.15249008956828145
总共花费的时间为：62.08
宣城市
2316A
2317A
2318A
3470A
[flaml.automl: 09-18 04:32:56] {2390} INFO - task = regression
[flaml.automl: 09-18 04:32:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:32:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:32:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:32:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:32:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:32:57] {3025} INFO - Estimated sufficient time budget=51384s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 04:32:57] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.9094,	best estimator xgboost's best error=3.9094
[flaml.automl: 09-18 04:32:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:33:00] {3072} INFO -  at 4.4s,	estimator xgboost's best error=1.9253,	best estimator xgboost's best error=1.9253
[flaml.automl: 09-18 04:33:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:33:02] {3072} INFO -  at 6.5s,	estimator xgboost's best error=1.9253,	best estimator xgboost's best error=1.9253
[flaml.automl: 09-18 04:33:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:33:08] {3072} INFO -  at 12.0s,	estimator xgboost's best error=1.9253,	best estimator xgboost's best error=1.9253
[flaml.automl: 09-18 04:33:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:33:10] {3072} INFO -  at 14.3s,	estimator xgboost's best error=1.3626,	best estimator xgboost's best error=1.3626
[flaml.automl: 09-18 04:33:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:33:13] {3072} INFO -  at 17.1s,	estimator xgboost's best error=1.3058,	best estimator xgboost's best error=1.3058
[flaml.automl: 09-18 04:33:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:33:15] {3072} INFO -  at 19.3s,	estimator xgboost's best error=1.2602,	best estimator xgboost's best error=1.2602
[flaml.automl: 09-18 04:33:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:33:19] {3072} INFO -  at 23.8s,	estimator xgboost's best error=1.2602,	best estimator xgboost's best error=1.2602
[flaml.automl: 09-18 04:33:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:33:22] {3072} INFO -  at 26.6s,	estimator xgboost's best error=1.2515,	best estimator xgboost's best error=1.2515
[flaml.automl: 09-18 04:33:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:33:26] {3072} INFO -  at 30.3s,	estimator xgboost's best error=1.2459,	best estimator xgboost's best error=1.2459
[flaml.automl: 09-18 04:33:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:33:28] {3072} INFO -  at 32.3s,	estimator xgboost's best error=1.2459,	best estimator xgboost's best error=1.2459
[flaml.automl: 09-18 04:33:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:33:30] {3072} INFO -  at 34.2s,	estimator xgboost's best error=1.2459,	best estimator xgboost's best error=1.2459
[flaml.automl: 09-18 04:33:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:33:32] {3072} INFO -  at 36.7s,	estimator xgboost's best error=1.2459,	best estimator xgboost's best error=1.2459
[flaml.automl: 09-18 04:33:32] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:33:37] {3072} INFO -  at 41.6s,	estimator xgboost's best error=1.2459,	best estimator xgboost's best error=1.2459
[flaml.automl: 09-18 04:33:37] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 04:33:45] {3072} INFO -  at 49.7s,	estimator xgboost's best error=1.2369,	best estimator xgboost's best error=1.2369
[flaml.automl: 09-18 04:33:45] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 04:33:50] {3072} INFO -  at 54.8s,	estimator xgboost's best error=1.2369,	best estimator xgboost's best error=1.2369
[flaml.automl: 09-18 04:33:59] {3335} INFO - retrain xgboost for 8.8s
[flaml.automl: 09-18 04:33:59] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.4133599162347451, colsample_bynode=1,
             colsample_bytree=0.7592639642076335, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6172511938767024,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.22715509478483517, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007128224391292669, reg_lambda=18.560362963807897,
             scale_pos_weight=1, subsample=0.8750838741026672,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 04:33:59] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:33:59] {2637} INFO - Time taken to find the best model: 49.69037938117981
[flaml.automl: 09-18 04:33:59] {2648} WARNING - Time taken to find the best model is 83% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 12, 'min_child_weight': 0.22715509478483517, 'learning_rate': 0.6172511938767024, 'subsample': 0.8750838741026672, 'colsample_bylevel': 0.4133599162347451, 'colsample_bytree': 0.7592639642076335, 'reg_alpha': 0.007128224391292669, 'reg_lambda': 18.560362963807897, 'FLAML_sample_size': 42606}
SO2(0)最佳损失：-0.23686135313810053
SO2(0)最好结果：{'pred_time': 1.3039391102244898e-05, 'wall_clock_time': 49.69037938117981, 'metric_for_logging': {'pred_time': 1.3039391102244898e-05}, 'val_loss': 1.2368613531381005, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 12, 'min_child_weight': 0.22715509478483517, 'learning_rate': 0.6172511938767024, 'subsample': 0.8750838741026672, 'colsample_bylevel': 0.4133599162347451, 'colsample_bytree': 0.7592639642076335, 'reg_alpha': 0.007128224391292669, 'reg_lambda': 18.560362963807897, 'FLAML_sample_size': 42606}, 'config/n_estimators': 7, 'config/max_leaves': 12, 'config/min_child_weight': 0.22715509478483517, 'config/learning_rate': 0.6172511938767024, 'config/subsample': 0.8750838741026672, 'config/colsample_bylevel': 0.4133599162347451, 'config/colsample_bytree': 0.7592639642076335, 'config/reg_alpha': 0.007128224391292669, 'config/reg_lambda': 18.560362963807897, 'config/FLAML_sample_size': 42606, 'experiment_tag': 'exp', 'time_total_s': 8.049324035644531}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.4133599162347451, colsample_bynode=1,
             colsample_bytree=0.7592639642076335, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6172511938767024,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.22715509478483517, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007128224391292669, reg_lambda=18.560362963807897,
             scale_pos_weight=1, subsample=0.8750838741026672,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.4091064662462245
SO2(0)的mse=3.3352001306753345
SO2(0)的mae=1.2310143031548184
SO2(0)的mar=0.20446208620380749
总共花费的时间为：64.36
莆田市
2319A
2320A
2321A
2322A
2323A
[flaml.automl: 09-18 04:50:21] {2390} INFO - task = regression
[flaml.automl: 09-18 04:50:21] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 04:50:21] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 04:50:21] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 04:50:21] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 04:50:21] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 04:50:23] {3025} INFO - Estimated sufficient time budget=65995s. Estimated necessary time budget=66s.
[flaml.automl: 09-18 04:50:23] {3072} INFO -  at 1.4s,	estimator xgboost's best error=2.8373,	best estimator xgboost's best error=2.8373
[flaml.automl: 09-18 04:50:23] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 04:50:25] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.3030,	best estimator xgboost's best error=1.3030
[flaml.automl: 09-18 04:50:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 04:50:26] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.3030,	best estimator xgboost's best error=1.3030
[flaml.automl: 09-18 04:50:26] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 04:50:31] {3072} INFO -  at 9.5s,	estimator xgboost's best error=1.3030,	best estimator xgboost's best error=1.3030
[flaml.automl: 09-18 04:50:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 04:50:32] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.6933,	best estimator xgboost's best error=0.6933
[flaml.automl: 09-18 04:50:32] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 04:50:33] {3072} INFO -  at 12.1s,	estimator xgboost's best error=0.5435,	best estimator xgboost's best error=0.5435
[flaml.automl: 09-18 04:50:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 04:50:35] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.5435,	best estimator xgboost's best error=0.5435
[flaml.automl: 09-18 04:50:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 04:50:37] {3072} INFO -  at 16.2s,	estimator xgboost's best error=0.5435,	best estimator xgboost's best error=0.5435
[flaml.automl: 09-18 04:50:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 04:50:38] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.5435,	best estimator xgboost's best error=0.5435
[flaml.automl: 09-18 04:50:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 04:50:41] {3072} INFO -  at 20.0s,	estimator xgboost's best error=0.5435,	best estimator xgboost's best error=0.5435
[flaml.automl: 09-18 04:50:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 04:50:43] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.5435,	best estimator xgboost's best error=0.5435
[flaml.automl: 09-18 04:50:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 04:50:44] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.5435,	best estimator xgboost's best error=0.5435
[flaml.automl: 09-18 04:50:44] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 04:50:50] {3072} INFO -  at 29.3s,	estimator xgboost's best error=0.4880,	best estimator xgboost's best error=0.4880
[flaml.automl: 09-18 04:50:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 04:51:02] {3072} INFO -  at 41.4s,	estimator xgboost's best error=0.4880,	best estimator xgboost's best error=0.4880
[flaml.automl: 09-18 04:51:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 04:51:06] {3072} INFO -  at 45.3s,	estimator xgboost's best error=0.4880,	best estimator xgboost's best error=0.4880
[flaml.automl: 09-18 04:51:06] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 04:51:12] {3072} INFO -  at 51.2s,	estimator xgboost's best error=0.4880,	best estimator xgboost's best error=0.4880
[flaml.automl: 09-18 04:51:12] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 04:51:15] {3072} INFO -  at 53.7s,	estimator xgboost's best error=0.4847,	best estimator xgboost's best error=0.4847
[flaml.automl: 09-18 04:51:15] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 04:51:16] {3072} INFO -  at 55.1s,	estimator xgboost's best error=0.4847,	best estimator xgboost's best error=0.4847
[flaml.automl: 09-18 04:51:16] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 04:51:20] {3072} INFO -  at 58.8s,	estimator xgboost's best error=0.4739,	best estimator xgboost's best error=0.4739
[flaml.automl: 09-18 04:51:26] {3335} INFO - retrain xgboost for 6.3s
[flaml.automl: 09-18 04:51:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7776987624171566, colsample_bynode=1,
             colsample_bytree=0.5988085351505671, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=22, min_child_weight=0.04219891859854448,
             missing=nan, monotone_constraints='()', n_estimators=5, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.004786363580107994, reg_lambda=3.538189463137466,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 04:51:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 04:51:26] {2637} INFO - Time taken to find the best model: 58.841004371643066
[flaml.automl: 09-18 04:51:26] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 5, 'max_leaves': 22, 'min_child_weight': 0.04219891859854448, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7776987624171566, 'colsample_bytree': 0.5988085351505671, 'reg_alpha': 0.004786363580107994, 'reg_lambda': 3.538189463137466, 'FLAML_sample_size': 53939}
SO2(0)最佳损失：0.5261383573333542
SO2(0)最好结果：{'pred_time': 6.868515485598717e-06, 'wall_clock_time': 58.841004371643066, 'metric_for_logging': {'pred_time': 6.868515485598717e-06}, 'val_loss': 0.4738616426666458, 'training_iteration': 1, 'config': {'n_estimators': 5, 'max_leaves': 22, 'min_child_weight': 0.04219891859854448, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7776987624171566, 'colsample_bytree': 0.5988085351505671, 'reg_alpha': 0.004786363580107994, 'reg_lambda': 3.538189463137466, 'FLAML_sample_size': 53939}, 'config/n_estimators': 5, 'config/max_leaves': 22, 'config/min_child_weight': 0.04219891859854448, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7776987624171566, 'config/colsample_bytree': 0.5988085351505671, 'config/reg_alpha': 0.004786363580107994, 'config/reg_lambda': 3.538189463137466, 'config/FLAML_sample_size': 53939, 'experiment_tag': 'exp', 'time_total_s': 3.752480983734131}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7776987624171566, colsample_bynode=1,
             colsample_bytree=0.5988085351505671, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=22, min_child_weight=0.04219891859854448,
             missing=nan, monotone_constraints='()', n_estimators=5, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.004786363580107994, reg_lambda=3.538189463137466,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7264714900700873
SO2(0)的mse=0.627119791714601
SO2(0)的mae=0.4976245462536454
SO2(0)的mar=0.11301660856429548
总共花费的时间为：66.17
三明市
2324A
2325A
2326A
2327A
[flaml.automl: 09-18 05:04:04] {2390} INFO - task = regression
[flaml.automl: 09-18 05:04:04] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:04:04] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:04:04] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:04:04] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:04:04] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:04:06] {3025} INFO - Estimated sufficient time budget=50720s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 05:04:06] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.9295,	best estimator xgboost's best error=3.9295
[flaml.automl: 09-18 05:04:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:04:08] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.0241,	best estimator xgboost's best error=2.0241
[flaml.automl: 09-18 05:04:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:04:09] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.0241,	best estimator xgboost's best error=2.0241
[flaml.automl: 09-18 05:04:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:04:15] {3072} INFO -  at 11.1s,	estimator xgboost's best error=2.0241,	best estimator xgboost's best error=2.0241
[flaml.automl: 09-18 05:04:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:04:16] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.6493,	best estimator xgboost's best error=1.6493
[flaml.automl: 09-18 05:04:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:04:18] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.5687,	best estimator xgboost's best error=1.5687
[flaml.automl: 09-18 05:04:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:04:20] {3072} INFO -  at 15.4s,	estimator xgboost's best error=1.5521,	best estimator xgboost's best error=1.5521
[flaml.automl: 09-18 05:04:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:04:22] {3072} INFO -  at 18.1s,	estimator xgboost's best error=1.5521,	best estimator xgboost's best error=1.5521
[flaml.automl: 09-18 05:04:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:04:24] {3072} INFO -  at 19.7s,	estimator xgboost's best error=1.5521,	best estimator xgboost's best error=1.5521
[flaml.automl: 09-18 05:04:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:04:27] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.4656,	best estimator xgboost's best error=1.4656
[flaml.automl: 09-18 05:04:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:04:28] {3072} INFO -  at 24.4s,	estimator xgboost's best error=1.4656,	best estimator xgboost's best error=1.4656
[flaml.automl: 09-18 05:04:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:04:30] {3072} INFO -  at 25.5s,	estimator xgboost's best error=1.4656,	best estimator xgboost's best error=1.4656
[flaml.automl: 09-18 05:04:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:04:33] {3072} INFO -  at 29.3s,	estimator xgboost's best error=1.4656,	best estimator xgboost's best error=1.4656
[flaml.automl: 09-18 05:04:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:04:37] {3072} INFO -  at 32.7s,	estimator xgboost's best error=1.4656,	best estimator xgboost's best error=1.4656
[flaml.automl: 09-18 05:04:37] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:04:40] {3072} INFO -  at 35.7s,	estimator xgboost's best error=1.4656,	best estimator xgboost's best error=1.4656
[flaml.automl: 09-18 05:04:40] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 05:04:45] {3072} INFO -  at 40.7s,	estimator xgboost's best error=1.4656,	best estimator xgboost's best error=1.4656
[flaml.automl: 09-18 05:04:45] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 05:04:47] {3072} INFO -  at 42.6s,	estimator xgboost's best error=1.4544,	best estimator xgboost's best error=1.4544
[flaml.automl: 09-18 05:04:47] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 05:04:48] {3072} INFO -  at 44.3s,	estimator xgboost's best error=1.4544,	best estimator xgboost's best error=1.4544
[flaml.automl: 09-18 05:04:48] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 05:04:53] {3072} INFO -  at 48.6s,	estimator xgboost's best error=1.4544,	best estimator xgboost's best error=1.4544
[flaml.automl: 09-18 05:04:53] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 05:04:54] {3072} INFO -  at 49.7s,	estimator xgboost's best error=1.4544,	best estimator xgboost's best error=1.4544
[flaml.automl: 09-18 05:04:54] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 05:04:59] {3072} INFO -  at 55.0s,	estimator xgboost's best error=1.4544,	best estimator xgboost's best error=1.4544
[flaml.automl: 09-18 05:04:59] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 05:05:04] {3072} INFO -  at 59.7s,	estimator xgboost's best error=1.4331,	best estimator xgboost's best error=1.4331
[flaml.automl: 09-18 05:05:11] {3335} INFO - retrain xgboost for 7.4s
[flaml.automl: 09-18 05:05:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.20585034445197764,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.04100391317183585, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.02343816342676462, scale_pos_weight=1,
             subsample=0.847359557027498, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:05:11] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:05:11] {2637} INFO - Time taken to find the best model: 59.67420697212219
[flaml.automl: 09-18 05:05:11] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 7, 'min_child_weight': 0.04100391317183585, 'learning_rate': 0.20585034445197764, 'subsample': 0.847359557027498, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.02343816342676462, 'FLAML_sample_size': 41978}
SO2(0)最佳损失：-0.43313643694690573
SO2(0)最好结果：{'pred_time': 9.46953621196849e-06, 'wall_clock_time': 59.67420697212219, 'metric_for_logging': {'pred_time': 9.46953621196849e-06}, 'val_loss': 1.4331364369469057, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 7, 'min_child_weight': 0.04100391317183585, 'learning_rate': 0.20585034445197764, 'subsample': 0.847359557027498, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.02343816342676462, 'FLAML_sample_size': 41978}, 'config/n_estimators': 18, 'config/max_leaves': 7, 'config/min_child_weight': 0.04100391317183585, 'config/learning_rate': 0.20585034445197764, 'config/subsample': 0.847359557027498, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.936172861729374, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.02343816342676462, 'config/FLAML_sample_size': 41978, 'experiment_tag': 'exp', 'time_total_s': 4.669746160507202}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.20585034445197764,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.04100391317183585, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.02343816342676462, scale_pos_weight=1,
             subsample=0.847359557027498, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.23504015083481078
SO2(0)的mse=7.145249254662211
SO2(0)的mae=1.548949138873934
SO2(0)的mar=0.2548348168033514
总共花费的时间为：67.80
南平市
2331A
2332A
2333A
2334A
[flaml.automl: 09-18 05:17:48] {2390} INFO - task = regression
[flaml.automl: 09-18 05:17:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:17:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:17:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:17:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:17:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:17:49] {3025} INFO - Estimated sufficient time budget=50440s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 05:17:49] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.0892,	best estimator xgboost's best error=3.0892
[flaml.automl: 09-18 05:17:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:17:51] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.4764,	best estimator xgboost's best error=1.4764
[flaml.automl: 09-18 05:17:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:17:52] {3072} INFO -  at 4.6s,	estimator xgboost's best error=1.4764,	best estimator xgboost's best error=1.4764
[flaml.automl: 09-18 05:17:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:17:58] {3072} INFO -  at 10.7s,	estimator xgboost's best error=1.4764,	best estimator xgboost's best error=1.4764
[flaml.automl: 09-18 05:17:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:17:59] {3072} INFO -  at 11.8s,	estimator xgboost's best error=0.9527,	best estimator xgboost's best error=0.9527
[flaml.automl: 09-18 05:17:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:18:01] {3072} INFO -  at 13.4s,	estimator xgboost's best error=0.9029,	best estimator xgboost's best error=0.9029
[flaml.automl: 09-18 05:18:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:18:02] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.8727,	best estimator xgboost's best error=0.8727
[flaml.automl: 09-18 05:18:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:18:05] {3072} INFO -  at 17.7s,	estimator xgboost's best error=0.8727,	best estimator xgboost's best error=0.8727
[flaml.automl: 09-18 05:18:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:18:07] {3072} INFO -  at 19.3s,	estimator xgboost's best error=0.8727,	best estimator xgboost's best error=0.8727
[flaml.automl: 09-18 05:18:07] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:18:10] {3072} INFO -  at 22.4s,	estimator xgboost's best error=0.8453,	best estimator xgboost's best error=0.8453
[flaml.automl: 09-18 05:18:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:18:11] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.8453,	best estimator xgboost's best error=0.8453
[flaml.automl: 09-18 05:18:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:18:13] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.8453,	best estimator xgboost's best error=0.8453
[flaml.automl: 09-18 05:18:13] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:18:16] {3072} INFO -  at 28.9s,	estimator xgboost's best error=0.8453,	best estimator xgboost's best error=0.8453
[flaml.automl: 09-18 05:18:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:18:20] {3072} INFO -  at 32.3s,	estimator xgboost's best error=0.8453,	best estimator xgboost's best error=0.8453
[flaml.automl: 09-18 05:18:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:18:23] {3072} INFO -  at 35.3s,	estimator xgboost's best error=0.8453,	best estimator xgboost's best error=0.8453
[flaml.automl: 09-18 05:18:23] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 05:18:28] {3072} INFO -  at 40.3s,	estimator xgboost's best error=0.8453,	best estimator xgboost's best error=0.8453
[flaml.automl: 09-18 05:18:28] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 05:18:30] {3072} INFO -  at 42.2s,	estimator xgboost's best error=0.8218,	best estimator xgboost's best error=0.8218
[flaml.automl: 09-18 05:18:30] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 05:18:31] {3072} INFO -  at 43.9s,	estimator xgboost's best error=0.8218,	best estimator xgboost's best error=0.8218
[flaml.automl: 09-18 05:18:31] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 05:18:36] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.8218,	best estimator xgboost's best error=0.8218
[flaml.automl: 09-18 05:18:36] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 05:18:37] {3072} INFO -  at 49.3s,	estimator xgboost's best error=0.8218,	best estimator xgboost's best error=0.8218
[flaml.automl: 09-18 05:18:37] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 05:18:42] {3072} INFO -  at 54.6s,	estimator xgboost's best error=0.8218,	best estimator xgboost's best error=0.8218
[flaml.automl: 09-18 05:18:42] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 05:18:47] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.8181,	best estimator xgboost's best error=0.8181
[flaml.automl: 09-18 05:18:55] {3335} INFO - retrain xgboost for 7.4s
[flaml.automl: 09-18 05:18:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.20585034445197764,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.04100391317183585, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.02343816342676462, scale_pos_weight=1,
             subsample=0.847359557027498, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:18:55] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:18:55] {2637} INFO - Time taken to find the best model: 59.66122817993164
[flaml.automl: 09-18 05:18:55] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 18, 'max_leaves': 7, 'min_child_weight': 0.04100391317183585, 'learning_rate': 0.20585034445197764, 'subsample': 0.847359557027498, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.02343816342676462, 'FLAML_sample_size': 42992}
SO2(0)最佳损失：0.1819456418688037
SO2(0)最好结果：{'pred_time': 8.338162009127599e-06, 'wall_clock_time': 59.66122817993164, 'metric_for_logging': {'pred_time': 8.338162009127599e-06}, 'val_loss': 0.8180543581311963, 'training_iteration': 1, 'config': {'n_estimators': 18, 'max_leaves': 7, 'min_child_weight': 0.04100391317183585, 'learning_rate': 0.20585034445197764, 'subsample': 0.847359557027498, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.02343816342676462, 'FLAML_sample_size': 42992}, 'config/n_estimators': 18, 'config/max_leaves': 7, 'config/min_child_weight': 0.04100391317183585, 'config/learning_rate': 0.20585034445197764, 'config/subsample': 0.847359557027498, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.936172861729374, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.02343816342676462, 'config/FLAML_sample_size': 42992, 'experiment_tag': 'exp', 'time_total_s': 5.051643133163452}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.20585034445197764,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.04100391317183585, missing=nan,
             monotone_constraints='()', n_estimators=18, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.02343816342676462, scale_pos_weight=1,
             subsample=0.847359557027498, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.3365891214787238
SO2(0)的mse=2.220285794907107
SO2(0)的mae=0.8411911996397293
SO2(0)的mar=0.15280879600081027
总共花费的时间为：67.73
龙岩市
2335A
2336A
2337A
2338A
[flaml.automl: 09-18 05:31:18] {2390} INFO - task = regression
[flaml.automl: 09-18 05:31:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:31:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:31:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:31:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:31:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:31:19] {3025} INFO - Estimated sufficient time budget=51978s. Estimated necessary time budget=52s.
[flaml.automl: 09-18 05:31:19] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.0920,	best estimator xgboost's best error=4.0920
[flaml.automl: 09-18 05:31:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:31:21] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.9360,	best estimator xgboost's best error=1.9360
[flaml.automl: 09-18 05:31:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:31:22] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.9360,	best estimator xgboost's best error=1.9360
[flaml.automl: 09-18 05:31:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:31:28] {3072} INFO -  at 11.0s,	estimator xgboost's best error=1.9360,	best estimator xgboost's best error=1.9360
[flaml.automl: 09-18 05:31:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:31:30] {3072} INFO -  at 12.1s,	estimator xgboost's best error=1.3027,	best estimator xgboost's best error=1.3027
[flaml.automl: 09-18 05:31:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:31:31] {3072} INFO -  at 13.7s,	estimator xgboost's best error=1.1914,	best estimator xgboost's best error=1.1914
[flaml.automl: 09-18 05:31:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:31:33] {3072} INFO -  at 15.3s,	estimator xgboost's best error=1.1190,	best estimator xgboost's best error=1.1190
[flaml.automl: 09-18 05:31:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:31:35] {3072} INFO -  at 18.0s,	estimator xgboost's best error=1.1190,	best estimator xgboost's best error=1.1190
[flaml.automl: 09-18 05:31:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:31:37] {3072} INFO -  at 19.6s,	estimator xgboost's best error=1.1190,	best estimator xgboost's best error=1.1190
[flaml.automl: 09-18 05:31:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:31:40] {3072} INFO -  at 22.6s,	estimator xgboost's best error=1.0397,	best estimator xgboost's best error=1.0397
[flaml.automl: 09-18 05:31:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:31:42] {3072} INFO -  at 24.3s,	estimator xgboost's best error=1.0397,	best estimator xgboost's best error=1.0397
[flaml.automl: 09-18 05:31:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:31:43] {3072} INFO -  at 25.4s,	estimator xgboost's best error=1.0397,	best estimator xgboost's best error=1.0397
[flaml.automl: 09-18 05:31:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:31:47] {3072} INFO -  at 29.2s,	estimator xgboost's best error=1.0397,	best estimator xgboost's best error=1.0397
[flaml.automl: 09-18 05:31:47] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:31:50] {3072} INFO -  at 32.6s,	estimator xgboost's best error=1.0397,	best estimator xgboost's best error=1.0397
[flaml.automl: 09-18 05:31:50] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:31:53] {3072} INFO -  at 35.6s,	estimator xgboost's best error=1.0397,	best estimator xgboost's best error=1.0397
[flaml.automl: 09-18 05:31:53] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 05:31:58] {3072} INFO -  at 40.5s,	estimator xgboost's best error=1.0397,	best estimator xgboost's best error=1.0397
[flaml.automl: 09-18 05:31:58] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 05:32:00] {3072} INFO -  at 42.4s,	estimator xgboost's best error=1.0311,	best estimator xgboost's best error=1.0311
[flaml.automl: 09-18 05:32:00] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 05:32:01] {3072} INFO -  at 44.0s,	estimator xgboost's best error=1.0311,	best estimator xgboost's best error=1.0311
[flaml.automl: 09-18 05:32:01] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 05:32:06] {3072} INFO -  at 48.3s,	estimator xgboost's best error=1.0311,	best estimator xgboost's best error=1.0311
[flaml.automl: 09-18 05:32:06] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 05:32:07] {3072} INFO -  at 49.5s,	estimator xgboost's best error=1.0311,	best estimator xgboost's best error=1.0311
[flaml.automl: 09-18 05:32:07] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 05:32:12] {3072} INFO -  at 54.7s,	estimator xgboost's best error=1.0261,	best estimator xgboost's best error=1.0261
[flaml.automl: 09-18 05:32:12] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 05:32:17] {3072} INFO -  at 59.1s,	estimator xgboost's best error=1.0060,	best estimator xgboost's best error=1.0060
[flaml.automl: 09-18 05:32:40] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-18 05:32:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.008381997180108987, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003886437262068607, scale_pos_weight=1,
             subsample=0.9390180412130811, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:32:40] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:32:40] {2637} INFO - Time taken to find the best model: 59.13029932975769
[flaml.automl: 09-18 05:32:40] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 28, 'max_leaves': 15, 'min_child_weight': 0.008381997180108987, 'learning_rate': 0.6023269513573992, 'subsample': 0.9390180412130811, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003886437262068607, 'FLAML_sample_size': 43137}
SO2(0)最佳损失：-0.005996721060414378
SO2(0)最好结果：{'pred_time': 9.597218131143747e-06, 'wall_clock_time': 59.13029932975769, 'metric_for_logging': {'pred_time': 9.597218131143747e-06}, 'val_loss': 1.0059967210604144, 'training_iteration': 1, 'config': {'n_estimators': 28, 'max_leaves': 15, 'min_child_weight': 0.008381997180108987, 'learning_rate': 0.6023269513573992, 'subsample': 0.9390180412130811, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003886437262068607, 'FLAML_sample_size': 43137}, 'config/n_estimators': 28, 'config/max_leaves': 15, 'config/min_child_weight': 0.008381997180108987, 'config/learning_rate': 0.6023269513573992, 'config/subsample': 0.9390180412130811, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.936172861729374, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.003886437262068607, 'config/FLAML_sample_size': 43137, 'experiment_tag': 'exp', 'time_total_s': 4.430002689361572}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.008381997180108987, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003886437262068607, scale_pos_weight=1,
             subsample=0.9390180412130811, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.753797920902279
SO2(0)的mse=2.4601509216936925
SO2(0)的mae=0.9971855247642956
SO2(0)的mar=0.15046054648488646
总共花费的时间为：83.68
宁德市
2339A
3209A
[flaml.automl: 09-18 05:39:11] {2390} INFO - task = regression
[flaml.automl: 09-18 05:39:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:39:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:39:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:39:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:39:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:39:13] {3025} INFO - Estimated sufficient time budget=17291s. Estimated necessary time budget=17s.
[flaml.automl: 09-18 05:39:13] {3072} INFO -  at 1.9s,	estimator xgboost's best error=3.4119,	best estimator xgboost's best error=3.4119
[flaml.automl: 09-18 05:39:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:39:16] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.7729,	best estimator xgboost's best error=1.7729
[flaml.automl: 09-18 05:39:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:39:18] {3072} INFO -  at 6.7s,	estimator xgboost's best error=1.7729,	best estimator xgboost's best error=1.7729
[flaml.automl: 09-18 05:39:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:39:31] {3072} INFO -  at 19.9s,	estimator xgboost's best error=1.7729,	best estimator xgboost's best error=1.7729
[flaml.automl: 09-18 05:39:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:39:33] {3072} INFO -  at 22.0s,	estimator xgboost's best error=0.7078,	best estimator xgboost's best error=0.7078
[flaml.automl: 09-18 05:39:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:39:36] {3072} INFO -  at 24.6s,	estimator xgboost's best error=0.6672,	best estimator xgboost's best error=0.6672
[flaml.automl: 09-18 05:39:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:39:39] {3072} INFO -  at 27.4s,	estimator xgboost's best error=0.5369,	best estimator xgboost's best error=0.5369
[flaml.automl: 09-18 05:39:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:39:43] {3072} INFO -  at 32.1s,	estimator xgboost's best error=0.5369,	best estimator xgboost's best error=0.5369
[flaml.automl: 09-18 05:39:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:39:48] {3072} INFO -  at 36.7s,	estimator xgboost's best error=0.5369,	best estimator xgboost's best error=0.5369
[flaml.automl: 09-18 05:39:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:39:56] {3072} INFO -  at 45.1s,	estimator xgboost's best error=0.5369,	best estimator xgboost's best error=0.5369
[flaml.automl: 09-18 05:39:56] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:40:01] {3072} INFO -  at 49.9s,	estimator xgboost's best error=0.5279,	best estimator xgboost's best error=0.5279
[flaml.automl: 09-18 05:40:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:40:04] {3072} INFO -  at 52.9s,	estimator xgboost's best error=0.5279,	best estimator xgboost's best error=0.5279
[flaml.automl: 09-18 05:40:04] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:40:10] {3072} INFO -  at 59.1s,	estimator xgboost's best error=0.4955,	best estimator xgboost's best error=0.4955
[flaml.automl: 09-18 05:40:22] {3335} INFO - retrain xgboost for 11.8s
[flaml.automl: 09-18 05:40:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 05:40:22] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:40:22] {2637} INFO - Time taken to find the best model: 59.058359146118164
[flaml.automl: 09-18 05:40:22] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}
SO2(0)最佳损失：0.5044671428255639
SO2(0)最好结果：{'pred_time': 2.5982835927904953e-05, 'wall_clock_time': 59.058359146118164, 'metric_for_logging': {'pred_time': 2.5982835927904953e-05}, 'val_loss': 0.49553285717443607, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'experiment_tag': 'exp', 'time_total_s': 6.163015127182007}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6832097779481014
SO2(0)的mse=0.8613711713640066
SO2(0)的mae=0.5278800635030435
SO2(0)的mar=0.09111255281173168
总共花费的时间为：71.34
景德镇市
2342A
2343A
2344A
2345A
2346A
[flaml.automl: 09-18 05:56:03] {2390} INFO - task = regression
[flaml.automl: 09-18 05:56:03] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 05:56:03] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 05:56:03] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 05:56:03] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 05:56:03] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 05:56:04] {3025} INFO - Estimated sufficient time budget=62991s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 05:56:04] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.5599,	best estimator xgboost's best error=5.5599
[flaml.automl: 09-18 05:56:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 05:56:07] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.8871,	best estimator xgboost's best error=2.8871
[flaml.automl: 09-18 05:56:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 05:56:08] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.8871,	best estimator xgboost's best error=2.8871
[flaml.automl: 09-18 05:56:08] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 05:56:13] {3072} INFO -  at 10.0s,	estimator xgboost's best error=2.8871,	best estimator xgboost's best error=2.8871
[flaml.automl: 09-18 05:56:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 05:56:14] {3072} INFO -  at 11.2s,	estimator xgboost's best error=2.4466,	best estimator xgboost's best error=2.4466
[flaml.automl: 09-18 05:56:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 05:56:16] {3072} INFO -  at 12.7s,	estimator xgboost's best error=2.4059,	best estimator xgboost's best error=2.4059
[flaml.automl: 09-18 05:56:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 05:56:17] {3072} INFO -  at 14.4s,	estimator xgboost's best error=2.3304,	best estimator xgboost's best error=2.3304
[flaml.automl: 09-18 05:56:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 05:56:20] {3072} INFO -  at 17.0s,	estimator xgboost's best error=2.3304,	best estimator xgboost's best error=2.3304
[flaml.automl: 09-18 05:56:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 05:56:22] {3072} INFO -  at 18.7s,	estimator xgboost's best error=2.3199,	best estimator xgboost's best error=2.3199
[flaml.automl: 09-18 05:56:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 05:56:25] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.3199,	best estimator xgboost's best error=2.3199
[flaml.automl: 09-18 05:56:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 05:56:26] {3072} INFO -  at 23.1s,	estimator xgboost's best error=2.3199,	best estimator xgboost's best error=2.3199
[flaml.automl: 09-18 05:56:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 05:56:27] {3072} INFO -  at 24.2s,	estimator xgboost's best error=2.3199,	best estimator xgboost's best error=2.3199
[flaml.automl: 09-18 05:56:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 05:56:31] {3072} INFO -  at 27.5s,	estimator xgboost's best error=2.3199,	best estimator xgboost's best error=2.3199
[flaml.automl: 09-18 05:56:31] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 05:56:32] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.2805,	best estimator xgboost's best error=2.2805
[flaml.automl: 09-18 05:56:32] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 05:56:36] {3072} INFO -  at 32.7s,	estimator xgboost's best error=2.2433,	best estimator xgboost's best error=2.2433
[flaml.automl: 09-18 05:56:36] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 05:56:38] {3072} INFO -  at 34.7s,	estimator xgboost's best error=2.2433,	best estimator xgboost's best error=2.2433
[flaml.automl: 09-18 05:56:38] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 05:56:44] {3072} INFO -  at 40.6s,	estimator xgboost's best error=2.1998,	best estimator xgboost's best error=2.1998
[flaml.automl: 09-18 05:56:44] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 05:56:47] {3072} INFO -  at 44.2s,	estimator xgboost's best error=2.1998,	best estimator xgboost's best error=2.1998
[flaml.automl: 09-18 05:56:47] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 05:56:50] {3072} INFO -  at 47.3s,	estimator xgboost's best error=2.1998,	best estimator xgboost's best error=2.1998
[flaml.automl: 09-18 05:56:50] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 05:57:03] {3072} INFO -  at 59.9s,	estimator xgboost's best error=2.1998,	best estimator xgboost's best error=2.1998
[flaml.automl: 09-18 05:57:13] {3335} INFO - retrain xgboost for 9.8s
[flaml.automl: 09-18 05:57:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6398475569876901, colsample_bynode=1,
             colsample_bytree=0.8064307818091642, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.44275994948107983,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=2.704955628867028, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0011817520129109049, reg_lambda=5.788018970244181,
             scale_pos_weight=1, subsample=0.8567104675814464,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 05:57:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 05:57:13] {2637} INFO - Time taken to find the best model: 40.622960805892944
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 7, 'min_child_weight': 2.704955628867028, 'learning_rate': 0.44275994948107983, 'subsample': 0.8567104675814464, 'colsample_bylevel': 0.6398475569876901, 'colsample_bytree': 0.8064307818091642, 'reg_alpha': 0.0011817520129109049, 'reg_lambda': 5.788018970244181, 'FLAML_sample_size': 52041}
SO2(0)最佳损失：-1.199816055287957
SO2(0)最好结果：{'pred_time': 6.893937323469821e-06, 'wall_clock_time': 40.622960805892944, 'metric_for_logging': {'pred_time': 6.893937323469821e-06}, 'val_loss': 2.199816055287957, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 7, 'min_child_weight': 2.704955628867028, 'learning_rate': 0.44275994948107983, 'subsample': 0.8567104675814464, 'colsample_bylevel': 0.6398475569876901, 'colsample_bytree': 0.8064307818091642, 'reg_alpha': 0.0011817520129109049, 'reg_lambda': 5.788018970244181, 'FLAML_sample_size': 52041}, 'config/n_estimators': 14, 'config/max_leaves': 7, 'config/min_child_weight': 2.704955628867028, 'config/learning_rate': 0.44275994948107983, 'config/subsample': 0.8567104675814464, 'config/colsample_bylevel': 0.6398475569876901, 'config/colsample_bytree': 0.8064307818091642, 'config/reg_alpha': 0.0011817520129109049, 'config/reg_lambda': 5.788018970244181, 'config/FLAML_sample_size': 52041, 'experiment_tag': 'exp', 'time_total_s': 5.898584365844727}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6398475569876901, colsample_bynode=1,
             colsample_bytree=0.8064307818091642, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.44275994948107983,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=2.704955628867028, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0011817520129109049, reg_lambda=5.788018970244181,
             scale_pos_weight=1, subsample=0.8567104675814464,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.47870293905994765
SO2(0)的mse=17.628406765967032
SO2(0)的mae=2.137787026987929
SO2(0)的mar=0.23103761791610405
总共花费的时间为：70.66
萍乡市
2347A
2348A
2349A
2350A
2351A
[flaml.automl: 09-18 06:14:19] {2390} INFO - task = regression
[flaml.automl: 09-18 06:14:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:14:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:14:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:14:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:14:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:14:21] {3025} INFO - Estimated sufficient time budget=62872s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 06:14:21] {3072} INFO -  at 1.4s,	estimator xgboost's best error=7.7412,	best estimator xgboost's best error=7.7412
[flaml.automl: 09-18 06:14:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:14:23] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.6895,	best estimator xgboost's best error=3.6895
[flaml.automl: 09-18 06:14:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:14:24] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.6895,	best estimator xgboost's best error=3.6895
[flaml.automl: 09-18 06:14:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:14:29] {3072} INFO -  at 9.5s,	estimator xgboost's best error=3.6895,	best estimator xgboost's best error=3.6895
[flaml.automl: 09-18 06:14:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:14:30] {3072} INFO -  at 10.7s,	estimator xgboost's best error=2.7328,	best estimator xgboost's best error=2.7328
[flaml.automl: 09-18 06:14:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:14:31] {3072} INFO -  at 12.2s,	estimator xgboost's best error=2.5009,	best estimator xgboost's best error=2.5009
[flaml.automl: 09-18 06:14:31] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:14:33] {3072} INFO -  at 13.8s,	estimator xgboost's best error=2.3906,	best estimator xgboost's best error=2.3906
[flaml.automl: 09-18 06:14:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:14:36] {3072} INFO -  at 16.5s,	estimator xgboost's best error=2.3906,	best estimator xgboost's best error=2.3906
[flaml.automl: 09-18 06:14:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:14:37] {3072} INFO -  at 18.1s,	estimator xgboost's best error=2.3906,	best estimator xgboost's best error=2.3906
[flaml.automl: 09-18 06:14:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:14:40] {3072} INFO -  at 21.1s,	estimator xgboost's best error=2.2885,	best estimator xgboost's best error=2.2885
[flaml.automl: 09-18 06:14:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:14:42] {3072} INFO -  at 22.7s,	estimator xgboost's best error=2.2885,	best estimator xgboost's best error=2.2885
[flaml.automl: 09-18 06:14:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:14:43] {3072} INFO -  at 23.8s,	estimator xgboost's best error=2.2885,	best estimator xgboost's best error=2.2885
[flaml.automl: 09-18 06:14:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:14:46] {3072} INFO -  at 26.7s,	estimator xgboost's best error=2.2523,	best estimator xgboost's best error=2.2523
[flaml.automl: 09-18 06:14:46] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:14:49] {3072} INFO -  at 29.5s,	estimator xgboost's best error=2.2014,	best estimator xgboost's best error=2.2014
[flaml.automl: 09-18 06:14:49] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:14:51] {3072} INFO -  at 32.2s,	estimator xgboost's best error=2.2014,	best estimator xgboost's best error=2.2014
[flaml.automl: 09-18 06:14:51] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 06:14:54] {3072} INFO -  at 34.7s,	estimator xgboost's best error=2.2014,	best estimator xgboost's best error=2.2014
[flaml.automl: 09-18 06:14:54] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 06:14:56] {3072} INFO -  at 37.0s,	estimator xgboost's best error=2.1299,	best estimator xgboost's best error=2.1299
[flaml.automl: 09-18 06:14:56] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 06:14:58] {3072} INFO -  at 39.1s,	estimator xgboost's best error=2.1299,	best estimator xgboost's best error=2.1299
[flaml.automl: 09-18 06:14:58] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 06:14:59] {3072} INFO -  at 40.1s,	estimator xgboost's best error=2.1299,	best estimator xgboost's best error=2.1299
[flaml.automl: 09-18 06:14:59] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 06:15:01] {3072} INFO -  at 41.8s,	estimator xgboost's best error=2.1299,	best estimator xgboost's best error=2.1299
[flaml.automl: 09-18 06:15:01] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 06:15:02] {3072} INFO -  at 42.7s,	estimator xgboost's best error=2.1299,	best estimator xgboost's best error=2.1299
[flaml.automl: 09-18 06:15:02] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 06:15:16] {3072} INFO -  at 57.2s,	estimator xgboost's best error=2.1098,	best estimator xgboost's best error=2.1098
[flaml.automl: 09-18 06:15:31] {3335} INFO - retrain xgboost for 14.4s
[flaml.automl: 09-18 06:15:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4512592128754277,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.003827601076117227, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010065608612606477, reg_lambda=0.020722552365849516,
             scale_pos_weight=1, subsample=0.749001334635897,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 06:15:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:15:31] {2637} INFO - Time taken to find the best model: 57.173503398895264
[flaml.automl: 09-18 06:15:31] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.003827601076117227, 'learning_rate': 0.4512592128754277, 'subsample': 0.749001334635897, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0010065608612606477, 'reg_lambda': 0.020722552365849516, 'FLAML_sample_size': 52206}
SO2(0)最佳损失：-1.1098285401983317
SO2(0)最好结果：{'pred_time': 6.979158306467063e-06, 'wall_clock_time': 57.173503398895264, 'metric_for_logging': {'pred_time': 6.979158306467063e-06}, 'val_loss': 2.1098285401983317, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 7, 'min_child_weight': 0.003827601076117227, 'learning_rate': 0.4512592128754277, 'subsample': 0.749001334635897, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0010065608612606477, 'reg_lambda': 0.020722552365849516, 'FLAML_sample_size': 52206}, 'config/n_estimators': 35, 'config/max_leaves': 7, 'config/min_child_weight': 0.003827601076117227, 'config/learning_rate': 0.4512592128754277, 'config/subsample': 0.749001334635897, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0010065608612606477, 'config/reg_lambda': 0.020722552365849516, 'config/FLAML_sample_size': 52206, 'experiment_tag': 'exp', 'time_total_s': 14.451696395874023}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4512592128754277,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.003827601076117227, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0010065608612606477, reg_lambda=0.020722552365849516,
             scale_pos_weight=1, subsample=0.749001334635897,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7056334206245409
SO2(0)的mse=13.502063378696139
SO2(0)的mae=2.1771732994092017
SO2(0)的mar=0.17638613132739012
总共花费的时间为：72.83
新余市
2352A
2353A
2354A
2355A
[flaml.automl: 09-18 06:27:39] {2390} INFO - task = regression
[flaml.automl: 09-18 06:27:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:27:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:27:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:27:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:27:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:27:41] {3025} INFO - Estimated sufficient time budget=87282s. Estimated necessary time budget=87s.
[flaml.automl: 09-18 06:27:41] {3072} INFO -  at 2.4s,	estimator xgboost's best error=8.0702,	best estimator xgboost's best error=8.0702
[flaml.automl: 09-18 06:27:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:27:44] {3072} INFO -  at 5.7s,	estimator xgboost's best error=4.2877,	best estimator xgboost's best error=4.2877
[flaml.automl: 09-18 06:27:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:27:46] {3072} INFO -  at 7.6s,	estimator xgboost's best error=4.2877,	best estimator xgboost's best error=4.2877
[flaml.automl: 09-18 06:27:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:27:52] {3072} INFO -  at 13.7s,	estimator xgboost's best error=4.2877,	best estimator xgboost's best error=4.2877
[flaml.automl: 09-18 06:27:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:27:54] {3072} INFO -  at 15.5s,	estimator xgboost's best error=4.0915,	best estimator xgboost's best error=4.0915
[flaml.automl: 09-18 06:27:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:27:57] {3072} INFO -  at 18.3s,	estimator xgboost's best error=4.0915,	best estimator xgboost's best error=4.0915
[flaml.automl: 09-18 06:27:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:27:59] {3072} INFO -  at 21.0s,	estimator xgboost's best error=3.4885,	best estimator xgboost's best error=3.4885
[flaml.automl: 09-18 06:27:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:28:04] {3072} INFO -  at 25.3s,	estimator xgboost's best error=3.4885,	best estimator xgboost's best error=3.4885
[flaml.automl: 09-18 06:28:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:28:06] {3072} INFO -  at 27.9s,	estimator xgboost's best error=3.4885,	best estimator xgboost's best error=3.4885
[flaml.automl: 09-18 06:28:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:28:10] {3072} INFO -  at 31.7s,	estimator xgboost's best error=3.4885,	best estimator xgboost's best error=3.4885
[flaml.automl: 09-18 06:28:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:28:12] {3072} INFO -  at 33.6s,	estimator xgboost's best error=3.4885,	best estimator xgboost's best error=3.4885
[flaml.automl: 09-18 06:28:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:28:15] {3072} INFO -  at 36.4s,	estimator xgboost's best error=3.4559,	best estimator xgboost's best error=3.4559
[flaml.automl: 09-18 06:28:15] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:28:17] {3072} INFO -  at 38.4s,	estimator xgboost's best error=3.4559,	best estimator xgboost's best error=3.4559
[flaml.automl: 09-18 06:28:17] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:28:30] {3072} INFO -  at 51.2s,	estimator xgboost's best error=3.4559,	best estimator xgboost's best error=3.4559
[flaml.automl: 09-18 06:28:30] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:28:38] {3072} INFO -  at 59.4s,	estimator xgboost's best error=3.4292,	best estimator xgboost's best error=3.4292
[flaml.automl: 09-18 06:28:46] {3335} INFO - retrain xgboost for 8.5s
[flaml.automl: 09-18 06:28:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:28:46] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:28:46] {2637} INFO - Time taken to find the best model: 59.39273476600647
[flaml.automl: 09-18 06:28:46] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445, 'FLAML_sample_size': 40261}
SO2(0)最佳损失：-2.4291570053390483
SO2(0)最好结果：{'pred_time': 1.7505913479923935e-05, 'wall_clock_time': 59.39273476600647, 'metric_for_logging': {'pred_time': 1.7505913479923935e-05}, 'val_loss': 3.4291570053390483, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445, 'FLAML_sample_size': 40261}, 'config/n_estimators': 14, 'config/max_leaves': 4, 'config/min_child_weight': 2.909605268359626, 'config/learning_rate': 0.1976415421521206, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040166475000702445, 'config/FLAML_sample_size': 40261, 'experiment_tag': 'exp', 'time_total_s': 8.193102359771729}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.44392993887522547
SO2(0)的mse=35.75798467766639
SO2(0)的mae=3.3341463639975504
SO2(0)的mar=0.2971912229472596
总共花费的时间为：69.02
鹰潭市
2357A
2358A
2359A
2360A
2361A
[flaml.automl: 09-18 06:44:12] {2390} INFO - task = regression
[flaml.automl: 09-18 06:44:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 06:44:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 06:44:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 06:44:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 06:44:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 06:44:13] {3025} INFO - Estimated sufficient time budget=62300s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 06:44:13] {3072} INFO -  at 1.4s,	estimator xgboost's best error=6.5526,	best estimator xgboost's best error=6.5526
[flaml.automl: 09-18 06:44:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 06:44:15] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.9453,	best estimator xgboost's best error=3.9453
[flaml.automl: 09-18 06:44:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 06:44:16] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.9453,	best estimator xgboost's best error=3.9453
[flaml.automl: 09-18 06:44:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 06:44:21] {3072} INFO -  at 9.5s,	estimator xgboost's best error=3.9453,	best estimator xgboost's best error=3.9453
[flaml.automl: 09-18 06:44:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 06:44:22] {3072} INFO -  at 10.7s,	estimator xgboost's best error=3.8528,	best estimator xgboost's best error=3.8528
[flaml.automl: 09-18 06:44:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 06:44:24] {3072} INFO -  at 12.2s,	estimator xgboost's best error=3.7282,	best estimator xgboost's best error=3.7282
[flaml.automl: 09-18 06:44:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 06:44:25] {3072} INFO -  at 13.9s,	estimator xgboost's best error=3.5957,	best estimator xgboost's best error=3.5957
[flaml.automl: 09-18 06:44:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 06:44:28] {3072} INFO -  at 16.5s,	estimator xgboost's best error=3.4663,	best estimator xgboost's best error=3.4663
[flaml.automl: 09-18 06:44:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 06:44:29] {3072} INFO -  at 18.2s,	estimator xgboost's best error=3.4663,	best estimator xgboost's best error=3.4663
[flaml.automl: 09-18 06:44:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 06:44:33] {3072} INFO -  at 21.7s,	estimator xgboost's best error=3.4663,	best estimator xgboost's best error=3.4663
[flaml.automl: 09-18 06:44:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 06:44:34] {3072} INFO -  at 23.2s,	estimator xgboost's best error=3.4663,	best estimator xgboost's best error=3.4663
[flaml.automl: 09-18 06:44:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 06:44:36] {3072} INFO -  at 24.3s,	estimator xgboost's best error=3.4663,	best estimator xgboost's best error=3.4663
[flaml.automl: 09-18 06:44:36] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 06:44:38] {3072} INFO -  at 27.0s,	estimator xgboost's best error=3.4632,	best estimator xgboost's best error=3.4632
[flaml.automl: 09-18 06:44:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 06:44:43] {3072} INFO -  at 31.4s,	estimator xgboost's best error=3.3261,	best estimator xgboost's best error=3.3261
[flaml.automl: 09-18 06:44:43] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 06:44:45] {3072} INFO -  at 33.8s,	estimator xgboost's best error=3.3261,	best estimator xgboost's best error=3.3261
[flaml.automl: 09-18 06:44:45] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 06:44:53] {3072} INFO -  at 41.2s,	estimator xgboost's best error=3.3261,	best estimator xgboost's best error=3.3261
[flaml.automl: 09-18 06:44:53] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 06:44:56] {3072} INFO -  at 44.8s,	estimator xgboost's best error=3.3261,	best estimator xgboost's best error=3.3261
[flaml.automl: 09-18 06:44:56] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 06:44:59] {3072} INFO -  at 47.9s,	estimator xgboost's best error=3.3261,	best estimator xgboost's best error=3.3261
[flaml.automl: 09-18 06:44:59] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 06:45:10] {3072} INFO -  at 58.7s,	estimator xgboost's best error=3.3261,	best estimator xgboost's best error=3.3261
[flaml.automl: 09-18 06:45:14] {3335} INFO - retrain xgboost for 4.3s
[flaml.automl: 09-18 06:45:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1284788539179643,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=3.258345592516902, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.016549616924443407, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 06:45:14] {2636} INFO - fit succeeded
[flaml.automl: 09-18 06:45:14] {2637} INFO - Time taken to find the best model: 31.390189170837402
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 5, 'min_child_weight': 3.258345592516902, 'learning_rate': 0.1284788539179643, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.016549616924443407, 'FLAML_sample_size': 51811}
SO2(0)最佳损失：-2.32613114236398
SO2(0)最好结果：{'pred_time': 7.105552841645056e-06, 'wall_clock_time': 31.390189170837402, 'metric_for_logging': {'pred_time': 7.105552841645056e-06}, 'val_loss': 3.32613114236398, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 5, 'min_child_weight': 3.258345592516902, 'learning_rate': 0.1284788539179643, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.016549616924443407, 'FLAML_sample_size': 51811}, 'config/n_estimators': 14, 'config/max_leaves': 5, 'config/min_child_weight': 3.258345592516902, 'config/learning_rate': 0.1284788539179643, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.016549616924443407, 'config/FLAML_sample_size': 51811, 'experiment_tag': 'exp', 'time_total_s': 4.358542203903198}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1284788539179643,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=3.258345592516902, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.016549616924443407, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.05937372106612604
SO2(0)的mse=44.39450561650794
SO2(0)的mae=3.2761534823464924
SO2(0)的mar=0.40583019028082873
总共花费的时间为：64.21
赣州市
2362A
2363A
2364A
2365A
2366A
3109A
[flaml.automl: 09-18 07:03:16] {2390} INFO - task = regression
[flaml.automl: 09-18 07:03:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:03:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:03:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:03:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:03:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:03:17] {3025} INFO - Estimated sufficient time budget=75344s. Estimated necessary time budget=75s.
[flaml.automl: 09-18 07:03:17] {3072} INFO -  at 1.5s,	estimator xgboost's best error=4.5327,	best estimator xgboost's best error=4.5327
[flaml.automl: 09-18 07:03:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:03:19] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.4864,	best estimator xgboost's best error=2.4864
[flaml.automl: 09-18 07:03:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:03:21] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.4864,	best estimator xgboost's best error=2.4864
[flaml.automl: 09-18 07:03:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:03:25] {3072} INFO -  at 9.0s,	estimator xgboost's best error=2.4864,	best estimator xgboost's best error=2.4864
[flaml.automl: 09-18 07:03:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:03:26] {3072} INFO -  at 10.2s,	estimator xgboost's best error=2.0445,	best estimator xgboost's best error=2.0445
[flaml.automl: 09-18 07:03:26] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:03:28] {3072} INFO -  at 11.7s,	estimator xgboost's best error=2.0445,	best estimator xgboost's best error=2.0445
[flaml.automl: 09-18 07:03:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:03:29] {3072} INFO -  at 13.4s,	estimator xgboost's best error=1.8444,	best estimator xgboost's best error=1.8444
[flaml.automl: 09-18 07:03:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:03:32] {3072} INFO -  at 16.1s,	estimator xgboost's best error=1.8444,	best estimator xgboost's best error=1.8444
[flaml.automl: 09-18 07:03:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:03:34] {3072} INFO -  at 17.7s,	estimator xgboost's best error=1.8444,	best estimator xgboost's best error=1.8444
[flaml.automl: 09-18 07:03:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:03:37] {3072} INFO -  at 20.8s,	estimator xgboost's best error=1.8222,	best estimator xgboost's best error=1.8222
[flaml.automl: 09-18 07:03:37] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:03:38] {3072} INFO -  at 22.4s,	estimator xgboost's best error=1.8222,	best estimator xgboost's best error=1.8222
[flaml.automl: 09-18 07:03:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:03:39] {3072} INFO -  at 23.6s,	estimator xgboost's best error=1.8222,	best estimator xgboost's best error=1.8222
[flaml.automl: 09-18 07:03:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:03:42] {3072} INFO -  at 26.4s,	estimator xgboost's best error=1.8138,	best estimator xgboost's best error=1.8138
[flaml.automl: 09-18 07:03:42] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:03:44] {3072} INFO -  at 28.6s,	estimator xgboost's best error=1.8138,	best estimator xgboost's best error=1.8138
[flaml.automl: 09-18 07:03:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:03:46] {3072} INFO -  at 30.0s,	estimator xgboost's best error=1.8138,	best estimator xgboost's best error=1.8138
[flaml.automl: 09-18 07:03:46] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 07:03:48] {3072} INFO -  at 31.8s,	estimator xgboost's best error=1.8138,	best estimator xgboost's best error=1.8138
[flaml.automl: 09-18 07:03:48] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 07:03:49] {3072} INFO -  at 33.5s,	estimator xgboost's best error=1.8138,	best estimator xgboost's best error=1.8138
[flaml.automl: 09-18 07:03:49] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 07:04:03] {3072} INFO -  at 47.2s,	estimator xgboost's best error=1.7869,	best estimator xgboost's best error=1.7869
[flaml.automl: 09-18 07:04:17] {3335} INFO - retrain xgboost for 13.7s
[flaml.automl: 09-18 07:04:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9227015137629297, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6727151764857845,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.05195093592765227, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.036950619226612795,
             scale_pos_weight=1, subsample=0.9485089295118937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 07:04:17] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:04:17] {2637} INFO - Time taken to find the best model: 47.201245069503784
[flaml.automl: 09-18 07:04:17] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.05195093592765227, 'learning_rate': 0.6727151764857845, 'subsample': 0.9485089295118937, 'colsample_bylevel': 0.9227015137629297, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.036950619226612795, 'FLAML_sample_size': 62656}
SO2(0)最佳损失：-0.7869179706846081
SO2(0)最好结果：{'pred_time': 5.679512736236388e-06, 'wall_clock_time': 47.201245069503784, 'metric_for_logging': {'pred_time': 5.679512736236388e-06}, 'val_loss': 1.7869179706846081, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.05195093592765227, 'learning_rate': 0.6727151764857845, 'subsample': 0.9485089295118937, 'colsample_bylevel': 0.9227015137629297, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.036950619226612795, 'FLAML_sample_size': 62656}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.05195093592765227, 'config/learning_rate': 0.6727151764857845, 'config/subsample': 0.9485089295118937, 'config/colsample_bylevel': 0.9227015137629297, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.036950619226612795, 'config/FLAML_sample_size': 62656, 'experiment_tag': 'exp', 'time_total_s': 13.70909309387207}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9227015137629297, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6727151764857845,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.05195093592765227, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.036950619226612795,
             scale_pos_weight=1, subsample=0.9485089295118937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.4935172387383856
SO2(0)的mse=10.938112070341166
SO2(0)的mae=1.8151841180431805
SO2(0)的mar=0.31060739523632613
总共花费的时间为：61.90
吉安市
2367A
2368A
2369A
2370A
[flaml.automl: 09-18 07:16:42] {2390} INFO - task = regression
[flaml.automl: 09-18 07:16:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:16:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:16:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:16:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:16:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:16:44] {3025} INFO - Estimated sufficient time budget=99382s. Estimated necessary time budget=99s.
[flaml.automl: 09-18 07:16:44] {3072} INFO -  at 2.5s,	estimator xgboost's best error=5.9840,	best estimator xgboost's best error=5.9840
[flaml.automl: 09-18 07:16:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:16:47] {3072} INFO -  at 5.1s,	estimator xgboost's best error=2.9485,	best estimator xgboost's best error=2.9485
[flaml.automl: 09-18 07:16:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:16:48] {3072} INFO -  at 6.3s,	estimator xgboost's best error=2.9485,	best estimator xgboost's best error=2.9485
[flaml.automl: 09-18 07:16:48] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:16:54] {3072} INFO -  at 12.1s,	estimator xgboost's best error=2.9485,	best estimator xgboost's best error=2.9485
[flaml.automl: 09-18 07:16:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:16:55] {3072} INFO -  at 13.3s,	estimator xgboost's best error=2.1450,	best estimator xgboost's best error=2.1450
[flaml.automl: 09-18 07:16:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:16:56] {3072} INFO -  at 14.9s,	estimator xgboost's best error=2.1450,	best estimator xgboost's best error=2.1450
[flaml.automl: 09-18 07:16:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:16:58] {3072} INFO -  at 16.5s,	estimator xgboost's best error=1.8350,	best estimator xgboost's best error=1.8350
[flaml.automl: 09-18 07:16:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:17:01] {3072} INFO -  at 19.2s,	estimator xgboost's best error=1.8350,	best estimator xgboost's best error=1.8350
[flaml.automl: 09-18 07:17:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:17:02] {3072} INFO -  at 20.9s,	estimator xgboost's best error=1.8350,	best estimator xgboost's best error=1.8350
[flaml.automl: 09-18 07:17:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:17:05] {3072} INFO -  at 23.9s,	estimator xgboost's best error=1.8350,	best estimator xgboost's best error=1.8350
[flaml.automl: 09-18 07:17:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:17:07] {3072} INFO -  at 25.3s,	estimator xgboost's best error=1.8350,	best estimator xgboost's best error=1.8350
[flaml.automl: 09-18 07:17:07] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:17:09] {3072} INFO -  at 27.0s,	estimator xgboost's best error=1.8350,	best estimator xgboost's best error=1.8350
[flaml.automl: 09-18 07:17:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:17:10] {3072} INFO -  at 28.2s,	estimator xgboost's best error=1.8350,	best estimator xgboost's best error=1.8350
[flaml.automl: 09-18 07:17:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:17:17] {3072} INFO -  at 35.2s,	estimator xgboost's best error=1.7981,	best estimator xgboost's best error=1.7981
[flaml.automl: 09-18 07:17:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:17:30] {3072} INFO -  at 48.1s,	estimator xgboost's best error=1.7766,	best estimator xgboost's best error=1.7766
[flaml.automl: 09-18 07:17:42] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 07:17:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:17:42] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:17:42] {2637} INFO - Time taken to find the best model: 48.064197301864624
[flaml.automl: 09-18 07:17:42] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43395}
SO2(0)最佳损失：-0.7765863570421379
SO2(0)最好结果：{'pred_time': 9.028954112346812e-06, 'wall_clock_time': 48.064197301864624, 'metric_for_logging': {'pred_time': 9.028954112346812e-06}, 'val_loss': 1.7765863570421379, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 43395}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 43395, 'experiment_tag': 'exp', 'time_total_s': 12.817500829696655}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6488915051452748
SO2(0)的mse=7.729914536996154
SO2(0)的mae=1.7770871544071167
SO2(0)的mar=0.22381466331570574
总共花费的时间为：61.65
宜春市
2371A
2374A
2375A
3151A
3415A
[flaml.automl: 09-18 07:33:42] {2390} INFO - task = regression
[flaml.automl: 09-18 07:33:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:33:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:33:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:33:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:33:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:33:43] {3025} INFO - Estimated sufficient time budget=62368s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 07:33:43] {3072} INFO -  at 1.5s,	estimator xgboost's best error=6.0459,	best estimator xgboost's best error=6.0459
[flaml.automl: 09-18 07:33:43] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:33:45] {3072} INFO -  at 3.6s,	estimator xgboost's best error=3.1444,	best estimator xgboost's best error=3.1444
[flaml.automl: 09-18 07:33:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:33:46] {3072} INFO -  at 4.8s,	estimator xgboost's best error=3.1444,	best estimator xgboost's best error=3.1444
[flaml.automl: 09-18 07:33:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:33:51] {3072} INFO -  at 9.6s,	estimator xgboost's best error=3.1444,	best estimator xgboost's best error=3.1444
[flaml.automl: 09-18 07:33:51] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:33:52] {3072} INFO -  at 10.8s,	estimator xgboost's best error=2.4605,	best estimator xgboost's best error=2.4605
[flaml.automl: 09-18 07:33:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:33:54] {3072} INFO -  at 12.4s,	estimator xgboost's best error=2.2906,	best estimator xgboost's best error=2.2906
[flaml.automl: 09-18 07:33:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:33:56] {3072} INFO -  at 14.0s,	estimator xgboost's best error=2.2209,	best estimator xgboost's best error=2.2209
[flaml.automl: 09-18 07:33:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:33:58] {3072} INFO -  at 16.7s,	estimator xgboost's best error=2.2209,	best estimator xgboost's best error=2.2209
[flaml.automl: 09-18 07:33:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:34:00] {3072} INFO -  at 18.3s,	estimator xgboost's best error=2.2209,	best estimator xgboost's best error=2.2209
[flaml.automl: 09-18 07:34:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:34:03] {3072} INFO -  at 21.3s,	estimator xgboost's best error=2.0932,	best estimator xgboost's best error=2.0932
[flaml.automl: 09-18 07:34:03] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:34:05] {3072} INFO -  at 23.0s,	estimator xgboost's best error=2.0932,	best estimator xgboost's best error=2.0932
[flaml.automl: 09-18 07:34:05] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:34:06] {3072} INFO -  at 24.1s,	estimator xgboost's best error=2.0932,	best estimator xgboost's best error=2.0932
[flaml.automl: 09-18 07:34:06] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:34:09] {3072} INFO -  at 27.0s,	estimator xgboost's best error=2.0932,	best estimator xgboost's best error=2.0932
[flaml.automl: 09-18 07:34:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:34:12] {3072} INFO -  at 29.8s,	estimator xgboost's best error=2.0932,	best estimator xgboost's best error=2.0932
[flaml.automl: 09-18 07:34:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:34:15] {3072} INFO -  at 32.9s,	estimator xgboost's best error=2.0791,	best estimator xgboost's best error=2.0791
[flaml.automl: 09-18 07:34:15] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 07:34:20] {3072} INFO -  at 37.8s,	estimator xgboost's best error=2.0791,	best estimator xgboost's best error=2.0791
[flaml.automl: 09-18 07:34:20] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 07:34:21] {3072} INFO -  at 39.7s,	estimator xgboost's best error=2.0791,	best estimator xgboost's best error=2.0791
[flaml.automl: 09-18 07:34:21] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 07:34:23] {3072} INFO -  at 41.3s,	estimator xgboost's best error=2.0791,	best estimator xgboost's best error=2.0791
[flaml.automl: 09-18 07:34:23] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 07:34:30] {3072} INFO -  at 48.7s,	estimator xgboost's best error=2.0791,	best estimator xgboost's best error=2.0791
[flaml.automl: 09-18 07:34:30] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 07:34:32] {3072} INFO -  at 49.9s,	estimator xgboost's best error=2.0791,	best estimator xgboost's best error=2.0791
[flaml.automl: 09-18 07:34:32] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 07:34:40] {3072} INFO -  at 58.7s,	estimator xgboost's best error=2.0398,	best estimator xgboost's best error=2.0398
[flaml.automl: 09-18 07:34:49] {3335} INFO - retrain xgboost for 8.9s
[flaml.automl: 09-18 07:34:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8105661245809422, colsample_bynode=1,
             colsample_bytree=0.9736248890816903, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.025507582899487728,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001173430399890411, reg_lambda=0.11802693660028775,
             scale_pos_weight=1, subsample=0.963672855918478,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 07:34:49] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:34:49] {2637} INFO - Time taken to find the best model: 58.7392840385437
[flaml.automl: 09-18 07:34:49] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 14, 'min_child_weight': 0.025507582899487728, 'learning_rate': 1.0, 'subsample': 0.963672855918478, 'colsample_bylevel': 0.8105661245809422, 'colsample_bytree': 0.9736248890816903, 'reg_alpha': 0.001173430399890411, 'reg_lambda': 0.11802693660028775, 'FLAML_sample_size': 51868}
SO2(0)最佳损失：-1.0398288878362103
SO2(0)最好结果：{'pred_time': 7.069292869276673e-06, 'wall_clock_time': 58.7392840385437, 'metric_for_logging': {'pred_time': 7.069292869276673e-06}, 'val_loss': 2.0398288878362103, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 14, 'min_child_weight': 0.025507582899487728, 'learning_rate': 1.0, 'subsample': 0.963672855918478, 'colsample_bylevel': 0.8105661245809422, 'colsample_bytree': 0.9736248890816903, 'reg_alpha': 0.001173430399890411, 'reg_lambda': 0.11802693660028775, 'FLAML_sample_size': 51868}, 'config/n_estimators': 11, 'config/max_leaves': 14, 'config/min_child_weight': 0.025507582899487728, 'config/learning_rate': 1.0, 'config/subsample': 0.963672855918478, 'config/colsample_bylevel': 0.8105661245809422, 'config/colsample_bytree': 0.9736248890816903, 'config/reg_alpha': 0.001173430399890411, 'config/reg_lambda': 0.11802693660028775, 'config/FLAML_sample_size': 51868, 'experiment_tag': 'exp', 'time_total_s': 8.881052494049072}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8105661245809422, colsample_bynode=1,
             colsample_bytree=0.9736248890816903, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.025507582899487728,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001173430399890411, reg_lambda=0.11802693660028775,
             scale_pos_weight=1, subsample=0.963672855918478,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6895056619671647
SO2(0)的mse=10.321844750654696
SO2(0)的mae=1.9930954264671188
SO2(0)的mar=0.2990297443510984
总共花费的时间为：68.47
抚州市
2376A
2377A
2378A
2379A
2380A
[flaml.automl: 09-18 07:52:00] {2390} INFO - task = regression
[flaml.automl: 09-18 07:52:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 07:52:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 07:52:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 07:52:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 07:52:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 07:52:01] {3025} INFO - Estimated sufficient time budget=63247s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 07:52:01] {3072} INFO -  at 1.5s,	estimator xgboost's best error=3.9719,	best estimator xgboost's best error=3.9719
[flaml.automl: 09-18 07:52:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 07:52:03] {3072} INFO -  at 3.6s,	estimator xgboost's best error=1.9448,	best estimator xgboost's best error=1.9448
[flaml.automl: 09-18 07:52:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 07:52:04] {3072} INFO -  at 4.8s,	estimator xgboost's best error=1.9448,	best estimator xgboost's best error=1.9448
[flaml.automl: 09-18 07:52:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 07:52:09] {3072} INFO -  at 9.5s,	estimator xgboost's best error=1.9448,	best estimator xgboost's best error=1.9448
[flaml.automl: 09-18 07:52:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 07:52:10] {3072} INFO -  at 11.0s,	estimator xgboost's best error=1.3489,	best estimator xgboost's best error=1.3489
[flaml.automl: 09-18 07:52:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 07:52:13] {3072} INFO -  at 13.4s,	estimator xgboost's best error=1.2687,	best estimator xgboost's best error=1.2687
[flaml.automl: 09-18 07:52:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 07:52:16] {3072} INFO -  at 16.2s,	estimator xgboost's best error=1.2564,	best estimator xgboost's best error=1.2564
[flaml.automl: 09-18 07:52:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 07:52:20] {3072} INFO -  at 20.3s,	estimator xgboost's best error=1.2564,	best estimator xgboost's best error=1.2564
[flaml.automl: 09-18 07:52:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 07:52:22] {3072} INFO -  at 22.9s,	estimator xgboost's best error=1.2564,	best estimator xgboost's best error=1.2564
[flaml.automl: 09-18 07:52:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 07:52:25] {3072} INFO -  at 26.0s,	estimator xgboost's best error=1.2564,	best estimator xgboost's best error=1.2564
[flaml.automl: 09-18 07:52:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 07:52:28] {3072} INFO -  at 28.4s,	estimator xgboost's best error=1.2564,	best estimator xgboost's best error=1.2564
[flaml.automl: 09-18 07:52:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 07:52:31] {3072} INFO -  at 31.4s,	estimator xgboost's best error=1.2457,	best estimator xgboost's best error=1.2457
[flaml.automl: 09-18 07:52:31] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 07:52:33] {3072} INFO -  at 33.7s,	estimator xgboost's best error=1.2457,	best estimator xgboost's best error=1.2457
[flaml.automl: 09-18 07:52:33] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 07:52:45] {3072} INFO -  at 45.3s,	estimator xgboost's best error=1.2249,	best estimator xgboost's best error=1.2249
[flaml.automl: 09-18 07:52:45] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 07:52:59] {3072} INFO -  at 59.8s,	estimator xgboost's best error=1.2180,	best estimator xgboost's best error=1.2180
[flaml.automl: 09-18 07:53:21] {3335} INFO - retrain xgboost for 21.9s
[flaml.automl: 09-18 07:53:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 07:53:21] {2636} INFO - fit succeeded
[flaml.automl: 09-18 07:53:21] {2637} INFO - Time taken to find the best model: 59.777287006378174
[flaml.automl: 09-18 07:53:21] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 52708}
SO2(0)最佳损失：-0.2179510813840566
SO2(0)最好结果：{'pred_time': 1.4331655451843147e-05, 'wall_clock_time': 59.777287006378174, 'metric_for_logging': {'pred_time': 1.4331655451843147e-05}, 'val_loss': 1.2179510813840566, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 52708}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 52708, 'experiment_tag': 'exp', 'time_total_s': 14.517127513885498}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.2924988718508559
SO2(0)的mse=6.259577778187806
SO2(0)的mae=1.1639655147803365
SO2(0)的mar=0.18352087876021042
总共花费的时间为：82.48
上饶市
2381A
2382A
2383A
3685A
[flaml.automl: 09-18 08:05:47] {2390} INFO - task = regression
[flaml.automl: 09-18 08:05:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:05:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:05:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:05:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:05:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:05:48] {3025} INFO - Estimated sufficient time budget=50352s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 08:05:48] {3072} INFO -  at 1.4s,	estimator xgboost's best error=9.0446,	best estimator xgboost's best error=9.0446
[flaml.automl: 09-18 08:05:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:05:50] {3072} INFO -  at 3.5s,	estimator xgboost's best error=5.4798,	best estimator xgboost's best error=5.4798
[flaml.automl: 09-18 08:05:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:05:51] {3072} INFO -  at 4.7s,	estimator xgboost's best error=5.4798,	best estimator xgboost's best error=5.4798
[flaml.automl: 09-18 08:05:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:05:57] {3072} INFO -  at 11.0s,	estimator xgboost's best error=5.4798,	best estimator xgboost's best error=5.4798
[flaml.automl: 09-18 08:05:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:05:59] {3072} INFO -  at 12.1s,	estimator xgboost's best error=5.4396,	best estimator xgboost's best error=5.4396
[flaml.automl: 09-18 08:05:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:06:00] {3072} INFO -  at 13.7s,	estimator xgboost's best error=5.4396,	best estimator xgboost's best error=5.4396
[flaml.automl: 09-18 08:06:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:06:02] {3072} INFO -  at 15.4s,	estimator xgboost's best error=4.9187,	best estimator xgboost's best error=4.9187
[flaml.automl: 09-18 08:06:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:06:05] {3072} INFO -  at 18.1s,	estimator xgboost's best error=4.9187,	best estimator xgboost's best error=4.9187
[flaml.automl: 09-18 08:06:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:06:06] {3072} INFO -  at 19.7s,	estimator xgboost's best error=4.9187,	best estimator xgboost's best error=4.9187
[flaml.automl: 09-18 08:06:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:06:09] {3072} INFO -  at 22.7s,	estimator xgboost's best error=4.8018,	best estimator xgboost's best error=4.8018
[flaml.automl: 09-18 08:06:09] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:06:11] {3072} INFO -  at 24.4s,	estimator xgboost's best error=4.8018,	best estimator xgboost's best error=4.8018
[flaml.automl: 09-18 08:06:11] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:06:12] {3072} INFO -  at 25.6s,	estimator xgboost's best error=4.8018,	best estimator xgboost's best error=4.8018
[flaml.automl: 09-18 08:06:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:06:15] {3072} INFO -  at 28.8s,	estimator xgboost's best error=4.8018,	best estimator xgboost's best error=4.8018
[flaml.automl: 09-18 08:06:15] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:06:19] {3072} INFO -  at 32.1s,	estimator xgboost's best error=4.8018,	best estimator xgboost's best error=4.8018
[flaml.automl: 09-18 08:06:19] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:06:24] {3072} INFO -  at 37.1s,	estimator xgboost's best error=4.7479,	best estimator xgboost's best error=4.7479
[flaml.automl: 09-18 08:06:24] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 08:06:32] {3072} INFO -  at 45.5s,	estimator xgboost's best error=4.7479,	best estimator xgboost's best error=4.7479
[flaml.automl: 09-18 08:06:32] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 08:06:35] {3072} INFO -  at 49.0s,	estimator xgboost's best error=4.7479,	best estimator xgboost's best error=4.7479
[flaml.automl: 09-18 08:06:35] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 08:06:39] {3072} INFO -  at 52.2s,	estimator xgboost's best error=4.7479,	best estimator xgboost's best error=4.7479
[flaml.automl: 09-18 08:06:39] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 08:06:45] {3072} INFO -  at 58.9s,	estimator xgboost's best error=4.7479,	best estimator xgboost's best error=4.7479
[flaml.automl: 09-18 08:06:50] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-18 08:06:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 08:06:50] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:06:50] {2637} INFO - Time taken to find the best model: 37.147955894470215
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768, 'FLAML_sample_size': 40933}
SO2(0)最佳损失：-3.7479439197883266
SO2(0)最好结果：{'pred_time': 1.6886052206915325e-05, 'wall_clock_time': 37.147955894470215, 'metric_for_logging': {'pred_time': 1.6886052206915325e-05}, 'val_loss': 4.747943919788327, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768, 'FLAML_sample_size': 40933}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'config/FLAML_sample_size': 40933, 'experiment_tag': 'exp', 'time_total_s': 5.04813289642334}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.03685115543686324
SO2(0)的mse=72.97185663393287
SO2(0)的mae=4.771159407363185
SO2(0)的mar=0.44415045669367886
总共花费的时间为：64.69
鹤壁市
2385A
2386A
2387A
3474A
3596A
[flaml.automl: 09-18 08:21:55] {2390} INFO - task = regression
[flaml.automl: 09-18 08:21:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:21:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:21:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:21:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:21:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:21:56] {3025} INFO - Estimated sufficient time budget=64552s. Estimated necessary time budget=65s.
[flaml.automl: 09-18 08:21:56] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.9039,	best estimator xgboost's best error=5.9039
[flaml.automl: 09-18 08:21:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:21:58] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.8067,	best estimator xgboost's best error=2.8067
[flaml.automl: 09-18 08:21:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:21:59] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.8067,	best estimator xgboost's best error=2.8067
[flaml.automl: 09-18 08:21:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:22:04] {3072} INFO -  at 9.5s,	estimator xgboost's best error=2.8067,	best estimator xgboost's best error=2.8067
[flaml.automl: 09-18 08:22:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:22:05] {3072} INFO -  at 10.7s,	estimator xgboost's best error=2.0056,	best estimator xgboost's best error=2.0056
[flaml.automl: 09-18 08:22:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:22:07] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.7930,	best estimator xgboost's best error=1.7930
[flaml.automl: 09-18 08:22:07] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:22:08] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.7034,	best estimator xgboost's best error=1.7034
[flaml.automl: 09-18 08:22:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:22:11] {3072} INFO -  at 16.5s,	estimator xgboost's best error=1.7034,	best estimator xgboost's best error=1.7034
[flaml.automl: 09-18 08:22:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:22:13] {3072} INFO -  at 18.1s,	estimator xgboost's best error=1.7034,	best estimator xgboost's best error=1.7034
[flaml.automl: 09-18 08:22:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:22:16] {3072} INFO -  at 21.1s,	estimator xgboost's best error=1.6704,	best estimator xgboost's best error=1.6704
[flaml.automl: 09-18 08:22:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:22:17] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.6704,	best estimator xgboost's best error=1.6704
[flaml.automl: 09-18 08:22:17] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:22:18] {3072} INFO -  at 23.9s,	estimator xgboost's best error=1.6704,	best estimator xgboost's best error=1.6704
[flaml.automl: 09-18 08:22:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:22:21] {3072} INFO -  at 26.7s,	estimator xgboost's best error=1.5789,	best estimator xgboost's best error=1.5789
[flaml.automl: 09-18 08:22:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:22:24] {3072} INFO -  at 29.6s,	estimator xgboost's best error=1.5789,	best estimator xgboost's best error=1.5789
[flaml.automl: 09-18 08:22:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:22:27] {3072} INFO -  at 32.2s,	estimator xgboost's best error=1.5789,	best estimator xgboost's best error=1.5789
[flaml.automl: 09-18 08:22:27] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 08:22:30] {3072} INFO -  at 35.1s,	estimator xgboost's best error=1.5789,	best estimator xgboost's best error=1.5789
[flaml.automl: 09-18 08:22:30] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 08:22:31] {3072} INFO -  at 36.9s,	estimator xgboost's best error=1.5789,	best estimator xgboost's best error=1.5789
[flaml.automl: 09-18 08:22:31] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 08:22:53] {3072} INFO -  at 58.8s,	estimator xgboost's best error=1.5540,	best estimator xgboost's best error=1.5540
[flaml.automl: 09-18 08:23:15] {3335} INFO - retrain xgboost for 21.4s
[flaml.automl: 09-18 08:23:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 08:23:15] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:23:15] {2637} INFO - Time taken to find the best model: 58.77769136428833
[flaml.automl: 09-18 08:23:15] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007, 'FLAML_sample_size': 54083}
SO2(0)最佳损失：-0.5539937542996272
SO2(0)最好结果：{'pred_time': 1.2047556593096792e-05, 'wall_clock_time': 58.77769136428833, 'metric_for_logging': {'pred_time': 1.2047556593096792e-05}, 'val_loss': 1.5539937542996272, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007, 'FLAML_sample_size': 54083}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'config/FLAML_sample_size': 54083, 'experiment_tag': 'exp', 'time_total_s': 21.907158136367798}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6826877086660753
SO2(0)的mse=7.153205473003692
SO2(0)的mae=1.6509934726786777
SO2(0)的mar=0.19567765985575652
总共花费的时间为：81.06
新乡市
2390A
2391A
3054A
3475A
3476A
[flaml.automl: 09-18 08:39:01] {2390} INFO - task = regression
[flaml.automl: 09-18 08:39:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:39:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:39:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:39:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:39:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:39:02] {3025} INFO - Estimated sufficient time budget=63097s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 08:39:02] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.8048,	best estimator xgboost's best error=5.8048
[flaml.automl: 09-18 08:39:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:39:05] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.6904,	best estimator xgboost's best error=2.6904
[flaml.automl: 09-18 08:39:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:39:06] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.6904,	best estimator xgboost's best error=2.6904
[flaml.automl: 09-18 08:39:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:39:11] {3072} INFO -  at 9.5s,	estimator xgboost's best error=2.6904,	best estimator xgboost's best error=2.6904
[flaml.automl: 09-18 08:39:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:39:12] {3072} INFO -  at 10.7s,	estimator xgboost's best error=1.6723,	best estimator xgboost's best error=1.6723
[flaml.automl: 09-18 08:39:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:39:13] {3072} INFO -  at 12.3s,	estimator xgboost's best error=1.4869,	best estimator xgboost's best error=1.4869
[flaml.automl: 09-18 08:39:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:39:15] {3072} INFO -  at 13.9s,	estimator xgboost's best error=1.3550,	best estimator xgboost's best error=1.3550
[flaml.automl: 09-18 08:39:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:39:18] {3072} INFO -  at 16.6s,	estimator xgboost's best error=1.3550,	best estimator xgboost's best error=1.3550
[flaml.automl: 09-18 08:39:18] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:39:19] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.3550,	best estimator xgboost's best error=1.3550
[flaml.automl: 09-18 08:39:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:39:22] {3072} INFO -  at 21.2s,	estimator xgboost's best error=1.2537,	best estimator xgboost's best error=1.2537
[flaml.automl: 09-18 08:39:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:39:24] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.2537,	best estimator xgboost's best error=1.2537
[flaml.automl: 09-18 08:39:24] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:39:25] {3072} INFO -  at 23.9s,	estimator xgboost's best error=1.2537,	best estimator xgboost's best error=1.2537
[flaml.automl: 09-18 08:39:25] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:39:28] {3072} INFO -  at 26.8s,	estimator xgboost's best error=1.2116,	best estimator xgboost's best error=1.2116
[flaml.automl: 09-18 08:39:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:39:31] {3072} INFO -  at 29.9s,	estimator xgboost's best error=1.2116,	best estimator xgboost's best error=1.2116
[flaml.automl: 09-18 08:39:31] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:39:33] {3072} INFO -  at 32.1s,	estimator xgboost's best error=1.2068,	best estimator xgboost's best error=1.2068
[flaml.automl: 09-18 08:39:33] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 08:39:37] {3072} INFO -  at 35.6s,	estimator xgboost's best error=1.2068,	best estimator xgboost's best error=1.2068
[flaml.automl: 09-18 08:39:37] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 08:39:38] {3072} INFO -  at 37.2s,	estimator xgboost's best error=1.2068,	best estimator xgboost's best error=1.2068
[flaml.automl: 09-18 08:39:38] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 08:39:39] {3072} INFO -  at 38.4s,	estimator xgboost's best error=1.2068,	best estimator xgboost's best error=1.2068
[flaml.automl: 09-18 08:39:39] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 08:39:44] {3072} INFO -  at 42.8s,	estimator xgboost's best error=1.2068,	best estimator xgboost's best error=1.2068
[flaml.automl: 09-18 08:39:44] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 08:39:56] {3072} INFO -  at 54.8s,	estimator xgboost's best error=1.1598,	best estimator xgboost's best error=1.1598
[flaml.automl: 09-18 08:40:09] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 08:40:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7063431279374065, colsample_bynode=1,
             colsample_bytree=0.7891928823597631, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=21, min_child_weight=0.04606527892183358,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.011846412336189025, reg_lambda=1.0350015394258016,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:40:09] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:40:09] {2637} INFO - Time taken to find the best model: 54.83256912231445
[flaml.automl: 09-18 08:40:09] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 21, 'min_child_weight': 0.04606527892183358, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7063431279374065, 'colsample_bytree': 0.7891928823597631, 'reg_alpha': 0.011846412336189025, 'reg_lambda': 1.0350015394258016, 'FLAML_sample_size': 52552}
SO2(0)最佳损失：-0.1598148008936071
SO2(0)最好结果：{'pred_time': 1.5600821743272755e-05, 'wall_clock_time': 54.83256912231445, 'metric_for_logging': {'pred_time': 1.5600821743272755e-05}, 'val_loss': 1.1598148008936071, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 21, 'min_child_weight': 0.04606527892183358, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7063431279374065, 'colsample_bytree': 0.7891928823597631, 'reg_alpha': 0.011846412336189025, 'reg_lambda': 1.0350015394258016, 'FLAML_sample_size': 52552}, 'config/n_estimators': 6, 'config/max_leaves': 21, 'config/min_child_weight': 0.04606527892183358, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7063431279374065, 'config/colsample_bytree': 0.7891928823597631, 'config/reg_alpha': 0.011846412336189025, 'config/reg_lambda': 1.0350015394258016, 'config/FLAML_sample_size': 52552, 'experiment_tag': 'exp', 'time_total_s': 12.013845920562744}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7063431279374065, colsample_bynode=1,
             colsample_bytree=0.7891928823597631, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=21, min_child_weight=0.04606527892183358,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.011846412336189025, reg_lambda=1.0350015394258016,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.842774912049378
SO2(0)的mse=3.1380108311684656
SO2(0)的mae=1.1472494231787327
SO2(0)的mar=0.1371468959445175
总共花费的时间为：68.75
濮阳市
2392A
2395A
3021A
[flaml.automl: 09-18 08:49:27] {2390} INFO - task = regression
[flaml.automl: 09-18 08:49:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 08:49:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 08:49:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 08:49:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 08:49:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 08:49:28] {3025} INFO - Estimated sufficient time budget=12176s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 08:49:28] {3072} INFO -  at 1.3s,	estimator xgboost's best error=5.0399,	best estimator xgboost's best error=5.0399
[flaml.automl: 09-18 08:49:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 08:49:30] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.3430,	best estimator xgboost's best error=2.3430
[flaml.automl: 09-18 08:49:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 08:49:31] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.3430,	best estimator xgboost's best error=2.3430
[flaml.automl: 09-18 08:49:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 08:49:41] {3072} INFO -  at 14.6s,	estimator xgboost's best error=2.3430,	best estimator xgboost's best error=2.3430
[flaml.automl: 09-18 08:49:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 08:49:43] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.4087,	best estimator xgboost's best error=1.4087
[flaml.automl: 09-18 08:49:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 08:49:44] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.4087,	best estimator xgboost's best error=1.4087
[flaml.automl: 09-18 08:49:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 08:49:46] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.0932,	best estimator xgboost's best error=1.0932
[flaml.automl: 09-18 08:49:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 08:49:49] {3072} INFO -  at 21.8s,	estimator xgboost's best error=1.0932,	best estimator xgboost's best error=1.0932
[flaml.automl: 09-18 08:49:49] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 08:49:50] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.0932,	best estimator xgboost's best error=1.0932
[flaml.automl: 09-18 08:49:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 08:49:53] {3072} INFO -  at 26.4s,	estimator xgboost's best error=1.0932,	best estimator xgboost's best error=1.0932
[flaml.automl: 09-18 08:49:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 08:49:55] {3072} INFO -  at 27.8s,	estimator xgboost's best error=1.0911,	best estimator xgboost's best error=1.0911
[flaml.automl: 09-18 08:49:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 08:49:56] {3072} INFO -  at 29.0s,	estimator xgboost's best error=1.0911,	best estimator xgboost's best error=1.0911
[flaml.automl: 09-18 08:49:56] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 08:50:02] {3072} INFO -  at 34.9s,	estimator xgboost's best error=1.0098,	best estimator xgboost's best error=1.0098
[flaml.automl: 09-18 08:50:02] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 08:50:12] {3072} INFO -  at 45.7s,	estimator xgboost's best error=0.9696,	best estimator xgboost's best error=0.9696
[flaml.automl: 09-18 08:50:12] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 08:50:18] {3072} INFO -  at 51.6s,	estimator xgboost's best error=0.9696,	best estimator xgboost's best error=0.9696
[flaml.automl: 09-18 08:50:29] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-18 08:50:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 08:50:29] {2636} INFO - fit succeeded
[flaml.automl: 09-18 08:50:29] {2637} INFO - Time taken to find the best model: 45.67370343208313
[flaml.automl: 09-18 08:50:29] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}
SO2(0)最佳损失：0.030413326027632026
SO2(0)最好结果：{'pred_time': 1.1801981952193022e-05, 'wall_clock_time': 45.67370343208313, 'metric_for_logging': {'pred_time': 1.1801981952193022e-05}, 'val_loss': 0.969586673972368, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'experiment_tag': 'exp', 'time_total_s': 10.743160247802734}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8109629684865627
SO2(0)的mse=2.576559154841757
SO2(0)的mae=0.9315929718831978
SO2(0)的mar=0.11905401379828555
总共花费的时间为：62.97
许昌市
2396A
3134A
3337A
3338A
3597A
[flaml.automl: 09-18 09:05:47] {2390} INFO - task = regression
[flaml.automl: 09-18 09:05:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:05:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:05:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:05:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:05:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:05:48] {3025} INFO - Estimated sufficient time budget=66368s. Estimated necessary time budget=66s.
[flaml.automl: 09-18 09:05:48] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.8601,	best estimator xgboost's best error=4.8601
[flaml.automl: 09-18 09:05:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:05:50] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.3300,	best estimator xgboost's best error=2.3300
[flaml.automl: 09-18 09:05:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:05:52] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.3300,	best estimator xgboost's best error=2.3300
[flaml.automl: 09-18 09:05:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:05:56] {3072} INFO -  at 9.6s,	estimator xgboost's best error=2.3300,	best estimator xgboost's best error=2.3300
[flaml.automl: 09-18 09:05:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:05:58] {3072} INFO -  at 10.7s,	estimator xgboost's best error=1.6037,	best estimator xgboost's best error=1.6037
[flaml.automl: 09-18 09:05:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:05:59] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.4432,	best estimator xgboost's best error=1.4432
[flaml.automl: 09-18 09:05:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:06:01] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.4058,	best estimator xgboost's best error=1.4058
[flaml.automl: 09-18 09:06:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:06:03] {3072} INFO -  at 16.5s,	estimator xgboost's best error=1.4058,	best estimator xgboost's best error=1.4058
[flaml.automl: 09-18 09:06:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:06:05] {3072} INFO -  at 18.1s,	estimator xgboost's best error=1.4058,	best estimator xgboost's best error=1.4058
[flaml.automl: 09-18 09:06:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:06:08] {3072} INFO -  at 21.1s,	estimator xgboost's best error=1.2988,	best estimator xgboost's best error=1.2988
[flaml.automl: 09-18 09:06:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:06:10] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.2988,	best estimator xgboost's best error=1.2988
[flaml.automl: 09-18 09:06:10] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:06:11] {3072} INFO -  at 23.8s,	estimator xgboost's best error=1.2988,	best estimator xgboost's best error=1.2988
[flaml.automl: 09-18 09:06:11] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:06:14] {3072} INFO -  at 26.7s,	estimator xgboost's best error=1.2988,	best estimator xgboost's best error=1.2988
[flaml.automl: 09-18 09:06:14] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:06:16] {3072} INFO -  at 29.5s,	estimator xgboost's best error=1.2988,	best estimator xgboost's best error=1.2988
[flaml.automl: 09-18 09:06:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 09:06:19] {3072} INFO -  at 32.5s,	estimator xgboost's best error=1.2293,	best estimator xgboost's best error=1.2293
[flaml.automl: 09-18 09:06:19] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 09:06:24] {3072} INFO -  at 37.5s,	estimator xgboost's best error=1.2293,	best estimator xgboost's best error=1.2293
[flaml.automl: 09-18 09:06:24] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 09:06:26] {3072} INFO -  at 39.4s,	estimator xgboost's best error=1.2268,	best estimator xgboost's best error=1.2268
[flaml.automl: 09-18 09:06:26] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 09:06:28] {3072} INFO -  at 41.0s,	estimator xgboost's best error=1.2268,	best estimator xgboost's best error=1.2268
[flaml.automl: 09-18 09:06:28] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 09:06:32] {3072} INFO -  at 45.3s,	estimator xgboost's best error=1.2268,	best estimator xgboost's best error=1.2268
[flaml.automl: 09-18 09:06:32] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 09:06:33] {3072} INFO -  at 46.5s,	estimator xgboost's best error=1.2268,	best estimator xgboost's best error=1.2268
[flaml.automl: 09-18 09:06:33] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 09:06:39] {3072} INFO -  at 51.8s,	estimator xgboost's best error=1.2063,	best estimator xgboost's best error=1.2063
[flaml.automl: 09-18 09:06:39] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 09:06:46] {3072} INFO -  at 59.6s,	estimator xgboost's best error=1.1924,	best estimator xgboost's best error=1.1924
[flaml.automl: 09-18 09:07:10] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-18 09:07:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.008381997180108987, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003886437262068607, scale_pos_weight=1,
             subsample=0.9390180412130811, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 09:07:10] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:07:10] {2637} INFO - Time taken to find the best model: 59.6072039604187
[flaml.automl: 09-18 09:07:10] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 28, 'max_leaves': 15, 'min_child_weight': 0.008381997180108987, 'learning_rate': 0.6023269513573992, 'subsample': 0.9390180412130811, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003886437262068607, 'FLAML_sample_size': 55573}
SO2(0)最佳损失：-0.19236686928552182
SO2(0)最好结果：{'pred_time': 7.374566576258856e-06, 'wall_clock_time': 59.6072039604187, 'metric_for_logging': {'pred_time': 7.374566576258856e-06}, 'val_loss': 1.1923668692855218, 'training_iteration': 1, 'config': {'n_estimators': 28, 'max_leaves': 15, 'min_child_weight': 0.008381997180108987, 'learning_rate': 0.6023269513573992, 'subsample': 0.9390180412130811, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.936172861729374, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.003886437262068607, 'FLAML_sample_size': 55573}, 'config/n_estimators': 28, 'config/max_leaves': 15, 'config/min_child_weight': 0.008381997180108987, 'config/learning_rate': 0.6023269513573992, 'config/subsample': 0.9390180412130811, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.936172861729374, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.003886437262068607, 'config/FLAML_sample_size': 55573, 'experiment_tag': 'exp', 'time_total_s': 7.833909511566162}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.936172861729374, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6023269513573992,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.008381997180108987, missing=nan,
             monotone_constraints='()', n_estimators=28, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.003886437262068607, scale_pos_weight=1,
             subsample=0.9390180412130811, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7016465769224808
SO2(0)的mse=5.035275136637971
SO2(0)的mae=1.1788117309089932
SO2(0)的mar=0.1500670662448293
总共花费的时间为：84.33
漯河市
2399A
2400A
2401A
2402A
3478A
3479A
[flaml.automl: 09-18 09:25:40] {2390} INFO - task = regression
[flaml.automl: 09-18 09:25:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:25:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:25:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:25:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:25:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:25:41] {3025} INFO - Estimated sufficient time budget=77092s. Estimated necessary time budget=77s.
[flaml.automl: 09-18 09:25:41] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.4927,	best estimator xgboost's best error=4.4927
[flaml.automl: 09-18 09:25:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:25:43] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.0996,	best estimator xgboost's best error=2.0996
[flaml.automl: 09-18 09:25:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:25:44] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.0996,	best estimator xgboost's best error=2.0996
[flaml.automl: 09-18 09:25:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:25:48] {3072} INFO -  at 8.5s,	estimator xgboost's best error=2.0996,	best estimator xgboost's best error=2.0996
[flaml.automl: 09-18 09:25:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:25:49] {3072} INFO -  at 9.6s,	estimator xgboost's best error=1.4119,	best estimator xgboost's best error=1.4119
[flaml.automl: 09-18 09:25:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:25:51] {3072} INFO -  at 11.2s,	estimator xgboost's best error=1.3234,	best estimator xgboost's best error=1.3234
[flaml.automl: 09-18 09:25:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:25:53] {3072} INFO -  at 12.8s,	estimator xgboost's best error=1.2405,	best estimator xgboost's best error=1.2405
[flaml.automl: 09-18 09:25:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:25:55] {3072} INFO -  at 15.5s,	estimator xgboost's best error=1.2405,	best estimator xgboost's best error=1.2405
[flaml.automl: 09-18 09:25:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:25:57] {3072} INFO -  at 17.1s,	estimator xgboost's best error=1.2405,	best estimator xgboost's best error=1.2405
[flaml.automl: 09-18 09:25:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:26:00] {3072} INFO -  at 20.1s,	estimator xgboost's best error=1.1662,	best estimator xgboost's best error=1.1662
[flaml.automl: 09-18 09:26:00] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:26:01] {3072} INFO -  at 21.7s,	estimator xgboost's best error=1.1662,	best estimator xgboost's best error=1.1662
[flaml.automl: 09-18 09:26:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:26:03] {3072} INFO -  at 22.9s,	estimator xgboost's best error=1.1662,	best estimator xgboost's best error=1.1662
[flaml.automl: 09-18 09:26:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:26:05] {3072} INFO -  at 24.9s,	estimator xgboost's best error=1.1332,	best estimator xgboost's best error=1.1332
[flaml.automl: 09-18 09:26:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:26:07] {3072} INFO -  at 27.1s,	estimator xgboost's best error=1.1332,	best estimator xgboost's best error=1.1332
[flaml.automl: 09-18 09:26:07] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 09:26:08] {3072} INFO -  at 28.4s,	estimator xgboost's best error=1.1332,	best estimator xgboost's best error=1.1332
[flaml.automl: 09-18 09:26:08] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 09:26:10] {3072} INFO -  at 30.2s,	estimator xgboost's best error=1.1332,	best estimator xgboost's best error=1.1332
[flaml.automl: 09-18 09:26:10] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 09:26:12] {3072} INFO -  at 32.0s,	estimator xgboost's best error=1.1332,	best estimator xgboost's best error=1.1332
[flaml.automl: 09-18 09:26:12] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 09:26:25] {3072} INFO -  at 45.6s,	estimator xgboost's best error=1.1332,	best estimator xgboost's best error=1.1332
[flaml.automl: 09-18 09:26:25] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 09:26:31] {3072} INFO -  at 51.1s,	estimator xgboost's best error=1.0957,	best estimator xgboost's best error=1.0957
[flaml.automl: 09-18 09:26:31] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 09:26:39] {3072} INFO -  at 59.4s,	estimator xgboost's best error=1.0957,	best estimator xgboost's best error=1.0957
[flaml.automl: 09-18 09:26:45] {3335} INFO - retrain xgboost for 5.5s
[flaml.automl: 09-18 09:26:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8892974922321603, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.25328141559296163,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.07660315947987856, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00898010782559076,
             reg_lambda=0.2683813756090025, scale_pos_weight=1,
             subsample=0.8468546263965728, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 09:26:45] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:26:45] {2637} INFO - Time taken to find the best model: 51.111992597579956
[flaml.automl: 09-18 09:26:45] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 7, 'min_child_weight': 0.07660315947987856, 'learning_rate': 0.25328141559296163, 'subsample': 0.8468546263965728, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8892974922321603, 'reg_alpha': 0.00898010782559076, 'reg_lambda': 0.2683813756090025, 'FLAML_sample_size': 65184}
SO2(0)最佳损失：-0.09573777280228413
SO2(0)最好结果：{'pred_time': 5.639423969189271e-06, 'wall_clock_time': 51.111992597579956, 'metric_for_logging': {'pred_time': 5.639423969189271e-06}, 'val_loss': 1.0957377728022841, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 7, 'min_child_weight': 0.07660315947987856, 'learning_rate': 0.25328141559296163, 'subsample': 0.8468546263965728, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8892974922321603, 'reg_alpha': 0.00898010782559076, 'reg_lambda': 0.2683813756090025, 'FLAML_sample_size': 65184}, 'config/n_estimators': 13, 'config/max_leaves': 7, 'config/min_child_weight': 0.07660315947987856, 'config/learning_rate': 0.25328141559296163, 'config/subsample': 0.8468546263965728, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8892974922321603, 'config/reg_alpha': 0.00898010782559076, 'config/reg_lambda': 0.2683813756090025, 'config/FLAML_sample_size': 65184, 'experiment_tag': 'exp', 'time_total_s': 5.4718334674835205}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8892974922321603, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.25328141559296163,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.07660315947987856, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00898010782559076,
             reg_lambda=0.2683813756090025, scale_pos_weight=1,
             subsample=0.8468546263965728, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6505139340643527
SO2(0)的mse=3.481354549411354
SO2(0)的mae=1.1074711226563805
SO2(0)的mar=0.1610226121763744
总共花费的时间为：65.76
南阳市
2403A
2404A
2405A
2406A
2407A
[flaml.automl: 09-18 09:42:47] {2390} INFO - task = regression
[flaml.automl: 09-18 09:42:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:42:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:42:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:42:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:42:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:42:48] {3025} INFO - Estimated sufficient time budget=65941s. Estimated necessary time budget=66s.
[flaml.automl: 09-18 09:42:48] {3072} INFO -  at 1.5s,	estimator xgboost's best error=4.8431,	best estimator xgboost's best error=4.8431
[flaml.automl: 09-18 09:42:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:42:50] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.1924,	best estimator xgboost's best error=2.1924
[flaml.automl: 09-18 09:42:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:42:52] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.1924,	best estimator xgboost's best error=2.1924
[flaml.automl: 09-18 09:42:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:42:56] {3072} INFO -  at 9.5s,	estimator xgboost's best error=2.1924,	best estimator xgboost's best error=2.1924
[flaml.automl: 09-18 09:42:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:42:58] {3072} INFO -  at 10.6s,	estimator xgboost's best error=1.0143,	best estimator xgboost's best error=1.0143
[flaml.automl: 09-18 09:42:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:42:59] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.8102,	best estimator xgboost's best error=0.8102
[flaml.automl: 09-18 09:42:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:43:01] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.7282,	best estimator xgboost's best error=0.7282
[flaml.automl: 09-18 09:43:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:43:03] {3072} INFO -  at 16.5s,	estimator xgboost's best error=0.7282,	best estimator xgboost's best error=0.7282
[flaml.automl: 09-18 09:43:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:43:05] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.7108,	best estimator xgboost's best error=0.7108
[flaml.automl: 09-18 09:43:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:43:08] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.7108,	best estimator xgboost's best error=0.7108
[flaml.automl: 09-18 09:43:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:43:09] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.7108,	best estimator xgboost's best error=0.7108
[flaml.automl: 09-18 09:43:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:43:10] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.7108,	best estimator xgboost's best error=0.7108
[flaml.automl: 09-18 09:43:10] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:43:13] {3072} INFO -  at 26.2s,	estimator xgboost's best error=0.5761,	best estimator xgboost's best error=0.5761
[flaml.automl: 09-18 09:43:13] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:43:16] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.5722,	best estimator xgboost's best error=0.5722
[flaml.automl: 09-18 09:43:16] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 09:43:18] {3072} INFO -  at 31.0s,	estimator xgboost's best error=0.5697,	best estimator xgboost's best error=0.5697
[flaml.automl: 09-18 09:43:18] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 09:43:20] {3072} INFO -  at 33.0s,	estimator xgboost's best error=0.5697,	best estimator xgboost's best error=0.5697
[flaml.automl: 09-18 09:43:20] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 09:43:22] {3072} INFO -  at 35.3s,	estimator xgboost's best error=0.5697,	best estimator xgboost's best error=0.5697
[flaml.automl: 09-18 09:43:22] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 09:43:24] {3072} INFO -  at 37.4s,	estimator xgboost's best error=0.5697,	best estimator xgboost's best error=0.5697
[flaml.automl: 09-18 09:43:24] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 09:43:26] {3072} INFO -  at 38.7s,	estimator xgboost's best error=0.5697,	best estimator xgboost's best error=0.5697
[flaml.automl: 09-18 09:43:26] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 09:43:33] {3072} INFO -  at 45.7s,	estimator xgboost's best error=0.5566,	best estimator xgboost's best error=0.5566
[flaml.automl: 09-18 09:43:33] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 09:43:35] {3072} INFO -  at 48.2s,	estimator xgboost's best error=0.5566,	best estimator xgboost's best error=0.5566
[flaml.automl: 09-18 09:43:35] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 09:43:47] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.5566,	best estimator xgboost's best error=0.5566
[flaml.automl: 09-18 09:43:53] {3335} INFO - retrain xgboost for 6.9s
[flaml.automl: 09-18 09:43:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8415627790092132, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609399,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 09:43:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:43:53] {2637} INFO - Time taken to find the best model: 45.655070066452026
[flaml.automl: 09-18 09:43:53] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.2501491252609399, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8415627790092132, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 55281}
SO2(0)最佳损失：0.4434111885036336
SO2(0)最好结果：{'pred_time': 6.518184643856733e-06, 'wall_clock_time': 45.655070066452026, 'metric_for_logging': {'pred_time': 6.518184643856733e-06}, 'val_loss': 0.5565888114963664, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.2501491252609399, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8415627790092132, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 55281}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609399, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8415627790092132, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466859, 'config/FLAML_sample_size': 55281, 'experiment_tag': 'exp', 'time_total_s': 6.938637018203735}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8415627790092132, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609399,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8735960910789052
SO2(0)的mse=0.8013579371858353
SO2(0)的mae=0.5638430258293292
SO2(0)的mar=0.07911881372216827
总共花费的时间为：67.38
商丘市
2408A
2409A
2410A
[flaml.automl: 09-18 09:53:24] {2390} INFO - task = regression
[flaml.automl: 09-18 09:53:24] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 09:53:24] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 09:53:24] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 09:53:24] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 09:53:24] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 09:53:26] {3025} INFO - Estimated sufficient time budget=12081s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 09:53:26] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.4299,	best estimator xgboost's best error=4.4299
[flaml.automl: 09-18 09:53:26] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 09:53:28] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.0462,	best estimator xgboost's best error=2.0462
[flaml.automl: 09-18 09:53:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 09:53:29] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.0462,	best estimator xgboost's best error=2.0462
[flaml.automl: 09-18 09:53:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 09:53:39] {3072} INFO -  at 14.6s,	estimator xgboost's best error=2.0462,	best estimator xgboost's best error=2.0462
[flaml.automl: 09-18 09:53:39] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 09:53:40] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.2076,	best estimator xgboost's best error=1.2076
[flaml.automl: 09-18 09:53:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 09:53:42] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.0414,	best estimator xgboost's best error=1.0414
[flaml.automl: 09-18 09:53:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 09:53:43] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.9379,	best estimator xgboost's best error=0.9379
[flaml.automl: 09-18 09:53:43] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 09:53:46] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.9379,	best estimator xgboost's best error=0.9379
[flaml.automl: 09-18 09:53:46] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 09:53:48] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.8966,	best estimator xgboost's best error=0.8966
[flaml.automl: 09-18 09:53:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 09:53:51] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.8966,	best estimator xgboost's best error=0.8966
[flaml.automl: 09-18 09:53:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 09:53:52] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.8966,	best estimator xgboost's best error=0.8966
[flaml.automl: 09-18 09:53:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 09:53:53] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.8966,	best estimator xgboost's best error=0.8966
[flaml.automl: 09-18 09:53:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 09:54:00] {3072} INFO -  at 35.9s,	estimator xgboost's best error=0.7567,	best estimator xgboost's best error=0.7567
[flaml.automl: 09-18 09:54:00] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 09:54:13] {3072} INFO -  at 48.7s,	estimator xgboost's best error=0.7340,	best estimator xgboost's best error=0.7340
[flaml.automl: 09-18 09:54:26] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 09:54:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 09:54:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 09:54:26] {2637} INFO - Time taken to find the best model: 48.693843364715576
[flaml.automl: 09-18 09:54:26] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}
SO2(0)最佳损失：0.26600155648643886
SO2(0)最好结果：{'pred_time': 1.0918289594595355e-05, 'wall_clock_time': 48.693843364715576, 'metric_for_logging': {'pred_time': 1.0918289594595355e-05}, 'val_loss': 0.7339984435135611, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'experiment_tag': 'exp', 'time_total_s': 12.806010723114014}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8729649069701778
SO2(0)的mse=1.4180664535566958
SO2(0)的mae=0.713260089009559
SO2(0)的mar=0.1078115300526299
总共花费的时间为：62.01
驻马店市
2420A
2421A
2422A
3339A
[flaml.automl: 09-18 10:06:44] {2390} INFO - task = regression
[flaml.automl: 09-18 10:06:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:06:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:06:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:06:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:06:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:06:46] {3025} INFO - Estimated sufficient time budget=98652s. Estimated necessary time budget=99s.
[flaml.automl: 09-18 10:06:46] {3072} INFO -  at 2.5s,	estimator xgboost's best error=4.7940,	best estimator xgboost's best error=4.7940
[flaml.automl: 09-18 10:06:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:06:49] {3072} INFO -  at 5.9s,	estimator xgboost's best error=2.2135,	best estimator xgboost's best error=2.2135
[flaml.automl: 09-18 10:06:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:06:51] {3072} INFO -  at 8.1s,	estimator xgboost's best error=2.2135,	best estimator xgboost's best error=2.2135
[flaml.automl: 09-18 10:06:52] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:06:57] {3072} INFO -  at 13.2s,	estimator xgboost's best error=2.2135,	best estimator xgboost's best error=2.2135
[flaml.automl: 09-18 10:06:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:06:58] {3072} INFO -  at 15.0s,	estimator xgboost's best error=1.1903,	best estimator xgboost's best error=1.1903
[flaml.automl: 09-18 10:06:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:07:01] {3072} INFO -  at 17.6s,	estimator xgboost's best error=0.9427,	best estimator xgboost's best error=0.9427
[flaml.automl: 09-18 10:07:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:07:04] {3072} INFO -  at 20.7s,	estimator xgboost's best error=0.9427,	best estimator xgboost's best error=0.9427
[flaml.automl: 09-18 10:07:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:07:08] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.9427,	best estimator xgboost's best error=0.9427
[flaml.automl: 09-18 10:07:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:07:09] {3072} INFO -  at 26.0s,	estimator xgboost's best error=0.9427,	best estimator xgboost's best error=0.9427
[flaml.automl: 09-18 10:07:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:07:12] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.9427,	best estimator xgboost's best error=0.9427
[flaml.automl: 09-18 10:07:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:07:15] {3072} INFO -  at 31.3s,	estimator xgboost's best error=0.9427,	best estimator xgboost's best error=0.9427
[flaml.automl: 09-18 10:07:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:07:17] {3072} INFO -  at 33.5s,	estimator xgboost's best error=0.9427,	best estimator xgboost's best error=0.9427
[flaml.automl: 09-18 10:07:17] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:07:24] {3072} INFO -  at 40.3s,	estimator xgboost's best error=0.8156,	best estimator xgboost's best error=0.8156
[flaml.automl: 09-18 10:07:24] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:07:36] {3072} INFO -  at 52.4s,	estimator xgboost's best error=0.7996,	best estimator xgboost's best error=0.7996
[flaml.automl: 09-18 10:07:48] {3335} INFO - retrain xgboost for 12.0s
[flaml.automl: 09-18 10:07:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:07:48] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:07:48] {2637} INFO - Time taken to find the best model: 52.44520616531372
[flaml.automl: 09-18 10:07:48] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 44047}
SO2(0)最佳损失：0.20042939551395345
SO2(0)最好结果：{'pred_time': 9.213039409882933e-06, 'wall_clock_time': 52.44520616531372, 'metric_for_logging': {'pred_time': 9.213039409882933e-06}, 'val_loss': 0.7995706044860466, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 44047}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 44047, 'experiment_tag': 'exp', 'time_total_s': 12.115630388259888}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8121138535962013
SO2(0)的mse=1.6591449693114153
SO2(0)的mae=0.7795613497090103
SO2(0)的mar=0.10569003157538093
总共花费的时间为：65.22
黄石市
2423A
2424A
2427A
3149A
[flaml.automl: 09-18 10:20:20] {2390} INFO - task = regression
[flaml.automl: 09-18 10:20:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:20:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:20:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:20:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:20:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:20:21] {3025} INFO - Estimated sufficient time budget=52103s. Estimated necessary time budget=52s.
[flaml.automl: 09-18 10:20:21] {3072} INFO -  at 1.4s,	estimator xgboost's best error=6.9704,	best estimator xgboost's best error=6.9704
[flaml.automl: 09-18 10:20:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:20:23] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.2344,	best estimator xgboost's best error=3.2344
[flaml.automl: 09-18 10:20:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:20:24] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.2344,	best estimator xgboost's best error=3.2344
[flaml.automl: 09-18 10:20:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:20:30] {3072} INFO -  at 10.5s,	estimator xgboost's best error=3.2344,	best estimator xgboost's best error=3.2344
[flaml.automl: 09-18 10:20:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:20:31] {3072} INFO -  at 11.6s,	estimator xgboost's best error=2.0985,	best estimator xgboost's best error=2.0985
[flaml.automl: 09-18 10:20:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:20:33] {3072} INFO -  at 13.1s,	estimator xgboost's best error=1.8188,	best estimator xgboost's best error=1.8188
[flaml.automl: 09-18 10:20:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:20:34] {3072} INFO -  at 14.7s,	estimator xgboost's best error=1.8188,	best estimator xgboost's best error=1.8188
[flaml.automl: 09-18 10:20:34] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:20:37] {3072} INFO -  at 17.1s,	estimator xgboost's best error=1.8188,	best estimator xgboost's best error=1.8188
[flaml.automl: 09-18 10:20:37] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:20:38] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.8188,	best estimator xgboost's best error=1.8188
[flaml.automl: 09-18 10:20:38] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:20:40] {3072} INFO -  at 20.8s,	estimator xgboost's best error=1.8188,	best estimator xgboost's best error=1.8188
[flaml.automl: 09-18 10:20:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:20:42] {3072} INFO -  at 22.4s,	estimator xgboost's best error=1.8188,	best estimator xgboost's best error=1.8188
[flaml.automl: 09-18 10:20:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:20:43] {3072} INFO -  at 23.6s,	estimator xgboost's best error=1.8188,	best estimator xgboost's best error=1.8188
[flaml.automl: 09-18 10:20:43] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:20:50] {3072} INFO -  at 30.1s,	estimator xgboost's best error=1.8010,	best estimator xgboost's best error=1.8010
[flaml.automl: 09-18 10:20:50] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:21:02] {3072} INFO -  at 42.2s,	estimator xgboost's best error=1.7417,	best estimator xgboost's best error=1.7417
[flaml.automl: 09-18 10:21:02] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 10:21:08] {3072} INFO -  at 48.7s,	estimator xgboost's best error=1.7417,	best estimator xgboost's best error=1.7417
[flaml.automl: 09-18 10:21:20] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 10:21:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:21:20] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:21:20] {2637} INFO - Time taken to find the best model: 42.20019841194153
[flaml.automl: 09-18 10:21:20] {2648} WARNING - Time taken to find the best model is 70% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43875}
SO2(0)最佳损失：-0.7417483954179278
SO2(0)最好结果：{'pred_time': 9.373478267503822e-06, 'wall_clock_time': 42.20019841194153, 'metric_for_logging': {'pred_time': 9.373478267503822e-06}, 'val_loss': 1.7417483954179278, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 43875}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 43875, 'experiment_tag': 'exp', 'time_total_s': 12.069583177566528}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6644077443074367
SO2(0)的mse=7.681341332022912
SO2(0)的mae=1.7041391054818567
SO2(0)的mar=0.1568307385465445
总共花费的时间为：61.55
十堰市
2428A
2429A
2430A
2431A
3545A
[flaml.automl: 09-18 10:37:05] {2390} INFO - task = regression
[flaml.automl: 09-18 10:37:05] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:37:05] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:37:05] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:37:05] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:37:05] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:37:06] {3025} INFO - Estimated sufficient time budget=64725s. Estimated necessary time budget=65s.
[flaml.automl: 09-18 10:37:06] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.6021,	best estimator xgboost's best error=3.6021
[flaml.automl: 09-18 10:37:06] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:37:09] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.6217,	best estimator xgboost's best error=1.6217
[flaml.automl: 09-18 10:37:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:37:10] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.6217,	best estimator xgboost's best error=1.6217
[flaml.automl: 09-18 10:37:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:37:15] {3072} INFO -  at 9.6s,	estimator xgboost's best error=1.6217,	best estimator xgboost's best error=1.6217
[flaml.automl: 09-18 10:37:15] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:37:16] {3072} INFO -  at 10.7s,	estimator xgboost's best error=0.8053,	best estimator xgboost's best error=0.8053
[flaml.automl: 09-18 10:37:16] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:37:17] {3072} INFO -  at 12.2s,	estimator xgboost's best error=0.6115,	best estimator xgboost's best error=0.6115
[flaml.automl: 09-18 10:37:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:37:19] {3072} INFO -  at 13.8s,	estimator xgboost's best error=0.6115,	best estimator xgboost's best error=0.6115
[flaml.automl: 09-18 10:37:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:37:21] {3072} INFO -  at 16.3s,	estimator xgboost's best error=0.6115,	best estimator xgboost's best error=0.6115
[flaml.automl: 09-18 10:37:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:37:22] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.6115,	best estimator xgboost's best error=0.6115
[flaml.automl: 09-18 10:37:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:37:25] {3072} INFO -  at 19.6s,	estimator xgboost's best error=0.6115,	best estimator xgboost's best error=0.6115
[flaml.automl: 09-18 10:37:25] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:37:26] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.6115,	best estimator xgboost's best error=0.6115
[flaml.automl: 09-18 10:37:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:37:27] {3072} INFO -  at 22.4s,	estimator xgboost's best error=0.6115,	best estimator xgboost's best error=0.6115
[flaml.automl: 09-18 10:37:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:37:34] {3072} INFO -  at 28.9s,	estimator xgboost's best error=0.5357,	best estimator xgboost's best error=0.5357
[flaml.automl: 09-18 10:37:34] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:37:46] {3072} INFO -  at 41.0s,	estimator xgboost's best error=0.5357,	best estimator xgboost's best error=0.5357
[flaml.automl: 09-18 10:37:46] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 10:37:50] {3072} INFO -  at 44.8s,	estimator xgboost's best error=0.5357,	best estimator xgboost's best error=0.5357
[flaml.automl: 09-18 10:37:50] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 10:38:01] {3072} INFO -  at 55.7s,	estimator xgboost's best error=0.5357,	best estimator xgboost's best error=0.5357
[flaml.automl: 09-18 10:38:07] {3335} INFO - retrain xgboost for 6.5s
[flaml.automl: 09-18 10:38:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 10:38:07] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:38:07] {2637} INFO - Time taken to find the best model: 28.931143283843994
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 54314}
SO2(0)最佳损失：0.464292093062065
SO2(0)最好结果：{'pred_time': 7.425543683169791e-06, 'wall_clock_time': 28.931143283843994, 'metric_for_logging': {'pred_time': 7.425543683169791e-06}, 'val_loss': 0.535707906937935, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 14, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932, 'FLAML_sample_size': 54314}, 'config/n_estimators': 8, 'config/max_leaves': 14, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'config/FLAML_sample_size': 54314, 'experiment_tag': 'exp', 'time_total_s': 6.54211163520813}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=14, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8099181213340192
SO2(0)的mse=0.6972260033366567
SO2(0)的mae=0.5377825175403944
SO2(0)的mar=0.09328569390519557
总共花费的时间为：63.03
襄阳市
2432A
2433A
2434A
2435A
3396A
3397A
[flaml.automl: 09-18 10:57:55] {2390} INFO - task = regression
[flaml.automl: 09-18 10:57:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 10:57:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 10:57:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 10:57:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 10:57:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 10:57:57] {3025} INFO - Estimated sufficient time budget=81147s. Estimated necessary time budget=81s.
[flaml.automl: 09-18 10:57:57] {3072} INFO -  at 1.5s,	estimator xgboost's best error=5.9346,	best estimator xgboost's best error=5.9346
[flaml.automl: 09-18 10:57:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 10:57:59] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.7063,	best estimator xgboost's best error=2.7063
[flaml.automl: 09-18 10:57:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 10:58:00] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.7063,	best estimator xgboost's best error=2.7063
[flaml.automl: 09-18 10:58:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 10:58:04] {3072} INFO -  at 8.4s,	estimator xgboost's best error=2.7063,	best estimator xgboost's best error=2.7063
[flaml.automl: 09-18 10:58:04] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 10:58:05] {3072} INFO -  at 9.6s,	estimator xgboost's best error=1.5609,	best estimator xgboost's best error=1.5609
[flaml.automl: 09-18 10:58:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 10:58:06] {3072} INFO -  at 11.1s,	estimator xgboost's best error=1.5609,	best estimator xgboost's best error=1.5609
[flaml.automl: 09-18 10:58:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 10:58:08] {3072} INFO -  at 12.8s,	estimator xgboost's best error=1.2398,	best estimator xgboost's best error=1.2398
[flaml.automl: 09-18 10:58:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 10:58:11] {3072} INFO -  at 15.5s,	estimator xgboost's best error=1.2398,	best estimator xgboost's best error=1.2398
[flaml.automl: 09-18 10:58:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 10:58:13] {3072} INFO -  at 17.5s,	estimator xgboost's best error=1.2398,	best estimator xgboost's best error=1.2398
[flaml.automl: 09-18 10:58:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 10:58:15] {3072} INFO -  at 20.0s,	estimator xgboost's best error=1.2398,	best estimator xgboost's best error=1.2398
[flaml.automl: 09-18 10:58:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 10:58:18] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.2398,	best estimator xgboost's best error=1.2398
[flaml.automl: 09-18 10:58:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 10:58:21] {3072} INFO -  at 25.9s,	estimator xgboost's best error=1.2356,	best estimator xgboost's best error=1.2356
[flaml.automl: 09-18 10:58:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 10:58:23] {3072} INFO -  at 28.0s,	estimator xgboost's best error=1.2356,	best estimator xgboost's best error=1.2356
[flaml.automl: 09-18 10:58:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 10:58:36] {3072} INFO -  at 41.2s,	estimator xgboost's best error=1.1709,	best estimator xgboost's best error=1.1709
[flaml.automl: 09-18 10:58:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 10:58:55] {3072} INFO -  at 59.6s,	estimator xgboost's best error=1.1402,	best estimator xgboost's best error=1.1402
[flaml.automl: 09-18 10:59:30] {3335} INFO - retrain xgboost for 35.4s
[flaml.automl: 09-18 10:59:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 10:59:30] {2636} INFO - fit succeeded
[flaml.automl: 09-18 10:59:30] {2637} INFO - Time taken to find the best model: 59.55707263946533
[flaml.automl: 09-18 10:59:30] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 67194}
SO2(0)最佳损失：-0.14015387364695875
SO2(0)最好结果：{'pred_time': 1.7022754435177752e-05, 'wall_clock_time': 59.55707263946533, 'metric_for_logging': {'pred_time': 1.7022754435177752e-05}, 'val_loss': 1.1401538736469587, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 67194}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 67194, 'experiment_tag': 'exp', 'time_total_s': 18.337504863739014}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7772708809346557
SO2(0)的mse=4.091782719846484
SO2(0)的mae=1.1117574930938254
SO2(0)的mar=0.10859242385974072
总共花费的时间为：96.02
鄂州市
2436A
2437A
[flaml.automl: 09-18 11:06:55] {2390} INFO - task = regression
[flaml.automl: 09-18 11:06:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:06:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:06:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:06:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:06:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:06:57] {3025} INFO - Estimated sufficient time budget=22288s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 11:06:57] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.2259,	best estimator xgboost's best error=5.2259
[flaml.automl: 09-18 11:06:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:07:01] {3072} INFO -  at 6.2s,	estimator xgboost's best error=2.5788,	best estimator xgboost's best error=2.5788
[flaml.automl: 09-18 11:07:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:07:03] {3072} INFO -  at 8.4s,	estimator xgboost's best error=2.5788,	best estimator xgboost's best error=2.5788
[flaml.automl: 09-18 11:07:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:07:21] {3072} INFO -  at 26.2s,	estimator xgboost's best error=2.5788,	best estimator xgboost's best error=2.5788
[flaml.automl: 09-18 11:07:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:07:23] {3072} INFO -  at 28.3s,	estimator xgboost's best error=1.9137,	best estimator xgboost's best error=1.9137
[flaml.automl: 09-18 11:07:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:07:26] {3072} INFO -  at 31.3s,	estimator xgboost's best error=1.9137,	best estimator xgboost's best error=1.9137
[flaml.automl: 09-18 11:07:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:07:29] {3072} INFO -  at 34.5s,	estimator xgboost's best error=1.6537,	best estimator xgboost's best error=1.6537
[flaml.automl: 09-18 11:07:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:07:34] {3072} INFO -  at 39.5s,	estimator xgboost's best error=1.6537,	best estimator xgboost's best error=1.6537
[flaml.automl: 09-18 11:07:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:07:37] {3072} INFO -  at 42.5s,	estimator xgboost's best error=1.6537,	best estimator xgboost's best error=1.6537
[flaml.automl: 09-18 11:07:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:07:43] {3072} INFO -  at 48.1s,	estimator xgboost's best error=1.6537,	best estimator xgboost's best error=1.6537
[flaml.automl: 09-18 11:07:43] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:07:46] {3072} INFO -  at 51.2s,	estimator xgboost's best error=1.6537,	best estimator xgboost's best error=1.6537
[flaml.automl: 09-18 11:07:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:07:48] {3072} INFO -  at 53.4s,	estimator xgboost's best error=1.6537,	best estimator xgboost's best error=1.6537
[flaml.automl: 09-18 11:07:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:07:54] {3072} INFO -  at 59.6s,	estimator xgboost's best error=1.6342,	best estimator xgboost's best error=1.6342
[flaml.automl: 09-18 11:08:06] {3335} INFO - retrain xgboost for 11.8s
[flaml.automl: 09-18 11:08:06] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:08:06] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:08:06] {2637} INFO - Time taken to find the best model: 59.599602460861206
[flaml.automl: 09-18 11:08:06] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}
SO2(0)最佳损失：-0.6341970502572649
SO2(0)最好结果：{'pred_time': 2.8814630168243822e-05, 'wall_clock_time': 59.599602460861206, 'metric_for_logging': {'pred_time': 2.8814630168243822e-05}, 'val_loss': 1.634197050257265, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'experiment_tag': 'exp', 'time_total_s': 6.198936700820923}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5938362255937397
SO2(0)的mse=6.11236877515889
SO2(0)的mae=1.5776093198804397
SO2(0)的mar=0.2147443168792773
总共花费的时间为：71.88
荆门市
2439A
2440A
2441A
3547A
[flaml.automl: 09-18 11:20:51] {2390} INFO - task = regression
[flaml.automl: 09-18 11:20:51] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:20:51] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:20:51] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:20:51] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:20:51] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:20:52] {3025} INFO - Estimated sufficient time budget=53378s. Estimated necessary time budget=53s.
[flaml.automl: 09-18 11:20:52] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.8551,	best estimator xgboost's best error=3.8551
[flaml.automl: 09-18 11:20:52] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:20:54] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.0578,	best estimator xgboost's best error=2.0578
[flaml.automl: 09-18 11:20:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:20:55] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.0578,	best estimator xgboost's best error=2.0578
[flaml.automl: 09-18 11:20:55] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:21:01] {3072} INFO -  at 10.7s,	estimator xgboost's best error=2.0578,	best estimator xgboost's best error=2.0578
[flaml.automl: 09-18 11:21:01] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:21:03] {3072} INFO -  at 12.8s,	estimator xgboost's best error=1.7777,	best estimator xgboost's best error=1.7777
[flaml.automl: 09-18 11:21:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:21:06] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.7777,	best estimator xgboost's best error=1.7777
[flaml.automl: 09-18 11:21:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:21:09] {3072} INFO -  at 18.9s,	estimator xgboost's best error=1.4865,	best estimator xgboost's best error=1.4865
[flaml.automl: 09-18 11:21:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:21:13] {3072} INFO -  at 23.0s,	estimator xgboost's best error=1.4865,	best estimator xgboost's best error=1.4865
[flaml.automl: 09-18 11:21:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:21:16] {3072} INFO -  at 26.0s,	estimator xgboost's best error=1.4865,	best estimator xgboost's best error=1.4865
[flaml.automl: 09-18 11:21:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:21:20] {3072} INFO -  at 29.3s,	estimator xgboost's best error=1.4865,	best estimator xgboost's best error=1.4865
[flaml.automl: 09-18 11:21:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:21:22] {3072} INFO -  at 31.9s,	estimator xgboost's best error=1.4865,	best estimator xgboost's best error=1.4865
[flaml.automl: 09-18 11:21:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:21:26] {3072} INFO -  at 35.1s,	estimator xgboost's best error=1.4856,	best estimator xgboost's best error=1.4856
[flaml.automl: 09-18 11:21:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:21:28] {3072} INFO -  at 37.2s,	estimator xgboost's best error=1.4856,	best estimator xgboost's best error=1.4856
[flaml.automl: 09-18 11:21:28] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 11:21:41] {3072} INFO -  at 50.1s,	estimator xgboost's best error=1.4856,	best estimator xgboost's best error=1.4856
[flaml.automl: 09-18 11:21:41] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 11:21:47] {3072} INFO -  at 56.8s,	estimator xgboost's best error=1.4523,	best estimator xgboost's best error=1.4523
[flaml.automl: 09-18 11:21:54] {3335} INFO - retrain xgboost for 6.7s
[flaml.automl: 09-18 11:21:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:21:54] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:21:54] {2637} INFO - Time taken to find the best model: 56.79598879814148
[flaml.automl: 09-18 11:21:54] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445, 'FLAML_sample_size': 44444}
SO2(0)最佳损失：-0.45229274827378263
SO2(0)最好结果：{'pred_time': 1.7434147403992776e-05, 'wall_clock_time': 56.79598879814148, 'metric_for_logging': {'pred_time': 1.7434147403992776e-05}, 'val_loss': 1.4522927482737826, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445, 'FLAML_sample_size': 44444}, 'config/n_estimators': 14, 'config/max_leaves': 4, 'config/min_child_weight': 2.909605268359626, 'config/learning_rate': 0.1976415421521206, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040166475000702445, 'config/FLAML_sample_size': 44444, 'experiment_tag': 'exp', 'time_total_s': 6.653536081314087}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.36482855984686013
SO2(0)的mse=10.352509483732035
SO2(0)的mae=1.5522215580540566
SO2(0)的mar=0.3260930184778691
总共花费的时间为：64.28
孝感市
2443A
2444A
[flaml.automl: 09-18 11:28:31] {2390} INFO - task = regression
[flaml.automl: 09-18 11:28:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:28:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:28:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:28:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:28:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:28:33] {3025} INFO - Estimated sufficient time budget=22112s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 11:28:33] {3072} INFO -  at 2.3s,	estimator xgboost's best error=3.7151,	best estimator xgboost's best error=3.7151
[flaml.automl: 09-18 11:28:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:28:37] {3072} INFO -  at 6.2s,	estimator xgboost's best error=1.7518,	best estimator xgboost's best error=1.7518
[flaml.automl: 09-18 11:28:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:28:39] {3072} INFO -  at 8.4s,	estimator xgboost's best error=1.7518,	best estimator xgboost's best error=1.7518
[flaml.automl: 09-18 11:28:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:28:57] {3072} INFO -  at 26.2s,	estimator xgboost's best error=1.7518,	best estimator xgboost's best error=1.7518
[flaml.automl: 09-18 11:28:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:28:59] {3072} INFO -  at 28.3s,	estimator xgboost's best error=1.1305,	best estimator xgboost's best error=1.1305
[flaml.automl: 09-18 11:28:59] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:29:02] {3072} INFO -  at 31.3s,	estimator xgboost's best error=1.0499,	best estimator xgboost's best error=1.0499
[flaml.automl: 09-18 11:29:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:29:05] {3072} INFO -  at 34.4s,	estimator xgboost's best error=0.9818,	best estimator xgboost's best error=0.9818
[flaml.automl: 09-18 11:29:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:29:10] {3072} INFO -  at 39.4s,	estimator xgboost's best error=0.9818,	best estimator xgboost's best error=0.9818
[flaml.automl: 09-18 11:29:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:29:13] {3072} INFO -  at 42.9s,	estimator xgboost's best error=0.9818,	best estimator xgboost's best error=0.9818
[flaml.automl: 09-18 11:29:13] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:29:22] {3072} INFO -  at 51.3s,	estimator xgboost's best error=0.9479,	best estimator xgboost's best error=0.9479
[flaml.automl: 09-18 11:29:22] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:29:26] {3072} INFO -  at 55.9s,	estimator xgboost's best error=0.9479,	best estimator xgboost's best error=0.9479
[flaml.automl: 09-18 11:29:34] {3335} INFO - retrain xgboost for 7.4s
[flaml.automl: 09-18 11:29:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 11:29:34] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:29:34] {2637} INFO - Time taken to find the best model: 51.33170294761658
[flaml.automl: 09-18 11:29:34] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：0.05210840145004125
SO2(0)最好结果：{'pred_time': 5.015451120487416e-05, 'wall_clock_time': 51.33170294761658, 'metric_for_logging': {'pred_time': 5.015451120487416e-05}, 'val_loss': 0.9478915985499587, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 8.422323942184448}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6318576452264824
SO2(0)的mse=1.9702805311682703
SO2(0)的mae=0.9693272098980612
SO2(0)的mar=0.16183469619442573
总共花费的时间为：63.63
黄冈市
2929A
3398A
[flaml.automl: 09-18 11:36:29] {2390} INFO - task = regression
[flaml.automl: 09-18 11:36:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:36:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:36:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:36:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:36:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:36:31] {3025} INFO - Estimated sufficient time budget=22459s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 11:36:31] {3072} INFO -  at 2.3s,	estimator xgboost's best error=6.2375,	best estimator xgboost's best error=6.2375
[flaml.automl: 09-18 11:36:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:36:35] {3072} INFO -  at 6.4s,	estimator xgboost's best error=2.8616,	best estimator xgboost's best error=2.8616
[flaml.automl: 09-18 11:36:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:36:37] {3072} INFO -  at 8.6s,	estimator xgboost's best error=2.8616,	best estimator xgboost's best error=2.8616
[flaml.automl: 09-18 11:36:37] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:36:54] {3072} INFO -  at 25.8s,	estimator xgboost's best error=2.8616,	best estimator xgboost's best error=2.8616
[flaml.automl: 09-18 11:36:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:36:57] {3072} INFO -  at 28.0s,	estimator xgboost's best error=1.6534,	best estimator xgboost's best error=1.6534
[flaml.automl: 09-18 11:36:57] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:37:00] {3072} INFO -  at 30.9s,	estimator xgboost's best error=1.5752,	best estimator xgboost's best error=1.5752
[flaml.automl: 09-18 11:37:00] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:37:03] {3072} INFO -  at 34.0s,	estimator xgboost's best error=1.4623,	best estimator xgboost's best error=1.4623
[flaml.automl: 09-18 11:37:03] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:37:07] {3072} INFO -  at 38.9s,	estimator xgboost's best error=1.4623,	best estimator xgboost's best error=1.4623
[flaml.automl: 09-18 11:37:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:37:11] {3072} INFO -  at 41.9s,	estimator xgboost's best error=1.4623,	best estimator xgboost's best error=1.4623
[flaml.automl: 09-18 11:37:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:37:16] {3072} INFO -  at 47.5s,	estimator xgboost's best error=1.4329,	best estimator xgboost's best error=1.4329
[flaml.automl: 09-18 11:37:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:37:19] {3072} INFO -  at 50.5s,	estimator xgboost's best error=1.4329,	best estimator xgboost's best error=1.4329
[flaml.automl: 09-18 11:37:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:37:21] {3072} INFO -  at 52.7s,	estimator xgboost's best error=1.4329,	best estimator xgboost's best error=1.4329
[flaml.automl: 09-18 11:37:21] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:37:28] {3072} INFO -  at 59.4s,	estimator xgboost's best error=1.4329,	best estimator xgboost's best error=1.4329
[flaml.automl: 09-18 11:37:31] {3335} INFO - retrain xgboost for 3.1s
[flaml.automl: 09-18 11:37:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 11:37:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:37:31] {2637} INFO - Time taken to find the best model: 47.53734803199768
[flaml.automl: 09-18 11:37:31] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-0.43285446013173745
SO2(0)最好结果：{'pred_time': 3.391903624049487e-05, 'wall_clock_time': 47.53734803199768, 'metric_for_logging': {'pred_time': 3.391903624049487e-05}, 'val_loss': 1.4328544601317375, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 5.638167381286621}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.5127818039318212
SO2(0)的mse=5.2945878723315944
SO2(0)的mae=1.430725232348994
SO2(0)的mar=0.13325943020589023
总共花费的时间为：62.85
咸宁市
2447A
2448A
2449A
[flaml.automl: 09-18 11:47:02] {2390} INFO - task = regression
[flaml.automl: 09-18 11:47:02] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:47:02] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:47:02] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:47:02] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:47:02] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:47:04] {3025} INFO - Estimated sufficient time budget=21697s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 11:47:04] {3072} INFO -  at 2.3s,	estimator xgboost's best error=3.7805,	best estimator xgboost's best error=3.7805
[flaml.automl: 09-18 11:47:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:47:08] {3072} INFO -  at 6.3s,	estimator xgboost's best error=1.7125,	best estimator xgboost's best error=1.7125
[flaml.automl: 09-18 11:47:08] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:47:10] {3072} INFO -  at 8.5s,	estimator xgboost's best error=1.7125,	best estimator xgboost's best error=1.7125
[flaml.automl: 09-18 11:47:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:47:29] {3072} INFO -  at 27.1s,	estimator xgboost's best error=1.7125,	best estimator xgboost's best error=1.7125
[flaml.automl: 09-18 11:47:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:47:31] {3072} INFO -  at 29.2s,	estimator xgboost's best error=0.8071,	best estimator xgboost's best error=0.8071
[flaml.automl: 09-18 11:47:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:47:35] {3072} INFO -  at 33.5s,	estimator xgboost's best error=0.7685,	best estimator xgboost's best error=0.7685
[flaml.automl: 09-18 11:47:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:47:40] {3072} INFO -  at 38.2s,	estimator xgboost's best error=0.6186,	best estimator xgboost's best error=0.6186
[flaml.automl: 09-18 11:47:40] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:47:47] {3072} INFO -  at 45.5s,	estimator xgboost's best error=0.6186,	best estimator xgboost's best error=0.6186
[flaml.automl: 09-18 11:47:47] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:47:52] {3072} INFO -  at 50.1s,	estimator xgboost's best error=0.6186,	best estimator xgboost's best error=0.6186
[flaml.automl: 09-18 11:47:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:48:00] {3072} INFO -  at 58.5s,	estimator xgboost's best error=0.6186,	best estimator xgboost's best error=0.6186
[flaml.automl: 09-18 11:48:05] {3335} INFO - retrain xgboost for 4.5s
[flaml.automl: 09-18 11:48:05] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 11:48:05] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:48:05] {2637} INFO - Time taken to find the best model: 38.188652992248535
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：0.3813869489340095
SO2(0)最好结果：{'pred_time': 3.752488273758072e-05, 'wall_clock_time': 38.188652992248535, 'metric_for_logging': {'pred_time': 3.752488273758072e-05}, 'val_loss': 0.6186130510659905, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.6683690547943115}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.720449095359929
SO2(0)的mse=0.7925737234724215
SO2(0)的mae=0.625092245211652
SO2(0)的mar=0.10502118843255294
总共花费的时间为：63.71
随州市
2451A
2452A
2453A
[flaml.automl: 09-18 11:58:26] {2390} INFO - task = regression
[flaml.automl: 09-18 11:58:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 11:58:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 11:58:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 11:58:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 11:58:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 11:58:29] {3025} INFO - Estimated sufficient time budget=22477s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 11:58:29] {3072} INFO -  at 2.5s,	estimator xgboost's best error=4.3324,	best estimator xgboost's best error=4.3324
[flaml.automl: 09-18 11:58:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 11:58:33] {3072} INFO -  at 6.5s,	estimator xgboost's best error=1.9631,	best estimator xgboost's best error=1.9631
[flaml.automl: 09-18 11:58:33] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 11:58:35] {3072} INFO -  at 8.8s,	estimator xgboost's best error=1.9631,	best estimator xgboost's best error=1.9631
[flaml.automl: 09-18 11:58:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 11:58:53] {3072} INFO -  at 27.3s,	estimator xgboost's best error=1.9631,	best estimator xgboost's best error=1.9631
[flaml.automl: 09-18 11:58:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 11:58:55] {3072} INFO -  at 29.4s,	estimator xgboost's best error=1.0755,	best estimator xgboost's best error=1.0755
[flaml.automl: 09-18 11:58:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 11:58:58] {3072} INFO -  at 32.3s,	estimator xgboost's best error=0.9187,	best estimator xgboost's best error=0.9187
[flaml.automl: 09-18 11:58:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 11:59:01] {3072} INFO -  at 35.3s,	estimator xgboost's best error=0.8358,	best estimator xgboost's best error=0.8358
[flaml.automl: 09-18 11:59:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 11:59:06] {3072} INFO -  at 40.1s,	estimator xgboost's best error=0.8358,	best estimator xgboost's best error=0.8358
[flaml.automl: 09-18 11:59:06] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 11:59:09] {3072} INFO -  at 43.2s,	estimator xgboost's best error=0.8358,	best estimator xgboost's best error=0.8358
[flaml.automl: 09-18 11:59:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 11:59:15] {3072} INFO -  at 48.7s,	estimator xgboost's best error=0.8262,	best estimator xgboost's best error=0.8262
[flaml.automl: 09-18 11:59:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 11:59:18] {3072} INFO -  at 51.8s,	estimator xgboost's best error=0.8262,	best estimator xgboost's best error=0.8262
[flaml.automl: 09-18 11:59:18] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 11:59:20] {3072} INFO -  at 53.9s,	estimator xgboost's best error=0.8262,	best estimator xgboost's best error=0.8262
[flaml.automl: 09-18 11:59:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 11:59:25] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.7444,	best estimator xgboost's best error=0.7444
[flaml.automl: 09-18 11:59:43] {3335} INFO - retrain xgboost for 17.2s
[flaml.automl: 09-18 11:59:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 11:59:43] {2636} INFO - fit succeeded
[flaml.automl: 09-18 11:59:43] {2637} INFO - Time taken to find the best model: 59.304123401641846
[flaml.automl: 09-18 11:59:43] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
SO2(0)最佳损失：0.2556067406731378
SO2(0)最好结果：{'pred_time': 1.9864291392480652e-05, 'wall_clock_time': 59.304123401641846, 'metric_for_logging': {'pred_time': 1.9864291392480652e-05}, 'val_loss': 0.7443932593268622, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 16, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 15, 'config/max_leaves': 16, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 5.363220453262329}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=16,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.784330631246658
SO2(0)的mse=1.6882880429154317
SO2(0)的mae=0.7760734535651171
SO2(0)的mar=0.11115271249982114
总共花费的时间为：77.15
恩施土家族苗族自治州
2454A
2455A
3549A
[flaml.automl: 09-18 12:09:25] {2390} INFO - task = regression
[flaml.automl: 09-18 12:09:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:09:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:09:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:09:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:09:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:09:29] {3025} INFO - Estimated sufficient time budget=41747s. Estimated necessary time budget=42s.
[flaml.automl: 09-18 12:09:29] {3072} INFO -  at 4.4s,	estimator xgboost's best error=4.0306,	best estimator xgboost's best error=4.0306
[flaml.automl: 09-18 12:09:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:09:37] {3072} INFO -  at 11.7s,	estimator xgboost's best error=1.8279,	best estimator xgboost's best error=1.8279
[flaml.automl: 09-18 12:09:37] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:09:41] {3072} INFO -  at 16.0s,	estimator xgboost's best error=1.8279,	best estimator xgboost's best error=1.8279
[flaml.automl: 09-18 12:09:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:10:03] {3072} INFO -  at 38.4s,	estimator xgboost's best error=1.8279,	best estimator xgboost's best error=1.8279
[flaml.automl: 09-18 12:10:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:10:06] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.8413,	best estimator xgboost's best error=0.8413
[flaml.automl: 09-18 12:10:06] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:10:10] {3072} INFO -  at 44.8s,	estimator xgboost's best error=0.7510,	best estimator xgboost's best error=0.7510
[flaml.automl: 09-18 12:10:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:10:12] {3072} INFO -  at 47.5s,	estimator xgboost's best error=0.6712,	best estimator xgboost's best error=0.6712
[flaml.automl: 09-18 12:10:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:10:17] {3072} INFO -  at 52.1s,	estimator xgboost's best error=0.6712,	best estimator xgboost's best error=0.6712
[flaml.automl: 09-18 12:10:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:10:21] {3072} INFO -  at 55.8s,	estimator xgboost's best error=0.6712,	best estimator xgboost's best error=0.6712
[flaml.automl: 09-18 12:10:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:10:25] {3072} INFO -  at 59.7s,	estimator xgboost's best error=0.6712,	best estimator xgboost's best error=0.6712
[flaml.automl: 09-18 12:10:29] {3335} INFO - retrain xgboost for 4.5s
[flaml.automl: 09-18 12:10:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 12:10:29] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:10:29] {2637} INFO - Time taken to find the best model: 47.547237157821655
[flaml.automl: 09-18 12:10:29] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：0.32876756633811755
SO2(0)最好结果：{'pred_time': 2.152277676332662e-05, 'wall_clock_time': 47.547237157821655, 'metric_for_logging': {'pred_time': 2.152277676332662e-05}, 'val_loss': 0.6712324336618825, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 2.738469123840332}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.30343096000253045
SO2(0)的mse=0.9457451152403323
SO2(0)的mae=0.6770949029853411
SO2(0)的mar=0.10366625752083944
总共花费的时间为：65.11
衡阳市
2456A
2457A
2458A
2459A
2460A
2461A
3399A
[flaml.automl: 09-18 12:32:46] {2390} INFO - task = regression
[flaml.automl: 09-18 12:32:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:32:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:32:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:32:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:32:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:32:49] {3025} INFO - Estimated sufficient time budget=161453s. Estimated necessary time budget=161s.
[flaml.automl: 09-18 12:32:49] {3072} INFO -  at 2.6s,	estimator xgboost's best error=6.0731,	best estimator xgboost's best error=6.0731
[flaml.automl: 09-18 12:32:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:32:52] {3072} INFO -  at 6.0s,	estimator xgboost's best error=3.4097,	best estimator xgboost's best error=3.4097
[flaml.automl: 09-18 12:32:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:32:54] {3072} INFO -  at 8.2s,	estimator xgboost's best error=3.4097,	best estimator xgboost's best error=3.4097
[flaml.automl: 09-18 12:32:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:32:57] {3072} INFO -  at 11.5s,	estimator xgboost's best error=3.4097,	best estimator xgboost's best error=3.4097
[flaml.automl: 09-18 12:32:57] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:33:00] {3072} INFO -  at 13.7s,	estimator xgboost's best error=2.3887,	best estimator xgboost's best error=2.3887
[flaml.automl: 09-18 12:33:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:33:02] {3072} INFO -  at 16.6s,	estimator xgboost's best error=2.3887,	best estimator xgboost's best error=2.3887
[flaml.automl: 09-18 12:33:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:33:05] {3072} INFO -  at 19.3s,	estimator xgboost's best error=2.1253,	best estimator xgboost's best error=2.1253
[flaml.automl: 09-18 12:33:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:33:07] {3072} INFO -  at 21.1s,	estimator xgboost's best error=2.1253,	best estimator xgboost's best error=2.1253
[flaml.automl: 09-18 12:33:07] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:33:09] {3072} INFO -  at 23.3s,	estimator xgboost's best error=2.1253,	best estimator xgboost's best error=2.1253
[flaml.automl: 09-18 12:33:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:33:11] {3072} INFO -  at 25.1s,	estimator xgboost's best error=2.1253,	best estimator xgboost's best error=2.1253
[flaml.automl: 09-18 12:33:11] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:33:13] {3072} INFO -  at 27.0s,	estimator xgboost's best error=2.1253,	best estimator xgboost's best error=2.1253
[flaml.automl: 09-18 12:33:13] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:33:16] {3072} INFO -  at 30.2s,	estimator xgboost's best error=2.0820,	best estimator xgboost's best error=2.0820
[flaml.automl: 09-18 12:33:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:33:19] {3072} INFO -  at 32.7s,	estimator xgboost's best error=2.0820,	best estimator xgboost's best error=2.0820
[flaml.automl: 09-18 12:33:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:33:36] {3072} INFO -  at 50.5s,	estimator xgboost's best error=2.0820,	best estimator xgboost's best error=2.0820
[flaml.automl: 09-18 12:33:36] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 12:33:43] {3072} INFO -  at 57.2s,	estimator xgboost's best error=2.0587,	best estimator xgboost's best error=2.0587
[flaml.automl: 09-18 12:33:49] {3335} INFO - retrain xgboost for 6.4s
[flaml.automl: 09-18 12:33:49] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 12:33:49] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:33:49] {2637} INFO - Time taken to find the best model: 57.21556329727173
[flaml.automl: 09-18 12:33:49] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445, 'FLAML_sample_size': 74601}
SO2(0)最佳损失：-1.0587428171250108
SO2(0)最好结果：{'pred_time': 8.50112234845727e-06, 'wall_clock_time': 57.21556329727173, 'metric_for_logging': {'pred_time': 8.50112234845727e-06}, 'val_loss': 2.058742817125011, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445, 'FLAML_sample_size': 74601}, 'config/n_estimators': 14, 'config/max_leaves': 4, 'config/min_child_weight': 2.909605268359626, 'config/learning_rate': 0.1976415421521206, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040166475000702445, 'config/FLAML_sample_size': 74601, 'experiment_tag': 'exp', 'time_total_s': 6.713503122329712}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.20695210348843018
SO2(0)的mse=13.64494436499975
SO2(0)的mae=1.9669607486751772
SO2(0)的mar=0.1993191222550348
总共花费的时间为：65.06
邵阳市
2462A
2463A
2464A
2465A
2466A
[flaml.automl: 09-18 12:50:17] {2390} INFO - task = regression
[flaml.automl: 09-18 12:50:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 12:50:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 12:50:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 12:50:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 12:50:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 12:50:20] {3025} INFO - Estimated sufficient time budget=173782s. Estimated necessary time budget=174s.
[flaml.automl: 09-18 12:50:20] {3072} INFO -  at 3.8s,	estimator xgboost's best error=6.6033,	best estimator xgboost's best error=6.6033
[flaml.automl: 09-18 12:50:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 12:50:25] {3072} INFO -  at 8.3s,	estimator xgboost's best error=4.0347,	best estimator xgboost's best error=4.0347
[flaml.automl: 09-18 12:50:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 12:50:28] {3072} INFO -  at 11.7s,	estimator xgboost's best error=4.0347,	best estimator xgboost's best error=4.0347
[flaml.automl: 09-18 12:50:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 12:50:33] {3072} INFO -  at 16.1s,	estimator xgboost's best error=4.0347,	best estimator xgboost's best error=4.0347
[flaml.automl: 09-18 12:50:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 12:50:36] {3072} INFO -  at 19.8s,	estimator xgboost's best error=2.4461,	best estimator xgboost's best error=2.4461
[flaml.automl: 09-18 12:50:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 12:50:40] {3072} INFO -  at 23.1s,	estimator xgboost's best error=2.4461,	best estimator xgboost's best error=2.4461
[flaml.automl: 09-18 12:50:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 12:50:42] {3072} INFO -  at 25.4s,	estimator xgboost's best error=2.4461,	best estimator xgboost's best error=2.4461
[flaml.automl: 09-18 12:50:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 12:50:45] {3072} INFO -  at 28.2s,	estimator xgboost's best error=2.4461,	best estimator xgboost's best error=2.4461
[flaml.automl: 09-18 12:50:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 12:50:47] {3072} INFO -  at 30.7s,	estimator xgboost's best error=2.0765,	best estimator xgboost's best error=2.0765
[flaml.automl: 09-18 12:50:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 12:50:49] {3072} INFO -  at 32.8s,	estimator xgboost's best error=2.0765,	best estimator xgboost's best error=2.0765
[flaml.automl: 09-18 12:50:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 12:50:52] {3072} INFO -  at 35.1s,	estimator xgboost's best error=2.0765,	best estimator xgboost's best error=2.0765
[flaml.automl: 09-18 12:50:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 12:50:53] {3072} INFO -  at 36.9s,	estimator xgboost's best error=2.0765,	best estimator xgboost's best error=2.0765
[flaml.automl: 09-18 12:50:53] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 12:50:55] {3072} INFO -  at 38.8s,	estimator xgboost's best error=1.8160,	best estimator xgboost's best error=1.8160
[flaml.automl: 09-18 12:50:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 12:50:57] {3072} INFO -  at 40.4s,	estimator xgboost's best error=1.8160,	best estimator xgboost's best error=1.8160
[flaml.automl: 09-18 12:50:57] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 12:50:59] {3072} INFO -  at 42.2s,	estimator xgboost's best error=1.8160,	best estimator xgboost's best error=1.8160
[flaml.automl: 09-18 12:50:59] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 12:51:00] {3072} INFO -  at 43.4s,	estimator xgboost's best error=1.8160,	best estimator xgboost's best error=1.8160
[flaml.automl: 09-18 12:51:00] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 12:51:01] {3072} INFO -  at 44.7s,	estimator xgboost's best error=1.8160,	best estimator xgboost's best error=1.8160
[flaml.automl: 09-18 12:51:01] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 12:51:09] {3072} INFO -  at 52.6s,	estimator xgboost's best error=1.7075,	best estimator xgboost's best error=1.7075
[flaml.automl: 09-18 12:51:17] {3335} INFO - retrain xgboost for 8.0s
[flaml.automl: 09-18 12:51:17] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 12:51:17] {2636} INFO - fit succeeded
[flaml.automl: 09-18 12:51:17] {2637} INFO - Time taken to find the best model: 52.57710266113281
[flaml.automl: 09-18 12:51:17] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 51868}
SO2(0)最佳损失：-0.7075336802593128
SO2(0)最好结果：{'pred_time': 1.0740541898898166e-05, 'wall_clock_time': 52.57710266113281, 'metric_for_logging': {'pred_time': 1.0740541898898166e-05}, 'val_loss': 1.7075336802593128, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466859, 'FLAML_sample_size': 51868}, 'config/n_estimators': 8, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609401, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466859, 'config/FLAML_sample_size': 51868, 'experiment_tag': 'exp', 'time_total_s': 7.88102650642395}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466859,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.5981913883870376
SO2(0)的mse=7.5886858076075185
SO2(0)的mae=1.6649402944949327
SO2(0)的mar=0.15388521446755024
总共花费的时间为：61.73
益阳市
2467A
2468A
2469A
2470A
2471A
[flaml.automl: 09-18 13:06:40] {2390} INFO - task = regression
[flaml.automl: 09-18 13:06:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:06:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:06:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:06:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:06:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:06:41] {3025} INFO - Estimated sufficient time budget=62148s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 13:06:41] {3072} INFO -  at 1.4s,	estimator xgboost's best error=2.1393,	best estimator xgboost's best error=2.1393
[flaml.automl: 09-18 13:06:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:06:43] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.2817,	best estimator xgboost's best error=1.2817
[flaml.automl: 09-18 13:06:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:06:44] {3072} INFO -  at 4.8s,	estimator xgboost's best error=1.2817,	best estimator xgboost's best error=1.2817
[flaml.automl: 09-18 13:06:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:06:49] {3072} INFO -  at 9.6s,	estimator xgboost's best error=1.2817,	best estimator xgboost's best error=1.2817
[flaml.automl: 09-18 13:06:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:06:50] {3072} INFO -  at 10.7s,	estimator xgboost's best error=1.0891,	best estimator xgboost's best error=1.0891
[flaml.automl: 09-18 13:06:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:06:52] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.9968,	best estimator xgboost's best error=0.9968
[flaml.automl: 09-18 13:06:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:06:53] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.9433,	best estimator xgboost's best error=0.9433
[flaml.automl: 09-18 13:06:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:06:56] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.9433,	best estimator xgboost's best error=0.9433
[flaml.automl: 09-18 13:06:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:06:58] {3072} INFO -  at 18.2s,	estimator xgboost's best error=0.9433,	best estimator xgboost's best error=0.9433
[flaml.automl: 09-18 13:06:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:07:01] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.8933,	best estimator xgboost's best error=0.8933
[flaml.automl: 09-18 13:07:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:07:02] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.8933,	best estimator xgboost's best error=0.8933
[flaml.automl: 09-18 13:07:02] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:07:03] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.8933,	best estimator xgboost's best error=0.8933
[flaml.automl: 09-18 13:07:03] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:07:06] {3072} INFO -  at 26.9s,	estimator xgboost's best error=0.8933,	best estimator xgboost's best error=0.8933
[flaml.automl: 09-18 13:07:06] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:07:09] {3072} INFO -  at 29.7s,	estimator xgboost's best error=0.8933,	best estimator xgboost's best error=0.8933
[flaml.automl: 09-18 13:07:09] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:07:12] {3072} INFO -  at 32.7s,	estimator xgboost's best error=0.8933,	best estimator xgboost's best error=0.8933
[flaml.automl: 09-18 13:07:12] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 13:07:19] {3072} INFO -  at 39.5s,	estimator xgboost's best error=0.8933,	best estimator xgboost's best error=0.8933
[flaml.automl: 09-18 13:07:19] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 13:07:22] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.8933,	best estimator xgboost's best error=0.8933
[flaml.automl: 09-18 13:07:22] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 13:07:25] {3072} INFO -  at 46.0s,	estimator xgboost's best error=0.8933,	best estimator xgboost's best error=0.8933
[flaml.automl: 09-18 13:07:25] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 13:07:39] {3072} INFO -  at 59.6s,	estimator xgboost's best error=0.8933,	best estimator xgboost's best error=0.8933
[flaml.automl: 09-18 13:07:45] {3335} INFO - retrain xgboost for 5.6s
[flaml.automl: 09-18 13:07:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 13:07:45] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:07:45] {2637} INFO - Time taken to find the best model: 21.224267959594727
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：0.10669670243849128
SO2(0)最好结果：{'pred_time': 6.803577981199784e-06, 'wall_clock_time': 21.224267959594727, 'metric_for_logging': {'pred_time': 6.803577981199784e-06}, 'val_loss': 0.8933032975615087, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978, 'FLAML_sample_size': 10000}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 3.00052809715271}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6854460674255827
SO2(0)的mse=2.0495622432073426
SO2(0)的mae=0.8770346300440746
SO2(0)的mar=0.3273296038674455
总共花费的时间为：66.14
郴州市
2472A
2473A
2474A
2475A
2476A
[flaml.automl: 09-18 13:22:56] {2390} INFO - task = regression
[flaml.automl: 09-18 13:22:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:22:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:22:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:22:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:22:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:22:57] {3025} INFO - Estimated sufficient time budget=62203s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 13:22:57] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.6440,	best estimator xgboost's best error=4.6440
[flaml.automl: 09-18 13:22:57] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:22:59] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.3343,	best estimator xgboost's best error=2.3343
[flaml.automl: 09-18 13:22:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:23:01] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.3343,	best estimator xgboost's best error=2.3343
[flaml.automl: 09-18 13:23:01] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:23:05] {3072} INFO -  at 9.4s,	estimator xgboost's best error=2.3343,	best estimator xgboost's best error=2.3343
[flaml.automl: 09-18 13:23:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:23:07] {3072} INFO -  at 10.6s,	estimator xgboost's best error=1.9530,	best estimator xgboost's best error=1.9530
[flaml.automl: 09-18 13:23:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:23:08] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.9530,	best estimator xgboost's best error=1.9530
[flaml.automl: 09-18 13:23:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:23:10] {3072} INFO -  at 13.9s,	estimator xgboost's best error=1.6482,	best estimator xgboost's best error=1.6482
[flaml.automl: 09-18 13:23:10] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:23:13] {3072} INFO -  at 16.6s,	estimator xgboost's best error=1.6482,	best estimator xgboost's best error=1.6482
[flaml.automl: 09-18 13:23:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:23:14] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.6482,	best estimator xgboost's best error=1.6482
[flaml.automl: 09-18 13:23:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:23:17] {3072} INFO -  at 21.2s,	estimator xgboost's best error=1.6482,	best estimator xgboost's best error=1.6482
[flaml.automl: 09-18 13:23:17] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:23:19] {3072} INFO -  at 22.6s,	estimator xgboost's best error=1.6482,	best estimator xgboost's best error=1.6482
[flaml.automl: 09-18 13:23:19] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:23:20] {3072} INFO -  at 24.4s,	estimator xgboost's best error=1.6481,	best estimator xgboost's best error=1.6481
[flaml.automl: 09-18 13:23:20] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:23:21] {3072} INFO -  at 25.5s,	estimator xgboost's best error=1.6481,	best estimator xgboost's best error=1.6481
[flaml.automl: 09-18 13:23:21] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:23:29] {3072} INFO -  at 32.6s,	estimator xgboost's best error=1.6033,	best estimator xgboost's best error=1.6033
[flaml.automl: 09-18 13:23:29] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:23:41] {3072} INFO -  at 45.4s,	estimator xgboost's best error=1.5984,	best estimator xgboost's best error=1.5984
[flaml.automl: 09-18 13:23:41] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 13:23:48] {3072} INFO -  at 52.5s,	estimator xgboost's best error=1.5984,	best estimator xgboost's best error=1.5984
[flaml.automl: 09-18 13:24:01] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 13:24:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:24:01] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:24:01] {2637} INFO - Time taken to find the best model: 45.42850852012634
[flaml.automl: 09-18 13:24:01] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52473}
SO2(0)最佳损失：-0.5984377171649387
SO2(0)最好结果：{'pred_time': 6.960342360123458e-06, 'wall_clock_time': 45.42850852012634, 'metric_for_logging': {'pred_time': 6.960342360123458e-06}, 'val_loss': 1.5984377171649387, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.6186300892591503, 'learning_rate': 0.48254477352294606, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.011522990442748649, 'FLAML_sample_size': 52473}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.6186300892591503, 'config/learning_rate': 0.48254477352294606, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.011522990442748649, 'config/FLAML_sample_size': 52473, 'experiment_tag': 'exp', 'time_total_s': 12.82555627822876}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.48254477352294606,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.6186300892591503, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.011522990442748649, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6612875450786798
SO2(0)的mse=7.976010511118471
SO2(0)的mae=1.619398938963711
SO2(0)的mar=0.1997083935490164
总共花费的时间为：66.12
永州市
2477A
2478A
2479A
2480A
2481A
[flaml.automl: 09-18 13:39:40] {2390} INFO - task = regression
[flaml.automl: 09-18 13:39:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:39:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:39:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:39:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:39:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:39:41] {3025} INFO - Estimated sufficient time budget=62182s. Estimated necessary time budget=62s.
[flaml.automl: 09-18 13:39:41] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.7783,	best estimator xgboost's best error=4.7783
[flaml.automl: 09-18 13:39:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:39:43] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.2848,	best estimator xgboost's best error=2.2848
[flaml.automl: 09-18 13:39:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:39:45] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.2848,	best estimator xgboost's best error=2.2848
[flaml.automl: 09-18 13:39:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:39:49] {3072} INFO -  at 9.6s,	estimator xgboost's best error=2.2848,	best estimator xgboost's best error=2.2848
[flaml.automl: 09-18 13:39:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:39:51] {3072} INFO -  at 10.7s,	estimator xgboost's best error=1.6534,	best estimator xgboost's best error=1.6534
[flaml.automl: 09-18 13:39:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:39:52] {3072} INFO -  at 12.3s,	estimator xgboost's best error=1.5409,	best estimator xgboost's best error=1.5409
[flaml.automl: 09-18 13:39:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:39:54] {3072} INFO -  at 13.9s,	estimator xgboost's best error=1.4787,	best estimator xgboost's best error=1.4787
[flaml.automl: 09-18 13:39:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:39:56] {3072} INFO -  at 16.6s,	estimator xgboost's best error=1.4787,	best estimator xgboost's best error=1.4787
[flaml.automl: 09-18 13:39:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:39:58] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.4787,	best estimator xgboost's best error=1.4787
[flaml.automl: 09-18 13:39:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:40:01] {3072} INFO -  at 21.2s,	estimator xgboost's best error=1.4212,	best estimator xgboost's best error=1.4212
[flaml.automl: 09-18 13:40:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:40:03] {3072} INFO -  at 22.9s,	estimator xgboost's best error=1.4212,	best estimator xgboost's best error=1.4212
[flaml.automl: 09-18 13:40:03] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:40:04] {3072} INFO -  at 24.0s,	estimator xgboost's best error=1.4212,	best estimator xgboost's best error=1.4212
[flaml.automl: 09-18 13:40:04] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:40:07] {3072} INFO -  at 26.9s,	estimator xgboost's best error=1.3997,	best estimator xgboost's best error=1.3997
[flaml.automl: 09-18 13:40:07] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:40:10] {3072} INFO -  at 29.8s,	estimator xgboost's best error=1.3997,	best estimator xgboost's best error=1.3997
[flaml.automl: 09-18 13:40:10] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:40:12] {3072} INFO -  at 32.3s,	estimator xgboost's best error=1.3851,	best estimator xgboost's best error=1.3851
[flaml.automl: 09-18 13:40:12] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 13:40:14] {3072} INFO -  at 34.6s,	estimator xgboost's best error=1.3851,	best estimator xgboost's best error=1.3851
[flaml.automl: 09-18 13:40:14] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 13:40:17] {3072} INFO -  at 36.8s,	estimator xgboost's best error=1.3851,	best estimator xgboost's best error=1.3851
[flaml.automl: 09-18 13:40:17] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 13:40:18] {3072} INFO -  at 38.5s,	estimator xgboost's best error=1.3851,	best estimator xgboost's best error=1.3851
[flaml.automl: 09-18 13:40:18] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 13:40:21] {3072} INFO -  at 41.6s,	estimator xgboost's best error=1.3851,	best estimator xgboost's best error=1.3851
[flaml.automl: 09-18 13:40:21] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 13:40:35] {3072} INFO -  at 55.0s,	estimator xgboost's best error=1.3650,	best estimator xgboost's best error=1.3650
[flaml.automl: 09-18 13:40:48] {3335} INFO - retrain xgboost for 13.4s
[flaml.automl: 09-18 13:40:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7063431279374065, colsample_bynode=1,
             colsample_bytree=0.7891928823597631, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=21, min_child_weight=0.04606527892183358,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.011846412336189025, reg_lambda=1.0350015394258016,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 13:40:48] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:40:48] {2637} INFO - Time taken to find the best model: 54.991522789001465
[flaml.automl: 09-18 13:40:48] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 21, 'min_child_weight': 0.04606527892183358, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7063431279374065, 'colsample_bytree': 0.7891928823597631, 'reg_alpha': 0.011846412336189025, 'reg_lambda': 1.0350015394258016, 'FLAML_sample_size': 52371}
SO2(0)最佳损失：-0.36502982607656453
SO2(0)最好结果：{'pred_time': 1.534589645194319e-05, 'wall_clock_time': 54.991522789001465, 'metric_for_logging': {'pred_time': 1.534589645194319e-05}, 'val_loss': 1.3650298260765645, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 21, 'min_child_weight': 0.04606527892183358, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7063431279374065, 'colsample_bytree': 0.7891928823597631, 'reg_alpha': 0.011846412336189025, 'reg_lambda': 1.0350015394258016, 'FLAML_sample_size': 52371}, 'config/n_estimators': 6, 'config/max_leaves': 21, 'config/min_child_weight': 0.04606527892183358, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7063431279374065, 'config/colsample_bytree': 0.7891928823597631, 'config/reg_alpha': 0.011846412336189025, 'config/reg_lambda': 1.0350015394258016, 'config/FLAML_sample_size': 52371, 'experiment_tag': 'exp', 'time_total_s': 13.399152755737305}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7063431279374065, colsample_bynode=1,
             colsample_bytree=0.7891928823597631, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=21, min_child_weight=0.04606527892183358,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.011846412336189025, reg_lambda=1.0350015394258016,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6570591851443857
SO2(0)的mse=4.480123182585029
SO2(0)的mae=1.3495223850721625
SO2(0)的mar=0.18028332960733875
总共花费的时间为：69.38
怀化市
2482A
2483A
2484A
2485A
2486A
[flaml.automl: 09-18 13:55:34] {2390} INFO - task = regression
[flaml.automl: 09-18 13:55:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 13:55:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 13:55:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 13:55:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 13:55:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 13:55:36] {3025} INFO - Estimated sufficient time budget=63017s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 13:55:36] {3072} INFO -  at 1.5s,	estimator xgboost's best error=3.3867,	best estimator xgboost's best error=3.3867
[flaml.automl: 09-18 13:55:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 13:55:38] {3072} INFO -  at 3.6s,	estimator xgboost's best error=1.6686,	best estimator xgboost's best error=1.6686
[flaml.automl: 09-18 13:55:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 13:55:39] {3072} INFO -  at 4.8s,	estimator xgboost's best error=1.6686,	best estimator xgboost's best error=1.6686
[flaml.automl: 09-18 13:55:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 13:55:44] {3072} INFO -  at 9.6s,	estimator xgboost's best error=1.6686,	best estimator xgboost's best error=1.6686
[flaml.automl: 09-18 13:55:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 13:55:45] {3072} INFO -  at 10.7s,	estimator xgboost's best error=1.2065,	best estimator xgboost's best error=1.2065
[flaml.automl: 09-18 13:55:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 13:55:46] {3072} INFO -  at 12.3s,	estimator xgboost's best error=1.1537,	best estimator xgboost's best error=1.1537
[flaml.automl: 09-18 13:55:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 13:55:48] {3072} INFO -  at 13.9s,	estimator xgboost's best error=1.0895,	best estimator xgboost's best error=1.0895
[flaml.automl: 09-18 13:55:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 13:55:51] {3072} INFO -  at 16.6s,	estimator xgboost's best error=1.0895,	best estimator xgboost's best error=1.0895
[flaml.automl: 09-18 13:55:51] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 13:55:52] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.0544,	best estimator xgboost's best error=1.0544
[flaml.automl: 09-18 13:55:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 13:55:55] {3072} INFO -  at 21.2s,	estimator xgboost's best error=1.0544,	best estimator xgboost's best error=1.0544
[flaml.automl: 09-18 13:55:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 13:55:57] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.0544,	best estimator xgboost's best error=1.0544
[flaml.automl: 09-18 13:55:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 13:55:58] {3072} INFO -  at 23.8s,	estimator xgboost's best error=1.0544,	best estimator xgboost's best error=1.0544
[flaml.automl: 09-18 13:55:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 13:56:01] {3072} INFO -  at 27.1s,	estimator xgboost's best error=1.0329,	best estimator xgboost's best error=1.0329
[flaml.automl: 09-18 13:56:01] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 13:56:04] {3072} INFO -  at 30.1s,	estimator xgboost's best error=0.9595,	best estimator xgboost's best error=0.9595
[flaml.automl: 09-18 13:56:04] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 13:56:07] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.9595,	best estimator xgboost's best error=0.9595
[flaml.automl: 09-18 13:56:07] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 13:56:09] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.9595,	best estimator xgboost's best error=0.9595
[flaml.automl: 09-18 13:56:09] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 13:56:11] {3072} INFO -  at 37.2s,	estimator xgboost's best error=0.9595,	best estimator xgboost's best error=0.9595
[flaml.automl: 09-18 13:56:11] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 13:56:13] {3072} INFO -  at 39.0s,	estimator xgboost's best error=0.9595,	best estimator xgboost's best error=0.9595
[flaml.automl: 09-18 13:56:13] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 13:56:30] {3072} INFO -  at 55.7s,	estimator xgboost's best error=0.9496,	best estimator xgboost's best error=0.9496
[flaml.automl: 09-18 13:56:54] {3335} INFO - retrain xgboost for 23.9s
[flaml.automl: 09-18 13:56:54] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 13:56:54] {2636} INFO - fit succeeded
[flaml.automl: 09-18 13:56:54] {2637} INFO - Time taken to find the best model: 55.70319151878357
[flaml.automl: 09-18 13:56:54] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 52565}
SO2(0)最佳损失：0.05041230859627488
SO2(0)最好结果：{'pred_time': 1.388448412502859e-05, 'wall_clock_time': 55.70319151878357, 'metric_for_logging': {'pred_time': 1.388448412502859e-05}, 'val_loss': 0.9495876914037251, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 52565}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'config/FLAML_sample_size': 52565, 'experiment_tag': 'exp', 'time_total_s': 16.678862810134888}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6253143215529213
SO2(0)的mse=3.5222609849990905
SO2(0)的mae=0.9052356418916735
SO2(0)的mar=0.1716900873434979
总共花费的时间为：80.48
娄底市
2487A
2488A
2489A
2490A
2491A
[flaml.automl: 09-18 14:11:37] {2390} INFO - task = regression
[flaml.automl: 09-18 14:11:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:11:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:11:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:11:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:11:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:11:38] {3025} INFO - Estimated sufficient time budget=63374s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 14:11:38] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.8559,	best estimator xgboost's best error=3.8559
[flaml.automl: 09-18 14:11:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:11:40] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.1079,	best estimator xgboost's best error=2.1079
[flaml.automl: 09-18 14:11:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:11:42] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.1079,	best estimator xgboost's best error=2.1079
[flaml.automl: 09-18 14:11:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:11:46] {3072} INFO -  at 9.5s,	estimator xgboost's best error=2.1079,	best estimator xgboost's best error=2.1079
[flaml.automl: 09-18 14:11:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:11:48] {3072} INFO -  at 10.7s,	estimator xgboost's best error=1.8208,	best estimator xgboost's best error=1.8208
[flaml.automl: 09-18 14:11:48] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:11:49] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.6746,	best estimator xgboost's best error=1.6746
[flaml.automl: 09-18 14:11:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:11:51] {3072} INFO -  at 13.9s,	estimator xgboost's best error=1.6216,	best estimator xgboost's best error=1.6216
[flaml.automl: 09-18 14:11:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:11:54] {3072} INFO -  at 16.6s,	estimator xgboost's best error=1.6216,	best estimator xgboost's best error=1.6216
[flaml.automl: 09-18 14:11:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:11:55] {3072} INFO -  at 18.2s,	estimator xgboost's best error=1.6216,	best estimator xgboost's best error=1.6216
[flaml.automl: 09-18 14:11:55] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:11:58] {3072} INFO -  at 21.2s,	estimator xgboost's best error=1.5194,	best estimator xgboost's best error=1.5194
[flaml.automl: 09-18 14:11:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:12:00] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.5194,	best estimator xgboost's best error=1.5194
[flaml.automl: 09-18 14:12:00] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:12:01] {3072} INFO -  at 24.0s,	estimator xgboost's best error=1.5194,	best estimator xgboost's best error=1.5194
[flaml.automl: 09-18 14:12:01] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:12:04] {3072} INFO -  at 26.9s,	estimator xgboost's best error=1.5194,	best estimator xgboost's best error=1.5194
[flaml.automl: 09-18 14:12:04] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:12:07] {3072} INFO -  at 29.7s,	estimator xgboost's best error=1.5194,	best estimator xgboost's best error=1.5194
[flaml.automl: 09-18 14:12:07] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 14:12:10] {3072} INFO -  at 32.8s,	estimator xgboost's best error=1.4795,	best estimator xgboost's best error=1.4795
[flaml.automl: 09-18 14:12:10] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 14:12:15] {3072} INFO -  at 37.7s,	estimator xgboost's best error=1.4795,	best estimator xgboost's best error=1.4795
[flaml.automl: 09-18 14:12:15] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 14:12:18] {3072} INFO -  at 41.2s,	estimator xgboost's best error=1.4795,	best estimator xgboost's best error=1.4795
[flaml.automl: 09-18 14:12:18] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 14:12:21] {3072} INFO -  at 44.2s,	estimator xgboost's best error=1.4795,	best estimator xgboost's best error=1.4795
[flaml.automl: 09-18 14:12:21] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 14:12:35] {3072} INFO -  at 58.0s,	estimator xgboost's best error=1.4795,	best estimator xgboost's best error=1.4795
[flaml.automl: 09-18 14:12:40] {3335} INFO - retrain xgboost for 5.5s
[flaml.automl: 09-18 14:12:40] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 14:12:40] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:12:40] {2637} INFO - Time taken to find the best model: 32.7596001625061
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978, 'FLAML_sample_size': 52723}
SO2(0)最佳损失：-0.4794896347243176
SO2(0)最好结果：{'pred_time': 7.294045920501412e-06, 'wall_clock_time': 32.7596001625061, 'metric_for_logging': {'pred_time': 7.294045920501412e-06}, 'val_loss': 1.4794896347243176, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978, 'FLAML_sample_size': 52723}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'config/FLAML_sample_size': 52723, 'experiment_tag': 'exp', 'time_total_s': 3.0558605194091797}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6102467423880966
SO2(0)的mse=9.406927435225652
SO2(0)的mae=1.4190283964672763
SO2(0)的mar=0.2539730337023516
总共花费的时间为：64.38
湘西州
2492A
2493A
2494A
[flaml.automl: 09-18 14:21:58] {2390} INFO - task = regression
[flaml.automl: 09-18 14:21:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:21:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:21:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:21:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:21:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:21:59] {3025} INFO - Estimated sufficient time budget=12146s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 14:21:59] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.5508,	best estimator xgboost's best error=3.5508
[flaml.automl: 09-18 14:21:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:22:01] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.6260,	best estimator xgboost's best error=1.6260
[flaml.automl: 09-18 14:22:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:22:02] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.6260,	best estimator xgboost's best error=1.6260
[flaml.automl: 09-18 14:22:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:22:12] {3072} INFO -  at 14.6s,	estimator xgboost's best error=1.6260,	best estimator xgboost's best error=1.6260
[flaml.automl: 09-18 14:22:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:22:14] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.8151,	best estimator xgboost's best error=0.8151
[flaml.automl: 09-18 14:22:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:22:15] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.7632,	best estimator xgboost's best error=0.7632
[flaml.automl: 09-18 14:22:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:22:17] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.6055,	best estimator xgboost's best error=0.6055
[flaml.automl: 09-18 14:22:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:22:20] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.6055,	best estimator xgboost's best error=0.6055
[flaml.automl: 09-18 14:22:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:22:21] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.6055,	best estimator xgboost's best error=0.6055
[flaml.automl: 09-18 14:22:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:22:24] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.6055,	best estimator xgboost's best error=0.6055
[flaml.automl: 09-18 14:22:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:22:26] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.6055,	best estimator xgboost's best error=0.6055
[flaml.automl: 09-18 14:22:26] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:22:27] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.6055,	best estimator xgboost's best error=0.6055
[flaml.automl: 09-18 14:22:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:22:41] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.5355,	best estimator xgboost's best error=0.5355
[flaml.automl: 09-18 14:22:41] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:22:56] {3072} INFO -  at 58.6s,	estimator xgboost's best error=0.5268,	best estimator xgboost's best error=0.5268
[flaml.automl: 09-18 14:23:32] {3335} INFO - retrain xgboost for 35.4s
[flaml.automl: 09-18 14:23:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:23:32] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:23:32] {2637} INFO - Time taken to find the best model: 58.620134115219116
[flaml.automl: 09-18 14:23:32] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767}
SO2(0)最佳损失：0.47323842573096886
SO2(0)最好结果：{'pred_time': 3.551691554907262e-05, 'wall_clock_time': 58.620134115219116, 'metric_for_logging': {'pred_time': 3.551691554907262e-05}, 'val_loss': 0.5267615742690311, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'experiment_tag': 'exp', 'time_total_s': 15.640801429748535}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8528544126432298
SO2(0)的mse=0.7530183555822938
SO2(0)的mae=0.5386143875606484
SO2(0)的mar=0.11214838305423151
总共花费的时间为：94.63
梧州市
2495A
2496A
2497A
2498A
[flaml.automl: 09-18 14:35:52] {2390} INFO - task = regression
[flaml.automl: 09-18 14:35:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:35:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:35:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:35:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:35:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:35:53] {3025} INFO - Estimated sufficient time budget=52549s. Estimated necessary time budget=53s.
[flaml.automl: 09-18 14:35:53] {3072} INFO -  at 1.4s,	estimator xgboost's best error=6.1074,	best estimator xgboost's best error=6.1074
[flaml.automl: 09-18 14:35:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:35:55] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.8933,	best estimator xgboost's best error=2.8933
[flaml.automl: 09-18 14:35:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:35:56] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.8933,	best estimator xgboost's best error=2.8933
[flaml.automl: 09-18 14:35:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:36:02] {3072} INFO -  at 10.5s,	estimator xgboost's best error=2.8933,	best estimator xgboost's best error=2.8933
[flaml.automl: 09-18 14:36:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:36:03] {3072} INFO -  at 11.7s,	estimator xgboost's best error=1.9993,	best estimator xgboost's best error=1.9993
[flaml.automl: 09-18 14:36:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:36:05] {3072} INFO -  at 13.4s,	estimator xgboost's best error=1.9993,	best estimator xgboost's best error=1.9993
[flaml.automl: 09-18 14:36:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:36:08] {3072} INFO -  at 16.2s,	estimator xgboost's best error=1.6526,	best estimator xgboost's best error=1.6526
[flaml.automl: 09-18 14:36:08] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:36:13] {3072} INFO -  at 21.0s,	estimator xgboost's best error=1.6526,	best estimator xgboost's best error=1.6526
[flaml.automl: 09-18 14:36:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:36:15] {3072} INFO -  at 23.7s,	estimator xgboost's best error=1.6526,	best estimator xgboost's best error=1.6526
[flaml.automl: 09-18 14:36:15] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:36:19] {3072} INFO -  at 27.3s,	estimator xgboost's best error=1.6526,	best estimator xgboost's best error=1.6526
[flaml.automl: 09-18 14:36:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:36:21] {3072} INFO -  at 29.6s,	estimator xgboost's best error=1.6526,	best estimator xgboost's best error=1.6526
[flaml.automl: 09-18 14:36:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:36:24] {3072} INFO -  at 32.6s,	estimator xgboost's best error=1.6526,	best estimator xgboost's best error=1.6526
[flaml.automl: 09-18 14:36:24] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:36:26] {3072} INFO -  at 34.4s,	estimator xgboost's best error=1.6526,	best estimator xgboost's best error=1.6526
[flaml.automl: 09-18 14:36:26] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:36:37] {3072} INFO -  at 45.8s,	estimator xgboost's best error=1.6526,	best estimator xgboost's best error=1.6526
[flaml.automl: 09-18 14:36:37] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 14:36:44] {3072} INFO -  at 52.0s,	estimator xgboost's best error=1.6439,	best estimator xgboost's best error=1.6439
[flaml.automl: 09-18 14:36:44] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 14:36:47] {3072} INFO -  at 55.7s,	estimator xgboost's best error=1.6439,	best estimator xgboost's best error=1.6439
[flaml.automl: 09-18 14:36:57] {3335} INFO - retrain xgboost for 9.5s
[flaml.automl: 09-18 14:36:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:36:57] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:36:57] {2637} INFO - Time taken to find the best model: 52.0379536151886
[flaml.automl: 09-18 14:36:57] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445, 'FLAML_sample_size': 43780}
SO2(0)最佳损失：-0.6439416468817307
SO2(0)最好结果：{'pred_time': 1.2719006357065575e-05, 'wall_clock_time': 52.0379536151886, 'metric_for_logging': {'pred_time': 1.2719006357065575e-05}, 'val_loss': 1.6439416468817307, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445, 'FLAML_sample_size': 43780}, 'config/n_estimators': 14, 'config/max_leaves': 4, 'config/min_child_weight': 2.909605268359626, 'config/learning_rate': 0.1976415421521206, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040166475000702445, 'config/FLAML_sample_size': 43780, 'experiment_tag': 'exp', 'time_total_s': 6.268610239028931}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5079773828429557
SO2(0)的mse=8.830734799926162
SO2(0)的mae=1.6106645881412869
SO2(0)的mar=0.1499538396182862
总共花费的时间为：65.96
防城港市
2499A
2500A
2501A
[flaml.automl: 09-18 14:46:06] {2390} INFO - task = regression
[flaml.automl: 09-18 14:46:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:46:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:46:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:46:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:46:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:46:07] {3025} INFO - Estimated sufficient time budget=12275s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 14:46:07] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.3381,	best estimator xgboost's best error=5.3381
[flaml.automl: 09-18 14:46:07] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:46:09] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.4216,	best estimator xgboost's best error=2.4216
[flaml.automl: 09-18 14:46:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:46:11] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.4216,	best estimator xgboost's best error=2.4216
[flaml.automl: 09-18 14:46:11] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:46:21] {3072} INFO -  at 14.8s,	estimator xgboost's best error=2.4216,	best estimator xgboost's best error=2.4216
[flaml.automl: 09-18 14:46:21] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:46:22] {3072} INFO -  at 15.9s,	estimator xgboost's best error=1.2879,	best estimator xgboost's best error=1.2879
[flaml.automl: 09-18 14:46:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:46:23] {3072} INFO -  at 17.5s,	estimator xgboost's best error=1.2127,	best estimator xgboost's best error=1.2127
[flaml.automl: 09-18 14:46:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:46:25] {3072} INFO -  at 19.2s,	estimator xgboost's best error=1.1288,	best estimator xgboost's best error=1.1288
[flaml.automl: 09-18 14:46:25] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:46:28] {3072} INFO -  at 21.9s,	estimator xgboost's best error=1.1288,	best estimator xgboost's best error=1.1288
[flaml.automl: 09-18 14:46:28] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:46:29] {3072} INFO -  at 23.5s,	estimator xgboost's best error=1.1288,	best estimator xgboost's best error=1.1288
[flaml.automl: 09-18 14:46:29] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:46:32] {3072} INFO -  at 26.6s,	estimator xgboost's best error=1.1209,	best estimator xgboost's best error=1.1209
[flaml.automl: 09-18 14:46:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:46:34] {3072} INFO -  at 28.2s,	estimator xgboost's best error=1.1209,	best estimator xgboost's best error=1.1209
[flaml.automl: 09-18 14:46:34] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:46:35] {3072} INFO -  at 29.4s,	estimator xgboost's best error=1.1209,	best estimator xgboost's best error=1.1209
[flaml.automl: 09-18 14:46:35] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:46:49] {3072} INFO -  at 43.0s,	estimator xgboost's best error=1.1209,	best estimator xgboost's best error=1.1209
[flaml.automl: 09-18 14:46:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:46:54] {3072} INFO -  at 48.3s,	estimator xgboost's best error=1.0691,	best estimator xgboost's best error=1.0691
[flaml.automl: 09-18 14:46:54] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 14:46:57] {3072} INFO -  at 51.0s,	estimator xgboost's best error=1.0691,	best estimator xgboost's best error=1.0691
[flaml.automl: 09-18 14:46:57] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 14:47:05] {3072} INFO -  at 59.4s,	estimator xgboost's best error=1.0691,	best estimator xgboost's best error=1.0691
[flaml.automl: 09-18 14:47:10] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-18 14:47:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 14:47:10] {2636} INFO - fit succeeded
[flaml.automl: 09-18 14:47:10] {2637} INFO - Time taken to find the best model: 48.293193340301514
[flaml.automl: 09-18 14:47:10] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 17, 'max_leaves': 5, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}
SO2(0)最佳损失：-0.06913129123833661
SO2(0)最好结果：{'pred_time': 1.1026273008252753e-05, 'wall_clock_time': 48.293193340301514, 'metric_for_logging': {'pred_time': 1.1026273008252753e-05}, 'val_loss': 1.0691312912383366, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 5, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}, 'config/n_estimators': 17, 'config/max_leaves': 5, 'config/min_child_weight': 0.07186500551974488, 'config/learning_rate': 0.18147476059072173, 'config/subsample': 0.7641710216408443, 'config/colsample_bylevel': 0.8883913141026579, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.14043216453256416, 'experiment_tag': 'exp', 'time_total_s': 5.246535062789917}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.3427918808684465
SO2(0)的mse=3.9587560465300164
SO2(0)的mae=1.108045616107694
SO2(0)的mar=0.11244079359615582
总共花费的时间为：65.14
钦州市
2502A
2503A
2504A
3404A
[flaml.automl: 09-18 14:59:10] {2390} INFO - task = regression
[flaml.automl: 09-18 14:59:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 14:59:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 14:59:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 14:59:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 14:59:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 14:59:11] {3025} INFO - Estimated sufficient time budget=52542s. Estimated necessary time budget=53s.
[flaml.automl: 09-18 14:59:11] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.3166,	best estimator xgboost's best error=5.3166
[flaml.automl: 09-18 14:59:11] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 14:59:13] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.4598,	best estimator xgboost's best error=2.4598
[flaml.automl: 09-18 14:59:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 14:59:14] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.4598,	best estimator xgboost's best error=2.4598
[flaml.automl: 09-18 14:59:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 14:59:20] {3072} INFO -  at 10.5s,	estimator xgboost's best error=2.4598,	best estimator xgboost's best error=2.4598
[flaml.automl: 09-18 14:59:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 14:59:21] {3072} INFO -  at 11.6s,	estimator xgboost's best error=1.6596,	best estimator xgboost's best error=1.6596
[flaml.automl: 09-18 14:59:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 14:59:23] {3072} INFO -  at 13.1s,	estimator xgboost's best error=1.4667,	best estimator xgboost's best error=1.4667
[flaml.automl: 09-18 14:59:23] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 14:59:24] {3072} INFO -  at 14.7s,	estimator xgboost's best error=1.4667,	best estimator xgboost's best error=1.4667
[flaml.automl: 09-18 14:59:24] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 14:59:27] {3072} INFO -  at 17.2s,	estimator xgboost's best error=1.4667,	best estimator xgboost's best error=1.4667
[flaml.automl: 09-18 14:59:27] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 14:59:28] {3072} INFO -  at 18.3s,	estimator xgboost's best error=1.4667,	best estimator xgboost's best error=1.4667
[flaml.automl: 09-18 14:59:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 14:59:31] {3072} INFO -  at 21.0s,	estimator xgboost's best error=1.4667,	best estimator xgboost's best error=1.4667
[flaml.automl: 09-18 14:59:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 14:59:32] {3072} INFO -  at 22.6s,	estimator xgboost's best error=1.4667,	best estimator xgboost's best error=1.4667
[flaml.automl: 09-18 14:59:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 14:59:33] {3072} INFO -  at 23.8s,	estimator xgboost's best error=1.4667,	best estimator xgboost's best error=1.4667
[flaml.automl: 09-18 14:59:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 14:59:40] {3072} INFO -  at 30.3s,	estimator xgboost's best error=1.4575,	best estimator xgboost's best error=1.4575
[flaml.automl: 09-18 14:59:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 14:59:52] {3072} INFO -  at 42.4s,	estimator xgboost's best error=1.4400,	best estimator xgboost's best error=1.4400
[flaml.automl: 09-18 14:59:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 14:59:59] {3072} INFO -  at 48.9s,	estimator xgboost's best error=1.4400,	best estimator xgboost's best error=1.4400
[flaml.automl: 09-18 15:00:11] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 15:00:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:00:11] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:00:11] {2637} INFO - Time taken to find the best model: 42.4032826423645
[flaml.automl: 09-18 15:00:11] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 44089}
SO2(0)最佳损失：-0.43997297483601105
SO2(0)最好结果：{'pred_time': 8.36328964716173e-06, 'wall_clock_time': 42.4032826423645, 'metric_for_logging': {'pred_time': 8.36328964716173e-06}, 'val_loss': 1.439972974836011, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 44089}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 44089, 'experiment_tag': 'exp', 'time_total_s': 12.095470905303955}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5439667170104987
SO2(0)的mse=4.9988461938008735
SO2(0)的mae=1.4386879320008015
SO2(0)的mar=0.16184204314319914
总共花费的时间为：61.68
贵港市
2505A
2506A
2507A
2508A
3405A
[flaml.automl: 09-18 15:15:25] {2390} INFO - task = regression
[flaml.automl: 09-18 15:15:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:15:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:15:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:15:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:15:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:15:27] {3025} INFO - Estimated sufficient time budget=62639s. Estimated necessary time budget=63s.
[flaml.automl: 09-18 15:15:27] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.2805,	best estimator xgboost's best error=3.2805
[flaml.automl: 09-18 15:15:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:15:29] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.5769,	best estimator xgboost's best error=1.5769
[flaml.automl: 09-18 15:15:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:15:30] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.5769,	best estimator xgboost's best error=1.5769
[flaml.automl: 09-18 15:15:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:15:35] {3072} INFO -  at 9.5s,	estimator xgboost's best error=1.5769,	best estimator xgboost's best error=1.5769
[flaml.automl: 09-18 15:15:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:15:36] {3072} INFO -  at 10.6s,	estimator xgboost's best error=1.0726,	best estimator xgboost's best error=1.0726
[flaml.automl: 09-18 15:15:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:15:37] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.0726,	best estimator xgboost's best error=1.0726
[flaml.automl: 09-18 15:15:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:15:39] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.8806,	best estimator xgboost's best error=0.8806
[flaml.automl: 09-18 15:15:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:15:42] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.8806,	best estimator xgboost's best error=0.8806
[flaml.automl: 09-18 15:15:42] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:15:43] {3072} INFO -  at 18.2s,	estimator xgboost's best error=0.8806,	best estimator xgboost's best error=0.8806
[flaml.automl: 09-18 15:15:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:15:46] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.8806,	best estimator xgboost's best error=0.8806
[flaml.automl: 09-18 15:15:46] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:15:48] {3072} INFO -  at 22.6s,	estimator xgboost's best error=0.8652,	best estimator xgboost's best error=0.8652
[flaml.automl: 09-18 15:15:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:15:49] {3072} INFO -  at 23.8s,	estimator xgboost's best error=0.8652,	best estimator xgboost's best error=0.8652
[flaml.automl: 09-18 15:15:49] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:15:52] {3072} INFO -  at 26.6s,	estimator xgboost's best error=0.8213,	best estimator xgboost's best error=0.8213
[flaml.automl: 09-18 15:15:52] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:15:55] {3072} INFO -  at 29.6s,	estimator xgboost's best error=0.8118,	best estimator xgboost's best error=0.8118
[flaml.automl: 09-18 15:15:55] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:15:57] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.8118,	best estimator xgboost's best error=0.8118
[flaml.automl: 09-18 15:15:57] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 15:15:59] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.8118,	best estimator xgboost's best error=0.8118
[flaml.automl: 09-18 15:15:59] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 15:16:02] {3072} INFO -  at 36.4s,	estimator xgboost's best error=0.8118,	best estimator xgboost's best error=0.8118
[flaml.automl: 09-18 15:16:02] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 15:16:04] {3072} INFO -  at 38.6s,	estimator xgboost's best error=0.8118,	best estimator xgboost's best error=0.8118
[flaml.automl: 09-18 15:16:04] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 15:16:14] {3072} INFO -  at 49.3s,	estimator xgboost's best error=0.8081,	best estimator xgboost's best error=0.8081
[flaml.automl: 09-18 15:16:25] {3335} INFO - retrain xgboost for 10.7s
[flaml.automl: 09-18 15:16:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:16:25] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:16:25] {2637} INFO - Time taken to find the best model: 49.3047890663147
[flaml.automl: 09-18 15:16:25] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52609}
SO2(0)最佳损失：0.19188143149288472
SO2(0)最好结果：{'pred_time': 6.803861024891152e-06, 'wall_clock_time': 49.3047890663147, 'metric_for_logging': {'pred_time': 6.803861024891152e-06}, 'val_loss': 0.8081185685071153, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 52609}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 52609, 'experiment_tag': 'exp', 'time_total_s': 10.744165897369385}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7371912606080017
SO2(0)的mse=1.8162134356492514
SO2(0)的mae=0.8133623665886853
SO2(0)的mar=0.1684917370028656
总共花费的时间为：60.86
玉林市
2509A
2510A
2511A
3532A
3533A
[flaml.automl: 09-18 15:32:20] {2390} INFO - task = regression
[flaml.automl: 09-18 15:32:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:32:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:32:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:32:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:32:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:32:22] {3025} INFO - Estimated sufficient time budget=60776s. Estimated necessary time budget=61s.
[flaml.automl: 09-18 15:32:22] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.8718,	best estimator xgboost's best error=4.8718
[flaml.automl: 09-18 15:32:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:32:24] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.1289,	best estimator xgboost's best error=3.1289
[flaml.automl: 09-18 15:32:24] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:32:25] {3072} INFO -  at 4.6s,	estimator xgboost's best error=3.1289,	best estimator xgboost's best error=3.1289
[flaml.automl: 09-18 15:32:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:32:30] {3072} INFO -  at 9.4s,	estimator xgboost's best error=3.1289,	best estimator xgboost's best error=3.1289
[flaml.automl: 09-18 15:32:30] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:32:31] {3072} INFO -  at 10.5s,	estimator xgboost's best error=3.1289,	best estimator xgboost's best error=3.1289
[flaml.automl: 09-18 15:32:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:32:33] {3072} INFO -  at 12.4s,	estimator xgboost's best error=3.1289,	best estimator xgboost's best error=3.1289
[flaml.automl: 09-18 15:32:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:32:35] {3072} INFO -  at 14.5s,	estimator xgboost's best error=3.0983,	best estimator xgboost's best error=3.0983
[flaml.automl: 09-18 15:32:35] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:32:38] {3072} INFO -  at 18.0s,	estimator xgboost's best error=3.0983,	best estimator xgboost's best error=3.0983
[flaml.automl: 09-18 15:32:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:32:40] {3072} INFO -  at 20.1s,	estimator xgboost's best error=2.9924,	best estimator xgboost's best error=2.9924
[flaml.automl: 09-18 15:32:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:32:44] {3072} INFO -  at 24.0s,	estimator xgboost's best error=2.7527,	best estimator xgboost's best error=2.7527
[flaml.automl: 09-18 15:32:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:32:46] {3072} INFO -  at 26.1s,	estimator xgboost's best error=2.7527,	best estimator xgboost's best error=2.7527
[flaml.automl: 09-18 15:32:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:32:48] {3072} INFO -  at 27.3s,	estimator xgboost's best error=2.7527,	best estimator xgboost's best error=2.7527
[flaml.automl: 09-18 15:32:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:33:05] {3072} INFO -  at 44.6s,	estimator xgboost's best error=2.7527,	best estimator xgboost's best error=2.7527
[flaml.automl: 09-18 15:33:05] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:33:11] {3072} INFO -  at 51.1s,	estimator xgboost's best error=2.7527,	best estimator xgboost's best error=2.7527
[flaml.automl: 09-18 15:33:11] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 15:33:14] {3072} INFO -  at 53.3s,	estimator xgboost's best error=2.7527,	best estimator xgboost's best error=2.7527
[flaml.automl: 09-18 15:33:14] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 15:33:20] {3072} INFO -  at 59.7s,	estimator xgboost's best error=2.7527,	best estimator xgboost's best error=2.7527
[flaml.automl: 09-18 15:33:24] {3335} INFO - retrain xgboost for 3.8s
[flaml.automl: 09-18 15:33:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7193376949298272, colsample_bynode=1,
             colsample_bytree=0.9042542631072509, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.20265494048848212,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.05093412065388937, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=34.96602211247175,
             scale_pos_weight=1, subsample=0.7952713816212716,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 15:33:24] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:33:24] {2637} INFO - Time taken to find the best model: 23.95849061012268
SO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 7, 'min_child_weight': 0.05093412065388937, 'learning_rate': 0.20265494048848212, 'subsample': 0.7952713816212716, 'colsample_bylevel': 0.7193376949298272, 'colsample_bytree': 0.9042542631072509, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 34.96602211247175, 'FLAML_sample_size': 51918}
SO2(0)最佳损失：-1.7527266785432691
SO2(0)最好结果：{'pred_time': 6.9107499356533505e-06, 'wall_clock_time': 23.95849061012268, 'metric_for_logging': {'pred_time': 6.9107499356533505e-06}, 'val_loss': 2.752726678543269, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 7, 'min_child_weight': 0.05093412065388937, 'learning_rate': 0.20265494048848212, 'subsample': 0.7952713816212716, 'colsample_bylevel': 0.7193376949298272, 'colsample_bytree': 0.9042542631072509, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 34.96602211247175, 'FLAML_sample_size': 51918}, 'config/n_estimators': 9, 'config/max_leaves': 7, 'config/min_child_weight': 0.05093412065388937, 'config/learning_rate': 0.20265494048848212, 'config/subsample': 0.7952713816212716, 'config/colsample_bylevel': 0.7193376949298272, 'config/colsample_bytree': 0.9042542631072509, 'config/reg_alpha': 0.007347191482940116, 'config/reg_lambda': 34.96602211247175, 'config/FLAML_sample_size': 51918, 'experiment_tag': 'exp', 'time_total_s': 3.8535654544830322}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7193376949298272, colsample_bynode=1,
             colsample_bytree=0.9042542631072509, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.20265494048848212,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.05093412065388937, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=34.96602211247175,
             scale_pos_weight=1, subsample=0.7952713816212716,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.12902149794678364
SO2(0)的mse=27.994482638950227
SO2(0)的mae=2.6519466779161354
SO2(0)的mar=0.4209157625956745
总共花费的时间为：64.24
百色市
2512A
2513A
3406A
[flaml.automl: 09-18 15:43:08] {2390} INFO - task = regression
[flaml.automl: 09-18 15:43:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:43:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:43:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:43:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:43:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:43:09] {3025} INFO - Estimated sufficient time budget=12104s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 15:43:09] {3072} INFO -  at 1.3s,	estimator xgboost's best error=6.8394,	best estimator xgboost's best error=6.8394
[flaml.automl: 09-18 15:43:09] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:43:11] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.2644,	best estimator xgboost's best error=3.2644
[flaml.automl: 09-18 15:43:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:43:13] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.2644,	best estimator xgboost's best error=3.2644
[flaml.automl: 09-18 15:43:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:43:23] {3072} INFO -  at 14.7s,	estimator xgboost's best error=3.2644,	best estimator xgboost's best error=3.2644
[flaml.automl: 09-18 15:43:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:43:24] {3072} INFO -  at 15.8s,	estimator xgboost's best error=2.4009,	best estimator xgboost's best error=2.4009
[flaml.automl: 09-18 15:43:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:43:25] {3072} INFO -  at 17.4s,	estimator xgboost's best error=2.4009,	best estimator xgboost's best error=2.4009
[flaml.automl: 09-18 15:43:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:43:27] {3072} INFO -  at 19.1s,	estimator xgboost's best error=2.1811,	best estimator xgboost's best error=2.1811
[flaml.automl: 09-18 15:43:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:43:30] {3072} INFO -  at 21.8s,	estimator xgboost's best error=2.1811,	best estimator xgboost's best error=2.1811
[flaml.automl: 09-18 15:43:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:43:31] {3072} INFO -  at 23.4s,	estimator xgboost's best error=2.1811,	best estimator xgboost's best error=2.1811
[flaml.automl: 09-18 15:43:31] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:43:34] {3072} INFO -  at 26.5s,	estimator xgboost's best error=2.1786,	best estimator xgboost's best error=2.1786
[flaml.automl: 09-18 15:43:34] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:43:36] {3072} INFO -  at 28.2s,	estimator xgboost's best error=2.1786,	best estimator xgboost's best error=2.1786
[flaml.automl: 09-18 15:43:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:43:37] {3072} INFO -  at 29.3s,	estimator xgboost's best error=2.1786,	best estimator xgboost's best error=2.1786
[flaml.automl: 09-18 15:43:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:43:51] {3072} INFO -  at 42.9s,	estimator xgboost's best error=2.1720,	best estimator xgboost's best error=2.1720
[flaml.automl: 09-18 15:43:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:44:07] {3072} INFO -  at 59.4s,	estimator xgboost's best error=2.1632,	best estimator xgboost's best error=2.1632
[flaml.automl: 09-18 15:44:31] {3335} INFO - retrain xgboost for 24.0s
[flaml.automl: 09-18 15:44:31] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.3638133431214387,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.029920148019616434, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.007290129763188211, scale_pos_weight=1,
             subsample=0.840665579419843, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:44:31] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:44:31] {2637} INFO - Time taken to find the best model: 59.44919276237488
[flaml.automl: 09-18 15:44:31] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.029920148019616434, 'learning_rate': 0.3638133431214387, 'subsample': 0.840665579419843, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.007290129763188211}
SO2(0)最佳损失：-1.1632189692615973
SO2(0)最好结果：{'pred_time': 1.1057338375399443e-05, 'wall_clock_time': 59.44919276237488, 'metric_for_logging': {'pred_time': 1.1057338375399443e-05}, 'val_loss': 2.1632189692615973, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.029920148019616434, 'learning_rate': 0.3638133431214387, 'subsample': 0.840665579419843, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9831203893408513, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.007290129763188211}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.029920148019616434, 'config/learning_rate': 0.3638133431214387, 'config/subsample': 0.840665579419843, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9831203893408513, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.007290129763188211, 'experiment_tag': 'exp', 'time_total_s': 16.505760669708252}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9831203893408513, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.3638133431214387,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.029920148019616434, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.007290129763188211, scale_pos_weight=1,
             subsample=0.840665579419843, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6393543894061009
SO2(0)的mse=11.163255495576905
SO2(0)的mae=2.1248626583377344
SO2(0)的mar=0.1967210359960282
总共花费的时间为：83.99
贺州市
2514A
2515A
3534A
[flaml.automl: 09-18 15:53:55] {2390} INFO - task = regression
[flaml.automl: 09-18 15:53:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 15:53:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 15:53:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 15:53:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 15:53:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 15:53:56] {3025} INFO - Estimated sufficient time budget=12220s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 15:53:56] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.0567,	best estimator xgboost's best error=5.0567
[flaml.automl: 09-18 15:53:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 15:53:58] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.3081,	best estimator xgboost's best error=2.3081
[flaml.automl: 09-18 15:53:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 15:53:59] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.3081,	best estimator xgboost's best error=2.3081
[flaml.automl: 09-18 15:53:59] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 15:54:09] {3072} INFO -  at 14.6s,	estimator xgboost's best error=2.3081,	best estimator xgboost's best error=2.3081
[flaml.automl: 09-18 15:54:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 15:54:10] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.2671,	best estimator xgboost's best error=1.2671
[flaml.automl: 09-18 15:54:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 15:54:12] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.2463,	best estimator xgboost's best error=1.2463
[flaml.automl: 09-18 15:54:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 15:54:13] {3072} INFO -  at 19.0s,	estimator xgboost's best error=1.0762,	best estimator xgboost's best error=1.0762
[flaml.automl: 09-18 15:54:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 15:54:16] {3072} INFO -  at 21.7s,	estimator xgboost's best error=1.0762,	best estimator xgboost's best error=1.0762
[flaml.automl: 09-18 15:54:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 15:54:18] {3072} INFO -  at 23.3s,	estimator xgboost's best error=1.0762,	best estimator xgboost's best error=1.0762
[flaml.automl: 09-18 15:54:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 15:54:21] {3072} INFO -  at 26.3s,	estimator xgboost's best error=1.0618,	best estimator xgboost's best error=1.0618
[flaml.automl: 09-18 15:54:21] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 15:54:22] {3072} INFO -  at 27.9s,	estimator xgboost's best error=1.0618,	best estimator xgboost's best error=1.0618
[flaml.automl: 09-18 15:54:22] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 15:54:23] {3072} INFO -  at 29.1s,	estimator xgboost's best error=1.0618,	best estimator xgboost's best error=1.0618
[flaml.automl: 09-18 15:54:23] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 15:54:37] {3072} INFO -  at 42.7s,	estimator xgboost's best error=1.0458,	best estimator xgboost's best error=1.0458
[flaml.automl: 09-18 15:54:37] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 15:54:54] {3072} INFO -  at 59.3s,	estimator xgboost's best error=1.0160,	best estimator xgboost's best error=1.0160
[flaml.automl: 09-18 15:55:32] {3335} INFO - retrain xgboost for 38.8s
[flaml.automl: 09-18 15:55:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 15:55:32] {2636} INFO - fit succeeded
[flaml.automl: 09-18 15:55:32] {2637} INFO - Time taken to find the best model: 59.31496787071228
[flaml.automl: 09-18 15:55:32] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
SO2(0)最佳损失：-0.015975467051825065
SO2(0)最好结果：{'pred_time': 2.4595289235549486e-05, 'wall_clock_time': 59.31496787071228, 'metric_for_logging': {'pred_time': 2.4595289235549486e-05}, 'val_loss': 1.015975467051825, 'training_iteration': 1, 'config': {'n_estimators': 35, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 35, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 16.572643756866455}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=35, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5505927283765295
SO2(0)的mse=2.4427960877948656
SO2(0)的mae=1.01146912334769
SO2(0)的mar=0.12125380110964976
总共花费的时间为：98.70
河池市
2516A
2517A
2518A
[flaml.automl: 09-18 16:05:20] {2390} INFO - task = regression
[flaml.automl: 09-18 16:05:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:05:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:05:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:05:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:05:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:05:21] {3025} INFO - Estimated sufficient time budget=11946s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:05:21] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.6614,	best estimator xgboost's best error=4.6614
[flaml.automl: 09-18 16:05:21] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:05:23] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.0637,	best estimator xgboost's best error=2.0637
[flaml.automl: 09-18 16:05:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:05:24] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.0637,	best estimator xgboost's best error=2.0637
[flaml.automl: 09-18 16:05:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:05:32] {3072} INFO -  at 12.6s,	estimator xgboost's best error=2.0637,	best estimator xgboost's best error=2.0637
[flaml.automl: 09-18 16:05:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:05:34] {3072} INFO -  at 13.7s,	estimator xgboost's best error=0.8058,	best estimator xgboost's best error=0.8058
[flaml.automl: 09-18 16:05:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:05:35] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.5933,	best estimator xgboost's best error=0.5933
[flaml.automl: 09-18 16:05:35] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:05:37] {3072} INFO -  at 17.0s,	estimator xgboost's best error=0.5571,	best estimator xgboost's best error=0.5571
[flaml.automl: 09-18 16:05:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:05:39] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.5571,	best estimator xgboost's best error=0.5571
[flaml.automl: 09-18 16:05:39] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:05:41] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.5038,	best estimator xgboost's best error=0.5038
[flaml.automl: 09-18 16:05:41] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:05:44] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.5038,	best estimator xgboost's best error=0.5038
[flaml.automl: 09-18 16:05:44] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:05:45] {3072} INFO -  at 25.5s,	estimator xgboost's best error=0.5038,	best estimator xgboost's best error=0.5038
[flaml.automl: 09-18 16:05:45] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:05:46] {3072} INFO -  at 26.5s,	estimator xgboost's best error=0.5038,	best estimator xgboost's best error=0.5038
[flaml.automl: 09-18 16:05:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:05:54] {3072} INFO -  at 34.1s,	estimator xgboost's best error=0.5020,	best estimator xgboost's best error=0.5020
[flaml.automl: 09-18 16:05:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:06:15] {3072} INFO -  at 55.3s,	estimator xgboost's best error=0.4937,	best estimator xgboost's best error=0.4937
[flaml.automl: 09-18 16:06:36] {3335} INFO - retrain xgboost for 20.6s
[flaml.automl: 09-18 16:06:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 16:06:36] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:06:36] {2637} INFO - Time taken to find the best model: 55.2986159324646
[flaml.automl: 09-18 16:06:36] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}
SO2(0)最佳损失：0.5062657638469095
SO2(0)最好结果：{'pred_time': 2.3640711938777883e-05, 'wall_clock_time': 55.2986159324646, 'metric_for_logging': {'pred_time': 2.3640711938777883e-05}, 'val_loss': 0.4937342361530906, 'training_iteration': 1, 'config': {'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}, 'config/n_estimators': 30, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'experiment_tag': 'exp', 'time_total_s': 21.233393907546997}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6558733260638467
SO2(0)的mse=0.5927603650045592
SO2(0)的mae=0.4921429360931243
SO2(0)的mar=0.062483872819692365
总共花费的时间为：76.50
来宾市
2519A
2520A
3535A
[flaml.automl: 09-18 16:15:40] {2390} INFO - task = regression
[flaml.automl: 09-18 16:15:40] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:15:40] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:15:40] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:15:40] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:15:40] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:15:41] {3025} INFO - Estimated sufficient time budget=12071s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:15:41] {3072} INFO -  at 1.3s,	estimator xgboost's best error=6.5374,	best estimator xgboost's best error=6.5374
[flaml.automl: 09-18 16:15:41] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:15:43] {3072} INFO -  at 3.4s,	estimator xgboost's best error=3.2515,	best estimator xgboost's best error=3.2515
[flaml.automl: 09-18 16:15:43] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:15:44] {3072} INFO -  at 4.6s,	estimator xgboost's best error=3.2515,	best estimator xgboost's best error=3.2515
[flaml.automl: 09-18 16:15:44] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:15:54] {3072} INFO -  at 14.6s,	estimator xgboost's best error=3.2515,	best estimator xgboost's best error=3.2515
[flaml.automl: 09-18 16:15:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:15:55] {3072} INFO -  at 15.7s,	estimator xgboost's best error=2.4813,	best estimator xgboost's best error=2.4813
[flaml.automl: 09-18 16:15:55] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:15:57] {3072} INFO -  at 17.3s,	estimator xgboost's best error=2.4813,	best estimator xgboost's best error=2.4813
[flaml.automl: 09-18 16:15:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:15:59] {3072} INFO -  at 19.0s,	estimator xgboost's best error=2.3155,	best estimator xgboost's best error=2.3155
[flaml.automl: 09-18 16:15:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:16:01] {3072} INFO -  at 21.7s,	estimator xgboost's best error=2.3155,	best estimator xgboost's best error=2.3155
[flaml.automl: 09-18 16:16:01] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:16:03] {3072} INFO -  at 23.3s,	estimator xgboost's best error=2.3155,	best estimator xgboost's best error=2.3155
[flaml.automl: 09-18 16:16:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:16:06] {3072} INFO -  at 26.4s,	estimator xgboost's best error=2.2906,	best estimator xgboost's best error=2.2906
[flaml.automl: 09-18 16:16:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:16:08] {3072} INFO -  at 28.1s,	estimator xgboost's best error=2.2906,	best estimator xgboost's best error=2.2906
[flaml.automl: 09-18 16:16:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:16:09] {3072} INFO -  at 29.2s,	estimator xgboost's best error=2.2906,	best estimator xgboost's best error=2.2906
[flaml.automl: 09-18 16:16:09] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:16:23] {3072} INFO -  at 42.9s,	estimator xgboost's best error=2.2906,	best estimator xgboost's best error=2.2906
[flaml.automl: 09-18 16:16:23] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:16:28] {3072} INFO -  at 48.2s,	estimator xgboost's best error=2.2806,	best estimator xgboost's best error=2.2806
[flaml.automl: 09-18 16:16:28] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 16:16:30] {3072} INFO -  at 50.8s,	estimator xgboost's best error=2.2806,	best estimator xgboost's best error=2.2806
[flaml.automl: 09-18 16:16:30] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 16:16:39] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.2806,	best estimator xgboost's best error=2.2806
[flaml.automl: 09-18 16:16:48] {3335} INFO - retrain xgboost for 9.3s
[flaml.automl: 09-18 16:16:48] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9463124789373943, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.14901131280540222, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.14072354678420318, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0254117033542491, scale_pos_weight=1,
             subsample=0.8026867063371195, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:16:48] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:16:48] {2637} INFO - Time taken to find the best model: 48.15601086616516
[flaml.automl: 09-18 16:16:48] {2648} WARNING - Time taken to find the best model is 80% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 17, 'max_leaves': 5, 'min_child_weight': 0.14072354678420318, 'learning_rate': 0.14901131280540222, 'subsample': 0.8026867063371195, 'colsample_bylevel': 0.9463124789373943, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0254117033542491}
SO2(0)最佳损失：-1.2805651305891939
SO2(0)最好结果：{'pred_time': 1.1229889142818436e-05, 'wall_clock_time': 48.15601086616516, 'metric_for_logging': {'pred_time': 1.1229889142818436e-05}, 'val_loss': 2.280565130589194, 'training_iteration': 1, 'config': {'n_estimators': 17, 'max_leaves': 5, 'min_child_weight': 0.14072354678420318, 'learning_rate': 0.14901131280540222, 'subsample': 0.8026867063371195, 'colsample_bylevel': 0.9463124789373943, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.0254117033542491}, 'config/n_estimators': 17, 'config/max_leaves': 5, 'config/min_child_weight': 0.14072354678420318, 'config/learning_rate': 0.14901131280540222, 'config/subsample': 0.8026867063371195, 'config/colsample_bylevel': 0.9463124789373943, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.0254117033542491, 'experiment_tag': 'exp', 'time_total_s': 5.240628480911255}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9463124789373943, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.14901131280540222, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.14072354678420318, missing=nan,
             monotone_constraints='()', n_estimators=17, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.0254117033542491, scale_pos_weight=1,
             subsample=0.8026867063371195, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=-0.022431546917810907
SO2(0)的mse=18.33334717062525
SO2(0)的mae=2.2032729718895987
SO2(0)的mar=0.1808350133081012
总共花费的时间为：69.41
崇左市
2521A
2522A
[flaml.automl: 09-18 16:23:09] {2390} INFO - task = regression
[flaml.automl: 09-18 16:23:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:23:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:23:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:23:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:23:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:23:10] {3025} INFO - Estimated sufficient time budget=12014s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:23:10] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.6252,	best estimator xgboost's best error=4.6252
[flaml.automl: 09-18 16:23:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:23:12] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.1400,	best estimator xgboost's best error=2.1400
[flaml.automl: 09-18 16:23:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:23:13] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.1400,	best estimator xgboost's best error=2.1400
[flaml.automl: 09-18 16:23:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:23:23] {3072} INFO -  at 14.1s,	estimator xgboost's best error=2.1400,	best estimator xgboost's best error=2.1400
[flaml.automl: 09-18 16:23:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:23:24] {3072} INFO -  at 15.2s,	estimator xgboost's best error=1.2852,	best estimator xgboost's best error=1.2852
[flaml.automl: 09-18 16:23:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:23:26] {3072} INFO -  at 16.8s,	estimator xgboost's best error=1.2852,	best estimator xgboost's best error=1.2852
[flaml.automl: 09-18 16:23:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:23:27] {3072} INFO -  at 18.5s,	estimator xgboost's best error=1.0666,	best estimator xgboost's best error=1.0666
[flaml.automl: 09-18 16:23:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:23:32] {3072} INFO -  at 22.9s,	estimator xgboost's best error=1.0666,	best estimator xgboost's best error=1.0666
[flaml.automl: 09-18 16:23:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:23:34] {3072} INFO -  at 25.6s,	estimator xgboost's best error=1.0666,	best estimator xgboost's best error=1.0666
[flaml.automl: 09-18 16:23:34] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:23:40] {3072} INFO -  at 31.0s,	estimator xgboost's best error=1.0666,	best estimator xgboost's best error=1.0666
[flaml.automl: 09-18 16:23:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:23:43] {3072} INFO -  at 34.1s,	estimator xgboost's best error=1.0666,	best estimator xgboost's best error=1.0666
[flaml.automl: 09-18 16:23:43] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:23:45] {3072} INFO -  at 36.0s,	estimator xgboost's best error=1.0666,	best estimator xgboost's best error=1.0666
[flaml.automl: 09-18 16:23:45] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:23:55] {3072} INFO -  at 46.5s,	estimator xgboost's best error=1.0666,	best estimator xgboost's best error=1.0666
[flaml.automl: 09-18 16:23:55] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:24:01] {3072} INFO -  at 52.7s,	estimator xgboost's best error=1.0356,	best estimator xgboost's best error=1.0356
[flaml.automl: 09-18 16:24:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 16:24:05] {3072} INFO -  at 55.9s,	estimator xgboost's best error=1.0356,	best estimator xgboost's best error=1.0356
[flaml.automl: 09-18 16:24:11] {3335} INFO - retrain xgboost for 6.4s
[flaml.automl: 09-18 16:24:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:24:11] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:24:11] {2637} INFO - Time taken to find the best model: 52.702805519104004
[flaml.automl: 09-18 16:24:11] {2648} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445}
SO2(0)最佳损失：-0.03559931518118442
SO2(0)最好结果：{'pred_time': 2.4409708001887243e-05, 'wall_clock_time': 52.702805519104004, 'metric_for_logging': {'pred_time': 2.4409708001887243e-05}, 'val_loss': 1.0355993151811844, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445}, 'config/n_estimators': 14, 'config/max_leaves': 4, 'config/min_child_weight': 2.909605268359626, 'config/learning_rate': 0.1976415421521206, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040166475000702445, 'experiment_tag': 'exp', 'time_total_s': 6.173386096954346}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.40347514708281074
SO2(0)的mse=4.765824217771237
SO2(0)的mae=1.0779629980621845
SO2(0)的mar=0.12800682198562086
总共花费的时间为：62.79
广元市
2523A
2524A
3617A
[flaml.automl: 09-18 16:33:48] {2390} INFO - task = regression
[flaml.automl: 09-18 16:33:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:33:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:33:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:33:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:33:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:33:49] {3025} INFO - Estimated sufficient time budget=12015s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 16:33:49] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.7907,	best estimator xgboost's best error=3.7907
[flaml.automl: 09-18 16:33:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:33:51] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.0423,	best estimator xgboost's best error=2.0423
[flaml.automl: 09-18 16:33:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:33:53] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.0423,	best estimator xgboost's best error=2.0423
[flaml.automl: 09-18 16:33:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:34:03] {3072} INFO -  at 14.6s,	estimator xgboost's best error=2.0423,	best estimator xgboost's best error=2.0423
[flaml.automl: 09-18 16:34:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:34:04] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.6742,	best estimator xgboost's best error=1.6742
[flaml.automl: 09-18 16:34:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:34:05] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.6354,	best estimator xgboost's best error=1.6354
[flaml.automl: 09-18 16:34:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:34:07] {3072} INFO -  at 19.0s,	estimator xgboost's best error=1.5485,	best estimator xgboost's best error=1.5485
[flaml.automl: 09-18 16:34:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:34:10] {3072} INFO -  at 21.7s,	estimator xgboost's best error=1.5485,	best estimator xgboost's best error=1.5485
[flaml.automl: 09-18 16:34:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:34:11] {3072} INFO -  at 23.4s,	estimator xgboost's best error=1.5485,	best estimator xgboost's best error=1.5485
[flaml.automl: 09-18 16:34:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:34:14] {3072} INFO -  at 26.4s,	estimator xgboost's best error=1.4391,	best estimator xgboost's best error=1.4391
[flaml.automl: 09-18 16:34:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:34:16] {3072} INFO -  at 28.1s,	estimator xgboost's best error=1.4391,	best estimator xgboost's best error=1.4391
[flaml.automl: 09-18 16:34:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:34:18] {3072} INFO -  at 29.7s,	estimator xgboost's best error=1.4391,	best estimator xgboost's best error=1.4391
[flaml.automl: 09-18 16:34:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:34:39] {3072} INFO -  at 50.7s,	estimator xgboost's best error=1.4391,	best estimator xgboost's best error=1.4391
[flaml.automl: 09-18 16:34:39] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:34:47] {3072} INFO -  at 59.4s,	estimator xgboost's best error=1.3893,	best estimator xgboost's best error=1.3893
[flaml.automl: 09-18 16:34:56] {3335} INFO - retrain xgboost for 8.5s
[flaml.automl: 09-18 16:34:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 16:34:56] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:34:56] {2637} INFO - Time taken to find the best model: 59.44406270980835
[flaml.automl: 09-18 16:34:56] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 16, 'max_leaves': 5, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}
SO2(0)最佳损失：-0.38925639078608176
SO2(0)最好结果：{'pred_time': 2.382492107120368e-05, 'wall_clock_time': 59.44406270980835, 'metric_for_logging': {'pred_time': 2.382492107120368e-05}, 'val_loss': 1.3892563907860818, 'training_iteration': 1, 'config': {'n_estimators': 16, 'max_leaves': 5, 'min_child_weight': 0.07186500551974488, 'learning_rate': 0.18147476059072173, 'subsample': 0.7641710216408443, 'colsample_bylevel': 0.8883913141026579, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.14043216453256416}, 'config/n_estimators': 16, 'config/max_leaves': 5, 'config/min_child_weight': 0.07186500551974488, 'config/learning_rate': 0.18147476059072173, 'config/subsample': 0.7641710216408443, 'config/colsample_bylevel': 0.8883913141026579, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.14043216453256416, 'experiment_tag': 'exp', 'time_total_s': 8.71246337890625}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8883913141026579, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.18147476059072173, max_delta_step=0, max_depth=0,
             max_leaves=5, min_child_weight=0.07186500551974488, missing=nan,
             monotone_constraints='()', n_estimators=16, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.14043216453256416, scale_pos_weight=1,
             subsample=0.7641710216408443, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.47812412297084195
SO2(0)的mse=8.177349365517737
SO2(0)的mae=1.3979764958598162
SO2(0)的mar=0.28095584942623214
总共花费的时间为：68.60
遂宁市
2527A
2528A
2529A
2530A
[flaml.automl: 09-18 16:47:35] {2390} INFO - task = regression
[flaml.automl: 09-18 16:47:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 16:47:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 16:47:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 16:47:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 16:47:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 16:47:37] {3025} INFO - Estimated sufficient time budget=84829s. Estimated necessary time budget=85s.
[flaml.automl: 09-18 16:47:37] {3072} INFO -  at 2.2s,	estimator xgboost's best error=4.8454,	best estimator xgboost's best error=4.8454
[flaml.automl: 09-18 16:47:37] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 16:47:41] {3072} INFO -  at 6.1s,	estimator xgboost's best error=2.1946,	best estimator xgboost's best error=2.1946
[flaml.automl: 09-18 16:47:41] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 16:47:43] {3072} INFO -  at 8.2s,	estimator xgboost's best error=2.1946,	best estimator xgboost's best error=2.1946
[flaml.automl: 09-18 16:47:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 16:47:49] {3072} INFO -  at 13.8s,	estimator xgboost's best error=2.1946,	best estimator xgboost's best error=2.1946
[flaml.automl: 09-18 16:47:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 16:47:50] {3072} INFO -  at 15.5s,	estimator xgboost's best error=1.2853,	best estimator xgboost's best error=1.2853
[flaml.automl: 09-18 16:47:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 16:47:53] {3072} INFO -  at 18.0s,	estimator xgboost's best error=1.1630,	best estimator xgboost's best error=1.1630
[flaml.automl: 09-18 16:47:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 16:47:55] {3072} INFO -  at 20.3s,	estimator xgboost's best error=1.1002,	best estimator xgboost's best error=1.1002
[flaml.automl: 09-18 16:47:55] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 16:47:59] {3072} INFO -  at 24.5s,	estimator xgboost's best error=1.1002,	best estimator xgboost's best error=1.1002
[flaml.automl: 09-18 16:47:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 16:48:01] {3072} INFO -  at 26.1s,	estimator xgboost's best error=1.1002,	best estimator xgboost's best error=1.1002
[flaml.automl: 09-18 16:48:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 16:48:04] {3072} INFO -  at 29.1s,	estimator xgboost's best error=1.0540,	best estimator xgboost's best error=1.0540
[flaml.automl: 09-18 16:48:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 16:48:06] {3072} INFO -  at 30.7s,	estimator xgboost's best error=1.0540,	best estimator xgboost's best error=1.0540
[flaml.automl: 09-18 16:48:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 16:48:07] {3072} INFO -  at 31.9s,	estimator xgboost's best error=1.0540,	best estimator xgboost's best error=1.0540
[flaml.automl: 09-18 16:48:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 16:48:10] {3072} INFO -  at 34.8s,	estimator xgboost's best error=1.0540,	best estimator xgboost's best error=1.0540
[flaml.automl: 09-18 16:48:10] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 16:48:13] {3072} INFO -  at 37.6s,	estimator xgboost's best error=1.0540,	best estimator xgboost's best error=1.0540
[flaml.automl: 09-18 16:48:13] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 16:48:16] {3072} INFO -  at 40.7s,	estimator xgboost's best error=1.0441,	best estimator xgboost's best error=1.0441
[flaml.automl: 09-18 16:48:16] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 16:48:21] {3072} INFO -  at 45.6s,	estimator xgboost's best error=1.0441,	best estimator xgboost's best error=1.0441
[flaml.automl: 09-18 16:48:21] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 16:48:22] {3072} INFO -  at 47.5s,	estimator xgboost's best error=1.0408,	best estimator xgboost's best error=1.0408
[flaml.automl: 09-18 16:48:22] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 16:48:24] {3072} INFO -  at 49.2s,	estimator xgboost's best error=1.0408,	best estimator xgboost's best error=1.0408
[flaml.automl: 09-18 16:48:24] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 16:48:28] {3072} INFO -  at 53.5s,	estimator xgboost's best error=1.0408,	best estimator xgboost's best error=1.0408
[flaml.automl: 09-18 16:48:28] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 16:48:30] {3072} INFO -  at 54.6s,	estimator xgboost's best error=1.0408,	best estimator xgboost's best error=1.0408
[flaml.automl: 09-18 16:48:30] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 16:48:35] {3072} INFO -  at 59.9s,	estimator xgboost's best error=1.0408,	best estimator xgboost's best error=1.0408
[flaml.automl: 09-18 16:48:37] {3335} INFO - retrain xgboost for 1.9s
[flaml.automl: 09-18 16:48:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 16:48:37] {2636} INFO - fit succeeded
[flaml.automl: 09-18 16:48:37] {2637} INFO - Time taken to find the best model: 47.51203894615173
[flaml.automl: 09-18 16:48:37] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 42048}
SO2(0)最佳损失：-0.04084367168401215
SO2(0)最好结果：{'pred_time': 8.664293409592841e-06, 'wall_clock_time': 47.51203894615173, 'metric_for_logging': {'pred_time': 8.664293409592841e-06}, 'val_loss': 1.0408436716840122, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 4, 'min_child_weight': 0.031257867156220746, 'learning_rate': 0.3417584818146937, 'subsample': 0.8188658116452241, 'colsample_bylevel': 0.9718368347363348, 'colsample_bytree': 1.0, 'reg_alpha': 0.001562580531679857, 'reg_lambda': 0.36612402315272025, 'FLAML_sample_size': 42048}, 'config/n_estimators': 7, 'config/max_leaves': 4, 'config/min_child_weight': 0.031257867156220746, 'config/learning_rate': 0.3417584818146937, 'config/subsample': 0.8188658116452241, 'config/colsample_bylevel': 0.9718368347363348, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.001562580531679857, 'config/reg_lambda': 0.36612402315272025, 'config/FLAML_sample_size': 42048, 'experiment_tag': 'exp', 'time_total_s': 1.8997926712036133}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9718368347363348, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.3417584818146937, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=0.031257867156220746, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.001562580531679857, reg_lambda=0.36612402315272025,
             scale_pos_weight=1, subsample=0.8188658116452241,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.3466534026922846
SO2(0)的mse=3.5010925494140888
SO2(0)的mae=1.0530573203126528
SO2(0)的mar=0.11778686883859181
总共花费的时间为：62.56
内江市
2531A
2532A
2533A
2534A
[flaml.automl: 09-18 17:00:52] {2390} INFO - task = regression
[flaml.automl: 09-18 17:00:52] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:00:52] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:00:52] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:00:52] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:00:52] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:00:53] {3025} INFO - Estimated sufficient time budget=52547s. Estimated necessary time budget=53s.
[flaml.automl: 09-18 17:00:53] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.4789,	best estimator xgboost's best error=5.4789
[flaml.automl: 09-18 17:00:53] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:00:55] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.5162,	best estimator xgboost's best error=2.5162
[flaml.automl: 09-18 17:00:55] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:00:56] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.5162,	best estimator xgboost's best error=2.5162
[flaml.automl: 09-18 17:00:56] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:01:02] {3072} INFO -  at 10.5s,	estimator xgboost's best error=2.5162,	best estimator xgboost's best error=2.5162
[flaml.automl: 09-18 17:01:02] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:01:03] {3072} INFO -  at 11.6s,	estimator xgboost's best error=1.4811,	best estimator xgboost's best error=1.4811
[flaml.automl: 09-18 17:01:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:01:05] {3072} INFO -  at 13.2s,	estimator xgboost's best error=1.2700,	best estimator xgboost's best error=1.2700
[flaml.automl: 09-18 17:01:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:01:06] {3072} INFO -  at 14.8s,	estimator xgboost's best error=1.1681,	best estimator xgboost's best error=1.1681
[flaml.automl: 09-18 17:01:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:01:09] {3072} INFO -  at 17.4s,	estimator xgboost's best error=1.1681,	best estimator xgboost's best error=1.1681
[flaml.automl: 09-18 17:01:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:01:11] {3072} INFO -  at 19.1s,	estimator xgboost's best error=1.1617,	best estimator xgboost's best error=1.1617
[flaml.automl: 09-18 17:01:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:01:14] {3072} INFO -  at 22.1s,	estimator xgboost's best error=1.1617,	best estimator xgboost's best error=1.1617
[flaml.automl: 09-18 17:01:14] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:01:15] {3072} INFO -  at 23.5s,	estimator xgboost's best error=1.1617,	best estimator xgboost's best error=1.1617
[flaml.automl: 09-18 17:01:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:01:16] {3072} INFO -  at 24.7s,	estimator xgboost's best error=1.1617,	best estimator xgboost's best error=1.1617
[flaml.automl: 09-18 17:01:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:01:20] {3072} INFO -  at 28.5s,	estimator xgboost's best error=1.0072,	best estimator xgboost's best error=1.0072
[flaml.automl: 09-18 17:01:20] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:01:24] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.9634,	best estimator xgboost's best error=0.9634
[flaml.automl: 09-18 17:01:24] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 17:01:26] {3072} INFO -  at 34.6s,	estimator xgboost's best error=0.9634,	best estimator xgboost's best error=0.9634
[flaml.automl: 09-18 17:01:26] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 17:01:29] {3072} INFO -  at 36.8s,	estimator xgboost's best error=0.9634,	best estimator xgboost's best error=0.9634
[flaml.automl: 09-18 17:01:29] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 17:01:31] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.9634,	best estimator xgboost's best error=0.9634
[flaml.automl: 09-18 17:01:31] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 17:01:33] {3072} INFO -  at 41.2s,	estimator xgboost's best error=0.9634,	best estimator xgboost's best error=0.9634
[flaml.automl: 09-18 17:01:33] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 17:01:46] {3072} INFO -  at 53.9s,	estimator xgboost's best error=0.9387,	best estimator xgboost's best error=0.9387
[flaml.automl: 09-18 17:01:58] {3335} INFO - retrain xgboost for 12.8s
[flaml.automl: 09-18 17:01:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 17:01:58] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:01:58] {2637} INFO - Time taken to find the best model: 53.94108438491821
[flaml.automl: 09-18 17:01:58] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 44221}
SO2(0)最佳损失：0.061331707746166475
SO2(0)最好结果：{'pred_time': 8.17954273402424e-06, 'wall_clock_time': 53.94108438491821, 'metric_for_logging': {'pred_time': 8.17954273402424e-06}, 'val_loss': 0.9386682922538335, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537, 'FLAML_sample_size': 44221}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'config/FLAML_sample_size': 44221, 'experiment_tag': 'exp', 'time_total_s': 12.690960884094238}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8375586191269078
SO2(0)的mse=2.361468129042076
SO2(0)的mae=0.946329472113005
SO2(0)的mar=0.11879712087017652
总共花费的时间为：67.51
眉山市
2539A
2540A
3137A
3148A
[flaml.automl: 09-18 17:14:00] {2390} INFO - task = regression
[flaml.automl: 09-18 17:14:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:14:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:14:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:14:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:14:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:14:02] {3025} INFO - Estimated sufficient time budget=94823s. Estimated necessary time budget=95s.
[flaml.automl: 09-18 17:14:02] {3072} INFO -  at 2.4s,	estimator xgboost's best error=5.5051,	best estimator xgboost's best error=5.5051
[flaml.automl: 09-18 17:14:02] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:14:06] {3072} INFO -  at 5.8s,	estimator xgboost's best error=2.4852,	best estimator xgboost's best error=2.4852
[flaml.automl: 09-18 17:14:06] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:14:07] {3072} INFO -  at 7.7s,	estimator xgboost's best error=2.4852,	best estimator xgboost's best error=2.4852
[flaml.automl: 09-18 17:14:07] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:14:13] {3072} INFO -  at 13.5s,	estimator xgboost's best error=2.4852,	best estimator xgboost's best error=2.4852
[flaml.automl: 09-18 17:14:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:14:15] {3072} INFO -  at 15.2s,	estimator xgboost's best error=1.2645,	best estimator xgboost's best error=1.2645
[flaml.automl: 09-18 17:14:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:14:17] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.9891,	best estimator xgboost's best error=0.9891
[flaml.automl: 09-18 17:14:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:14:20] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.9891,	best estimator xgboost's best error=0.9891
[flaml.automl: 09-18 17:14:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:14:24] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.9891,	best estimator xgboost's best error=0.9891
[flaml.automl: 09-18 17:14:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:14:25] {3072} INFO -  at 25.7s,	estimator xgboost's best error=0.9891,	best estimator xgboost's best error=0.9891
[flaml.automl: 09-18 17:14:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:14:29] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.9891,	best estimator xgboost's best error=0.9891
[flaml.automl: 09-18 17:14:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:14:31] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.9891,	best estimator xgboost's best error=0.9891
[flaml.automl: 09-18 17:14:31] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:14:33] {3072} INFO -  at 33.3s,	estimator xgboost's best error=0.9891,	best estimator xgboost's best error=0.9891
[flaml.automl: 09-18 17:14:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:14:44] {3072} INFO -  at 44.0s,	estimator xgboost's best error=0.9203,	best estimator xgboost's best error=0.9203
[flaml.automl: 09-18 17:14:44] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:14:59] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.8758,	best estimator xgboost's best error=0.8758
[flaml.automl: 09-18 17:15:11] {3335} INFO - retrain xgboost for 12.1s
[flaml.automl: 09-18 17:15:11] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:15:11] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:15:11] {2637} INFO - Time taken to find the best model: 58.98108673095703
[flaml.automl: 09-18 17:15:11] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42265}
SO2(0)最佳损失：0.12417515932764933
SO2(0)最好结果：{'pred_time': 8.455306539034015e-06, 'wall_clock_time': 58.98108673095703, 'metric_for_logging': {'pred_time': 8.455306539034015e-06}, 'val_loss': 0.8758248406723507, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 42265}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 42265, 'experiment_tag': 'exp', 'time_total_s': 15.022650241851807}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7731582303973876
SO2(0)的mse=1.7393967310626863
SO2(0)的mae=0.8160987009511503
SO2(0)的mar=0.09381782723169906
总共花费的时间为：71.86
广安市
2543A
2544A
2545A
2902A
[flaml.automl: 09-18 17:27:45] {2390} INFO - task = regression
[flaml.automl: 09-18 17:27:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:27:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:27:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:27:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:27:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:27:47] {3025} INFO - Estimated sufficient time budget=91676s. Estimated necessary time budget=92s.
[flaml.automl: 09-18 17:27:47] {3072} INFO -  at 2.5s,	estimator xgboost's best error=3.9494,	best estimator xgboost's best error=3.9494
[flaml.automl: 09-18 17:27:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:27:51] {3072} INFO -  at 6.4s,	estimator xgboost's best error=1.8712,	best estimator xgboost's best error=1.8712
[flaml.automl: 09-18 17:27:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:27:53] {3072} INFO -  at 8.7s,	estimator xgboost's best error=1.8712,	best estimator xgboost's best error=1.8712
[flaml.automl: 09-18 17:27:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:27:58] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.8712,	best estimator xgboost's best error=1.8712
[flaml.automl: 09-18 17:27:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:28:00] {3072} INFO -  at 15.9s,	estimator xgboost's best error=1.2647,	best estimator xgboost's best error=1.2647
[flaml.automl: 09-18 17:28:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:28:03] {3072} INFO -  at 18.9s,	estimator xgboost's best error=1.1507,	best estimator xgboost's best error=1.1507
[flaml.automl: 09-18 17:28:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:28:06] {3072} INFO -  at 21.6s,	estimator xgboost's best error=1.1119,	best estimator xgboost's best error=1.1119
[flaml.automl: 09-18 17:28:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:28:09] {3072} INFO -  at 24.3s,	estimator xgboost's best error=1.1119,	best estimator xgboost's best error=1.1119
[flaml.automl: 09-18 17:28:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:28:10] {3072} INFO -  at 25.9s,	estimator xgboost's best error=1.1119,	best estimator xgboost's best error=1.1119
[flaml.automl: 09-18 17:28:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:28:13] {3072} INFO -  at 28.9s,	estimator xgboost's best error=1.0761,	best estimator xgboost's best error=1.0761
[flaml.automl: 09-18 17:28:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:28:15] {3072} INFO -  at 30.5s,	estimator xgboost's best error=1.0761,	best estimator xgboost's best error=1.0761
[flaml.automl: 09-18 17:28:15] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:28:16] {3072} INFO -  at 31.7s,	estimator xgboost's best error=1.0761,	best estimator xgboost's best error=1.0761
[flaml.automl: 09-18 17:28:16] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:28:19] {3072} INFO -  at 34.5s,	estimator xgboost's best error=1.0595,	best estimator xgboost's best error=1.0595
[flaml.automl: 09-18 17:28:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:28:22] {3072} INFO -  at 37.4s,	estimator xgboost's best error=1.0595,	best estimator xgboost's best error=1.0595
[flaml.automl: 09-18 17:28:22] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 17:28:25] {3072} INFO -  at 40.0s,	estimator xgboost's best error=1.0595,	best estimator xgboost's best error=1.0595
[flaml.automl: 09-18 17:28:25] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 17:28:26] {3072} INFO -  at 41.8s,	estimator xgboost's best error=1.0595,	best estimator xgboost's best error=1.0595
[flaml.automl: 09-18 17:28:26] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 17:28:28] {3072} INFO -  at 43.5s,	estimator xgboost's best error=1.0338,	best estimator xgboost's best error=1.0338
[flaml.automl: 09-18 17:28:28] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 17:28:30] {3072} INFO -  at 45.4s,	estimator xgboost's best error=1.0338,	best estimator xgboost's best error=1.0338
[flaml.automl: 09-18 17:28:30] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 17:28:31] {3072} INFO -  at 46.6s,	estimator xgboost's best error=1.0338,	best estimator xgboost's best error=1.0338
[flaml.automl: 09-18 17:28:31] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 17:28:33] {3072} INFO -  at 48.0s,	estimator xgboost's best error=1.0338,	best estimator xgboost's best error=1.0338
[flaml.automl: 09-18 17:28:33] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 17:28:34] {3072} INFO -  at 49.2s,	estimator xgboost's best error=1.0338,	best estimator xgboost's best error=1.0338
[flaml.automl: 09-18 17:28:34] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 17:28:42] {3072} INFO -  at 57.1s,	estimator xgboost's best error=1.0232,	best estimator xgboost's best error=1.0232
[flaml.automl: 09-18 17:28:50] {3335} INFO - retrain xgboost for 7.9s
[flaml.automl: 09-18 17:28:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9508280259547836, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8344084316033451,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=0.0066459383199444525, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0031424921315017056, reg_lambda=0.10503395230912585,
             scale_pos_weight=1, subsample=0.8568446847279476,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 17:28:50] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:28:50] {2637} INFO - Time taken to find the best model: 57.12206792831421
[flaml.automl: 09-18 17:28:50] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 15, 'max_leaves': 9, 'min_child_weight': 0.0066459383199444525, 'learning_rate': 0.8344084316033451, 'subsample': 0.8568446847279476, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9508280259547836, 'reg_alpha': 0.0031424921315017056, 'reg_lambda': 0.10503395230912585, 'FLAML_sample_size': 41048}
SO2(0)最佳损失：-0.023161239227791786
SO2(0)最好结果：{'pred_time': 9.13225227059881e-06, 'wall_clock_time': 57.12206792831421, 'metric_for_logging': {'pred_time': 9.13225227059881e-06}, 'val_loss': 1.0231612392277918, 'training_iteration': 1, 'config': {'n_estimators': 15, 'max_leaves': 9, 'min_child_weight': 0.0066459383199444525, 'learning_rate': 0.8344084316033451, 'subsample': 0.8568446847279476, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9508280259547836, 'reg_alpha': 0.0031424921315017056, 'reg_lambda': 0.10503395230912585, 'FLAML_sample_size': 41048}, 'config/n_estimators': 15, 'config/max_leaves': 9, 'config/min_child_weight': 0.0066459383199444525, 'config/learning_rate': 0.8344084316033451, 'config/subsample': 0.8568446847279476, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9508280259547836, 'config/reg_alpha': 0.0031424921315017056, 'config/reg_lambda': 0.10503395230912585, 'config/FLAML_sample_size': 41048, 'experiment_tag': 'exp', 'time_total_s': 7.941686630249023}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9508280259547836, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8344084316033451,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=0.0066459383199444525, missing=nan,
             monotone_constraints='()', n_estimators=15, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0031424921315017056, reg_lambda=0.10503395230912585,
             scale_pos_weight=1, subsample=0.8568446847279476,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6210523032953559
SO2(0)的mse=3.3611563089693597
SO2(0)的mae=1.0554355757741731
SO2(0)的mar=0.16211056636150775
总共花费的时间为：65.75
达州市
2548A
2549A
2550A
2551A
2552A
[flaml.automl: 09-18 17:44:16] {2390} INFO - task = regression
[flaml.automl: 09-18 17:44:16] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:44:16] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:44:16] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:44:16] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:44:16] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:44:18] {3025} INFO - Estimated sufficient time budget=118744s. Estimated necessary time budget=119s.
[flaml.automl: 09-18 17:44:18] {3072} INFO -  at 2.5s,	estimator xgboost's best error=5.0375,	best estimator xgboost's best error=5.0375
[flaml.automl: 09-18 17:44:18] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:44:22] {3072} INFO -  at 6.5s,	estimator xgboost's best error=2.3966,	best estimator xgboost's best error=2.3966
[flaml.automl: 09-18 17:44:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:44:24] {3072} INFO -  at 8.8s,	estimator xgboost's best error=2.3966,	best estimator xgboost's best error=2.3966
[flaml.automl: 09-18 17:44:24] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:44:28] {3072} INFO -  at 13.0s,	estimator xgboost's best error=2.3966,	best estimator xgboost's best error=2.3966
[flaml.automl: 09-18 17:44:28] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:44:30] {3072} INFO -  at 15.0s,	estimator xgboost's best error=1.5732,	best estimator xgboost's best error=1.5732
[flaml.automl: 09-18 17:44:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:44:33] {3072} INFO -  at 17.9s,	estimator xgboost's best error=1.5570,	best estimator xgboost's best error=1.5570
[flaml.automl: 09-18 17:44:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:44:36] {3072} INFO -  at 20.9s,	estimator xgboost's best error=1.4270,	best estimator xgboost's best error=1.4270
[flaml.automl: 09-18 17:44:36] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:44:40] {3072} INFO -  at 24.4s,	estimator xgboost's best error=1.4270,	best estimator xgboost's best error=1.4270
[flaml.automl: 09-18 17:44:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:44:43] {3072} INFO -  at 27.4s,	estimator xgboost's best error=1.4270,	best estimator xgboost's best error=1.4270
[flaml.automl: 09-18 17:44:43] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:44:45] {3072} INFO -  at 30.0s,	estimator xgboost's best error=1.4270,	best estimator xgboost's best error=1.4270
[flaml.automl: 09-18 17:44:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:44:48] {3072} INFO -  at 32.2s,	estimator xgboost's best error=1.4270,	best estimator xgboost's best error=1.4270
[flaml.automl: 09-18 17:44:48] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:44:51] {3072} INFO -  at 35.2s,	estimator xgboost's best error=1.4191,	best estimator xgboost's best error=1.4191
[flaml.automl: 09-18 17:44:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:44:53] {3072} INFO -  at 37.4s,	estimator xgboost's best error=1.4191,	best estimator xgboost's best error=1.4191
[flaml.automl: 09-18 17:44:53] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 17:45:06] {3072} INFO -  at 50.3s,	estimator xgboost's best error=1.3518,	best estimator xgboost's best error=1.3518
[flaml.automl: 09-18 17:45:14] {3335} INFO - retrain xgboost for 8.8s
[flaml.automl: 09-18 17:45:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 17:45:14] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:45:14] {2637} INFO - Time taken to find the best model: 50.257943868637085
[flaml.automl: 09-18 17:45:14] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 53532}
SO2(0)最佳损失：-0.35175502476601617
SO2(0)最好结果：{'pred_time': 1.1913675002679842e-05, 'wall_clock_time': 50.257943868637085, 'metric_for_logging': {'pred_time': 1.1913675002679842e-05}, 'val_loss': 1.3517550247660162, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 53532}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 53532, 'experiment_tag': 'exp', 'time_total_s': 12.838379621505737}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6313949459524635
SO2(0)的mse=4.53381760917103
SO2(0)的mae=1.3856496699098018
SO2(0)的mar=0.17444597801685707
总共花费的时间为：59.95
雅安市
2555A
2556A
[flaml.automl: 09-18 17:51:54] {2390} INFO - task = regression
[flaml.automl: 09-18 17:51:54] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 17:51:54] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 17:51:54] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 17:51:54] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 17:51:54] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 17:51:56] {3025} INFO - Estimated sufficient time budget=22308s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 17:51:56] {3072} INFO -  at 2.3s,	estimator xgboost's best error=4.3206,	best estimator xgboost's best error=4.3206
[flaml.automl: 09-18 17:51:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 17:52:00] {3072} INFO -  at 6.3s,	estimator xgboost's best error=1.9363,	best estimator xgboost's best error=1.9363
[flaml.automl: 09-18 17:52:00] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 17:52:02] {3072} INFO -  at 8.6s,	estimator xgboost's best error=1.9363,	best estimator xgboost's best error=1.9363
[flaml.automl: 09-18 17:52:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 17:52:19] {3072} INFO -  at 25.6s,	estimator xgboost's best error=1.9363,	best estimator xgboost's best error=1.9363
[flaml.automl: 09-18 17:52:19] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 17:52:21] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.8149,	best estimator xgboost's best error=0.8149
[flaml.automl: 09-18 17:52:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 17:52:24] {3072} INFO -  at 30.6s,	estimator xgboost's best error=0.6791,	best estimator xgboost's best error=0.6791
[flaml.automl: 09-18 17:52:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 17:52:27] {3072} INFO -  at 33.6s,	estimator xgboost's best error=0.5512,	best estimator xgboost's best error=0.5512
[flaml.automl: 09-18 17:52:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 17:52:32] {3072} INFO -  at 38.7s,	estimator xgboost's best error=0.5512,	best estimator xgboost's best error=0.5512
[flaml.automl: 09-18 17:52:32] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 17:52:35] {3072} INFO -  at 41.5s,	estimator xgboost's best error=0.5512,	best estimator xgboost's best error=0.5512
[flaml.automl: 09-18 17:52:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 17:52:41] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.5512,	best estimator xgboost's best error=0.5512
[flaml.automl: 09-18 17:52:41] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 17:52:44] {3072} INFO -  at 50.3s,	estimator xgboost's best error=0.4907,	best estimator xgboost's best error=0.4907
[flaml.automl: 09-18 17:52:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 17:52:46] {3072} INFO -  at 52.3s,	estimator xgboost's best error=0.4907,	best estimator xgboost's best error=0.4907
[flaml.automl: 09-18 17:52:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 17:52:53] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.4536,	best estimator xgboost's best error=0.4536
[flaml.automl: 09-18 17:53:07] {3335} INFO - retrain xgboost for 14.3s
[flaml.automl: 09-18 17:53:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 17:53:07] {2636} INFO - fit succeeded
[flaml.automl: 09-18 17:53:07] {2637} INFO - Time taken to find the best model: 59.26230597496033
[flaml.automl: 09-18 17:53:07] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}
SO2(0)最佳损失：0.5463715164714176
SO2(0)最好结果：{'pred_time': 3.057840604417521e-05, 'wall_clock_time': 59.26230597496033, 'metric_for_logging': {'pred_time': 3.057840604417521e-05}, 'val_loss': 0.45362848352858237, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'experiment_tag': 'exp', 'time_total_s': 6.915371894836426}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8242619088531631
SO2(0)的mse=0.5309116432640264
SO2(0)的mae=0.47549348532052543
SO2(0)的mar=0.07925206660472758
总共花费的时间为：74.14
巴中市
2914A
3183A
3616A
[flaml.automl: 09-18 18:03:26] {2390} INFO - task = regression
[flaml.automl: 09-18 18:03:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:03:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:03:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:03:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:03:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:03:27] {3025} INFO - Estimated sufficient time budget=12244s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 18:03:27] {3072} INFO -  at 1.4s,	estimator xgboost's best error=2.4770,	best estimator xgboost's best error=2.4770
[flaml.automl: 09-18 18:03:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:03:29] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.1114,	best estimator xgboost's best error=1.1114
[flaml.automl: 09-18 18:03:29] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:03:30] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.1114,	best estimator xgboost's best error=1.1114
[flaml.automl: 09-18 18:03:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:03:44] {3072} INFO -  at 18.9s,	estimator xgboost's best error=1.1114,	best estimator xgboost's best error=1.1114
[flaml.automl: 09-18 18:03:44] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:03:47] {3072} INFO -  at 21.0s,	estimator xgboost's best error=0.5085,	best estimator xgboost's best error=0.5085
[flaml.automl: 09-18 18:03:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:03:49] {3072} INFO -  at 23.7s,	estimator xgboost's best error=0.3874,	best estimator xgboost's best error=0.3874
[flaml.automl: 09-18 18:03:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:03:52] {3072} INFO -  at 26.7s,	estimator xgboost's best error=0.3684,	best estimator xgboost's best error=0.3684
[flaml.automl: 09-18 18:03:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:03:57] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.3684,	best estimator xgboost's best error=0.3684
[flaml.automl: 09-18 18:03:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:04:00] {3072} INFO -  at 34.7s,	estimator xgboost's best error=0.3175,	best estimator xgboost's best error=0.3175
[flaml.automl: 09-18 18:04:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:04:06] {3072} INFO -  at 40.0s,	estimator xgboost's best error=0.3175,	best estimator xgboost's best error=0.3175
[flaml.automl: 09-18 18:04:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:04:08] {3072} INFO -  at 42.7s,	estimator xgboost's best error=0.3175,	best estimator xgboost's best error=0.3175
[flaml.automl: 09-18 18:04:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:04:10] {3072} INFO -  at 44.9s,	estimator xgboost's best error=0.3175,	best estimator xgboost's best error=0.3175
[flaml.automl: 09-18 18:04:10] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:04:23] {3072} INFO -  at 57.5s,	estimator xgboost's best error=0.3175,	best estimator xgboost's best error=0.3175
[flaml.automl: 09-18 18:04:26] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-18 18:04:26] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:04:26] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:04:26] {2637} INFO - Time taken to find the best model: 34.74376201629639
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}
SO2(0)最佳损失：0.6824884294834577
SO2(0)最好结果：{'pred_time': 2.1502098352569438e-05, 'wall_clock_time': 34.74376201629639, 'metric_for_logging': {'pred_time': 2.1502098352569438e-05}, 'val_loss': 0.31751157051654233, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 1.1765273389893034, 'config/learning_rate': 0.8338182546732649, 'config/subsample': 0.9114052575858262, 'config/colsample_bylevel': 0.7232930776213671, 'config/colsample_bytree': 0.8046603913292237, 'config/reg_alpha': 0.00473280911734172, 'config/reg_lambda': 15.090081382165623, 'experiment_tag': 'exp', 'time_total_s': 3.0175933837890625}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5272306824328643
SO2(0)的mse=0.25902688823046083
SO2(0)的mae=0.30825457543738033
SO2(0)的mar=0.0762350177377843
总共花费的时间为：61.11
资阳市
2561A
2562A
2563A
2564A
2565A
[flaml.automl: 09-18 18:20:22] {2390} INFO - task = regression
[flaml.automl: 09-18 18:20:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:20:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:20:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:20:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:20:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:20:24] {3025} INFO - Estimated sufficient time budget=118277s. Estimated necessary time budget=118s.
[flaml.automl: 09-18 18:20:24] {3072} INFO -  at 2.6s,	estimator xgboost's best error=3.9460,	best estimator xgboost's best error=3.9460
[flaml.automl: 09-18 18:20:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:20:28] {3072} INFO -  at 6.1s,	estimator xgboost's best error=1.8174,	best estimator xgboost's best error=1.8174
[flaml.automl: 09-18 18:20:28] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:20:30] {3072} INFO -  at 8.3s,	estimator xgboost's best error=1.8174,	best estimator xgboost's best error=1.8174
[flaml.automl: 09-18 18:20:30] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:20:34] {3072} INFO -  at 12.4s,	estimator xgboost's best error=1.8174,	best estimator xgboost's best error=1.8174
[flaml.automl: 09-18 18:20:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:20:36] {3072} INFO -  at 14.5s,	estimator xgboost's best error=1.0174,	best estimator xgboost's best error=1.0174
[flaml.automl: 09-18 18:20:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:20:39] {3072} INFO -  at 17.1s,	estimator xgboost's best error=0.8824,	best estimator xgboost's best error=0.8824
[flaml.automl: 09-18 18:20:39] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:20:42] {3072} INFO -  at 20.3s,	estimator xgboost's best error=0.8824,	best estimator xgboost's best error=0.8824
[flaml.automl: 09-18 18:20:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:20:44] {3072} INFO -  at 22.4s,	estimator xgboost's best error=0.8824,	best estimator xgboost's best error=0.8824
[flaml.automl: 09-18 18:20:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:20:47] {3072} INFO -  at 25.3s,	estimator xgboost's best error=0.8620,	best estimator xgboost's best error=0.8620
[flaml.automl: 09-18 18:20:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:20:49] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.8620,	best estimator xgboost's best error=0.8620
[flaml.automl: 09-18 18:20:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:20:52] {3072} INFO -  at 30.2s,	estimator xgboost's best error=0.8620,	best estimator xgboost's best error=0.8620
[flaml.automl: 09-18 18:20:52] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:20:54] {3072} INFO -  at 32.7s,	estimator xgboost's best error=0.8620,	best estimator xgboost's best error=0.8620
[flaml.automl: 09-18 18:20:54] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:20:56] {3072} INFO -  at 34.7s,	estimator xgboost's best error=0.8620,	best estimator xgboost's best error=0.8620
[flaml.automl: 09-18 18:20:56] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:21:00] {3072} INFO -  at 38.0s,	estimator xgboost's best error=0.8620,	best estimator xgboost's best error=0.8620
[flaml.automl: 09-18 18:21:00] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 18:21:07] {3072} INFO -  at 44.9s,	estimator xgboost's best error=0.8261,	best estimator xgboost's best error=0.8261
[flaml.automl: 09-18 18:21:07] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 18:21:10] {3072} INFO -  at 48.8s,	estimator xgboost's best error=0.8261,	best estimator xgboost's best error=0.8261
[flaml.automl: 09-18 18:21:10] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 18:21:20] {3072} INFO -  at 57.9s,	estimator xgboost's best error=0.8261,	best estimator xgboost's best error=0.8261
[flaml.automl: 09-18 18:21:24] {3335} INFO - retrain xgboost for 4.3s
[flaml.automl: 09-18 18:21:24] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8238091337774175, colsample_bynode=1,
             colsample_bytree=0.6689842268708346, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=0.3460374283279869, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.002183206969860811, reg_lambda=16.45272856610453,
             scale_pos_weight=1, subsample=0.7019598455137446,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 18:21:24] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:21:24] {2637} INFO - Time taken to find the best model: 44.93329095840454
[flaml.automl: 09-18 18:21:24] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 9, 'max_leaves': 4, 'min_child_weight': 0.3460374283279869, 'learning_rate': 0.5408133424638543, 'subsample': 0.7019598455137446, 'colsample_bylevel': 0.8238091337774175, 'colsample_bytree': 0.6689842268708346, 'reg_alpha': 0.002183206969860811, 'reg_lambda': 16.45272856610453, 'FLAML_sample_size': 52828}
SO2(0)最佳损失：0.17388813061039943
SO2(0)最好结果：{'pred_time': 2.1959732094655857e-05, 'wall_clock_time': 44.93329095840454, 'metric_for_logging': {'pred_time': 2.1959732094655857e-05}, 'val_loss': 0.8261118693896006, 'training_iteration': 1, 'config': {'n_estimators': 9, 'max_leaves': 4, 'min_child_weight': 0.3460374283279869, 'learning_rate': 0.5408133424638543, 'subsample': 0.7019598455137446, 'colsample_bylevel': 0.8238091337774175, 'colsample_bytree': 0.6689842268708346, 'reg_alpha': 0.002183206969860811, 'reg_lambda': 16.45272856610453, 'FLAML_sample_size': 52828}, 'config/n_estimators': 9, 'config/max_leaves': 4, 'config/min_child_weight': 0.3460374283279869, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7019598455137446, 'config/colsample_bylevel': 0.8238091337774175, 'config/colsample_bytree': 0.6689842268708346, 'config/reg_alpha': 0.002183206969860811, 'config/reg_lambda': 16.45272856610453, 'config/FLAML_sample_size': 52828, 'experiment_tag': 'exp', 'time_total_s': 6.910229206085205}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8238091337774175, colsample_bynode=1,
             colsample_bytree=0.6689842268708346, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=0.3460374283279869, missing=nan,
             monotone_constraints='()', n_estimators=9, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.002183206969860811, reg_lambda=16.45272856610453,
             scale_pos_weight=1, subsample=0.7019598455137446,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6614581024969888
SO2(0)的mse=1.7556072017357396
SO2(0)的mae=0.8255493895108031
SO2(0)的mar=0.1295716979993284
总共花费的时间为：63.25
阿坝藏族羌族自治州
2566A
2567A
2568A
[flaml.automl: 09-18 18:30:59] {2390} INFO - task = regression
[flaml.automl: 09-18 18:30:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:30:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:30:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:30:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:30:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:31:00] {3025} INFO - Estimated sufficient time budget=12050s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 18:31:00] {3072} INFO -  at 1.4s,	estimator xgboost's best error=7.8041,	best estimator xgboost's best error=7.8041
[flaml.automl: 09-18 18:31:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:31:02] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.4682,	best estimator xgboost's best error=3.4682
[flaml.automl: 09-18 18:31:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:31:03] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.4682,	best estimator xgboost's best error=3.4682
[flaml.automl: 09-18 18:31:03] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:31:13] {3072} INFO -  at 14.5s,	estimator xgboost's best error=3.4682,	best estimator xgboost's best error=3.4682
[flaml.automl: 09-18 18:31:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:31:14] {3072} INFO -  at 15.7s,	estimator xgboost's best error=1.3734,	best estimator xgboost's best error=1.3734
[flaml.automl: 09-18 18:31:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:31:16] {3072} INFO -  at 17.3s,	estimator xgboost's best error=0.9752,	best estimator xgboost's best error=0.9752
[flaml.automl: 09-18 18:31:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:31:18] {3072} INFO -  at 18.9s,	estimator xgboost's best error=0.8198,	best estimator xgboost's best error=0.8198
[flaml.automl: 09-18 18:31:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:31:20] {3072} INFO -  at 21.6s,	estimator xgboost's best error=0.8198,	best estimator xgboost's best error=0.8198
[flaml.automl: 09-18 18:31:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:31:22] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.8198,	best estimator xgboost's best error=0.8198
[flaml.automl: 09-18 18:31:22] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:31:27] {3072} INFO -  at 28.7s,	estimator xgboost's best error=0.8198,	best estimator xgboost's best error=0.8198
[flaml.automl: 09-18 18:31:27] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:31:30] {3072} INFO -  at 31.3s,	estimator xgboost's best error=0.8198,	best estimator xgboost's best error=0.8198
[flaml.automl: 09-18 18:31:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:31:32] {3072} INFO -  at 33.4s,	estimator xgboost's best error=0.8198,	best estimator xgboost's best error=0.8198
[flaml.automl: 09-18 18:31:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:31:45] {3072} INFO -  at 46.0s,	estimator xgboost's best error=0.5889,	best estimator xgboost's best error=0.5889
[flaml.automl: 09-18 18:31:45] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:31:58] {3072} INFO -  at 59.8s,	estimator xgboost's best error=0.5811,	best estimator xgboost's best error=0.5811
[flaml.automl: 09-18 18:32:21] {3335} INFO - retrain xgboost for 22.9s
[flaml.automl: 09-18 18:32:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:32:21] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:32:21] {2637} INFO - Time taken to find the best model: 59.78731417655945
[flaml.automl: 09-18 18:32:21] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767}
SO2(0)最佳损失：0.4189036710706999
SO2(0)最好结果：{'pred_time': 1.882421452372588e-05, 'wall_clock_time': 59.78731417655945, 'metric_for_logging': {'pred_time': 1.882421452372588e-05}, 'val_loss': 0.5810963289293001, 'training_iteration': 1, 'config': {'n_estimators': 30, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767}, 'config/n_estimators': 30, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'experiment_tag': 'exp', 'time_total_s': 13.74618148803711}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=30, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8861435642587131
SO2(0)的mse=0.9060075936593394
SO2(0)的mae=0.5800703692271266
SO2(0)的mar=0.04888902116768775
总共花费的时间为：83.32
甘孜藏族自治州
3065A
[flaml.automl: 09-18 18:35:53] {2390} INFO - task = regression
[flaml.automl: 09-18 18:35:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:35:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:35:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:35:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:35:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:35:54] {3025} INFO - Estimated sufficient time budget=11908s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 18:35:54] {3072} INFO -  at 1.2s,	estimator xgboost's best error=4.6273,	best estimator xgboost's best error=4.6273
[flaml.automl: 09-18 18:35:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:35:56] {3072} INFO -  at 3.1s,	estimator xgboost's best error=2.4001,	best estimator xgboost's best error=2.4001
[flaml.automl: 09-18 18:35:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:35:57] {3072} INFO -  at 4.3s,	estimator xgboost's best error=2.4001,	best estimator xgboost's best error=2.4001
[flaml.automl: 09-18 18:35:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:36:13] {3072} INFO -  at 20.2s,	estimator xgboost's best error=2.4001,	best estimator xgboost's best error=2.4001
[flaml.automl: 09-18 18:36:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:36:17] {3072} INFO -  at 24.1s,	estimator xgboost's best error=0.7457,	best estimator xgboost's best error=0.7457
[flaml.automl: 09-18 18:36:17] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:36:21] {3072} INFO -  at 28.4s,	estimator xgboost's best error=0.4445,	best estimator xgboost's best error=0.4445
[flaml.automl: 09-18 18:36:21] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:36:28] {3072} INFO -  at 34.8s,	estimator xgboost's best error=0.3830,	best estimator xgboost's best error=0.3830
[flaml.automl: 09-18 18:36:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:36:38] {3072} INFO -  at 44.6s,	estimator xgboost's best error=0.3830,	best estimator xgboost's best error=0.3830
[flaml.automl: 09-18 18:36:38] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:36:45] {3072} INFO -  at 51.7s,	estimator xgboost's best error=0.3300,	best estimator xgboost's best error=0.3300
[flaml.automl: 09-18 18:36:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:36:52] {3072} INFO -  at 58.9s,	estimator xgboost's best error=0.3300,	best estimator xgboost's best error=0.3300
[flaml.automl: 09-18 18:37:00] {3335} INFO - retrain xgboost for 8.2s
[flaml.automl: 09-18 18:37:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:37:00] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:37:00] {2637} INFO - Time taken to find the best model: 51.730170011520386
[flaml.automl: 09-18 18:37:00] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}
SO2(0)最佳损失：0.6700266365799608
SO2(0)最好结果：{'pred_time': 0.00019388754021798215, 'wall_clock_time': 51.730170011520386, 'metric_for_logging': {'pred_time': 0.00019388754021798215}, 'val_loss': 0.32997336342003925, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 1.1765273389893034, 'config/learning_rate': 0.8338182546732649, 'config/subsample': 0.9114052575858262, 'config/colsample_bylevel': 0.7232930776213671, 'config/colsample_bytree': 0.8046603913292237, 'config/reg_alpha': 0.00473280911734172, 'config/reg_lambda': 15.090081382165623, 'experiment_tag': 'exp', 'time_total_s': 7.170647144317627}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.937850443684558
SO2(0)的mse=0.22804624635681475
SO2(0)的mae=0.33325955941363883
SO2(0)的mar=0.04776720424250694
总共花费的时间为：67.45
凉山彝族自治州
2571A
2572A
2573A
2574A
2575A
[flaml.automl: 09-18 18:52:59] {2390} INFO - task = regression
[flaml.automl: 09-18 18:52:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 18:52:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 18:52:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 18:52:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 18:52:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 18:53:00] {3025} INFO - Estimated sufficient time budget=67850s. Estimated necessary time budget=68s.
[flaml.automl: 09-18 18:53:00] {3072} INFO -  at 1.5s,	estimator xgboost's best error=6.3611,	best estimator xgboost's best error=6.3611
[flaml.automl: 09-18 18:53:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 18:53:03] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.9196,	best estimator xgboost's best error=2.9196
[flaml.automl: 09-18 18:53:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 18:53:04] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.9196,	best estimator xgboost's best error=2.9196
[flaml.automl: 09-18 18:53:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 18:53:09] {3072} INFO -  at 9.9s,	estimator xgboost's best error=2.9196,	best estimator xgboost's best error=2.9196
[flaml.automl: 09-18 18:53:09] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 18:53:10] {3072} INFO -  at 11.0s,	estimator xgboost's best error=1.7313,	best estimator xgboost's best error=1.7313
[flaml.automl: 09-18 18:53:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 18:53:11] {3072} INFO -  at 12.5s,	estimator xgboost's best error=1.4948,	best estimator xgboost's best error=1.4948
[flaml.automl: 09-18 18:53:11] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 18:53:13] {3072} INFO -  at 14.1s,	estimator xgboost's best error=1.4948,	best estimator xgboost's best error=1.4948
[flaml.automl: 09-18 18:53:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 18:53:15] {3072} INFO -  at 16.5s,	estimator xgboost's best error=1.4948,	best estimator xgboost's best error=1.4948
[flaml.automl: 09-18 18:53:15] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 18:53:17] {3072} INFO -  at 17.8s,	estimator xgboost's best error=1.4948,	best estimator xgboost's best error=1.4948
[flaml.automl: 09-18 18:53:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 18:53:20] {3072} INFO -  at 21.5s,	estimator xgboost's best error=1.4948,	best estimator xgboost's best error=1.4948
[flaml.automl: 09-18 18:53:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 18:53:23] {3072} INFO -  at 24.5s,	estimator xgboost's best error=1.4948,	best estimator xgboost's best error=1.4948
[flaml.automl: 09-18 18:53:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 18:53:26] {3072} INFO -  at 26.7s,	estimator xgboost's best error=1.4948,	best estimator xgboost's best error=1.4948
[flaml.automl: 09-18 18:53:26] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 18:53:38] {3072} INFO -  at 38.8s,	estimator xgboost's best error=1.4303,	best estimator xgboost's best error=1.4303
[flaml.automl: 09-18 18:53:38] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 18:53:58] {3072} INFO -  at 58.7s,	estimator xgboost's best error=1.4227,	best estimator xgboost's best error=1.4227
[flaml.automl: 09-18 18:54:20] {3335} INFO - retrain xgboost for 22.0s
[flaml.automl: 09-18 18:54:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 18:54:20] {2636} INFO - fit succeeded
[flaml.automl: 09-18 18:54:20] {2637} INFO - Time taken to find the best model: 58.72629427909851
[flaml.automl: 09-18 18:54:20] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52544}
SO2(0)最佳损失：-0.4227355768631982
SO2(0)最好结果：{'pred_time': 1.3114124317041794e-05, 'wall_clock_time': 58.72629427909851, 'metric_for_logging': {'pred_time': 1.3114124317041794e-05}, 'val_loss': 1.4227355768631982, 'training_iteration': 1, 'config': {'n_estimators': 19, 'max_leaves': 11, 'min_child_weight': 0.1613364219629652, 'learning_rate': 0.5408133424638543, 'subsample': 0.7900177763143668, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7006835355029045, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.35190936751303176, 'FLAML_sample_size': 52544}, 'config/n_estimators': 19, 'config/max_leaves': 11, 'config/min_child_weight': 0.1613364219629652, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.7900177763143668, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7006835355029045, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.35190936751303176, 'config/FLAML_sample_size': 52544, 'experiment_tag': 'exp', 'time_total_s': 19.966171503067017}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7006835355029045, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=11,
             min_child_weight=0.1613364219629652, missing=nan,
             monotone_constraints='()', n_estimators=19, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.35190936751303176, scale_pos_weight=1,
             subsample=0.7900177763143668, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5712254018205019
SO2(0)的mse=4.459230381393791
SO2(0)的mae=1.4116721639872811
SO2(0)的mar=0.13607798729888143
总共花费的时间为：81.68
六盘水市
2576A
2577A
2578A
2579A
2580A
[flaml.automl: 09-18 19:10:37] {2390} INFO - task = regression
[flaml.automl: 09-18 19:10:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:10:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:10:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:10:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:10:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:10:38] {3025} INFO - Estimated sufficient time budget=63939s. Estimated necessary time budget=64s.
[flaml.automl: 09-18 19:10:38] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.3607,	best estimator xgboost's best error=3.3607
[flaml.automl: 09-18 19:10:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:10:40] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.6058,	best estimator xgboost's best error=1.6058
[flaml.automl: 09-18 19:10:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:10:41] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.6058,	best estimator xgboost's best error=1.6058
[flaml.automl: 09-18 19:10:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:10:46] {3072} INFO -  at 9.5s,	estimator xgboost's best error=1.6058,	best estimator xgboost's best error=1.6058
[flaml.automl: 09-18 19:10:46] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:10:47] {3072} INFO -  at 10.6s,	estimator xgboost's best error=1.0966,	best estimator xgboost's best error=1.0966
[flaml.automl: 09-18 19:10:47] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:10:49] {3072} INFO -  at 12.2s,	estimator xgboost's best error=1.0800,	best estimator xgboost's best error=1.0800
[flaml.automl: 09-18 19:10:49] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:10:51] {3072} INFO -  at 14.9s,	estimator xgboost's best error=0.9465,	best estimator xgboost's best error=0.9465
[flaml.automl: 09-18 19:10:51] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:10:55] {3072} INFO -  at 18.3s,	estimator xgboost's best error=0.9465,	best estimator xgboost's best error=0.9465
[flaml.automl: 09-18 19:10:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:10:58] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.9465,	best estimator xgboost's best error=0.9465
[flaml.automl: 09-18 19:10:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:11:01] {3072} INFO -  at 24.7s,	estimator xgboost's best error=0.9465,	best estimator xgboost's best error=0.9465
[flaml.automl: 09-18 19:11:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:11:04] {3072} INFO -  at 27.3s,	estimator xgboost's best error=0.9465,	best estimator xgboost's best error=0.9465
[flaml.automl: 09-18 19:11:04] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:11:07] {3072} INFO -  at 30.4s,	estimator xgboost's best error=0.9410,	best estimator xgboost's best error=0.9410
[flaml.automl: 09-18 19:11:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:11:09] {3072} INFO -  at 32.5s,	estimator xgboost's best error=0.9410,	best estimator xgboost's best error=0.9410
[flaml.automl: 09-18 19:11:09] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:11:21] {3072} INFO -  at 45.0s,	estimator xgboost's best error=0.8736,	best estimator xgboost's best error=0.8736
[flaml.automl: 09-18 19:11:21] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 19:11:36] {3072} INFO -  at 59.6s,	estimator xgboost's best error=0.8603,	best estimator xgboost's best error=0.8603
[flaml.automl: 09-18 19:11:57] {3335} INFO - retrain xgboost for 21.1s
[flaml.automl: 09-18 19:11:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:11:57] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:11:57] {2637} INFO - Time taken to find the best model: 59.64859938621521
[flaml.automl: 09-18 19:11:57] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 53775}
SO2(0)最佳损失：0.13973573520480387
SO2(0)最好结果：{'pred_time': 1.4229711279811629e-05, 'wall_clock_time': 59.64859938621521, 'metric_for_logging': {'pred_time': 1.4229711279811629e-05}, 'val_loss': 0.8602642647951961, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767, 'FLAML_sample_size': 53775}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'config/FLAML_sample_size': 53775, 'experiment_tag': 'exp', 'time_total_s': 14.602842330932617}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6547407707414864
SO2(0)的mse=2.2320571816425443
SO2(0)的mae=0.8789146661758422
SO2(0)的mar=0.16632216143794798
总共花费的时间为：81.68
安顺市
3122A
3123A
3124A
3125A
[flaml.automl: 09-18 19:25:34] {2390} INFO - task = regression
[flaml.automl: 09-18 19:25:34] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:25:34] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:25:34] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:25:34] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:25:34] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:25:36] {3025} INFO - Estimated sufficient time budget=94806s. Estimated necessary time budget=95s.
[flaml.automl: 09-18 19:25:36] {3072} INFO -  at 2.6s,	estimator xgboost's best error=7.2761,	best estimator xgboost's best error=7.2761
[flaml.automl: 09-18 19:25:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:25:40] {3072} INFO -  at 6.4s,	estimator xgboost's best error=3.5091,	best estimator xgboost's best error=3.5091
[flaml.automl: 09-18 19:25:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:25:42] {3072} INFO -  at 8.6s,	estimator xgboost's best error=3.5091,	best estimator xgboost's best error=3.5091
[flaml.automl: 09-18 19:25:42] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:25:47] {3072} INFO -  at 13.7s,	estimator xgboost's best error=3.5091,	best estimator xgboost's best error=3.5091
[flaml.automl: 09-18 19:25:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:25:49] {3072} INFO -  at 15.9s,	estimator xgboost's best error=2.5782,	best estimator xgboost's best error=2.5782
[flaml.automl: 09-18 19:25:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:25:52] {3072} INFO -  at 18.8s,	estimator xgboost's best error=2.5782,	best estimator xgboost's best error=2.5782
[flaml.automl: 09-18 19:25:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:25:56] {3072} INFO -  at 21.9s,	estimator xgboost's best error=2.1119,	best estimator xgboost's best error=2.1119
[flaml.automl: 09-18 19:25:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:26:00] {3072} INFO -  at 26.1s,	estimator xgboost's best error=2.1119,	best estimator xgboost's best error=2.1119
[flaml.automl: 09-18 19:26:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:26:03] {3072} INFO -  at 29.0s,	estimator xgboost's best error=2.1119,	best estimator xgboost's best error=2.1119
[flaml.automl: 09-18 19:26:03] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:26:06] {3072} INFO -  at 32.6s,	estimator xgboost's best error=2.1119,	best estimator xgboost's best error=2.1119
[flaml.automl: 09-18 19:26:06] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:26:09] {3072} INFO -  at 35.2s,	estimator xgboost's best error=2.1119,	best estimator xgboost's best error=2.1119
[flaml.automl: 09-18 19:26:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:26:14] {3072} INFO -  at 40.4s,	estimator xgboost's best error=2.1069,	best estimator xgboost's best error=2.1069
[flaml.automl: 09-18 19:26:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:26:19] {3072} INFO -  at 45.0s,	estimator xgboost's best error=2.1069,	best estimator xgboost's best error=2.1069
[flaml.automl: 09-18 19:26:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:26:33] {3072} INFO -  at 59.3s,	estimator xgboost's best error=2.0408,	best estimator xgboost's best error=2.0408
[flaml.automl: 09-18 19:26:58] {3335} INFO - retrain xgboost for 25.2s
[flaml.automl: 09-18 19:26:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:26:58] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:26:58] {2637} INFO - Time taken to find the best model: 59.291407108306885
[flaml.automl: 09-18 19:26:58] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 42811}
SO2(0)最佳损失：-1.0408153499854271
SO2(0)最好结果：{'pred_time': 3.396159138578263e-05, 'wall_clock_time': 59.291407108306885, 'metric_for_logging': {'pred_time': 3.396159138578263e-05}, 'val_loss': 2.040815349985427, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 1.074139476480836, 'learning_rate': 0.8922575233158153, 'subsample': 1.0, 'colsample_bylevel': 0.9907519335250881, 'colsample_bytree': 0.901417622303371, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.058405219938869164, 'FLAML_sample_size': 42811}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 1.074139476480836, 'config/learning_rate': 0.8922575233158153, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9907519335250881, 'config/colsample_bytree': 0.901417622303371, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.058405219938869164, 'config/FLAML_sample_size': 42811, 'experiment_tag': 'exp', 'time_total_s': 14.32896876335144}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9907519335250881, colsample_bynode=1,
             colsample_bytree=0.901417622303371, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8922575233158153,
             max_delta_step=0, max_depth=0, max_leaves=9,
             min_child_weight=1.074139476480836, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.058405219938869164,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8096964203489718
SO2(0)的mse=10.97239171723709
SO2(0)的mae=1.9495090158248192
SO2(0)的mar=0.16919128441130496
总共花费的时间为：85.74
铜仁地区
2585A
2586A
[flaml.automl: 09-18 19:34:08] {2390} INFO - task = regression
[flaml.automl: 09-18 19:34:08] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:34:08] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:34:08] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:34:08] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:34:08] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:34:10] {3025} INFO - Estimated sufficient time budget=22709s. Estimated necessary time budget=23s.
[flaml.automl: 09-18 19:34:10] {3072} INFO -  at 2.4s,	estimator xgboost's best error=1.6679,	best estimator xgboost's best error=1.6679
[flaml.automl: 09-18 19:34:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:34:14] {3072} INFO -  at 6.3s,	estimator xgboost's best error=0.8610,	best estimator xgboost's best error=0.8610
[flaml.automl: 09-18 19:34:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:34:17] {3072} INFO -  at 8.5s,	estimator xgboost's best error=0.8610,	best estimator xgboost's best error=0.8610
[flaml.automl: 09-18 19:34:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:34:37] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.8610,	best estimator xgboost's best error=0.8610
[flaml.automl: 09-18 19:34:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:34:40] {3072} INFO -  at 32.0s,	estimator xgboost's best error=0.5620,	best estimator xgboost's best error=0.5620
[flaml.automl: 09-18 19:34:40] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:34:44] {3072} INFO -  at 36.2s,	estimator xgboost's best error=0.4963,	best estimator xgboost's best error=0.4963
[flaml.automl: 09-18 19:34:44] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:34:49] {3072} INFO -  at 40.6s,	estimator xgboost's best error=0.4490,	best estimator xgboost's best error=0.4490
[flaml.automl: 09-18 19:34:49] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:34:56] {3072} INFO -  at 47.8s,	estimator xgboost's best error=0.4490,	best estimator xgboost's best error=0.4490
[flaml.automl: 09-18 19:34:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:35:00] {3072} INFO -  at 52.2s,	estimator xgboost's best error=0.4121,	best estimator xgboost's best error=0.4121
[flaml.automl: 09-18 19:35:00] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:35:06] {3072} INFO -  at 58.1s,	estimator xgboost's best error=0.4121,	best estimator xgboost's best error=0.4121
[flaml.automl: 09-18 19:35:09] {3335} INFO - retrain xgboost for 2.8s
[flaml.automl: 09-18 19:35:09] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:35:09] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:35:09] {2637} INFO - Time taken to find the best model: 52.15396738052368
[flaml.automl: 09-18 19:35:09] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}
SO2(0)最佳损失：0.5879446676715234
SO2(0)最好结果：{'pred_time': 4.312034697871712e-05, 'wall_clock_time': 52.15396738052368, 'metric_for_logging': {'pred_time': 4.312034697871712e-05}, 'val_loss': 0.4120553323284765, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 1.1765273389893034, 'config/learning_rate': 0.8338182546732649, 'config/subsample': 0.9114052575858262, 'config/colsample_bylevel': 0.7232930776213671, 'config/colsample_bytree': 0.8046603913292237, 'config/reg_alpha': 0.00473280911734172, 'config/reg_lambda': 15.090081382165623, 'experiment_tag': 'exp', 'time_total_s': 4.3363776206970215}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8578095847375097
SO2(0)的mse=0.4938049079705306
SO2(0)的mae=0.44244287532653975
SO2(0)的mar=0.18340250698093333
总共花费的时间为：61.36
毕节市
2587A
2588A
3537A
[flaml.automl: 09-18 19:45:19] {2390} INFO - task = regression
[flaml.automl: 09-18 19:45:19] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:45:19] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:45:19] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:45:19] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:45:19] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:45:20] {3025} INFO - Estimated sufficient time budget=12176s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 19:45:20] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.4526,	best estimator xgboost's best error=4.4526
[flaml.automl: 09-18 19:45:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:45:22] {3072} INFO -  at 3.5s,	estimator xgboost's best error=1.9835,	best estimator xgboost's best error=1.9835
[flaml.automl: 09-18 19:45:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:45:23] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.9835,	best estimator xgboost's best error=1.9835
[flaml.automl: 09-18 19:45:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:45:33] {3072} INFO -  at 14.5s,	estimator xgboost's best error=1.9835,	best estimator xgboost's best error=1.9835
[flaml.automl: 09-18 19:45:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:45:34] {3072} INFO -  at 15.6s,	estimator xgboost's best error=0.9924,	best estimator xgboost's best error=0.9924
[flaml.automl: 09-18 19:45:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:45:36] {3072} INFO -  at 17.2s,	estimator xgboost's best error=0.9797,	best estimator xgboost's best error=0.9797
[flaml.automl: 09-18 19:45:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:45:37] {3072} INFO -  at 18.8s,	estimator xgboost's best error=0.8322,	best estimator xgboost's best error=0.8322
[flaml.automl: 09-18 19:45:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:45:40] {3072} INFO -  at 21.5s,	estimator xgboost's best error=0.8322,	best estimator xgboost's best error=0.8322
[flaml.automl: 09-18 19:45:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:45:42] {3072} INFO -  at 23.2s,	estimator xgboost's best error=0.8322,	best estimator xgboost's best error=0.8322
[flaml.automl: 09-18 19:45:42] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:45:45] {3072} INFO -  at 26.2s,	estimator xgboost's best error=0.8322,	best estimator xgboost's best error=0.8322
[flaml.automl: 09-18 19:45:45] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:45:46] {3072} INFO -  at 27.7s,	estimator xgboost's best error=0.8322,	best estimator xgboost's best error=0.8322
[flaml.automl: 09-18 19:45:46] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:45:47] {3072} INFO -  at 28.8s,	estimator xgboost's best error=0.8322,	best estimator xgboost's best error=0.8322
[flaml.automl: 09-18 19:45:47] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:45:54] {3072} INFO -  at 35.7s,	estimator xgboost's best error=0.8054,	best estimator xgboost's best error=0.8054
[flaml.automl: 09-18 19:45:54] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:46:07] {3072} INFO -  at 48.5s,	estimator xgboost's best error=0.7859,	best estimator xgboost's best error=0.7859
[flaml.automl: 09-18 19:46:32] {3335} INFO - retrain xgboost for 25.2s
[flaml.automl: 09-18 19:46:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:46:32] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:46:32] {2637} INFO - Time taken to find the best model: 48.52805423736572
[flaml.automl: 09-18 19:46:32] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767}
SO2(0)最佳损失：0.21407831721343618
SO2(0)最好结果：{'pred_time': 1.0934888062923862e-05, 'wall_clock_time': 48.52805423736572, 'metric_for_logging': {'pred_time': 1.0934888062923862e-05}, 'val_loss': 0.7859216827865638, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'experiment_tag': 'exp', 'time_total_s': 12.793488264083862}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6322677487521434
SO2(0)的mse=1.7271603704093657
SO2(0)的mae=0.7930889428614515
SO2(0)的mar=0.09994460997001509
总共花费的时间为：74.44
黔西南布依族苗族自治州
2589A
2590A
[flaml.automl: 09-18 19:53:15] {2390} INFO - task = regression
[flaml.automl: 09-18 19:53:15] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:53:15] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:53:15] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:53:15] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:53:15] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:53:17] {3025} INFO - Estimated sufficient time budget=22183s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 19:53:17] {3072} INFO -  at 2.4s,	estimator xgboost's best error=3.6803,	best estimator xgboost's best error=3.6803
[flaml.automl: 09-18 19:53:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:53:21] {3072} INFO -  at 6.1s,	estimator xgboost's best error=1.7876,	best estimator xgboost's best error=1.7876
[flaml.automl: 09-18 19:53:21] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:53:23] {3072} INFO -  at 8.0s,	estimator xgboost's best error=1.7876,	best estimator xgboost's best error=1.7876
[flaml.automl: 09-18 19:53:23] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:53:42] {3072} INFO -  at 27.4s,	estimator xgboost's best error=1.7876,	best estimator xgboost's best error=1.7876
[flaml.automl: 09-18 19:53:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:53:45] {3072} INFO -  at 30.0s,	estimator xgboost's best error=1.1981,	best estimator xgboost's best error=1.1981
[flaml.automl: 09-18 19:53:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:53:48] {3072} INFO -  at 33.6s,	estimator xgboost's best error=1.0715,	best estimator xgboost's best error=1.0715
[flaml.automl: 09-18 19:53:48] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:53:52] {3072} INFO -  at 37.3s,	estimator xgboost's best error=1.0569,	best estimator xgboost's best error=1.0569
[flaml.automl: 09-18 19:53:52] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:53:58] {3072} INFO -  at 43.1s,	estimator xgboost's best error=1.0569,	best estimator xgboost's best error=1.0569
[flaml.automl: 09-18 19:53:58] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:54:02] {3072} INFO -  at 47.0s,	estimator xgboost's best error=1.0569,	best estimator xgboost's best error=1.0569
[flaml.automl: 09-18 19:54:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:54:11] {3072} INFO -  at 56.3s,	estimator xgboost's best error=0.9891,	best estimator xgboost's best error=0.9891
[flaml.automl: 09-18 19:54:22] {3335} INFO - retrain xgboost for 11.2s
[flaml.automl: 09-18 19:54:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 19:54:22] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:54:22] {2637} INFO - Time taken to find the best model: 56.3243203163147
[flaml.automl: 09-18 19:54:22] {2648} WARNING - Time taken to find the best model is 94% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：0.01086098513122069
SO2(0)最好结果：{'pred_time': 7.472877330326271e-05, 'wall_clock_time': 56.3243203163147, 'metric_for_logging': {'pred_time': 7.472877330326271e-05}, 'val_loss': 0.9891390148687793, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 9.319254636764526}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.5826161998535175
SO2(0)的mse=2.686334305955498
SO2(0)的mae=0.9344510811199865
SO2(0)的mar=0.17833228283823632
总共花费的时间为：68.08
黔东南苗族侗族自治州
2591A
[flaml.automl: 09-18 19:57:37] {2390} INFO - task = regression
[flaml.automl: 09-18 19:57:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 19:57:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 19:57:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 19:57:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 19:57:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 19:57:38] {3025} INFO - Estimated sufficient time budget=12009s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 19:57:38] {3072} INFO -  at 1.3s,	estimator xgboost's best error=2.3823,	best estimator xgboost's best error=2.3823
[flaml.automl: 09-18 19:57:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 19:57:40] {3072} INFO -  at 3.1s,	estimator xgboost's best error=1.4347,	best estimator xgboost's best error=1.4347
[flaml.automl: 09-18 19:57:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 19:57:41] {3072} INFO -  at 4.3s,	estimator xgboost's best error=1.4347,	best estimator xgboost's best error=1.4347
[flaml.automl: 09-18 19:57:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 19:57:48] {3072} INFO -  at 11.4s,	estimator xgboost's best error=1.4347,	best estimator xgboost's best error=1.4347
[flaml.automl: 09-18 19:57:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 19:57:49] {3072} INFO -  at 12.5s,	estimator xgboost's best error=1.0323,	best estimator xgboost's best error=1.0323
[flaml.automl: 09-18 19:57:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 19:57:51] {3072} INFO -  at 14.1s,	estimator xgboost's best error=1.0045,	best estimator xgboost's best error=1.0045
[flaml.automl: 09-18 19:57:51] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 19:57:53] {3072} INFO -  at 15.7s,	estimator xgboost's best error=0.9445,	best estimator xgboost's best error=0.9445
[flaml.automl: 09-18 19:57:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 19:57:55] {3072} INFO -  at 18.1s,	estimator xgboost's best error=0.9445,	best estimator xgboost's best error=0.9445
[flaml.automl: 09-18 19:57:55] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 19:57:57] {3072} INFO -  at 19.7s,	estimator xgboost's best error=0.9445,	best estimator xgboost's best error=0.9445
[flaml.automl: 09-18 19:57:57] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 19:57:59] {3072} INFO -  at 22.3s,	estimator xgboost's best error=0.9445,	best estimator xgboost's best error=0.9445
[flaml.automl: 09-18 19:57:59] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 19:58:01] {3072} INFO -  at 24.0s,	estimator xgboost's best error=0.8894,	best estimator xgboost's best error=0.8894
[flaml.automl: 09-18 19:58:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 19:58:02] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.8894,	best estimator xgboost's best error=0.8894
[flaml.automl: 09-18 19:58:02] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 19:58:08] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.8658,	best estimator xgboost's best error=0.8658
[flaml.automl: 09-18 19:58:08] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 19:58:17] {3072} INFO -  at 40.2s,	estimator xgboost's best error=0.8621,	best estimator xgboost's best error=0.8621
[flaml.automl: 09-18 19:58:17] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 19:58:22] {3072} INFO -  at 45.4s,	estimator xgboost's best error=0.8621,	best estimator xgboost's best error=0.8621
[flaml.automl: 09-18 19:58:22] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 19:58:37] {3072} INFO -  at 60.2s,	estimator xgboost's best error=0.8621,	best estimator xgboost's best error=0.8621
[flaml.automl: 09-18 19:59:02] {3335} INFO - retrain xgboost for 25.0s
[flaml.automl: 09-18 19:59:02] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 19:59:02] {2636} INFO - fit succeeded
[flaml.automl: 09-18 19:59:02] {2637} INFO - Time taken to find the best model: 40.18604588508606
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463}
SO2(0)最佳损失：0.1379459407014375
SO2(0)最好结果：{'pred_time': 3.484527173316875e-05, 'wall_clock_time': 40.18604588508606, 'metric_for_logging': {'pred_time': 3.484527173316875e-05}, 'val_loss': 0.8620540592985625, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 6, 'min_child_weight': 6.532042959250319, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8835265175628242, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.10065328594609463}, 'config/n_estimators': 26, 'config/max_leaves': 6, 'config/min_child_weight': 6.532042959250319, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8835265175628242, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.10065328594609463, 'experiment_tag': 'exp', 'time_total_s': 9.245709657669067}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8835265175628242, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=6.532042959250319, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.10065328594609463, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.772535080957586
SO2(0)的mse=2.8040424465605573
SO2(0)的mae=0.788223335690742
SO2(0)的mar=0.21164394647651996
总共花费的时间为：85.45
黔南布依族苗族自治州
2593A
3538A
[flaml.automl: 09-18 20:05:09] {2390} INFO - task = regression
[flaml.automl: 09-18 20:05:09] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:05:09] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:05:09] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:05:09] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:05:09] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:05:10] {3025} INFO - Estimated sufficient time budget=12078s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:05:10] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.2103,	best estimator xgboost's best error=3.2103
[flaml.automl: 09-18 20:05:10] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:05:13] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.4749,	best estimator xgboost's best error=1.4749
[flaml.automl: 09-18 20:05:13] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:05:14] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.4749,	best estimator xgboost's best error=1.4749
[flaml.automl: 09-18 20:05:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:05:23] {3072} INFO -  at 13.8s,	estimator xgboost's best error=1.4749,	best estimator xgboost's best error=1.4749
[flaml.automl: 09-18 20:05:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:05:24] {3072} INFO -  at 15.0s,	estimator xgboost's best error=0.8170,	best estimator xgboost's best error=0.8170
[flaml.automl: 09-18 20:05:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:05:26] {3072} INFO -  at 16.6s,	estimator xgboost's best error=0.7033,	best estimator xgboost's best error=0.7033
[flaml.automl: 09-18 20:05:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:05:27] {3072} INFO -  at 18.2s,	estimator xgboost's best error=0.6532,	best estimator xgboost's best error=0.6532
[flaml.automl: 09-18 20:05:27] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:05:30] {3072} INFO -  at 20.9s,	estimator xgboost's best error=0.6532,	best estimator xgboost's best error=0.6532
[flaml.automl: 09-18 20:05:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:05:32] {3072} INFO -  at 22.5s,	estimator xgboost's best error=0.6532,	best estimator xgboost's best error=0.6532
[flaml.automl: 09-18 20:05:32] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:05:35] {3072} INFO -  at 25.6s,	estimator xgboost's best error=0.6266,	best estimator xgboost's best error=0.6266
[flaml.automl: 09-18 20:05:35] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:05:36] {3072} INFO -  at 27.2s,	estimator xgboost's best error=0.6266,	best estimator xgboost's best error=0.6266
[flaml.automl: 09-18 20:05:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:05:37] {3072} INFO -  at 28.3s,	estimator xgboost's best error=0.6266,	best estimator xgboost's best error=0.6266
[flaml.automl: 09-18 20:05:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:05:49] {3072} INFO -  at 40.3s,	estimator xgboost's best error=0.5581,	best estimator xgboost's best error=0.5581
[flaml.automl: 09-18 20:05:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:06:10] {3072} INFO -  at 60.5s,	estimator xgboost's best error=0.5447,	best estimator xgboost's best error=0.5447
[flaml.automl: 09-18 20:07:07] {3335} INFO - retrain xgboost for 57.5s
[flaml.automl: 09-18 20:07:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:07:07] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:07:07] {2637} INFO - Time taken to find the best model: 60.503947496414185
[flaml.automl: 09-18 20:07:07] {2648} WARNING - Time taken to find the best model is 101% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
SO2(0)最佳损失：0.45531295910750336
SO2(0)最好结果：{'pred_time': 2.882072430114622e-05, 'wall_clock_time': 60.503947496414185, 'metric_for_logging': {'pred_time': 2.882072430114622e-05}, 'val_loss': 0.5446870408924966, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 20.182896852493286}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8331494941482626
SO2(0)的mse=0.7380801213220406
SO2(0)的mae=0.5632732442060641
SO2(0)的mar=0.11608646318831585
总共花费的时间为：118.56
保山市
2594A
2595A
[flaml.automl: 09-18 20:14:28] {2390} INFO - task = regression
[flaml.automl: 09-18 20:14:28] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:14:28] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:14:28] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:14:28] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:14:28] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:14:31] {3025} INFO - Estimated sufficient time budget=32881s. Estimated necessary time budget=33s.
[flaml.automl: 09-18 20:14:31] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.1562,	best estimator xgboost's best error=3.1562
[flaml.automl: 09-18 20:14:31] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:14:38] {3072} INFO -  at 10.7s,	estimator xgboost's best error=1.4172,	best estimator xgboost's best error=1.4172
[flaml.automl: 09-18 20:14:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:14:43] {3072} INFO -  at 15.3s,	estimator xgboost's best error=1.4172,	best estimator xgboost's best error=1.4172
[flaml.automl: 09-18 20:14:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:15:18] {3072} INFO -  at 50.6s,	estimator xgboost's best error=1.4172,	best estimator xgboost's best error=1.4172
[flaml.automl: 09-18 20:15:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:15:23] {3072} INFO -  at 55.0s,	estimator xgboost's best error=0.5889,	best estimator xgboost's best error=0.5889
[flaml.automl: 09-18 20:15:23] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:15:27] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.5889,	best estimator xgboost's best error=0.5889
[flaml.automl: 09-18 20:15:29] {3335} INFO - retrain xgboost for 2.1s
[flaml.automl: 09-18 20:15:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579228, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.125078938571443, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:15:29] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:15:29] {2637} INFO - Time taken to find the best model: 55.03131175041199
[flaml.automl: 09-18 20:15:29] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579228, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.125078938571443}
SO2(0)最佳损失：0.41112636468270924
SO2(0)最好结果：{'pred_time': 8.147491296004944e-05, 'wall_clock_time': 55.03131175041199, 'metric_for_logging': {'pred_time': 8.147491296004944e-05}, 'val_loss': 0.5888736353172908, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579228, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.125078938571443}, 'config/n_estimators': 4, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579228, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.125078938571443, 'experiment_tag': 'exp', 'time_total_s': 4.424854516983032}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579228, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.125078938571443, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.3459618495667226
SO2(0)的mse=0.5902850534515334
SO2(0)的mae=0.571134320929281
SO2(0)的mar=0.10542270522521559
总共花费的时间为：61.58
昭通市
2596A
2597A
[flaml.automl: 09-18 20:22:31] {2390} INFO - task = regression
[flaml.automl: 09-18 20:22:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:22:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:22:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:22:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:22:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:22:33] {3025} INFO - Estimated sufficient time budget=12360s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:22:33] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.9328,	best estimator xgboost's best error=4.9328
[flaml.automl: 09-18 20:22:33] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:22:35] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.3526,	best estimator xgboost's best error=2.3526
[flaml.automl: 09-18 20:22:35] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:22:36] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.3526,	best estimator xgboost's best error=2.3526
[flaml.automl: 09-18 20:22:36] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:22:56] {3072} INFO -  at 24.7s,	estimator xgboost's best error=2.3526,	best estimator xgboost's best error=2.3526
[flaml.automl: 09-18 20:22:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:23:00] {3072} INFO -  at 29.1s,	estimator xgboost's best error=1.6758,	best estimator xgboost's best error=1.6758
[flaml.automl: 09-18 20:23:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:23:06] {3072} INFO -  at 35.2s,	estimator xgboost's best error=1.6254,	best estimator xgboost's best error=1.6254
[flaml.automl: 09-18 20:23:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:23:13] {3072} INFO -  at 41.5s,	estimator xgboost's best error=1.5527,	best estimator xgboost's best error=1.5527
[flaml.automl: 09-18 20:23:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:23:24] {3072} INFO -  at 52.5s,	estimator xgboost's best error=1.5527,	best estimator xgboost's best error=1.5527
[flaml.automl: 09-18 20:23:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:23:30] {3072} INFO -  at 58.8s,	estimator xgboost's best error=1.5527,	best estimator xgboost's best error=1.5527
[flaml.automl: 09-18 20:23:36] {3335} INFO - retrain xgboost for 6.1s
[flaml.automl: 09-18 20:23:36] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:23:36] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:23:36] {2637} INFO - Time taken to find the best model: 41.54028916358948
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：-0.5527074973051866
SO2(0)最好结果：{'pred_time': 8.799304819936682e-05, 'wall_clock_time': 41.54028916358948, 'metric_for_logging': {'pred_time': 8.799304819936682e-05}, 'val_loss': 1.5527074973051866, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 6.327714443206787}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.4162883633846133
SO2(0)的mse=6.824135723206656
SO2(0)的mae=1.622923505030667
SO2(0)的mar=0.19011975185510757
总共花费的时间为：65.49
丽江市
2598A
2599A
2600A
[flaml.automl: 09-18 20:33:22] {2390} INFO - task = regression
[flaml.automl: 09-18 20:33:22] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:33:22] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:33:22] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:33:22] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:33:22] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:33:24] {3025} INFO - Estimated sufficient time budget=12129s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:33:24] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.7580,	best estimator xgboost's best error=3.7580
[flaml.automl: 09-18 20:33:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:33:26] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.6863,	best estimator xgboost's best error=1.6863
[flaml.automl: 09-18 20:33:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:33:27] {3072} INFO -  at 4.7s,	estimator xgboost's best error=1.6863,	best estimator xgboost's best error=1.6863
[flaml.automl: 09-18 20:33:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:33:37] {3072} INFO -  at 14.6s,	estimator xgboost's best error=1.6863,	best estimator xgboost's best error=1.6863
[flaml.automl: 09-18 20:33:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:33:38] {3072} INFO -  at 15.8s,	estimator xgboost's best error=0.7246,	best estimator xgboost's best error=0.7246
[flaml.automl: 09-18 20:33:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:33:40] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.5369,	best estimator xgboost's best error=0.5369
[flaml.automl: 09-18 20:33:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:33:41] {3072} INFO -  at 19.0s,	estimator xgboost's best error=0.4953,	best estimator xgboost's best error=0.4953
[flaml.automl: 09-18 20:33:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:33:44] {3072} INFO -  at 21.7s,	estimator xgboost's best error=0.4953,	best estimator xgboost's best error=0.4953
[flaml.automl: 09-18 20:33:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:33:45] {3072} INFO -  at 23.3s,	estimator xgboost's best error=0.4597,	best estimator xgboost's best error=0.4597
[flaml.automl: 09-18 20:33:45] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:33:49] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.4597,	best estimator xgboost's best error=0.4597
[flaml.automl: 09-18 20:33:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:33:50] {3072} INFO -  at 27.8s,	estimator xgboost's best error=0.4597,	best estimator xgboost's best error=0.4597
[flaml.automl: 09-18 20:33:50] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:33:51] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.4597,	best estimator xgboost's best error=0.4597
[flaml.automl: 09-18 20:33:51] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:33:59] {3072} INFO -  at 36.5s,	estimator xgboost's best error=0.4089,	best estimator xgboost's best error=0.4089
[flaml.automl: 09-18 20:33:59] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 20:34:21] {3072} INFO -  at 59.3s,	estimator xgboost's best error=0.4067,	best estimator xgboost's best error=0.4067
[flaml.automl: 09-18 20:34:52] {3335} INFO - retrain xgboost for 31.0s
[flaml.automl: 09-18 20:34:52] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 20:34:52] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:34:52] {2637} INFO - Time taken to find the best model: 59.30173110961914
[flaml.automl: 09-18 20:34:52] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}
SO2(0)最佳损失：0.5933481591863619
SO2(0)最好结果：{'pred_time': 2.2354912791787986e-05, 'wall_clock_time': 59.30173110961914, 'metric_for_logging': {'pred_time': 2.2354912791787986e-05}, 'val_loss': 0.40665184081363814, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.14406860475445393, 'learning_rate': 0.5408133424638543, 'subsample': 0.8415407805764991, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.7877807806700751, 'reg_alpha': 0.0024496167141905482, 'reg_lambda': 0.8540958305716537}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.14406860475445393, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8415407805764991, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.7877807806700751, 'config/reg_alpha': 0.0024496167141905482, 'config/reg_lambda': 0.8540958305716537, 'experiment_tag': 'exp', 'time_total_s': 22.816651344299316}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.7877807806700751, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.14406860475445393, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0024496167141905482, reg_lambda=0.8540958305716537,
             scale_pos_weight=1, subsample=0.8415407805764991,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8749005653922731
SO2(0)的mse=0.3630494985878538
SO2(0)的mae=0.4125605946129185
SO2(0)的mar=0.07468443135870231
总共花费的时间为：90.94
普洱市
2601A
2602A
[flaml.automl: 09-18 20:41:37] {2390} INFO - task = regression
[flaml.automl: 09-18 20:41:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:41:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:41:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:41:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:41:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:41:38] {3025} INFO - Estimated sufficient time budget=12023s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:41:38] {3072} INFO -  at 1.3s,	estimator xgboost's best error=3.8113,	best estimator xgboost's best error=3.8113
[flaml.automl: 09-18 20:41:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:41:40] {3072} INFO -  at 3.4s,	estimator xgboost's best error=1.7096,	best estimator xgboost's best error=1.7096
[flaml.automl: 09-18 20:41:40] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:41:41] {3072} INFO -  at 4.6s,	estimator xgboost's best error=1.7096,	best estimator xgboost's best error=1.7096
[flaml.automl: 09-18 20:41:41] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:41:58] {3072} INFO -  at 21.1s,	estimator xgboost's best error=1.7096,	best estimator xgboost's best error=1.7096
[flaml.automl: 09-18 20:41:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:42:01] {3072} INFO -  at 24.2s,	estimator xgboost's best error=0.6847,	best estimator xgboost's best error=0.6847
[flaml.automl: 09-18 20:42:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:42:05] {3072} INFO -  at 28.5s,	estimator xgboost's best error=0.4743,	best estimator xgboost's best error=0.4743
[flaml.automl: 09-18 20:42:05] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:42:09] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.4351,	best estimator xgboost's best error=0.4351
[flaml.automl: 09-18 20:42:09] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:42:17] {3072} INFO -  at 40.1s,	estimator xgboost's best error=0.4351,	best estimator xgboost's best error=0.4351
[flaml.automl: 09-18 20:42:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:42:21] {3072} INFO -  at 44.5s,	estimator xgboost's best error=0.3776,	best estimator xgboost's best error=0.3776
[flaml.automl: 09-18 20:42:21] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:42:29] {3072} INFO -  at 52.8s,	estimator xgboost's best error=0.3776,	best estimator xgboost's best error=0.3776
[flaml.automl: 09-18 20:42:29] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:42:34] {3072} INFO -  at 57.3s,	estimator xgboost's best error=0.3268,	best estimator xgboost's best error=0.3268
[flaml.automl: 09-18 20:42:38] {3335} INFO - retrain xgboost for 4.4s
[flaml.automl: 09-18 20:42:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7913434973835253, colsample_bynode=1,
             colsample_bytree=0.7701244418932739, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=4, min_child_weight=24.325922861664836,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0030487135381191033, reg_lambda=23.851820090366814,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:42:38] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:42:38] {2637} INFO - Time taken to find the best model: 57.33550047874451
[flaml.automl: 09-18 20:42:38] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 24.325922861664836, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7913434973835253, 'colsample_bytree': 0.7701244418932739, 'reg_alpha': 0.0030487135381191033, 'reg_lambda': 23.851820090366814}
SO2(0)最佳损失：0.6732453869502264
SO2(0)最好结果：{'pred_time': 4.814504999336403e-05, 'wall_clock_time': 57.33550047874451, 'metric_for_logging': {'pred_time': 4.814504999336403e-05}, 'val_loss': 0.32675461304977366, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 24.325922861664836, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 0.7913434973835253, 'colsample_bytree': 0.7701244418932739, 'reg_alpha': 0.0030487135381191033, 'reg_lambda': 23.851820090366814}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 24.325922861664836, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.7913434973835253, 'config/colsample_bytree': 0.7701244418932739, 'config/reg_alpha': 0.0030487135381191033, 'config/reg_lambda': 23.851820090366814, 'experiment_tag': 'exp', 'time_total_s': 4.5684425830841064}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7913434973835253, colsample_bynode=1,
             colsample_bytree=0.7701244418932739, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=4, min_child_weight=24.325922861664836,
             missing=nan, monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0030487135381191033, reg_lambda=23.851820090366814,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8498976219485144
SO2(0)的mse=0.3834418882788369
SO2(0)的mae=0.33227939046053
SO2(0)的mar=0.05545095449808639
总共花费的时间为：62.24
临沧市
2603A
2604A
[flaml.automl: 09-18 20:49:45] {2390} INFO - task = regression
[flaml.automl: 09-18 20:49:45] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:49:45] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:49:45] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:49:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:49:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:49:47] {3025} INFO - Estimated sufficient time budget=12094s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 20:49:47] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.5652,	best estimator xgboost's best error=4.5652
[flaml.automl: 09-18 20:49:47] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:49:49] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.0424,	best estimator xgboost's best error=2.0424
[flaml.automl: 09-18 20:49:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:49:50] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.0424,	best estimator xgboost's best error=2.0424
[flaml.automl: 09-18 20:49:50] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:50:05] {3072} INFO -  at 19.2s,	estimator xgboost's best error=2.0424,	best estimator xgboost's best error=2.0424
[flaml.automl: 09-18 20:50:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:50:07] {3072} INFO -  at 21.3s,	estimator xgboost's best error=1.0127,	best estimator xgboost's best error=1.0127
[flaml.automl: 09-18 20:50:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:50:10] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.8356,	best estimator xgboost's best error=0.8356
[flaml.automl: 09-18 20:50:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:50:13] {3072} INFO -  at 27.6s,	estimator xgboost's best error=0.8111,	best estimator xgboost's best error=0.8111
[flaml.automl: 09-18 20:50:13] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:50:20] {3072} INFO -  at 34.8s,	estimator xgboost's best error=0.8111,	best estimator xgboost's best error=0.8111
[flaml.automl: 09-18 20:50:20] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:50:25] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.8111,	best estimator xgboost's best error=0.8111
[flaml.automl: 09-18 20:50:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 20:50:33] {3072} INFO -  at 47.5s,	estimator xgboost's best error=0.8111,	best estimator xgboost's best error=0.8111
[flaml.automl: 09-18 20:50:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 20:50:38] {3072} INFO -  at 52.2s,	estimator xgboost's best error=0.7978,	best estimator xgboost's best error=0.7978
[flaml.automl: 09-18 20:50:38] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 20:50:41] {3072} INFO -  at 55.3s,	estimator xgboost's best error=0.7978,	best estimator xgboost's best error=0.7978
[flaml.automl: 09-18 20:50:41] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 20:50:44] {3072} INFO -  at 58.7s,	estimator xgboost's best error=0.7598,	best estimator xgboost's best error=0.7598
[flaml.automl: 09-18 20:51:01] {3335} INFO - retrain xgboost for 17.3s
[flaml.automl: 09-18 20:51:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 20:51:01] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:51:01] {2637} INFO - Time taken to find the best model: 58.66229510307312
[flaml.automl: 09-18 20:51:01] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}
SO2(0)最佳损失：0.24015438712883153
SO2(0)最好结果：{'pred_time': 4.945506237258099e-05, 'wall_clock_time': 58.66229510307312, 'metric_for_logging': {'pred_time': 4.945506237258099e-05}, 'val_loss': 0.7598456128711685, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'experiment_tag': 'exp', 'time_total_s': 3.3539748191833496}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7635200596283662
SO2(0)的mse=1.1349232301728103
SO2(0)的mae=0.7172561890077995
SO2(0)的mar=0.09936139319617426
总共花费的时间为：76.42
楚雄州
2605A
2606A
[flaml.automl: 09-18 20:58:17] {2390} INFO - task = regression
[flaml.automl: 09-18 20:58:17] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 20:58:17] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 20:58:17] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 20:58:17] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 20:58:17] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 20:58:19] {3025} INFO - Estimated sufficient time budget=18798s. Estimated necessary time budget=19s.
[flaml.automl: 09-18 20:58:19] {3072} INFO -  at 2.0s,	estimator xgboost's best error=5.1351,	best estimator xgboost's best error=5.1351
[flaml.automl: 09-18 20:58:19] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 20:58:23] {3072} INFO -  at 5.8s,	estimator xgboost's best error=2.6734,	best estimator xgboost's best error=2.6734
[flaml.automl: 09-18 20:58:23] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 20:58:25] {3072} INFO -  at 8.1s,	estimator xgboost's best error=2.6734,	best estimator xgboost's best error=2.6734
[flaml.automl: 09-18 20:58:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 20:58:50] {3072} INFO -  at 32.5s,	estimator xgboost's best error=2.6734,	best estimator xgboost's best error=2.6734
[flaml.automl: 09-18 20:58:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 20:58:53] {3072} INFO -  at 35.7s,	estimator xgboost's best error=2.0719,	best estimator xgboost's best error=2.0719
[flaml.automl: 09-18 20:58:53] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 20:58:57] {3072} INFO -  at 39.9s,	estimator xgboost's best error=2.0719,	best estimator xgboost's best error=2.0719
[flaml.automl: 09-18 20:58:57] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 20:59:02] {3072} INFO -  at 44.5s,	estimator xgboost's best error=1.9060,	best estimator xgboost's best error=1.9060
[flaml.automl: 09-18 20:59:02] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 20:59:09] {3072} INFO -  at 51.8s,	estimator xgboost's best error=1.9060,	best estimator xgboost's best error=1.9060
[flaml.automl: 09-18 20:59:09] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 20:59:13] {3072} INFO -  at 56.1s,	estimator xgboost's best error=1.9060,	best estimator xgboost's best error=1.9060
[flaml.automl: 09-18 20:59:18] {3335} INFO - retrain xgboost for 4.3s
[flaml.automl: 09-18 20:59:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 20:59:18] {2636} INFO - fit succeeded
[flaml.automl: 09-18 20:59:18] {2637} INFO - Time taken to find the best model: 44.52019166946411
[flaml.automl: 09-18 20:59:18] {2648} WARNING - Time taken to find the best model is 74% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-0.9059697431791958
SO2(0)最好结果：{'pred_time': 6.136433087258673e-05, 'wall_clock_time': 44.52019166946411, 'metric_for_logging': {'pred_time': 6.136433087258673e-05}, 'val_loss': 1.9059697431791958, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.582840204238892}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=-0.026725967863697964
SO2(0)的mse=13.427307372863266
SO2(0)的mae=1.7855589654710557
SO2(0)的mar=0.1891073433295185
总共花费的时间为：61.05
红河州
2609A
3038A
[flaml.automl: 09-18 21:06:44] {2390} INFO - task = regression
[flaml.automl: 09-18 21:06:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:06:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:06:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:06:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:06:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:06:46] {3025} INFO - Estimated sufficient time budget=12059s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:06:46] {3072} INFO -  at 1.3s,	estimator xgboost's best error=5.5713,	best estimator xgboost's best error=5.5713
[flaml.automl: 09-18 21:06:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:06:48] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.8967,	best estimator xgboost's best error=2.8967
[flaml.automl: 09-18 21:06:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:06:49] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.8967,	best estimator xgboost's best error=2.8967
[flaml.automl: 09-18 21:06:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:07:12] {3072} INFO -  at 27.3s,	estimator xgboost's best error=2.8967,	best estimator xgboost's best error=2.8967
[flaml.automl: 09-18 21:07:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:07:15] {3072} INFO -  at 30.4s,	estimator xgboost's best error=2.2858,	best estimator xgboost's best error=2.2858
[flaml.automl: 09-18 21:07:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:07:19] {3072} INFO -  at 34.6s,	estimator xgboost's best error=2.2736,	best estimator xgboost's best error=2.2736
[flaml.automl: 09-18 21:07:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:07:23] {3072} INFO -  at 38.9s,	estimator xgboost's best error=2.1231,	best estimator xgboost's best error=2.1231
[flaml.automl: 09-18 21:07:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:07:30] {3072} INFO -  at 46.1s,	estimator xgboost's best error=2.1231,	best estimator xgboost's best error=2.1231
[flaml.automl: 09-18 21:07:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:07:35] {3072} INFO -  at 50.5s,	estimator xgboost's best error=2.1231,	best estimator xgboost's best error=2.1231
[flaml.automl: 09-18 21:07:35] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:07:43] {3072} INFO -  at 58.8s,	estimator xgboost's best error=1.9572,	best estimator xgboost's best error=1.9572
[flaml.automl: 09-18 21:07:51] {3335} INFO - retrain xgboost for 8.2s
[flaml.automl: 09-18 21:07:51] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 21:07:51] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:07:51] {2637} INFO - Time taken to find the best model: 58.811590909957886
[flaml.automl: 09-18 21:07:51] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-0.9572412715215568
SO2(0)最好结果：{'pred_time': 4.7171120787040166e-05, 'wall_clock_time': 58.811590909957886, 'metric_for_logging': {'pred_time': 4.7171120787040166e-05}, 'val_loss': 1.9572412715215568, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 8.29949951171875}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.18127582817143217
SO2(0)的mse=16.977221413868833
SO2(0)的mae=1.9017160102354849
SO2(0)的mar=0.21016643385954412
总共花费的时间为：67.46
文山州
2610A
2611A
[flaml.automl: 09-18 21:15:11] {2390} INFO - task = regression
[flaml.automl: 09-18 21:15:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:15:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:15:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:15:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:15:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:15:13] {3025} INFO - Estimated sufficient time budget=21862s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 21:15:13] {3072} INFO -  at 2.3s,	estimator xgboost's best error=3.2431,	best estimator xgboost's best error=3.2431
[flaml.automl: 09-18 21:15:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:15:17] {3072} INFO -  at 6.3s,	estimator xgboost's best error=1.4648,	best estimator xgboost's best error=1.4648
[flaml.automl: 09-18 21:15:17] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:15:20] {3072} INFO -  at 8.7s,	estimator xgboost's best error=1.4648,	best estimator xgboost's best error=1.4648
[flaml.automl: 09-18 21:15:20] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:15:47] {3072} INFO -  at 36.2s,	estimator xgboost's best error=1.4648,	best estimator xgboost's best error=1.4648
[flaml.automl: 09-18 21:15:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:15:52] {3072} INFO -  at 40.7s,	estimator xgboost's best error=0.7203,	best estimator xgboost's best error=0.7203
[flaml.automl: 09-18 21:15:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:15:58] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.5814,	best estimator xgboost's best error=0.5814
[flaml.automl: 09-18 21:15:58] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:16:04] {3072} INFO -  at 52.5s,	estimator xgboost's best error=0.5814,	best estimator xgboost's best error=0.5814
[flaml.automl: 09-18 21:16:04] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:16:09] {3072} INFO -  at 57.6s,	estimator xgboost's best error=0.5814,	best estimator xgboost's best error=0.5814
[flaml.automl: 09-18 21:16:15] {3335} INFO - retrain xgboost for 5.8s
[flaml.automl: 09-18 21:16:15] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 21:16:15] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:16:15] {2637} INFO - Time taken to find the best model: 46.43253803253174
[flaml.automl: 09-18 21:16:15] {2648} WARNING - Time taken to find the best model is 77% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
SO2(0)最佳损失：0.4186309482993149
SO2(0)最好结果：{'pred_time': 5.002990970766641e-05, 'wall_clock_time': 46.43253803253174, 'metric_for_logging': {'pred_time': 5.002990970766641e-05}, 'val_loss': 0.5813690517006851, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 5.744668006896973}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.33469192907911305
SO2(0)的mse=0.9225644994106269
SO2(0)的mae=0.5732157711052781
SO2(0)的mar=0.09969318688697726
总共花费的时间为：63.96
西双版纳州
2612A
2613A
[flaml.automl: 09-18 21:22:58] {2390} INFO - task = regression
[flaml.automl: 09-18 21:22:58] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:22:58] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:22:58] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:22:58] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:22:58] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:22:59] {3025} INFO - Estimated sufficient time budget=12264s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 21:22:59] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.5749,	best estimator xgboost's best error=4.5749
[flaml.automl: 09-18 21:22:59] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:23:01] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.0249,	best estimator xgboost's best error=2.0249
[flaml.automl: 09-18 21:23:01] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:23:02] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.0249,	best estimator xgboost's best error=2.0249
[flaml.automl: 09-18 21:23:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:23:12] {3072} INFO -  at 14.1s,	estimator xgboost's best error=2.0249,	best estimator xgboost's best error=2.0249
[flaml.automl: 09-18 21:23:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:23:13] {3072} INFO -  at 15.3s,	estimator xgboost's best error=0.8133,	best estimator xgboost's best error=0.8133
[flaml.automl: 09-18 21:23:13] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:23:14] {3072} INFO -  at 16.9s,	estimator xgboost's best error=0.5710,	best estimator xgboost's best error=0.5710
[flaml.automl: 09-18 21:23:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:23:16] {3072} INFO -  at 18.5s,	estimator xgboost's best error=0.5156,	best estimator xgboost's best error=0.5156
[flaml.automl: 09-18 21:23:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:23:19] {3072} INFO -  at 21.2s,	estimator xgboost's best error=0.5156,	best estimator xgboost's best error=0.5156
[flaml.automl: 09-18 21:23:19] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:23:20] {3072} INFO -  at 22.8s,	estimator xgboost's best error=0.4627,	best estimator xgboost's best error=0.4627
[flaml.automl: 09-18 21:23:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:23:26] {3072} INFO -  at 28.2s,	estimator xgboost's best error=0.4164,	best estimator xgboost's best error=0.4164
[flaml.automl: 09-18 21:23:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 21:23:28] {3072} INFO -  at 30.2s,	estimator xgboost's best error=0.4164,	best estimator xgboost's best error=0.4164
[flaml.automl: 09-18 21:23:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 21:23:57] {3072} INFO -  at 59.9s,	estimator xgboost's best error=0.4139,	best estimator xgboost's best error=0.4139
[flaml.automl: 09-18 21:24:38] {3335} INFO - retrain xgboost for 40.6s
[flaml.automl: 09-18 21:24:38] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=15, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:24:38] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:24:38] {2637} INFO - Time taken to find the best model: 59.945056438446045
[flaml.automl: 09-18 21:24:38] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}
SO2(0)最佳损失：0.586145981337673
SO2(0)最好结果：{'pred_time': 7.079047409673945e-05, 'wall_clock_time': 59.945056438446045, 'metric_for_logging': {'pred_time': 7.079047409673945e-05}, 'val_loss': 0.413854018662327, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.012098504396622672, 'learning_rate': 1.0, 'subsample': 0.85991418709772, 'colsample_bylevel': 0.7900689136755311, 'colsample_bytree': 0.7406139630685445, 'reg_alpha': 0.014775872958659165, 'reg_lambda': 2.7388176684942858}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.012098504396622672, 'config/learning_rate': 1.0, 'config/subsample': 0.85991418709772, 'config/colsample_bylevel': 0.7900689136755311, 'config/colsample_bytree': 0.7406139630685445, 'config/reg_alpha': 0.014775872958659165, 'config/reg_lambda': 2.7388176684942858, 'experiment_tag': 'exp', 'time_total_s': 29.696478128433228}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7900689136755311, colsample_bynode=1,
             colsample_bytree=0.7406139630685445, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=15, min_child_weight=0.012098504396622672,
             missing=nan, monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.014775872958659165, reg_lambda=2.7388176684942858,
             scale_pos_weight=1, subsample=0.85991418709772, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.9003847098607752
SO2(0)的mse=0.3506342548025915
SO2(0)的mae=0.3951938752518653
SO2(0)的mar=0.05632517579884417
总共花费的时间为：101.09
大理州
2614A
2615A
[flaml.automl: 09-18 21:31:31] {2390} INFO - task = regression
[flaml.automl: 09-18 21:31:31] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:31:31] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:31:31] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:31:31] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:31:31] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:31:35] {3025} INFO - Estimated sufficient time budget=33786s. Estimated necessary time budget=34s.
[flaml.automl: 09-18 21:31:35] {3072} INFO -  at 3.5s,	estimator xgboost's best error=3.3976,	best estimator xgboost's best error=3.3976
[flaml.automl: 09-18 21:31:35] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:31:42] {3072} INFO -  at 10.5s,	estimator xgboost's best error=1.5430,	best estimator xgboost's best error=1.5430
[flaml.automl: 09-18 21:31:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:31:46] {3072} INFO -  at 15.1s,	estimator xgboost's best error=1.5430,	best estimator xgboost's best error=1.5430
[flaml.automl: 09-18 21:31:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:32:20] {3072} INFO -  at 49.3s,	estimator xgboost's best error=1.5430,	best estimator xgboost's best error=1.5430
[flaml.automl: 09-18 21:32:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:32:24] {3072} INFO -  at 52.4s,	estimator xgboost's best error=0.6597,	best estimator xgboost's best error=0.6597
[flaml.automl: 09-18 21:32:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:32:26] {3072} INFO -  at 55.4s,	estimator xgboost's best error=0.4896,	best estimator xgboost's best error=0.4896
[flaml.automl: 09-18 21:32:26] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:32:29] {3072} INFO -  at 58.2s,	estimator xgboost's best error=0.4405,	best estimator xgboost's best error=0.4405
[flaml.automl: 09-18 21:32:32] {3335} INFO - retrain xgboost for 2.8s
[flaml.automl: 09-18 21:32:32] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:32:32] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:32:32] {2637} INFO - Time taken to find the best model: 58.16683626174927
[flaml.automl: 09-18 21:32:32] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：0.5594504736393906
SO2(0)最好结果：{'pred_time': 2.5152007386532645e-05, 'wall_clock_time': 58.16683626174927, 'metric_for_logging': {'pred_time': 2.5152007386532645e-05}, 'val_loss': 0.4405495263606094, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 2.8040685653686523}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8484672884573968
SO2(0)的mse=0.3610820922687155
SO2(0)的mae=0.4249805488648667
SO2(0)的mar=0.08193767436783787
总共花费的时间为：61.46
德宏州
2616A
[flaml.automl: 09-18 21:36:20] {2390} INFO - task = regression
[flaml.automl: 09-18 21:36:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:36:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:36:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:36:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:36:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:36:22] {3025} INFO - Estimated sufficient time budget=22535s. Estimated necessary time budget=23s.
[flaml.automl: 09-18 21:36:22] {3072} INFO -  at 2.3s,	estimator xgboost's best error=4.3497,	best estimator xgboost's best error=4.3497
[flaml.automl: 09-18 21:36:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:36:26] {3072} INFO -  at 5.8s,	estimator xgboost's best error=2.7545,	best estimator xgboost's best error=2.7545
[flaml.automl: 09-18 21:36:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:36:28] {3072} INFO -  at 8.1s,	estimator xgboost's best error=2.7545,	best estimator xgboost's best error=2.7545
[flaml.automl: 09-18 21:36:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:36:47] {3072} INFO -  at 27.1s,	estimator xgboost's best error=2.7545,	best estimator xgboost's best error=2.7545
[flaml.automl: 09-18 21:36:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:36:50] {3072} INFO -  at 30.2s,	estimator xgboost's best error=2.2675,	best estimator xgboost's best error=2.2675
[flaml.automl: 09-18 21:36:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:36:54] {3072} INFO -  at 34.3s,	estimator xgboost's best error=2.2675,	best estimator xgboost's best error=2.2675
[flaml.automl: 09-18 21:36:54] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:36:59] {3072} INFO -  at 38.9s,	estimator xgboost's best error=2.1873,	best estimator xgboost's best error=2.1873
[flaml.automl: 09-18 21:36:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:37:05] {3072} INFO -  at 45.3s,	estimator xgboost's best error=2.1873,	best estimator xgboost's best error=2.1873
[flaml.automl: 09-18 21:37:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:37:10] {3072} INFO -  at 49.7s,	estimator xgboost's best error=2.1873,	best estimator xgboost's best error=2.1873
[flaml.automl: 09-18 21:37:10] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:37:17] {3072} INFO -  at 56.7s,	estimator xgboost's best error=2.1817,	best estimator xgboost's best error=2.1817
[flaml.automl: 09-18 21:37:22] {3335} INFO - retrain xgboost for 5.8s
[flaml.automl: 09-18 21:37:22] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 21:37:22] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:37:22] {2637} INFO - Time taken to find the best model: 56.74706983566284
[flaml.automl: 09-18 21:37:22] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}
SO2(0)最佳损失：-1.1816984316906023
SO2(0)最好结果：{'pred_time': 8.978708735052144e-05, 'wall_clock_time': 56.74706983566284, 'metric_for_logging': {'pred_time': 8.978708735052144e-05}, 'val_loss': 2.1816984316906023, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}, 'config/n_estimators': 6, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'experiment_tag': 'exp', 'time_total_s': 7.00340461730957}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=-0.09660491041996644
SO2(0)的mse=17.169763638551952
SO2(0)的mae=2.298217001704522
SO2(0)的mar=0.3247808890052707
总共花费的时间为：62.88
怒江州
2618A
2619A
[flaml.automl: 09-18 21:44:25] {2390} INFO - task = regression
[flaml.automl: 09-18 21:44:25] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:44:25] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:44:25] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:44:25] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:44:25] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:44:27] {3025} INFO - Estimated sufficient time budget=22417s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 21:44:27] {3072} INFO -  at 2.3s,	estimator xgboost's best error=4.6966,	best estimator xgboost's best error=4.6966
[flaml.automl: 09-18 21:44:27] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:44:31] {3072} INFO -  at 6.3s,	estimator xgboost's best error=2.1262,	best estimator xgboost's best error=2.1262
[flaml.automl: 09-18 21:44:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:44:33] {3072} INFO -  at 8.5s,	estimator xgboost's best error=2.1262,	best estimator xgboost's best error=2.1262
[flaml.automl: 09-18 21:44:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:44:58] {3072} INFO -  at 33.3s,	estimator xgboost's best error=2.1262,	best estimator xgboost's best error=2.1262
[flaml.automl: 09-18 21:44:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:45:03] {3072} INFO -  at 37.8s,	estimator xgboost's best error=0.9448,	best estimator xgboost's best error=0.9448
[flaml.automl: 09-18 21:45:03] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:45:09] {3072} INFO -  at 43.9s,	estimator xgboost's best error=0.6833,	best estimator xgboost's best error=0.6833
[flaml.automl: 09-18 21:45:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:45:15] {3072} INFO -  at 50.2s,	estimator xgboost's best error=0.6250,	best estimator xgboost's best error=0.6250
[flaml.automl: 09-18 21:45:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:45:24] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.6250,	best estimator xgboost's best error=0.6250
[flaml.automl: 09-18 21:45:29] {3335} INFO - retrain xgboost for 5.2s
[flaml.automl: 09-18 21:45:29] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:45:29] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:45:29] {2637} INFO - Time taken to find the best model: 50.24133038520813
[flaml.automl: 09-18 21:45:29] {2648} WARNING - Time taken to find the best model is 84% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：0.3749500155694706
SO2(0)最好结果：{'pred_time': 7.376238242867067e-05, 'wall_clock_time': 50.24133038520813, 'metric_for_logging': {'pred_time': 7.376238242867067e-05}, 'val_loss': 0.6250499844305294, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 6.340175151824951}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8768281583855848
SO2(0)的mse=0.8358170856314303
SO2(0)的mae=0.6168754576752049
SO2(0)的mar=0.11114070764814223
总共花费的时间为：65.05
迪庆州
迪庆州没有数据
昌都市
2622A
[flaml.automl: 09-18 21:49:10] {2390} INFO - task = regression
[flaml.automl: 09-18 21:49:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:49:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:49:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:49:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:49:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:49:13] {3025} INFO - Estimated sufficient time budget=22065s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 21:49:13] {3072} INFO -  at 2.3s,	estimator xgboost's best error=5.0627,	best estimator xgboost's best error=5.0627
[flaml.automl: 09-18 21:49:13] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:49:16] {3072} INFO -  at 5.8s,	estimator xgboost's best error=2.7162,	best estimator xgboost's best error=2.7162
[flaml.automl: 09-18 21:49:16] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:49:18] {3072} INFO -  at 7.9s,	estimator xgboost's best error=2.7162,	best estimator xgboost's best error=2.7162
[flaml.automl: 09-18 21:49:18] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:49:32] {3072} INFO -  at 21.3s,	estimator xgboost's best error=2.7162,	best estimator xgboost's best error=2.7162
[flaml.automl: 09-18 21:49:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:49:36] {3072} INFO -  at 25.4s,	estimator xgboost's best error=1.4327,	best estimator xgboost's best error=1.4327
[flaml.automl: 09-18 21:49:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:49:42] {3072} INFO -  at 31.3s,	estimator xgboost's best error=1.3046,	best estimator xgboost's best error=1.3046
[flaml.automl: 09-18 21:49:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:49:48] {3072} INFO -  at 37.8s,	estimator xgboost's best error=1.3046,	best estimator xgboost's best error=1.3046
[flaml.automl: 09-18 21:49:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:49:57] {3072} INFO -  at 46.8s,	estimator xgboost's best error=1.3046,	best estimator xgboost's best error=1.3046
[flaml.automl: 09-18 21:49:57] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:49:59] {3072} INFO -  at 49.1s,	estimator xgboost's best error=1.3046,	best estimator xgboost's best error=1.3046
[flaml.automl: 09-18 21:49:59] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 21:50:08] {3072} INFO -  at 57.8s,	estimator xgboost's best error=1.3046,	best estimator xgboost's best error=1.3046
[flaml.automl: 09-18 21:50:14] {3335} INFO - retrain xgboost for 6.3s
[flaml.automl: 09-18 21:50:14] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 21:50:14] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:50:14] {2637} INFO - Time taken to find the best model: 31.329891204833984
SO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
SO2(0)最佳损失：-0.3045918234932441
SO2(0)最好结果：{'pred_time': 0.00014082962283997453, 'wall_clock_time': 31.329891204833984, 'metric_for_logging': {'pred_time': 0.00014082962283997453}, 'val_loss': 1.304591823493244, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 5.957305669784546}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.30841297024158776
SO2(0)的mse=2.830466916601098
SO2(0)的mae=1.310203819574201
SO2(0)的mar=0.1871316561030053
总共花费的时间为：64.55
山南市
2624A
2625A
[flaml.automl: 09-18 21:57:12] {2390} INFO - task = regression
[flaml.automl: 09-18 21:57:12] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 21:57:12] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 21:57:12] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 21:57:12] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 21:57:12] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 21:57:14] {3025} INFO - Estimated sufficient time budget=22149s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 21:57:14] {3072} INFO -  at 2.4s,	estimator xgboost's best error=3.6168,	best estimator xgboost's best error=3.6168
[flaml.automl: 09-18 21:57:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 21:57:18] {3072} INFO -  at 6.4s,	estimator xgboost's best error=1.6144,	best estimator xgboost's best error=1.6144
[flaml.automl: 09-18 21:57:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 21:57:21] {3072} INFO -  at 8.7s,	estimator xgboost's best error=1.6144,	best estimator xgboost's best error=1.6144
[flaml.automl: 09-18 21:57:21] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 21:57:37] {3072} INFO -  at 25.0s,	estimator xgboost's best error=1.6144,	best estimator xgboost's best error=1.6144
[flaml.automl: 09-18 21:57:37] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 21:57:41] {3072} INFO -  at 29.1s,	estimator xgboost's best error=0.7409,	best estimator xgboost's best error=0.7409
[flaml.automl: 09-18 21:57:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 21:57:47] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.5822,	best estimator xgboost's best error=0.5822
[flaml.automl: 09-18 21:57:47] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 21:57:54] {3072} INFO -  at 41.5s,	estimator xgboost's best error=0.5712,	best estimator xgboost's best error=0.5712
[flaml.automl: 09-18 21:57:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 21:58:03] {3072} INFO -  at 50.9s,	estimator xgboost's best error=0.5712,	best estimator xgboost's best error=0.5712
[flaml.automl: 09-18 21:58:03] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 21:58:09] {3072} INFO -  at 57.0s,	estimator xgboost's best error=0.5634,	best estimator xgboost's best error=0.5634
[flaml.automl: 09-18 21:58:13] {3335} INFO - retrain xgboost for 4.3s
[flaml.automl: 09-18 21:58:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 21:58:13] {2636} INFO - fit succeeded
[flaml.automl: 09-18 21:58:13] {2637} INFO - Time taken to find the best model: 57.00619196891785
[flaml.automl: 09-18 21:58:13] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}
SO2(0)最佳损失：0.4365680410889561
SO2(0)最好结果：{'pred_time': 4.492141226561244e-05, 'wall_clock_time': 57.00619196891785, 'metric_for_logging': {'pred_time': 4.492141226561244e-05}, 'val_loss': 0.5634319589110439, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 1.1765273389893034, 'config/learning_rate': 0.8338182546732649, 'config/subsample': 0.9114052575858262, 'config/colsample_bylevel': 0.7232930776213671, 'config/colsample_bytree': 0.8046603913292237, 'config/reg_alpha': 0.00473280911734172, 'config/reg_lambda': 15.090081382165623, 'experiment_tag': 'exp', 'time_total_s': 6.0774900913238525}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.6736213132050409
SO2(0)的mse=0.7821457324505651
SO2(0)的mae=0.559357043632981
SO2(0)的mar=0.0928004009036307
总共花费的时间为：61.88
日喀则市
2626A
[flaml.automl: 09-18 22:01:44] {2390} INFO - task = regression
[flaml.automl: 09-18 22:01:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:01:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:01:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:01:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:01:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:01:45] {3025} INFO - Estimated sufficient time budget=14521s. Estimated necessary time budget=15s.
[flaml.automl: 09-18 22:01:45] {3072} INFO -  at 1.5s,	estimator xgboost's best error=2.3826,	best estimator xgboost's best error=2.3826
[flaml.automl: 09-18 22:01:45] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:01:49] {3072} INFO -  at 4.9s,	estimator xgboost's best error=1.3511,	best estimator xgboost's best error=1.3511
[flaml.automl: 09-18 22:01:49] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:01:51] {3072} INFO -  at 7.1s,	estimator xgboost's best error=1.3511,	best estimator xgboost's best error=1.3511
[flaml.automl: 09-18 22:01:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:02:03] {3072} INFO -  at 19.3s,	estimator xgboost's best error=1.3511,	best estimator xgboost's best error=1.3511
[flaml.automl: 09-18 22:02:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:02:05] {3072} INFO -  at 21.3s,	estimator xgboost's best error=0.8537,	best estimator xgboost's best error=0.8537
[flaml.automl: 09-18 22:02:05] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:02:08] {3072} INFO -  at 24.3s,	estimator xgboost's best error=0.8140,	best estimator xgboost's best error=0.8140
[flaml.automl: 09-18 22:02:08] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:02:11] {3072} INFO -  at 27.3s,	estimator xgboost's best error=0.8140,	best estimator xgboost's best error=0.8140
[flaml.automl: 09-18 22:02:11] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:02:16] {3072} INFO -  at 31.7s,	estimator xgboost's best error=0.8140,	best estimator xgboost's best error=0.8140
[flaml.automl: 09-18 22:02:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:02:18] {3072} INFO -  at 34.5s,	estimator xgboost's best error=0.8140,	best estimator xgboost's best error=0.8140
[flaml.automl: 09-18 22:02:18] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:02:28] {3072} INFO -  at 43.8s,	estimator xgboost's best error=0.8140,	best estimator xgboost's best error=0.8140
[flaml.automl: 09-18 22:02:28] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:02:32] {3072} INFO -  at 48.4s,	estimator xgboost's best error=0.8140,	best estimator xgboost's best error=0.8140
[flaml.automl: 09-18 22:02:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:02:37] {3072} INFO -  at 53.3s,	estimator xgboost's best error=0.8140,	best estimator xgboost's best error=0.8140
[flaml.automl: 09-18 22:02:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:02:43] {3072} INFO -  at 59.5s,	estimator xgboost's best error=0.8140,	best estimator xgboost's best error=0.8140
[flaml.automl: 09-18 22:02:47] {3335} INFO - retrain xgboost for 3.4s
[flaml.automl: 09-18 22:02:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:02:47] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:02:47] {2637} INFO - Time taken to find the best model: 24.262036085128784
SO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
SO2(0)最佳损失：0.185973011485992
SO2(0)最好结果：{'pred_time': 5.7162200250933246e-05, 'wall_clock_time': 24.262036085128784, 'metric_for_logging': {'pred_time': 5.7162200250933246e-05}, 'val_loss': 0.814026988514008, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 2.9308459758758545}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=-0.6132197305864517
SO2(0)的mse=1.2682275629895958
SO2(0)的mae=0.8143487445501872
SO2(0)的mar=0.22651500609152503
总共花费的时间为：63.14
那曲地区
2628A
[flaml.automl: 09-18 22:06:01] {2390} INFO - task = regression
[flaml.automl: 09-18 22:06:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:06:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:06:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:06:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:06:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:06:04] {3025} INFO - Estimated sufficient time budget=33925s. Estimated necessary time budget=34s.
[flaml.automl: 09-18 22:06:04] {3072} INFO -  at 3.4s,	estimator xgboost's best error=5.2264,	best estimator xgboost's best error=5.2264
[flaml.automl: 09-18 22:06:04] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:06:09] {3072} INFO -  at 8.7s,	estimator xgboost's best error=2.7396,	best estimator xgboost's best error=2.7396
[flaml.automl: 09-18 22:06:09] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:06:13] {3072} INFO -  at 12.2s,	estimator xgboost's best error=2.7396,	best estimator xgboost's best error=2.7396
[flaml.automl: 09-18 22:06:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:06:32] {3072} INFO -  at 31.8s,	estimator xgboost's best error=2.7396,	best estimator xgboost's best error=2.7396
[flaml.automl: 09-18 22:06:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:06:36] {3072} INFO -  at 35.0s,	estimator xgboost's best error=1.2873,	best estimator xgboost's best error=1.2873
[flaml.automl: 09-18 22:06:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:06:40] {3072} INFO -  at 39.6s,	estimator xgboost's best error=1.1271,	best estimator xgboost's best error=1.1271
[flaml.automl: 09-18 22:06:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:06:44] {3072} INFO -  at 43.2s,	estimator xgboost's best error=1.1271,	best estimator xgboost's best error=1.1271
[flaml.automl: 09-18 22:06:44] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:06:48] {3072} INFO -  at 47.0s,	estimator xgboost's best error=1.1271,	best estimator xgboost's best error=1.1271
[flaml.automl: 09-18 22:06:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:06:50] {3072} INFO -  at 49.7s,	estimator xgboost's best error=1.1271,	best estimator xgboost's best error=1.1271
[flaml.automl: 09-18 22:06:50] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:06:57] {3072} INFO -  at 56.3s,	estimator xgboost's best error=1.1271,	best estimator xgboost's best error=1.1271
[flaml.automl: 09-18 22:07:01] {3335} INFO - retrain xgboost for 4.1s
[flaml.automl: 09-18 22:07:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:07:01] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:07:01] {2637} INFO - Time taken to find the best model: 39.5948805809021
SO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
SO2(0)最佳损失：-0.1271141323772922
SO2(0)最好结果：{'pred_time': 0.00010268455072808872, 'wall_clock_time': 39.5948805809021, 'metric_for_logging': {'pred_time': 0.00010268455072808872}, 'val_loss': 1.1271141323772922, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 4.560944557189941}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.5716552448101713
SO2(0)的mse=2.9447719060147985
SO2(0)的mae=1.133047148673492
SO2(0)的mar=0.13281550861317368
总共花费的时间为：60.73
阿里地区
2630A
[flaml.automl: 09-18 22:10:53] {2390} INFO - task = regression
[flaml.automl: 09-18 22:10:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:10:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:10:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:10:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:10:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:10:58] {3025} INFO - Estimated sufficient time budget=42417s. Estimated necessary time budget=42s.
[flaml.automl: 09-18 22:10:58] {3072} INFO -  at 4.3s,	estimator xgboost's best error=5.0799,	best estimator xgboost's best error=5.0799
[flaml.automl: 09-18 22:10:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:11:05] {3072} INFO -  at 11.3s,	estimator xgboost's best error=2.7250,	best estimator xgboost's best error=2.7250
[flaml.automl: 09-18 22:11:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:11:09] {3072} INFO -  at 15.5s,	estimator xgboost's best error=2.7250,	best estimator xgboost's best error=2.7250
[flaml.automl: 09-18 22:11:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:11:32] {3072} INFO -  at 38.4s,	estimator xgboost's best error=2.7250,	best estimator xgboost's best error=2.7250
[flaml.automl: 09-18 22:11:32] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:11:34] {3072} INFO -  at 40.3s,	estimator xgboost's best error=1.3924,	best estimator xgboost's best error=1.3924
[flaml.automl: 09-18 22:11:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:11:36] {3072} INFO -  at 43.1s,	estimator xgboost's best error=1.2433,	best estimator xgboost's best error=1.2433
[flaml.automl: 09-18 22:11:36] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:11:39] {3072} INFO -  at 45.8s,	estimator xgboost's best error=1.2204,	best estimator xgboost's best error=1.2204
[flaml.automl: 09-18 22:11:39] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:11:44] {3072} INFO -  at 50.3s,	estimator xgboost's best error=1.2204,	best estimator xgboost's best error=1.2204
[flaml.automl: 09-18 22:11:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:11:48] {3072} INFO -  at 54.7s,	estimator xgboost's best error=1.2204,	best estimator xgboost's best error=1.2204
[flaml.automl: 09-18 22:11:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:11:53] {3072} INFO -  at 59.4s,	estimator xgboost's best error=1.2204,	best estimator xgboost's best error=1.2204
[flaml.automl: 09-18 22:11:57] {3335} INFO - retrain xgboost for 4.3s
[flaml.automl: 09-18 22:11:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:11:57] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:11:57] {2637} INFO - Time taken to find the best model: 45.814526081085205
[flaml.automl: 09-18 22:11:57] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：-0.2204340838735015
SO2(0)最好结果：{'pred_time': 5.8235568804952473e-05, 'wall_clock_time': 45.814526081085205, 'metric_for_logging': {'pred_time': 5.8235568804952473e-05}, 'val_loss': 1.2204340838735015, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 2.7585227489471436}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.014382074547989565
SO2(0)的mse=4.65660940900221
SO2(0)的mae=1.1455169133927763
SO2(0)的mar=0.12780355925734393
总共花费的时间为：64.08
林芝市
2632A
2633A
[flaml.automl: 09-18 22:18:33] {2390} INFO - task = regression
[flaml.automl: 09-18 22:18:33] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:18:33] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:18:33] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:18:33] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:18:33] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:18:38] {3025} INFO - Estimated sufficient time budget=41525s. Estimated necessary time budget=42s.
[flaml.automl: 09-18 22:18:38] {3072} INFO -  at 4.4s,	estimator xgboost's best error=5.3208,	best estimator xgboost's best error=5.3208
[flaml.automl: 09-18 22:18:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:18:45] {3072} INFO -  at 12.2s,	estimator xgboost's best error=2.3555,	best estimator xgboost's best error=2.3555
[flaml.automl: 09-18 22:18:45] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:18:49] {3072} INFO -  at 16.4s,	estimator xgboost's best error=2.3555,	best estimator xgboost's best error=2.3555
[flaml.automl: 09-18 22:18:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:19:08] {3072} INFO -  at 34.8s,	estimator xgboost's best error=2.3555,	best estimator xgboost's best error=2.3555
[flaml.automl: 09-18 22:19:08] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:19:11] {3072} INFO -  at 37.8s,	estimator xgboost's best error=0.8160,	best estimator xgboost's best error=0.8160
[flaml.automl: 09-18 22:19:11] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:19:15] {3072} INFO -  at 41.8s,	estimator xgboost's best error=0.5132,	best estimator xgboost's best error=0.5132
[flaml.automl: 09-18 22:19:15] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:19:19] {3072} INFO -  at 46.1s,	estimator xgboost's best error=0.4419,	best estimator xgboost's best error=0.4419
[flaml.automl: 09-18 22:19:19] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:19:26] {3072} INFO -  at 53.4s,	estimator xgboost's best error=0.4419,	best estimator xgboost's best error=0.4419
[flaml.automl: 09-18 22:19:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:19:31] {3072} INFO -  at 57.5s,	estimator xgboost's best error=0.3613,	best estimator xgboost's best error=0.3613
[flaml.automl: 09-18 22:19:34] {3335} INFO - retrain xgboost for 3.1s
[flaml.automl: 09-18 22:19:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:19:34] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:19:34] {2637} INFO - Time taken to find the best model: 57.5138099193573
[flaml.automl: 09-18 22:19:34] {2648} WARNING - Time taken to find the best model is 96% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}
SO2(0)最佳损失：0.6386944567697769
SO2(0)最好结果：{'pred_time': 4.1797243315598064e-05, 'wall_clock_time': 57.5138099193573, 'metric_for_logging': {'pred_time': 4.1797243315598064e-05}, 'val_loss': 0.3613055432302231, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 1.1765273389893034, 'config/learning_rate': 0.8338182546732649, 'config/subsample': 0.9114052575858262, 'config/colsample_bylevel': 0.7232930776213671, 'config/colsample_bytree': 0.8046603913292237, 'config/reg_alpha': 0.00473280911734172, 'config/reg_lambda': 15.090081382165623, 'experiment_tag': 'exp', 'time_total_s': 4.1137917041778564}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8281217177123783
SO2(0)的mse=0.292814076290751
SO2(0)的mae=0.3748227936284043
SO2(0)的mar=0.04492868239147417
总共花费的时间为：61.20
汉中市
2634A
2635A
2636A
2637A
[flaml.automl: 09-18 22:32:55] {2390} INFO - task = regression
[flaml.automl: 09-18 22:32:55] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:32:55] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:32:55] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:32:55] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:32:55] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:32:56] {3025} INFO - Estimated sufficient time budget=50472s. Estimated necessary time budget=50s.
[flaml.automl: 09-18 22:32:56] {3072} INFO -  at 1.4s,	estimator xgboost's best error=3.9816,	best estimator xgboost's best error=3.9816
[flaml.automl: 09-18 22:32:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:32:58] {3072} INFO -  at 3.7s,	estimator xgboost's best error=1.9033,	best estimator xgboost's best error=1.9033
[flaml.automl: 09-18 22:32:58] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:33:00] {3072} INFO -  at 5.9s,	estimator xgboost's best error=1.9033,	best estimator xgboost's best error=1.9033
[flaml.automl: 09-18 22:33:00] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:33:07] {3072} INFO -  at 12.0s,	estimator xgboost's best error=1.9033,	best estimator xgboost's best error=1.9033
[flaml.automl: 09-18 22:33:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:33:10] {3072} INFO -  at 15.0s,	estimator xgboost's best error=1.3550,	best estimator xgboost's best error=1.3550
[flaml.automl: 09-18 22:33:10] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:33:14] {3072} INFO -  at 19.0s,	estimator xgboost's best error=1.1174,	best estimator xgboost's best error=1.1174
[flaml.automl: 09-18 22:33:14] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:33:18] {3072} INFO -  at 23.3s,	estimator xgboost's best error=1.1174,	best estimator xgboost's best error=1.1174
[flaml.automl: 09-18 22:33:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:33:21] {3072} INFO -  at 26.8s,	estimator xgboost's best error=1.1174,	best estimator xgboost's best error=1.1174
[flaml.automl: 09-18 22:33:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:33:24] {3072} INFO -  at 29.6s,	estimator xgboost's best error=1.1174,	best estimator xgboost's best error=1.1174
[flaml.automl: 09-18 22:33:24] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:33:26] {3072} INFO -  at 31.6s,	estimator xgboost's best error=1.1174,	best estimator xgboost's best error=1.1174
[flaml.automl: 09-18 22:33:26] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:33:30] {3072} INFO -  at 35.8s,	estimator xgboost's best error=1.1174,	best estimator xgboost's best error=1.1174
[flaml.automl: 09-18 22:33:30] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:33:34] {3072} INFO -  at 39.0s,	estimator xgboost's best error=1.1174,	best estimator xgboost's best error=1.1174
[flaml.automl: 09-18 22:33:34] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:33:51] {3072} INFO -  at 56.3s,	estimator xgboost's best error=1.1174,	best estimator xgboost's best error=1.1174
[flaml.automl: 09-18 22:33:55] {3335} INFO - retrain xgboost for 4.2s
[flaml.automl: 09-18 22:33:55] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:33:55] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:33:55] {2637} INFO - Time taken to find the best model: 18.996376514434814
SO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953, 'FLAML_sample_size': 10000}
SO2(0)最佳损失：-0.11744212740836302
SO2(0)最好结果：{'pred_time': 2.4026020651162566e-05, 'wall_clock_time': 18.996376514434814, 'metric_for_logging': {'pred_time': 2.4026020651162566e-05}, 'val_loss': 1.117442127408363, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953, 'FLAML_sample_size': 10000}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'config/FLAML_sample_size': 10000, 'experiment_tag': 'exp', 'time_total_s': 4.013581275939941}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=-0.00782254247992964
SO2(0)的mse=5.957663920044313
SO2(0)的mae=1.3599754134322612
SO2(0)的mar=0.2121458024715235
总共花费的时间为：61.35
榆林市
2638A
2639A
2640A
2641A
[flaml.automl: 09-18 22:46:42] {2390} INFO - task = regression
[flaml.automl: 09-18 22:46:42] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:46:42] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:46:42] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:46:42] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:46:42] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:46:44] {3025} INFO - Estimated sufficient time budget=50896s. Estimated necessary time budget=51s.
[flaml.automl: 09-18 22:46:44] {3072} INFO -  at 1.4s,	estimator xgboost's best error=5.3372,	best estimator xgboost's best error=5.3372
[flaml.automl: 09-18 22:46:44] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:46:46] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.5232,	best estimator xgboost's best error=2.5232
[flaml.automl: 09-18 22:46:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:46:47] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.5232,	best estimator xgboost's best error=2.5232
[flaml.automl: 09-18 22:46:47] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:46:53] {3072} INFO -  at 10.9s,	estimator xgboost's best error=2.5232,	best estimator xgboost's best error=2.5232
[flaml.automl: 09-18 22:46:53] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:46:54] {3072} INFO -  at 12.0s,	estimator xgboost's best error=1.6145,	best estimator xgboost's best error=1.6145
[flaml.automl: 09-18 22:46:54] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:46:56] {3072} INFO -  at 13.6s,	estimator xgboost's best error=1.6145,	best estimator xgboost's best error=1.6145
[flaml.automl: 09-18 22:46:56] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:46:58] {3072} INFO -  at 15.3s,	estimator xgboost's best error=1.3157,	best estimator xgboost's best error=1.3157
[flaml.automl: 09-18 22:46:58] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:47:00] {3072} INFO -  at 18.0s,	estimator xgboost's best error=1.3157,	best estimator xgboost's best error=1.3157
[flaml.automl: 09-18 22:47:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:47:02] {3072} INFO -  at 19.6s,	estimator xgboost's best error=1.3157,	best estimator xgboost's best error=1.3157
[flaml.automl: 09-18 22:47:02] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:47:05] {3072} INFO -  at 22.6s,	estimator xgboost's best error=1.3157,	best estimator xgboost's best error=1.3157
[flaml.automl: 09-18 22:47:05] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:47:06] {3072} INFO -  at 24.1s,	estimator xgboost's best error=1.3049,	best estimator xgboost's best error=1.3049
[flaml.automl: 09-18 22:47:06] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:47:07] {3072} INFO -  at 25.2s,	estimator xgboost's best error=1.3049,	best estimator xgboost's best error=1.3049
[flaml.automl: 09-18 22:47:07] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:47:11] {3072} INFO -  at 29.0s,	estimator xgboost's best error=1.2797,	best estimator xgboost's best error=1.2797
[flaml.automl: 09-18 22:47:11] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:47:15] {3072} INFO -  at 32.5s,	estimator xgboost's best error=1.2466,	best estimator xgboost's best error=1.2466
[flaml.automl: 09-18 22:47:15] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 22:47:18] {3072} INFO -  at 35.3s,	estimator xgboost's best error=1.2466,	best estimator xgboost's best error=1.2466
[flaml.automl: 09-18 22:47:18] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 22:47:20] {3072} INFO -  at 37.5s,	estimator xgboost's best error=1.2466,	best estimator xgboost's best error=1.2466
[flaml.automl: 09-18 22:47:20] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 22:47:22] {3072} INFO -  at 40.1s,	estimator xgboost's best error=1.2466,	best estimator xgboost's best error=1.2466
[flaml.automl: 09-18 22:47:22] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 22:47:24] {3072} INFO -  at 42.2s,	estimator xgboost's best error=1.2466,	best estimator xgboost's best error=1.2466
[flaml.automl: 09-18 22:47:24] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 22:47:40] {3072} INFO -  at 57.3s,	estimator xgboost's best error=1.2273,	best estimator xgboost's best error=1.2273
[flaml.automl: 09-18 22:47:58] {3335} INFO - retrain xgboost for 18.8s
[flaml.automl: 09-18 22:47:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 22:47:58] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:47:58] {2637} INFO - Time taken to find the best model: 57.29202318191528
[flaml.automl: 09-18 22:47:58] {2648} WARNING - Time taken to find the best model is 95% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 42192}
SO2(0)最佳损失：-0.2273381375178367
SO2(0)最好结果：{'pred_time': 1.816450596160791e-05, 'wall_clock_time': 57.29202318191528, 'metric_for_logging': {'pred_time': 1.816450596160791e-05}, 'val_loss': 1.2273381375178367, 'training_iteration': 1, 'config': {'n_estimators': 26, 'max_leaves': 7, 'min_child_weight': 12.790818651227058, 'learning_rate': 0.5408133424638543, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9485844399049015, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.018213572742442938, 'FLAML_sample_size': 42192}, 'config/n_estimators': 26, 'config/max_leaves': 7, 'config/min_child_weight': 12.790818651227058, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9485844399049015, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.018213572742442938, 'config/FLAML_sample_size': 42192, 'experiment_tag': 'exp', 'time_total_s': 15.111777067184448}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9485844399049015, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=12.790818651227058, missing=nan,
             monotone_constraints='()', n_estimators=26, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.018213572742442938, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7486579193871985
SO2(0)的mse=3.585215349094411
SO2(0)的mae=1.1489005844934954
SO2(0)的mar=0.13049547391914731
总共花费的时间为：76.83
安康市
2642A
2643A
2644A
[flaml.automl: 09-18 22:57:48] {2390} INFO - task = regression
[flaml.automl: 09-18 22:57:48] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 22:57:48] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 22:57:48] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 22:57:48] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 22:57:48] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 22:57:50] {3025} INFO - Estimated sufficient time budget=12210s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 22:57:50] {3072} INFO -  at 1.3s,	estimator xgboost's best error=5.0695,	best estimator xgboost's best error=5.0695
[flaml.automl: 09-18 22:57:50] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 22:57:52] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.5808,	best estimator xgboost's best error=2.5808
[flaml.automl: 09-18 22:57:52] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 22:57:53] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.5808,	best estimator xgboost's best error=2.5808
[flaml.automl: 09-18 22:57:53] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 22:58:03] {3072} INFO -  at 14.7s,	estimator xgboost's best error=2.5808,	best estimator xgboost's best error=2.5808
[flaml.automl: 09-18 22:58:03] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 22:58:04] {3072} INFO -  at 15.9s,	estimator xgboost's best error=2.1750,	best estimator xgboost's best error=2.1750
[flaml.automl: 09-18 22:58:04] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 22:58:06] {3072} INFO -  at 17.5s,	estimator xgboost's best error=2.1750,	best estimator xgboost's best error=2.1750
[flaml.automl: 09-18 22:58:06] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 22:58:07] {3072} INFO -  at 19.2s,	estimator xgboost's best error=1.9850,	best estimator xgboost's best error=1.9850
[flaml.automl: 09-18 22:58:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 22:58:10] {3072} INFO -  at 21.9s,	estimator xgboost's best error=1.9850,	best estimator xgboost's best error=1.9850
[flaml.automl: 09-18 22:58:10] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 22:58:12] {3072} INFO -  at 23.5s,	estimator xgboost's best error=1.9850,	best estimator xgboost's best error=1.9850
[flaml.automl: 09-18 22:58:12] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 22:58:15] {3072} INFO -  at 26.5s,	estimator xgboost's best error=1.9484,	best estimator xgboost's best error=1.9484
[flaml.automl: 09-18 22:58:15] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 22:58:16] {3072} INFO -  at 28.3s,	estimator xgboost's best error=1.9484,	best estimator xgboost's best error=1.9484
[flaml.automl: 09-18 22:58:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 22:58:18] {3072} INFO -  at 29.4s,	estimator xgboost's best error=1.9484,	best estimator xgboost's best error=1.9484
[flaml.automl: 09-18 22:58:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 22:58:35] {3072} INFO -  at 46.6s,	estimator xgboost's best error=1.9484,	best estimator xgboost's best error=1.9484
[flaml.automl: 09-18 22:58:35] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 22:58:44] {3072} INFO -  at 55.8s,	estimator xgboost's best error=1.9484,	best estimator xgboost's best error=1.9484
[flaml.automl: 09-18 22:58:44] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 22:58:47] {3072} INFO -  at 59.1s,	estimator xgboost's best error=1.9484,	best estimator xgboost's best error=1.9484
[flaml.automl: 09-18 22:58:53] {3335} INFO - retrain xgboost for 5.6s
[flaml.automl: 09-18 22:58:53] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 22:58:53] {2636} INFO - fit succeeded
[flaml.automl: 09-18 22:58:53] {2637} INFO - Time taken to find the best model: 26.531489372253418
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}
SO2(0)最佳损失：-0.9484185167736292
SO2(0)最好结果：{'pred_time': 1.1628365565504566e-05, 'wall_clock_time': 26.531489372253418, 'metric_for_logging': {'pred_time': 1.1628365565504566e-05}, 'val_loss': 1.9484185167736292, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'experiment_tag': 'exp', 'time_total_s': 3.0345404148101807}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=-0.6065205043934885
SO2(0)的mse=16.224404805920212
SO2(0)的mae=1.9387474807471947
SO2(0)的mar=0.18840305437461974
总共花费的时间为：65.30
商洛市
2645A
[flaml.automl: 09-18 23:02:27] {2390} INFO - task = regression
[flaml.automl: 09-18 23:02:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:02:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:02:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:02:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:02:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:02:28] {3025} INFO - Estimated sufficient time budget=12148s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 23:02:28] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.5308,	best estimator xgboost's best error=4.5308
[flaml.automl: 09-18 23:02:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:02:30] {3072} INFO -  at 3.1s,	estimator xgboost's best error=2.5306,	best estimator xgboost's best error=2.5306
[flaml.automl: 09-18 23:02:30] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:02:31] {3072} INFO -  at 4.3s,	estimator xgboost's best error=2.5306,	best estimator xgboost's best error=2.5306
[flaml.automl: 09-18 23:02:31] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:02:38] {3072} INFO -  at 11.4s,	estimator xgboost's best error=2.5306,	best estimator xgboost's best error=2.5306
[flaml.automl: 09-18 23:02:38] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:02:39] {3072} INFO -  at 12.5s,	estimator xgboost's best error=1.8521,	best estimator xgboost's best error=1.8521
[flaml.automl: 09-18 23:02:39] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:02:41] {3072} INFO -  at 14.1s,	estimator xgboost's best error=1.8521,	best estimator xgboost's best error=1.8521
[flaml.automl: 09-18 23:02:41] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:02:42] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.7476,	best estimator xgboost's best error=1.7476
[flaml.automl: 09-18 23:02:42] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:02:45] {3072} INFO -  at 18.1s,	estimator xgboost's best error=1.7476,	best estimator xgboost's best error=1.7476
[flaml.automl: 09-18 23:02:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:02:46] {3072} INFO -  at 19.8s,	estimator xgboost's best error=1.7476,	best estimator xgboost's best error=1.7476
[flaml.automl: 09-18 23:02:46] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:02:49] {3072} INFO -  at 22.4s,	estimator xgboost's best error=1.7476,	best estimator xgboost's best error=1.7476
[flaml.automl: 09-18 23:02:49] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:02:51] {3072} INFO -  at 24.1s,	estimator xgboost's best error=1.7476,	best estimator xgboost's best error=1.7476
[flaml.automl: 09-18 23:02:51] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:02:52] {3072} INFO -  at 25.2s,	estimator xgboost's best error=1.7476,	best estimator xgboost's best error=1.7476
[flaml.automl: 09-18 23:02:52] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:02:58] {3072} INFO -  at 31.1s,	estimator xgboost's best error=1.7476,	best estimator xgboost's best error=1.7476
[flaml.automl: 09-18 23:02:58] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 23:03:01] {3072} INFO -  at 34.4s,	estimator xgboost's best error=1.7375,	best estimator xgboost's best error=1.7375
[flaml.automl: 09-18 23:03:01] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 23:03:03] {3072} INFO -  at 36.4s,	estimator xgboost's best error=1.7313,	best estimator xgboost's best error=1.7313
[flaml.automl: 09-18 23:03:03] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-18 23:03:06] {3072} INFO -  at 39.4s,	estimator xgboost's best error=1.7313,	best estimator xgboost's best error=1.7313
[flaml.automl: 09-18 23:03:06] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-18 23:03:08] {3072} INFO -  at 41.0s,	estimator xgboost's best error=1.7313,	best estimator xgboost's best error=1.7313
[flaml.automl: 09-18 23:03:08] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-18 23:03:09] {3072} INFO -  at 42.4s,	estimator xgboost's best error=1.7313,	best estimator xgboost's best error=1.7313
[flaml.automl: 09-18 23:03:09] {2897} INFO - iteration 18, current learner xgboost
[flaml.automl: 09-18 23:03:13] {3072} INFO -  at 46.7s,	estimator xgboost's best error=1.7313,	best estimator xgboost's best error=1.7313
[flaml.automl: 09-18 23:03:13] {2897} INFO - iteration 19, current learner xgboost
[flaml.automl: 09-18 23:03:14] {3072} INFO -  at 47.8s,	estimator xgboost's best error=1.7313,	best estimator xgboost's best error=1.7313
[flaml.automl: 09-18 23:03:14] {2897} INFO - iteration 20, current learner xgboost
[flaml.automl: 09-18 23:03:20] {3072} INFO -  at 53.6s,	estimator xgboost's best error=1.7313,	best estimator xgboost's best error=1.7313
[flaml.automl: 09-18 23:03:20] {2897} INFO - iteration 21, current learner xgboost
[flaml.automl: 09-18 23:03:26] {3072} INFO -  at 59.7s,	estimator xgboost's best error=1.7313,	best estimator xgboost's best error=1.7313
[flaml.automl: 09-18 23:03:30] {3335} INFO - retrain xgboost for 3.7s
[flaml.automl: 09-18 23:03:30] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8415627790092132, colsample_bynode=1,
             colsample_bytree=0.9182972329625196, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.36545241515621474,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003794483647048939, reg_lambda=0.20358706520158557,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:03:30] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:03:30] {2637} INFO - Time taken to find the best model: 36.39743995666504
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 5, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.36545241515621474, 'subsample': 1.0, 'colsample_bylevel': 0.8415627790092132, 'colsample_bytree': 0.9182972329625196, 'reg_alpha': 0.003794483647048939, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-0.7312749330378476
SO2(0)最好结果：{'pred_time': 3.2456950219706565e-05, 'wall_clock_time': 36.39743995666504, 'metric_for_logging': {'pred_time': 3.2456950219706565e-05}, 'val_loss': 1.7312749330378476, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 5, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.36545241515621474, 'subsample': 1.0, 'colsample_bylevel': 0.8415627790092132, 'colsample_bytree': 0.9182972329625196, 'reg_alpha': 0.003794483647048939, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 5, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.36545241515621474, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8415627790092132, 'config/colsample_bytree': 0.9182972329625196, 'config/reg_alpha': 0.003794483647048939, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 2.031261920928955}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8415627790092132, colsample_bynode=1,
             colsample_bytree=0.9182972329625196, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.36545241515621474,
             max_delta_step=0, max_depth=0, max_leaves=5,
             min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003794483647048939, reg_lambda=0.20358706520158557,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=-0.12066422071243221
SO2(0)的mse=9.474158494523756
SO2(0)的mae=1.80273157775446
SO2(0)的mar=0.22655620175853103
总共花费的时间为：63.72
白银市
2647A
2648A
[flaml.automl: 09-18 23:10:35] {2390} INFO - task = regression
[flaml.automl: 09-18 23:10:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:10:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:10:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:10:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:10:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:10:36] {3025} INFO - Estimated sufficient time budget=12011s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 23:10:36] {3072} INFO -  at 1.3s,	estimator xgboost's best error=14.7918,	best estimator xgboost's best error=14.7918
[flaml.automl: 09-18 23:10:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:10:38] {3072} INFO -  at 3.4s,	estimator xgboost's best error=10.3690,	best estimator xgboost's best error=10.3690
[flaml.automl: 09-18 23:10:38] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:10:39] {3072} INFO -  at 4.6s,	estimator xgboost's best error=10.3690,	best estimator xgboost's best error=10.3690
[flaml.automl: 09-18 23:10:39] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:10:50] {3072} INFO -  at 14.9s,	estimator xgboost's best error=10.3690,	best estimator xgboost's best error=10.3690
[flaml.automl: 09-18 23:10:50] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:10:52] {3072} INFO -  at 17.0s,	estimator xgboost's best error=10.3690,	best estimator xgboost's best error=10.3690
[flaml.automl: 09-18 23:10:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:10:55] {3072} INFO -  at 20.6s,	estimator xgboost's best error=10.3690,	best estimator xgboost's best error=10.3690
[flaml.automl: 09-18 23:10:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:11:01] {3072} INFO -  at 26.7s,	estimator xgboost's best error=10.1941,	best estimator xgboost's best error=10.1941
[flaml.automl: 09-18 23:11:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:11:12] {3072} INFO -  at 36.8s,	estimator xgboost's best error=10.1941,	best estimator xgboost's best error=10.1941
[flaml.automl: 09-18 23:11:12] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:11:20] {3072} INFO -  at 45.2s,	estimator xgboost's best error=10.1941,	best estimator xgboost's best error=10.1941
[flaml.automl: 09-18 23:11:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:11:34] {3072} INFO -  at 59.7s,	estimator xgboost's best error=10.1941,	best estimator xgboost's best error=10.1941
[flaml.automl: 09-18 23:11:43] {3335} INFO - retrain xgboost for 8.6s
[flaml.automl: 09-18 23:11:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9200207147793842, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.11780822872254525, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=4.522075448387905, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.7456495681908629, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:11:43] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:11:43] {2637} INFO - Time taken to find the best model: 26.654671669006348
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 4, 'min_child_weight': 4.522075448387905, 'learning_rate': 0.11780822872254525, 'subsample': 1.0, 'colsample_bylevel': 0.9200207147793842, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.7456495681908629}
SO2(0)最佳损失：-9.194088487956453
SO2(0)最好结果：{'pred_time': 3.0059702678822533e-05, 'wall_clock_time': 26.654671669006348, 'metric_for_logging': {'pred_time': 3.0059702678822533e-05}, 'val_loss': 10.194088487956453, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 4, 'min_child_weight': 4.522075448387905, 'learning_rate': 0.11780822872254525, 'subsample': 1.0, 'colsample_bylevel': 0.9200207147793842, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.7456495681908629}, 'config/n_estimators': 12, 'config/max_leaves': 4, 'config/min_child_weight': 4.522075448387905, 'config/learning_rate': 0.11780822872254525, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.9200207147793842, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.7456495681908629, 'experiment_tag': 'exp', 'time_total_s': 6.0432844161987305}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9200207147793842, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.11780822872254525, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=4.522075448387905, missing=nan,
             monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.7456495681908629, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=-1.0069948883739355
SO2(0)的mse=417.8456460418111
SO2(0)的mae=10.332788402392142
SO2(0)的mar=0.5303367106091726
总共花费的时间为：68.88
天水市
2649A
2650A
2651A
[flaml.automl: 09-18 23:21:37] {2390} INFO - task = regression
[flaml.automl: 09-18 23:21:37] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:21:37] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:21:37] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:21:37] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:21:37] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:21:40] {3025} INFO - Estimated sufficient time budget=23211s. Estimated necessary time budget=23s.
[flaml.automl: 09-18 23:21:40] {3072} INFO -  at 2.5s,	estimator xgboost's best error=5.5530,	best estimator xgboost's best error=5.5530
[flaml.automl: 09-18 23:21:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:21:44] {3072} INFO -  at 6.4s,	estimator xgboost's best error=2.5699,	best estimator xgboost's best error=2.5699
[flaml.automl: 09-18 23:21:44] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:21:46] {3072} INFO -  at 8.5s,	estimator xgboost's best error=2.5699,	best estimator xgboost's best error=2.5699
[flaml.automl: 09-18 23:21:46] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:22:05] {3072} INFO -  at 27.8s,	estimator xgboost's best error=2.5699,	best estimator xgboost's best error=2.5699
[flaml.automl: 09-18 23:22:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:22:08] {3072} INFO -  at 31.1s,	estimator xgboost's best error=1.5040,	best estimator xgboost's best error=1.5040
[flaml.automl: 09-18 23:22:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:22:13] {3072} INFO -  at 35.7s,	estimator xgboost's best error=1.3343,	best estimator xgboost's best error=1.3343
[flaml.automl: 09-18 23:22:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:22:18] {3072} INFO -  at 40.3s,	estimator xgboost's best error=1.3063,	best estimator xgboost's best error=1.3063
[flaml.automl: 09-18 23:22:18] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:22:25] {3072} INFO -  at 47.6s,	estimator xgboost's best error=1.3063,	best estimator xgboost's best error=1.3063
[flaml.automl: 09-18 23:22:25] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:22:28] {3072} INFO -  at 51.0s,	estimator xgboost's best error=1.3027,	best estimator xgboost's best error=1.3027
[flaml.automl: 09-18 23:22:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:22:35] {3072} INFO -  at 58.1s,	estimator xgboost's best error=1.2994,	best estimator xgboost's best error=1.2994
[flaml.automl: 09-18 23:22:43] {3335} INFO - retrain xgboost for 7.9s
[flaml.automl: 09-18 23:22:43] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:22:43] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:22:43] {2637} INFO - Time taken to find the best model: 58.074599266052246
[flaml.automl: 09-18 23:22:43] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}
SO2(0)最佳损失：-0.299441466001543
SO2(0)最好结果：{'pred_time': 3.303011155765083e-05, 'wall_clock_time': 58.074599266052246, 'metric_for_logging': {'pred_time': 3.303011155765083e-05}, 'val_loss': 1.299441466001543, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.056902942069697794, 'config/learning_rate': 0.6286550459839113, 'config/subsample': 0.8219353140149964, 'config/colsample_bylevel': 0.6552426578592088, 'config/colsample_bytree': 0.8391963407651736, 'config/reg_alpha': 0.007347191482940116, 'config/reg_lambda': 9.546883854467293, 'experiment_tag': 'exp', 'time_total_s': 7.117427110671997}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6539494374711271
SO2(0)的mse=3.5504709071350558
SO2(0)的mae=1.3296109077542326
SO2(0)的mar=0.1642845988357122
总共花费的时间为：66.69
武威市
2652A
[flaml.automl: 09-18 23:26:00] {2390} INFO - task = regression
[flaml.automl: 09-18 23:26:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:26:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:26:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:26:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:26:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:26:01] {3025} INFO - Estimated sufficient time budget=11938s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 23:26:01] {3072} INFO -  at 1.2s,	estimator xgboost's best error=3.5037,	best estimator xgboost's best error=3.5037
[flaml.automl: 09-18 23:26:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:26:03] {3072} INFO -  at 3.1s,	estimator xgboost's best error=1.8844,	best estimator xgboost's best error=1.8844
[flaml.automl: 09-18 23:26:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:26:04] {3072} INFO -  at 4.3s,	estimator xgboost's best error=1.8844,	best estimator xgboost's best error=1.8844
[flaml.automl: 09-18 23:26:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:26:11] {3072} INFO -  at 11.2s,	estimator xgboost's best error=1.8844,	best estimator xgboost's best error=1.8844
[flaml.automl: 09-18 23:26:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:26:12] {3072} INFO -  at 12.3s,	estimator xgboost's best error=0.9613,	best estimator xgboost's best error=0.9613
[flaml.automl: 09-18 23:26:12] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:26:13] {3072} INFO -  at 13.9s,	estimator xgboost's best error=0.9054,	best estimator xgboost's best error=0.9054
[flaml.automl: 09-18 23:26:13] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:26:15] {3072} INFO -  at 15.5s,	estimator xgboost's best error=0.8865,	best estimator xgboost's best error=0.8865
[flaml.automl: 09-18 23:26:15] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:26:17] {3072} INFO -  at 17.8s,	estimator xgboost's best error=0.8865,	best estimator xgboost's best error=0.8865
[flaml.automl: 09-18 23:26:17] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:26:20] {3072} INFO -  at 20.0s,	estimator xgboost's best error=0.8865,	best estimator xgboost's best error=0.8865
[flaml.automl: 09-18 23:26:20] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:26:24] {3072} INFO -  at 24.8s,	estimator xgboost's best error=0.8865,	best estimator xgboost's best error=0.8865
[flaml.automl: 09-18 23:26:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:26:28] {3072} INFO -  at 28.1s,	estimator xgboost's best error=0.8865,	best estimator xgboost's best error=0.8865
[flaml.automl: 09-18 23:26:28] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:26:30] {3072} INFO -  at 30.2s,	estimator xgboost's best error=0.8865,	best estimator xgboost's best error=0.8865
[flaml.automl: 09-18 23:26:30] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:26:40] {3072} INFO -  at 40.8s,	estimator xgboost's best error=0.8865,	best estimator xgboost's best error=0.8865
[flaml.automl: 09-18 23:26:40] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 23:26:48] {3072} INFO -  at 48.9s,	estimator xgboost's best error=0.8723,	best estimator xgboost's best error=0.8723
[flaml.automl: 09-18 23:26:48] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-18 23:26:54] {3072} INFO -  at 54.2s,	estimator xgboost's best error=0.8723,	best estimator xgboost's best error=0.8723
[flaml.automl: 09-18 23:27:03] {3335} INFO - retrain xgboost for 8.9s
[flaml.automl: 09-18 23:27:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9564417338648162, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2406995205234967, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=1.4858835173589837, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.22197115035378334, scale_pos_weight=1,
             subsample=0.8536409652116742, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:27:03] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:27:03] {2637} INFO - Time taken to find the best model: 48.8821120262146
[flaml.automl: 09-18 23:27:03] {2648} WARNING - Time taken to find the best model is 81% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 4, 'min_child_weight': 1.4858835173589837, 'learning_rate': 0.2406995205234967, 'subsample': 0.8536409652116742, 'colsample_bylevel': 0.9564417338648162, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.22197115035378334}
SO2(0)最佳损失：0.1277112830094932
SO2(0)最好结果：{'pred_time': 9.360211960812832e-05, 'wall_clock_time': 48.8821120262146, 'metric_for_logging': {'pred_time': 9.360211960812832e-05}, 'val_loss': 0.8722887169905068, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 4, 'min_child_weight': 1.4858835173589837, 'learning_rate': 0.2406995205234967, 'subsample': 0.8536409652116742, 'colsample_bylevel': 0.9564417338648162, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.22197115035378334}, 'config/n_estimators': 13, 'config/max_leaves': 4, 'config/min_child_weight': 1.4858835173589837, 'config/learning_rate': 0.2406995205234967, 'config/subsample': 0.8536409652116742, 'config/colsample_bylevel': 0.9564417338648162, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.22197115035378334, 'experiment_tag': 'exp', 'time_total_s': 8.061644792556763}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9564417338648162, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2406995205234967, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=1.4858835173589837, missing=nan,
             monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.22197115035378334, scale_pos_weight=1,
             subsample=0.8536409652116742, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.24145288954201816
SO2(0)的mse=2.2123045122167517
SO2(0)的mae=0.9167964154736459
SO2(0)的mar=0.1695040616192593
总共花费的时间为：63.47
张掖市
2654A
2655A
[flaml.automl: 09-18 23:33:35] {2390} INFO - task = regression
[flaml.automl: 09-18 23:33:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:33:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:33:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:33:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:33:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:33:36] {3025} INFO - Estimated sufficient time budget=12052s. Estimated necessary time budget=12s.
[flaml.automl: 09-18 23:33:36] {3072} INFO -  at 1.3s,	estimator xgboost's best error=5.0686,	best estimator xgboost's best error=5.0686
[flaml.automl: 09-18 23:33:36] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:33:39] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.3679,	best estimator xgboost's best error=2.3679
[flaml.automl: 09-18 23:33:39] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:33:40] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.3679,	best estimator xgboost's best error=2.3679
[flaml.automl: 09-18 23:33:40] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:33:49] {3072} INFO -  at 14.1s,	estimator xgboost's best error=2.3679,	best estimator xgboost's best error=2.3679
[flaml.automl: 09-18 23:33:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:33:50] {3072} INFO -  at 15.2s,	estimator xgboost's best error=1.4985,	best estimator xgboost's best error=1.4985
[flaml.automl: 09-18 23:33:50] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:33:52] {3072} INFO -  at 16.8s,	estimator xgboost's best error=1.3770,	best estimator xgboost's best error=1.3770
[flaml.automl: 09-18 23:33:52] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:33:54] {3072} INFO -  at 18.4s,	estimator xgboost's best error=1.2645,	best estimator xgboost's best error=1.2645
[flaml.automl: 09-18 23:33:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:33:56] {3072} INFO -  at 21.1s,	estimator xgboost's best error=1.2645,	best estimator xgboost's best error=1.2645
[flaml.automl: 09-18 23:33:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:33:58] {3072} INFO -  at 22.8s,	estimator xgboost's best error=1.2645,	best estimator xgboost's best error=1.2645
[flaml.automl: 09-18 23:33:58] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-18 23:34:01] {3072} INFO -  at 25.8s,	estimator xgboost's best error=1.2163,	best estimator xgboost's best error=1.2163
[flaml.automl: 09-18 23:34:01] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-18 23:34:03] {3072} INFO -  at 27.4s,	estimator xgboost's best error=1.2163,	best estimator xgboost's best error=1.2163
[flaml.automl: 09-18 23:34:03] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-18 23:34:04] {3072} INFO -  at 28.6s,	estimator xgboost's best error=1.2163,	best estimator xgboost's best error=1.2163
[flaml.automl: 09-18 23:34:04] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-18 23:34:16] {3072} INFO -  at 40.6s,	estimator xgboost's best error=1.1810,	best estimator xgboost's best error=1.1810
[flaml.automl: 09-18 23:34:16] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-18 23:34:35] {3072} INFO -  at 60.2s,	estimator xgboost's best error=1.1543,	best estimator xgboost's best error=1.1543
[flaml.automl: 09-18 23:35:28] {3335} INFO - retrain xgboost for 52.9s
[flaml.automl: 09-18 23:35:28] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:35:28] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:35:28] {2637} INFO - Time taken to find the best model: 60.242308616638184
[flaml.automl: 09-18 23:35:28] {2648} WARNING - Time taken to find the best model is 100% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}
SO2(0)最佳损失：-0.15427132904819385
SO2(0)最好结果：{'pred_time': 2.3089409647343416e-05, 'wall_clock_time': 60.242308616638184, 'metric_for_logging': {'pred_time': 2.3089409647343416e-05}, 'val_loss': 1.1542713290481939, 'training_iteration': 1, 'config': {'n_estimators': 32, 'max_leaves': 12, 'min_child_weight': 0.015279685963846733, 'learning_rate': 0.4430733351694866, 'subsample': 0.8021498947235678, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.9525984164347239, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040287291571763306}, 'config/n_estimators': 32, 'config/max_leaves': 12, 'config/min_child_weight': 0.015279685963846733, 'config/learning_rate': 0.4430733351694866, 'config/subsample': 0.8021498947235678, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.9525984164347239, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040287291571763306, 'experiment_tag': 'exp', 'time_total_s': 19.680352449417114}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.9525984164347239, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4430733351694866,
             max_delta_step=0, max_depth=0, max_leaves=12,
             min_child_weight=0.015279685963846733, missing=nan,
             monotone_constraints='()', n_estimators=32, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040287291571763306, scale_pos_weight=1,
             subsample=0.8021498947235678, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7266663935230002
SO2(0)的mse=2.77316600816899
SO2(0)的mae=1.1115097535256484
SO2(0)的mar=0.15578070107331057
总共花费的时间为：113.67
平凉市
2656A
2657A
[flaml.automl: 09-18 23:42:35] {2390} INFO - task = regression
[flaml.automl: 09-18 23:42:35] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:42:35] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:42:35] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:42:35] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:42:35] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:42:38] {3025} INFO - Estimated sufficient time budget=22258s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 23:42:38] {3072} INFO -  at 2.3s,	estimator xgboost's best error=3.5353,	best estimator xgboost's best error=3.5353
[flaml.automl: 09-18 23:42:38] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:42:42] {3072} INFO -  at 6.7s,	estimator xgboost's best error=1.8879,	best estimator xgboost's best error=1.8879
[flaml.automl: 09-18 23:42:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:42:45] {3072} INFO -  at 9.8s,	estimator xgboost's best error=1.8879,	best estimator xgboost's best error=1.8879
[flaml.automl: 09-18 23:42:45] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:43:11] {3072} INFO -  at 35.4s,	estimator xgboost's best error=1.8879,	best estimator xgboost's best error=1.8879
[flaml.automl: 09-18 23:43:11] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:43:14] {3072} INFO -  at 38.5s,	estimator xgboost's best error=1.5512,	best estimator xgboost's best error=1.5512
[flaml.automl: 09-18 23:43:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:43:18] {3072} INFO -  at 42.8s,	estimator xgboost's best error=1.5512,	best estimator xgboost's best error=1.5512
[flaml.automl: 09-18 23:43:18] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:43:23] {3072} INFO -  at 47.4s,	estimator xgboost's best error=1.3889,	best estimator xgboost's best error=1.3889
[flaml.automl: 09-18 23:43:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:43:30] {3072} INFO -  at 54.6s,	estimator xgboost's best error=1.3889,	best estimator xgboost's best error=1.3889
[flaml.automl: 09-18 23:43:30] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:43:34] {3072} INFO -  at 58.2s,	estimator xgboost's best error=1.3889,	best estimator xgboost's best error=1.3889
[flaml.automl: 09-18 23:43:37] {3335} INFO - retrain xgboost for 3.1s
[flaml.automl: 09-18 23:43:37] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:43:37] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:43:37] {2637} INFO - Time taken to find the best model: 47.40874671936035
[flaml.automl: 09-18 23:43:37] {2648} WARNING - Time taken to find the best model is 79% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-0.38889209743777964
SO2(0)最好结果：{'pred_time': 4.5160329729920317e-05, 'wall_clock_time': 47.40874671936035, 'metric_for_logging': {'pred_time': 4.5160329729920317e-05}, 'val_loss': 1.3888920974377796, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.594907760620117}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.47479024476616816
SO2(0)的mse=5.062807522716535
SO2(0)的mae=1.3391464546869807
SO2(0)的mar=0.27391299627701016
总共花费的时间为：61.84
酒泉市
2658A
2659A
[flaml.automl: 09-18 23:51:01] {2390} INFO - task = regression
[flaml.automl: 09-18 23:51:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:51:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:51:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:51:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:51:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:51:03] {3025} INFO - Estimated sufficient time budget=21568s. Estimated necessary time budget=22s.
[flaml.automl: 09-18 23:51:03] {3072} INFO -  at 2.3s,	estimator xgboost's best error=3.7326,	best estimator xgboost's best error=3.7326
[flaml.automl: 09-18 23:51:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:51:07] {3072} INFO -  at 6.2s,	estimator xgboost's best error=1.8321,	best estimator xgboost's best error=1.8321
[flaml.automl: 09-18 23:51:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:51:10] {3072} INFO -  at 9.1s,	estimator xgboost's best error=1.8321,	best estimator xgboost's best error=1.8321
[flaml.automl: 09-18 23:51:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:51:35] {3072} INFO -  at 34.7s,	estimator xgboost's best error=1.8321,	best estimator xgboost's best error=1.8321
[flaml.automl: 09-18 23:51:35] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:51:38] {3072} INFO -  at 37.6s,	estimator xgboost's best error=1.4204,	best estimator xgboost's best error=1.4204
[flaml.automl: 09-18 23:51:38] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:51:42] {3072} INFO -  at 41.9s,	estimator xgboost's best error=1.4204,	best estimator xgboost's best error=1.4204
[flaml.automl: 09-18 23:51:42] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:51:47] {3072} INFO -  at 46.5s,	estimator xgboost's best error=1.3431,	best estimator xgboost's best error=1.3431
[flaml.automl: 09-18 23:51:47] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:51:54] {3072} INFO -  at 53.7s,	estimator xgboost's best error=1.3431,	best estimator xgboost's best error=1.3431
[flaml.automl: 09-18 23:51:54] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-18 23:51:58] {3072} INFO -  at 57.4s,	estimator xgboost's best error=1.3431,	best estimator xgboost's best error=1.3431
[flaml.automl: 09-18 23:52:01] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-18 23:52:01] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-18 23:52:01] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:52:01] {2637} INFO - Time taken to find the best model: 46.51095724105835
[flaml.automl: 09-18 23:52:01] {2648} WARNING - Time taken to find the best model is 78% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-0.3431004340243604
SO2(0)最好结果：{'pred_time': 4.47468814512478e-05, 'wall_clock_time': 46.51095724105835, 'metric_for_logging': {'pred_time': 4.47468814512478e-05}, 'val_loss': 1.3431004340243604, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.61548376083374}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=-0.2024647032503586
SO2(0)的mse=6.169560077329097
SO2(0)的mae=1.4018080041286771
SO2(0)的mar=0.1980605277538957
总共花费的时间为：60.86
庆阳市
2662A
[flaml.automl: 09-18 23:55:57] {2390} INFO - task = regression
[flaml.automl: 09-18 23:55:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-18 23:55:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-18 23:55:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-18 23:55:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-18 23:55:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-18 23:56:00] {3025} INFO - Estimated sufficient time budget=33989s. Estimated necessary time budget=34s.
[flaml.automl: 09-18 23:56:00] {3072} INFO -  at 3.5s,	estimator xgboost's best error=4.9781,	best estimator xgboost's best error=4.9781
[flaml.automl: 09-18 23:56:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-18 23:56:05] {3072} INFO -  at 8.9s,	estimator xgboost's best error=2.6353,	best estimator xgboost's best error=2.6353
[flaml.automl: 09-18 23:56:05] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-18 23:56:09] {3072} INFO -  at 12.4s,	estimator xgboost's best error=2.6353,	best estimator xgboost's best error=2.6353
[flaml.automl: 09-18 23:56:09] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-18 23:56:36] {3072} INFO -  at 39.4s,	estimator xgboost's best error=2.6353,	best estimator xgboost's best error=2.6353
[flaml.automl: 09-18 23:56:36] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-18 23:56:41] {3072} INFO -  at 44.4s,	estimator xgboost's best error=1.2814,	best estimator xgboost's best error=1.2814
[flaml.automl: 09-18 23:56:41] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-18 23:56:46] {3072} INFO -  at 49.9s,	estimator xgboost's best error=1.1414,	best estimator xgboost's best error=1.1414
[flaml.automl: 09-18 23:56:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-18 23:56:50] {3072} INFO -  at 53.1s,	estimator xgboost's best error=1.1122,	best estimator xgboost's best error=1.1122
[flaml.automl: 09-18 23:56:50] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-18 23:56:55] {3072} INFO -  at 58.1s,	estimator xgboost's best error=1.1122,	best estimator xgboost's best error=1.1122
[flaml.automl: 09-18 23:56:58] {3335} INFO - retrain xgboost for 3.3s
[flaml.automl: 09-18 23:56:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-18 23:56:58] {2636} INFO - fit succeeded
[flaml.automl: 09-18 23:56:58] {2637} INFO - Time taken to find the best model: 53.10017442703247
[flaml.automl: 09-18 23:56:58] {2648} WARNING - Time taken to find the best model is 89% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：-0.11224867689688467
SO2(0)最好结果：{'pred_time': 8.935782293610226e-05, 'wall_clock_time': 53.10017442703247, 'metric_for_logging': {'pred_time': 8.935782293610226e-05}, 'val_loss': 1.1122486768968847, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 3.2332024574279785}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.44672197773745526
SO2(0)的mse=3.978037980839696
SO2(0)的mae=1.0743024350987915
SO2(0)的mar=0.11769724605244142
总共花费的时间为：61.77
定西市
2663A
2664A
[flaml.automl: 09-19 00:04:11] {2390} INFO - task = regression
[flaml.automl: 09-19 00:04:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:04:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:04:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:04:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:04:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:04:12] {3025} INFO - Estimated sufficient time budget=11966s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:04:12] {3072} INFO -  at 1.3s,	estimator xgboost's best error=5.7949,	best estimator xgboost's best error=5.7949
[flaml.automl: 09-19 00:04:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:04:14] {3072} INFO -  at 3.4s,	estimator xgboost's best error=3.1518,	best estimator xgboost's best error=3.1518
[flaml.automl: 09-19 00:04:14] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:04:16] {3072} INFO -  at 4.6s,	estimator xgboost's best error=3.1518,	best estimator xgboost's best error=3.1518
[flaml.automl: 09-19 00:04:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:04:29] {3072} INFO -  at 17.9s,	estimator xgboost's best error=3.1518,	best estimator xgboost's best error=3.1518
[flaml.automl: 09-19 00:04:29] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:04:31] {3072} INFO -  at 20.1s,	estimator xgboost's best error=2.6100,	best estimator xgboost's best error=2.6100
[flaml.automl: 09-19 00:04:31] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:04:34] {3072} INFO -  at 23.0s,	estimator xgboost's best error=2.6100,	best estimator xgboost's best error=2.6100
[flaml.automl: 09-19 00:04:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:04:38] {3072} INFO -  at 26.6s,	estimator xgboost's best error=2.3055,	best estimator xgboost's best error=2.3055
[flaml.automl: 09-19 00:04:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:04:44] {3072} INFO -  at 32.9s,	estimator xgboost's best error=2.3055,	best estimator xgboost's best error=2.3055
[flaml.automl: 09-19 00:04:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:04:48] {3072} INFO -  at 37.4s,	estimator xgboost's best error=2.3055,	best estimator xgboost's best error=2.3055
[flaml.automl: 09-19 00:04:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:04:57] {3072} INFO -  at 45.5s,	estimator xgboost's best error=2.2900,	best estimator xgboost's best error=2.2900
[flaml.automl: 09-19 00:04:57] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:05:01] {3072} INFO -  at 50.0s,	estimator xgboost's best error=2.2900,	best estimator xgboost's best error=2.2900
[flaml.automl: 09-19 00:05:01] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:05:04] {3072} INFO -  at 53.1s,	estimator xgboost's best error=2.2900,	best estimator xgboost's best error=2.2900
[flaml.automl: 09-19 00:05:12] {3335} INFO - retrain xgboost for 8.1s
[flaml.automl: 09-19 00:05:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:05:12] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:05:12] {2637} INFO - Time taken to find the best model: 45.47782874107361
[flaml.automl: 09-19 00:05:12] {2648} WARNING - Time taken to find the best model is 76% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}
SO2(0)最佳损失：-1.290047051520879
SO2(0)最好结果：{'pred_time': 4.1055459488567345e-05, 'wall_clock_time': 45.47782874107361, 'metric_for_logging': {'pred_time': 4.1055459488567345e-05}, 'val_loss': 2.290047051520879, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'experiment_tag': 'exp', 'time_total_s': 8.118618726730347}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.43585198361607136
SO2(0)的mse=18.838654015435257
SO2(0)的mae=2.0863031143064728
SO2(0)的mar=0.22451968417447069
总共花费的时间为：61.68
陇南市
2665A
3247A
[flaml.automl: 09-19 00:12:00] {2390} INFO - task = regression
[flaml.automl: 09-19 00:12:00] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:12:00] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:12:00] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:12:00] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:12:00] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:12:01] {3025} INFO - Estimated sufficient time budget=12129s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:12:01] {3072} INFO -  at 1.3s,	estimator xgboost's best error=7.3831,	best estimator xgboost's best error=7.3831
[flaml.automl: 09-19 00:12:01] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:12:03] {3072} INFO -  at 3.2s,	estimator xgboost's best error=3.9563,	best estimator xgboost's best error=3.9563
[flaml.automl: 09-19 00:12:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:12:04] {3072} INFO -  at 4.4s,	estimator xgboost's best error=3.9563,	best estimator xgboost's best error=3.9563
[flaml.automl: 09-19 00:12:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:12:13] {3072} INFO -  at 12.8s,	estimator xgboost's best error=3.9563,	best estimator xgboost's best error=3.9563
[flaml.automl: 09-19 00:12:13] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:12:14] {3072} INFO -  at 13.9s,	estimator xgboost's best error=1.9758,	best estimator xgboost's best error=1.9758
[flaml.automl: 09-19 00:12:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:12:16] {3072} INFO -  at 15.5s,	estimator xgboost's best error=1.9297,	best estimator xgboost's best error=1.9297
[flaml.automl: 09-19 00:12:16] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:12:17] {3072} INFO -  at 17.3s,	estimator xgboost's best error=1.6108,	best estimator xgboost's best error=1.6108
[flaml.automl: 09-19 00:12:17] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:12:22] {3072} INFO -  at 21.7s,	estimator xgboost's best error=1.6108,	best estimator xgboost's best error=1.6108
[flaml.automl: 09-19 00:12:22] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:12:25] {3072} INFO -  at 24.8s,	estimator xgboost's best error=1.5636,	best estimator xgboost's best error=1.5636
[flaml.automl: 09-19 00:12:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:12:31] {3072} INFO -  at 30.5s,	estimator xgboost's best error=1.4966,	best estimator xgboost's best error=1.4966
[flaml.automl: 09-19 00:12:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:12:33] {3072} INFO -  at 32.6s,	estimator xgboost's best error=1.4966,	best estimator xgboost's best error=1.4966
[flaml.automl: 09-19 00:12:33] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:12:59] {3072} INFO -  at 58.4s,	estimator xgboost's best error=1.4966,	best estimator xgboost's best error=1.4966
[flaml.automl: 09-19 00:13:10] {3335} INFO - retrain xgboost for 11.3s
[flaml.automl: 09-19 00:13:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:13:10] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:13:10] {2637} INFO - Time taken to find the best model: 30.495293140411377
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}
SO2(0)最佳损失：-0.4966306999189045
SO2(0)最好结果：{'pred_time': 3.889985040787163e-05, 'wall_clock_time': 30.495293140411377, 'metric_for_logging': {'pred_time': 3.889985040787163e-05}, 'val_loss': 1.4966306999189045, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.056902942069697794, 'config/learning_rate': 0.6286550459839113, 'config/subsample': 0.8219353140149964, 'config/colsample_bylevel': 0.6552426578592088, 'config/colsample_bytree': 0.8391963407651736, 'config/reg_alpha': 0.007347191482940116, 'config/reg_lambda': 9.546883854467293, 'experiment_tag': 'exp', 'time_total_s': 5.679194211959839}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7086949748449345
SO2(0)的mse=4.694301376674115
SO2(0)的mae=1.4958050256913402
SO2(0)的mar=0.16771340126780115
总共花费的时间为：70.34
临夏回族自治州
2667A
2668A
[flaml.automl: 09-19 00:19:53] {2390} INFO - task = regression
[flaml.automl: 09-19 00:19:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:19:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:19:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:19:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:19:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:19:54] {3025} INFO - Estimated sufficient time budget=12082s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:19:54] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.2922,	best estimator xgboost's best error=4.2922
[flaml.automl: 09-19 00:19:54] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:19:56] {3072} INFO -  at 3.4s,	estimator xgboost's best error=2.1205,	best estimator xgboost's best error=2.1205
[flaml.automl: 09-19 00:19:56] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:19:58] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.1205,	best estimator xgboost's best error=2.1205
[flaml.automl: 09-19 00:19:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:20:07] {3072} INFO -  at 14.1s,	estimator xgboost's best error=2.1205,	best estimator xgboost's best error=2.1205
[flaml.automl: 09-19 00:20:07] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:20:08] {3072} INFO -  at 15.3s,	estimator xgboost's best error=1.5559,	best estimator xgboost's best error=1.5559
[flaml.automl: 09-19 00:20:08] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:20:10] {3072} INFO -  at 16.8s,	estimator xgboost's best error=1.5559,	best estimator xgboost's best error=1.5559
[flaml.automl: 09-19 00:20:10] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:20:12] {3072} INFO -  at 18.5s,	estimator xgboost's best error=1.3050,	best estimator xgboost's best error=1.3050
[flaml.automl: 09-19 00:20:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:20:14] {3072} INFO -  at 21.2s,	estimator xgboost's best error=1.3050,	best estimator xgboost's best error=1.3050
[flaml.automl: 09-19 00:20:14] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:20:16] {3072} INFO -  at 22.9s,	estimator xgboost's best error=1.3050,	best estimator xgboost's best error=1.3050
[flaml.automl: 09-19 00:20:16] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:20:19] {3072} INFO -  at 25.9s,	estimator xgboost's best error=1.3050,	best estimator xgboost's best error=1.3050
[flaml.automl: 09-19 00:20:19] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:20:21] {3072} INFO -  at 27.6s,	estimator xgboost's best error=1.2903,	best estimator xgboost's best error=1.2903
[flaml.automl: 09-19 00:20:21] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:20:22] {3072} INFO -  at 28.7s,	estimator xgboost's best error=1.2903,	best estimator xgboost's best error=1.2903
[flaml.automl: 09-19 00:20:22] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:20:30] {3072} INFO -  at 36.8s,	estimator xgboost's best error=1.2306,	best estimator xgboost's best error=1.2306
[flaml.automl: 09-19 00:20:30] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 00:20:52] {3072} INFO -  at 59.4s,	estimator xgboost's best error=1.2306,	best estimator xgboost's best error=1.2306
[flaml.automl: 09-19 00:21:13] {3335} INFO - retrain xgboost for 20.4s
[flaml.automl: 09-19 00:21:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:21:13] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:21:13] {2637} INFO - Time taken to find the best model: 36.784098386764526
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
SO2(0)最佳损失：-0.2306451995762524
SO2(0)最好结果：{'pred_time': 3.279329463991398e-05, 'wall_clock_time': 36.784098386764526, 'metric_for_logging': {'pred_time': 3.279329463991398e-05}, 'val_loss': 1.2306451995762524, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 8.045299053192139}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7327299442788568
SO2(0)的mse=3.297004296888063
SO2(0)的mae=1.2157167156026998
SO2(0)的mar=0.21981468462246864
总共花费的时间为：80.26
甘南州
2669A
[flaml.automl: 09-19 00:24:44] {2390} INFO - task = regression
[flaml.automl: 09-19 00:24:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:24:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:24:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:24:45] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:24:45] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:24:46] {3025} INFO - Estimated sufficient time budget=11973s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:24:46] {3072} INFO -  at 1.2s,	estimator xgboost's best error=5.1225,	best estimator xgboost's best error=5.1225
[flaml.automl: 09-19 00:24:46] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:24:48] {3072} INFO -  at 3.1s,	estimator xgboost's best error=2.8651,	best estimator xgboost's best error=2.8651
[flaml.automl: 09-19 00:24:48] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:24:49] {3072} INFO -  at 4.3s,	estimator xgboost's best error=2.8651,	best estimator xgboost's best error=2.8651
[flaml.automl: 09-19 00:24:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:24:58] {3072} INFO -  at 13.9s,	estimator xgboost's best error=2.8651,	best estimator xgboost's best error=2.8651
[flaml.automl: 09-19 00:24:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:25:01] {3072} INFO -  at 16.1s,	estimator xgboost's best error=1.8025,	best estimator xgboost's best error=1.8025
[flaml.automl: 09-19 00:25:01] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:25:03] {3072} INFO -  at 19.0s,	estimator xgboost's best error=1.6955,	best estimator xgboost's best error=1.6955
[flaml.automl: 09-19 00:25:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:25:06] {3072} INFO -  at 22.0s,	estimator xgboost's best error=1.6955,	best estimator xgboost's best error=1.6955
[flaml.automl: 09-19 00:25:06] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:25:11] {3072} INFO -  at 26.4s,	estimator xgboost's best error=1.6955,	best estimator xgboost's best error=1.6955
[flaml.automl: 09-19 00:25:11] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:25:14] {3072} INFO -  at 29.4s,	estimator xgboost's best error=1.6955,	best estimator xgboost's best error=1.6955
[flaml.automl: 09-19 00:25:14] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:25:20] {3072} INFO -  at 35.9s,	estimator xgboost's best error=1.6955,	best estimator xgboost's best error=1.6955
[flaml.automl: 09-19 00:25:20] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:25:23] {3072} INFO -  at 39.0s,	estimator xgboost's best error=1.6955,	best estimator xgboost's best error=1.6955
[flaml.automl: 09-19 00:25:23] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:25:27] {3072} INFO -  at 42.1s,	estimator xgboost's best error=1.6955,	best estimator xgboost's best error=1.6955
[flaml.automl: 09-19 00:25:27] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:25:42] {3072} INFO -  at 57.3s,	estimator xgboost's best error=1.6955,	best estimator xgboost's best error=1.6955
[flaml.automl: 09-19 00:25:46] {3335} INFO - retrain xgboost for 4.6s
[flaml.automl: 09-19 00:25:46] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:25:46] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:25:46] {2637} INFO - Time taken to find the best model: 18.993263006210327
SO2(0)最佳参数：{'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}
SO2(0)最佳损失：-0.6955144530639139
SO2(0)最好结果：{'pred_time': 5.0750546300281976e-05, 'wall_clock_time': 18.993263006210327, 'metric_for_logging': {'pred_time': 5.0750546300281976e-05}, 'val_loss': 1.695514453063914, 'training_iteration': 1, 'config': {'n_estimators': 4, 'max_leaves': 6, 'min_child_weight': 1.3175438988783195, 'learning_rate': 0.5420318652130535, 'subsample': 0.8598822533236937, 'colsample_bylevel': 0.740083348039293, 'colsample_bytree': 0.7175631461620532, 'reg_alpha': 0.0017503649396942245, 'reg_lambda': 6.217500197095953}, 'config/n_estimators': 4, 'config/max_leaves': 6, 'config/min_child_weight': 1.3175438988783195, 'config/learning_rate': 0.5420318652130535, 'config/subsample': 0.8598822533236937, 'config/colsample_bylevel': 0.740083348039293, 'config/colsample_bytree': 0.7175631461620532, 'config/reg_alpha': 0.0017503649396942245, 'config/reg_lambda': 6.217500197095953, 'experiment_tag': 'exp', 'time_total_s': 2.918116807937622}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.740083348039293, colsample_bynode=1,
             colsample_bytree=0.7175631461620532, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5420318652130535,
             max_delta_step=0, max_depth=0, max_leaves=6,
             min_child_weight=1.3175438988783195, missing=nan,
             monotone_constraints='()', n_estimators=4, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0017503649396942245, reg_lambda=6.217500197095953,
             scale_pos_weight=1, subsample=0.8598822533236937,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=-0.10499004018849889
SO2(0)的mse=18.541991011287454
SO2(0)的mae=2.0734618572859436
SO2(0)的mar=0.21956641164109375
总共花费的时间为：62.27
海东地区
3867A
[flaml.automl: 09-19 00:29:06] {2390} INFO - task = regression
[flaml.automl: 09-19 00:29:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:29:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:29:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:29:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:29:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:29:08] {3025} INFO - Estimated sufficient time budget=17951s. Estimated necessary time budget=18s.
[flaml.automl: 09-19 00:29:08] {3072} INFO -  at 1.8s,	estimator xgboost's best error=7.5887,	best estimator xgboost's best error=7.5887
[flaml.automl: 09-19 00:29:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:29:12] {3072} INFO -  at 6.4s,	estimator xgboost's best error=4.2300,	best estimator xgboost's best error=4.2300
[flaml.automl: 09-19 00:29:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:29:16] {3072} INFO -  at 9.8s,	estimator xgboost's best error=4.2300,	best estimator xgboost's best error=4.2300
[flaml.automl: 09-19 00:29:16] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:29:31] {3072} INFO -  at 25.5s,	estimator xgboost's best error=4.2300,	best estimator xgboost's best error=4.2300
[flaml.automl: 09-19 00:29:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:29:34] {3072} INFO -  at 28.7s,	estimator xgboost's best error=2.4411,	best estimator xgboost's best error=2.4411
[flaml.automl: 09-19 00:29:34] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:29:40] {3072} INFO -  at 34.7s,	estimator xgboost's best error=2.3351,	best estimator xgboost's best error=2.3351
[flaml.automl: 09-19 00:29:40] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:29:46] {3072} INFO -  at 39.9s,	estimator xgboost's best error=2.2677,	best estimator xgboost's best error=2.2677
[flaml.automl: 09-19 00:29:46] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:29:56] {3072} INFO -  at 49.7s,	estimator xgboost's best error=2.2677,	best estimator xgboost's best error=2.2677
[flaml.automl: 09-19 00:29:56] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:30:02] {3072} INFO -  at 56.7s,	estimator xgboost's best error=2.2677,	best estimator xgboost's best error=2.2677
[flaml.automl: 09-19 00:30:08] {3335} INFO - retrain xgboost for 5.9s
[flaml.automl: 09-19 00:30:08] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=5, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:30:08] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:30:08] {2637} INFO - Time taken to find the best model: 39.897350549697876
SO2(0)最佳参数：{'n_estimators': 5, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：-1.267708931955144
SO2(0)最好结果：{'pred_time': 0.00027350497767181096, 'wall_clock_time': 39.897350549697876, 'metric_for_logging': {'pred_time': 0.00027350497767181096}, 'val_loss': 2.267708931955144, 'training_iteration': 1, 'config': {'n_estimators': 5, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 5, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 5.238758087158203}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=5, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.40285466376093537
SO2(0)的mse=11.941698625024888
SO2(0)的mae=2.234083733533064
SO2(0)的mar=0.2072871615529322
总共花费的时间为：62.95
海北藏族自治州
2671A
[flaml.automl: 09-19 00:33:56] {2390} INFO - task = regression
[flaml.automl: 09-19 00:33:56] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:33:56] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:33:56] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:33:56] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:33:56] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:33:58] {3025} INFO - Estimated sufficient time budget=22153s. Estimated necessary time budget=22s.
[flaml.automl: 09-19 00:33:58] {3072} INFO -  at 2.3s,	estimator xgboost's best error=7.5806,	best estimator xgboost's best error=7.5806
[flaml.automl: 09-19 00:33:58] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:34:02] {3072} INFO -  at 5.8s,	estimator xgboost's best error=4.0550,	best estimator xgboost's best error=4.0550
[flaml.automl: 09-19 00:34:02] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:34:04] {3072} INFO -  at 8.0s,	estimator xgboost's best error=4.0550,	best estimator xgboost's best error=4.0550
[flaml.automl: 09-19 00:34:04] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:34:22] {3072} INFO -  at 25.9s,	estimator xgboost's best error=4.0550,	best estimator xgboost's best error=4.0550
[flaml.automl: 09-19 00:34:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:34:25] {3072} INFO -  at 29.0s,	estimator xgboost's best error=2.0825,	best estimator xgboost's best error=2.0825
[flaml.automl: 09-19 00:34:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:34:29] {3072} INFO -  at 33.3s,	estimator xgboost's best error=1.8486,	best estimator xgboost's best error=1.8486
[flaml.automl: 09-19 00:34:29] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:34:33] {3072} INFO -  at 37.7s,	estimator xgboost's best error=1.7630,	best estimator xgboost's best error=1.7630
[flaml.automl: 09-19 00:34:33] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:34:40] {3072} INFO -  at 43.9s,	estimator xgboost's best error=1.7630,	best estimator xgboost's best error=1.7630
[flaml.automl: 09-19 00:34:40] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:34:44] {3072} INFO -  at 48.2s,	estimator xgboost's best error=1.7630,	best estimator xgboost's best error=1.7630
[flaml.automl: 09-19 00:34:44] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:34:51] {3072} INFO -  at 55.3s,	estimator xgboost's best error=1.7630,	best estimator xgboost's best error=1.7630
[flaml.automl: 09-19 00:34:51] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:34:54] {3072} INFO -  at 58.5s,	estimator xgboost's best error=1.6710,	best estimator xgboost's best error=1.6710
[flaml.automl: 09-19 00:34:57] {3335} INFO - retrain xgboost for 3.0s
[flaml.automl: 09-19 00:34:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8660549326361876, colsample_bynode=1,
             colsample_bytree=0.9004061282219729, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.590319080116618,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=53.34352431682733, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.778332386065348, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:34:57] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:34:57] {2637} INFO - Time taken to find the best model: 58.48967218399048
[flaml.automl: 09-19 00:34:57] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 53.34352431682733, 'learning_rate': 0.590319080116618, 'subsample': 1.0, 'colsample_bylevel': 0.8660549326361876, 'colsample_bytree': 0.9004061282219729, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.778332386065348}
SO2(0)最佳损失：-0.6710103373190124
SO2(0)最好结果：{'pred_time': 5.880849339061584e-05, 'wall_clock_time': 58.48967218399048, 'metric_for_logging': {'pred_time': 5.880849339061584e-05}, 'val_loss': 1.6710103373190124, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 53.34352431682733, 'learning_rate': 0.590319080116618, 'subsample': 1.0, 'colsample_bylevel': 0.8660549326361876, 'colsample_bytree': 0.9004061282219729, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.778332386065348}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 53.34352431682733, 'config/learning_rate': 0.590319080116618, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8660549326361876, 'config/colsample_bytree': 0.9004061282219729, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.778332386065348, 'experiment_tag': 'exp', 'time_total_s': 3.1502685546875}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8660549326361876, colsample_bynode=1,
             colsample_bytree=0.9004061282219729, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.590319080116618,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=53.34352431682733, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.778332386065348, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7147850112553091
SO2(0)的mse=6.638003106480026
SO2(0)的mae=1.5743778198043261
SO2(0)的mar=0.13938303493974022
总共花费的时间为：61.73
黄南藏族自治州
2672A
[flaml.automl: 09-19 00:38:44] {2390} INFO - task = regression
[flaml.automl: 09-19 00:38:44] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:38:44] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:38:44] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:38:44] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:38:44] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:38:48] {3025} INFO - Estimated sufficient time budget=42961s. Estimated necessary time budget=43s.
[flaml.automl: 09-19 00:38:48] {3072} INFO -  at 4.4s,	estimator xgboost's best error=4.7373,	best estimator xgboost's best error=4.7373
[flaml.automl: 09-19 00:38:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:38:54] {3072} INFO -  at 10.6s,	estimator xgboost's best error=2.6411,	best estimator xgboost's best error=2.6411
[flaml.automl: 09-19 00:38:54] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:38:58] {3072} INFO -  at 14.9s,	estimator xgboost's best error=2.6411,	best estimator xgboost's best error=2.6411
[flaml.automl: 09-19 00:38:58] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:39:25] {3072} INFO -  at 41.1s,	estimator xgboost's best error=2.6411,	best estimator xgboost's best error=2.6411
[flaml.automl: 09-19 00:39:25] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:39:29] {3072} INFO -  at 45.6s,	estimator xgboost's best error=1.6099,	best estimator xgboost's best error=1.6099
[flaml.automl: 09-19 00:39:29] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:39:34] {3072} INFO -  at 50.7s,	estimator xgboost's best error=1.5289,	best estimator xgboost's best error=1.5289
[flaml.automl: 09-19 00:39:34] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:39:38] {3072} INFO -  at 54.5s,	estimator xgboost's best error=1.4953,	best estimator xgboost's best error=1.4953
[flaml.automl: 09-19 00:39:38] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:39:42] {3072} INFO -  at 58.5s,	estimator xgboost's best error=1.4953,	best estimator xgboost's best error=1.4953
[flaml.automl: 09-19 00:39:45] {3335} INFO - retrain xgboost for 2.8s
[flaml.automl: 09-19 00:39:45] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:39:45] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:39:45] {2637} INFO - Time taken to find the best model: 54.542025566101074
[flaml.automl: 09-19 00:39:45] {2648} WARNING - Time taken to find the best model is 91% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：-0.49534292811592184
SO2(0)最好结果：{'pred_time': 5.47155574679676e-05, 'wall_clock_time': 54.542025566101074, 'metric_for_logging': {'pred_time': 5.47155574679676e-05}, 'val_loss': 1.4953429281159218, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 3.8397436141967773}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.49217643698962676
SO2(0)的mse=5.568510150217658
SO2(0)的mae=1.527968356788294
SO2(0)的mar=0.2327831390421047
总共花费的时间为：61.55
海南藏族自治州
2673A
[flaml.automl: 09-19 00:43:20] {2390} INFO - task = regression
[flaml.automl: 09-19 00:43:20] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:43:20] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:43:20] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:43:20] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:43:20] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:43:22] {3025} INFO - Estimated sufficient time budget=21953s. Estimated necessary time budget=22s.
[flaml.automl: 09-19 00:43:22] {3072} INFO -  at 2.3s,	estimator xgboost's best error=6.8520,	best estimator xgboost's best error=6.8520
[flaml.automl: 09-19 00:43:22] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:43:26] {3072} INFO -  at 5.7s,	estimator xgboost's best error=3.5876,	best estimator xgboost's best error=3.5876
[flaml.automl: 09-19 00:43:26] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:43:28] {3072} INFO -  at 7.9s,	estimator xgboost's best error=3.5876,	best estimator xgboost's best error=3.5876
[flaml.automl: 09-19 00:43:28] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:43:42] {3072} INFO -  at 22.1s,	estimator xgboost's best error=3.5876,	best estimator xgboost's best error=3.5876
[flaml.automl: 09-19 00:43:42] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:43:45] {3072} INFO -  at 25.2s,	estimator xgboost's best error=1.5320,	best estimator xgboost's best error=1.5320
[flaml.automl: 09-19 00:43:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:43:50] {3072} INFO -  at 29.4s,	estimator xgboost's best error=1.2624,	best estimator xgboost's best error=1.2624
[flaml.automl: 09-19 00:43:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:43:54] {3072} INFO -  at 33.8s,	estimator xgboost's best error=1.2201,	best estimator xgboost's best error=1.2201
[flaml.automl: 09-19 00:43:54] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:44:00] {3072} INFO -  at 40.2s,	estimator xgboost's best error=1.2201,	best estimator xgboost's best error=1.2201
[flaml.automl: 09-19 00:44:00] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:44:05] {3072} INFO -  at 44.6s,	estimator xgboost's best error=1.2145,	best estimator xgboost's best error=1.2145
[flaml.automl: 09-19 00:44:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:44:12] {3072} INFO -  at 51.3s,	estimator xgboost's best error=1.1459,	best estimator xgboost's best error=1.1459
[flaml.automl: 09-19 00:44:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:44:15] {3072} INFO -  at 54.3s,	estimator xgboost's best error=1.1459,	best estimator xgboost's best error=1.1459
[flaml.automl: 09-19 00:44:21] {3335} INFO - retrain xgboost for 6.8s
[flaml.automl: 09-19 00:44:21] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:44:21] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:44:21] {2637} INFO - Time taken to find the best model: 51.290716886520386
[flaml.automl: 09-19 00:44:21] {2648} WARNING - Time taken to find the best model is 85% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}
SO2(0)最佳损失：-0.14591498547289738
SO2(0)最好结果：{'pred_time': 9.5966829353547e-05, 'wall_clock_time': 51.290716886520386, 'metric_for_logging': {'pred_time': 9.5966829353547e-05}, 'val_loss': 1.1459149854728974, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 7, 'min_child_weight': 0.056902942069697794, 'learning_rate': 0.6286550459839113, 'subsample': 0.8219353140149964, 'colsample_bylevel': 0.6552426578592088, 'colsample_bytree': 0.8391963407651736, 'reg_alpha': 0.007347191482940116, 'reg_lambda': 9.546883854467293}, 'config/n_estimators': 6, 'config/max_leaves': 7, 'config/min_child_weight': 0.056902942069697794, 'config/learning_rate': 0.6286550459839113, 'config/subsample': 0.8219353140149964, 'config/colsample_bylevel': 0.6552426578592088, 'config/colsample_bytree': 0.8391963407651736, 'config/reg_alpha': 0.007347191482940116, 'config/reg_lambda': 9.546883854467293, 'experiment_tag': 'exp', 'time_total_s': 6.692927837371826}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.6552426578592088, colsample_bynode=1,
             colsample_bytree=0.8391963407651736, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.6286550459839113,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.056902942069697794, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.007347191482940116, reg_lambda=9.546883854467293,
             scale_pos_weight=1, subsample=0.8219353140149964,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7149520008913054
SO2(0)的mse=2.3987794976323307
SO2(0)的mae=1.143960136481152
SO2(0)的mar=0.11589942347781386
总共花费的时间为：61.43
果洛藏族自治州
2674A
[flaml.automl: 09-19 00:47:53] {2390} INFO - task = regression
[flaml.automl: 09-19 00:47:53] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:47:53] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:47:53] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:47:53] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:47:53] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:47:56] {3025} INFO - Estimated sufficient time budget=22764s. Estimated necessary time budget=23s.
[flaml.automl: 09-19 00:47:56] {3072} INFO -  at 2.3s,	estimator xgboost's best error=9.9723,	best estimator xgboost's best error=9.9723
[flaml.automl: 09-19 00:47:56] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:47:59] {3072} INFO -  at 5.9s,	estimator xgboost's best error=5.5611,	best estimator xgboost's best error=5.5611
[flaml.automl: 09-19 00:47:59] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:48:02] {3072} INFO -  at 8.1s,	estimator xgboost's best error=5.5611,	best estimator xgboost's best error=5.5611
[flaml.automl: 09-19 00:48:02] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:48:18] {3072} INFO -  at 24.2s,	estimator xgboost's best error=5.5611,	best estimator xgboost's best error=5.5611
[flaml.automl: 09-19 00:48:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:48:21] {3072} INFO -  at 27.2s,	estimator xgboost's best error=3.1349,	best estimator xgboost's best error=3.1349
[flaml.automl: 09-19 00:48:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:48:25] {3072} INFO -  at 31.5s,	estimator xgboost's best error=2.9013,	best estimator xgboost's best error=2.9013
[flaml.automl: 09-19 00:48:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:48:29] {3072} INFO -  at 35.9s,	estimator xgboost's best error=2.8382,	best estimator xgboost's best error=2.8382
[flaml.automl: 09-19 00:48:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:48:36] {3072} INFO -  at 42.2s,	estimator xgboost's best error=2.8382,	best estimator xgboost's best error=2.8382
[flaml.automl: 09-19 00:48:36] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:48:40] {3072} INFO -  at 46.5s,	estimator xgboost's best error=2.8382,	best estimator xgboost's best error=2.8382
[flaml.automl: 09-19 00:48:40] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:48:47] {3072} INFO -  at 53.5s,	estimator xgboost's best error=2.8382,	best estimator xgboost's best error=2.8382
[flaml.automl: 09-19 00:48:47] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:48:51] {3072} INFO -  at 58.0s,	estimator xgboost's best error=2.8382,	best estimator xgboost's best error=2.8382
[flaml.automl: 09-19 00:48:56] {3335} INFO - retrain xgboost for 4.4s
[flaml.automl: 09-19 00:48:56] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:48:56] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:48:56] {2637} INFO - Time taken to find the best model: 35.85050654411316
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：-1.838228670304924
SO2(0)最好结果：{'pred_time': 8.266975167199646e-05, 'wall_clock_time': 35.85050654411316, 'metric_for_logging': {'pred_time': 8.266975167199646e-05}, 'val_loss': 2.838228670304924, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.341488599777222}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.629402638595987
SO2(0)的mse=22.99136117113312
SO2(0)的mae=2.9661894556996344
SO2(0)的mar=0.2246099970667421
总共花费的时间为：62.74
玉树藏族自治州
2675A
[flaml.automl: 09-19 00:52:39] {2390} INFO - task = regression
[flaml.automl: 09-19 00:52:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:52:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:52:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:52:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:52:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:52:40] {3025} INFO - Estimated sufficient time budget=11983s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 00:52:40] {3072} INFO -  at 1.2s,	estimator xgboost's best error=6.9980,	best estimator xgboost's best error=6.9980
[flaml.automl: 09-19 00:52:40] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:52:42] {3072} INFO -  at 3.1s,	estimator xgboost's best error=3.7184,	best estimator xgboost's best error=3.7184
[flaml.automl: 09-19 00:52:42] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:52:43] {3072} INFO -  at 4.3s,	estimator xgboost's best error=3.7184,	best estimator xgboost's best error=3.7184
[flaml.automl: 09-19 00:52:43] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:52:56] {3072} INFO -  at 16.9s,	estimator xgboost's best error=3.7184,	best estimator xgboost's best error=3.7184
[flaml.automl: 09-19 00:52:56] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:52:58] {3072} INFO -  at 19.3s,	estimator xgboost's best error=1.9641,	best estimator xgboost's best error=1.9641
[flaml.automl: 09-19 00:52:58] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:53:03] {3072} INFO -  at 23.5s,	estimator xgboost's best error=1.7740,	best estimator xgboost's best error=1.7740
[flaml.automl: 09-19 00:53:03] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:53:07] {3072} INFO -  at 27.8s,	estimator xgboost's best error=1.6710,	best estimator xgboost's best error=1.6710
[flaml.automl: 09-19 00:53:07] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:53:13] {3072} INFO -  at 34.1s,	estimator xgboost's best error=1.6710,	best estimator xgboost's best error=1.6710
[flaml.automl: 09-19 00:53:13] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:53:17] {3072} INFO -  at 38.5s,	estimator xgboost's best error=1.6710,	best estimator xgboost's best error=1.6710
[flaml.automl: 09-19 00:53:17] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:53:24] {3072} INFO -  at 45.5s,	estimator xgboost's best error=1.6710,	best estimator xgboost's best error=1.6710
[flaml.automl: 09-19 00:53:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:53:29] {3072} INFO -  at 50.1s,	estimator xgboost's best error=1.6710,	best estimator xgboost's best error=1.6710
[flaml.automl: 09-19 00:53:29] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:53:32] {3072} INFO -  at 53.1s,	estimator xgboost's best error=1.6710,	best estimator xgboost's best error=1.6710
[flaml.automl: 09-19 00:53:32] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:53:38] {3072} INFO -  at 58.6s,	estimator xgboost's best error=1.6710,	best estimator xgboost's best error=1.6710
[flaml.automl: 09-19 00:53:42] {3335} INFO - retrain xgboost for 4.2s
[flaml.automl: 09-19 00:53:42] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 00:53:42] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:53:42] {2637} INFO - Time taken to find the best model: 27.832050561904907
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}
SO2(0)最佳损失：-0.671029279064574
SO2(0)最好结果：{'pred_time': 9.004128816534444e-05, 'wall_clock_time': 27.832050561904907, 'metric_for_logging': {'pred_time': 9.004128816534444e-05}, 'val_loss': 1.671029279064574, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 2.579968500010808, 'learning_rate': 0.4450694937127663, 'subsample': 0.9614843153037248, 'colsample_bylevel': 0.7980045128740293, 'colsample_bytree': 0.9349420776579227, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.1250789385714413}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 2.579968500010808, 'config/learning_rate': 0.4450694937127663, 'config/subsample': 0.9614843153037248, 'config/colsample_bylevel': 0.7980045128740293, 'config/colsample_bytree': 0.9349420776579227, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.1250789385714413, 'experiment_tag': 'exp', 'time_total_s': 4.281411647796631}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7980045128740293, colsample_bynode=1,
             colsample_bytree=0.9349420776579227, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.4450694937127663,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.579968500010808, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.1250789385714413, scale_pos_weight=1,
             subsample=0.9614843153037248, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.29351482444194166
SO2(0)的mse=13.338142041612869
SO2(0)的mae=1.7686826942196812
SO2(0)的mar=0.13528258776479055
总共花费的时间为：63.17
海西蒙古族藏族自治州
2676A
[flaml.automl: 09-19 00:57:18] {2390} INFO - task = regression
[flaml.automl: 09-19 00:57:18] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 00:57:18] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 00:57:18] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 00:57:18] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 00:57:18] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 00:57:20] {3025} INFO - Estimated sufficient time budget=20392s. Estimated necessary time budget=20s.
[flaml.automl: 09-19 00:57:20] {3072} INFO -  at 2.1s,	estimator xgboost's best error=7.5024,	best estimator xgboost's best error=7.5024
[flaml.automl: 09-19 00:57:20] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 00:57:25] {3072} INFO -  at 6.4s,	estimator xgboost's best error=3.9766,	best estimator xgboost's best error=3.9766
[flaml.automl: 09-19 00:57:25] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 00:57:27] {3072} INFO -  at 9.2s,	estimator xgboost's best error=3.9766,	best estimator xgboost's best error=3.9766
[flaml.automl: 09-19 00:57:27] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 00:57:43] {3072} INFO -  at 25.3s,	estimator xgboost's best error=3.9766,	best estimator xgboost's best error=3.9766
[flaml.automl: 09-19 00:57:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 00:57:46] {3072} INFO -  at 27.8s,	estimator xgboost's best error=1.6292,	best estimator xgboost's best error=1.6292
[flaml.automl: 09-19 00:57:46] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 00:57:50] {3072} INFO -  at 31.4s,	estimator xgboost's best error=1.1984,	best estimator xgboost's best error=1.1984
[flaml.automl: 09-19 00:57:50] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 00:57:53] {3072} INFO -  at 35.1s,	estimator xgboost's best error=1.1984,	best estimator xgboost's best error=1.1984
[flaml.automl: 09-19 00:57:53] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 00:57:59] {3072} INFO -  at 40.4s,	estimator xgboost's best error=1.1984,	best estimator xgboost's best error=1.1984
[flaml.automl: 09-19 00:57:59] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 00:58:01] {3072} INFO -  at 42.9s,	estimator xgboost's best error=1.1984,	best estimator xgboost's best error=1.1984
[flaml.automl: 09-19 00:58:01] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 00:58:07] {3072} INFO -  at 48.6s,	estimator xgboost's best error=1.1984,	best estimator xgboost's best error=1.1984
[flaml.automl: 09-19 00:58:07] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 00:58:09] {3072} INFO -  at 51.1s,	estimator xgboost's best error=1.1984,	best estimator xgboost's best error=1.1984
[flaml.automl: 09-19 00:58:09] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 00:58:12] {3072} INFO -  at 53.7s,	estimator xgboost's best error=1.1984,	best estimator xgboost's best error=1.1984
[flaml.automl: 09-19 00:58:12] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 00:58:17] {3072} INFO -  at 58.8s,	estimator xgboost's best error=1.0939,	best estimator xgboost's best error=1.0939
[flaml.automl: 09-19 00:58:25] {3335} INFO - retrain xgboost for 8.0s
[flaml.automl: 09-19 00:58:25] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 00:58:25] {2636} INFO - fit succeeded
[flaml.automl: 09-19 00:58:25] {2637} INFO - Time taken to find the best model: 58.81468462944031
[flaml.automl: 09-19 00:58:25] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
SO2(0)最佳损失：-0.09389008132394094
SO2(0)最好结果：{'pred_time': 9.501589189054938e-05, 'wall_clock_time': 58.81468462944031, 'metric_for_logging': {'pred_time': 9.501589189054938e-05}, 'val_loss': 1.093890081323941, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 12, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 5.13981294631958}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8769569432495539
SO2(0)的mse=2.6635987968486137
SO2(0)的mae=1.073298754561292
SO2(0)的mar=0.11466533984357383
总共花费的时间为：67.03
吴忠市
2677A
3648A
[flaml.automl: 09-19 01:06:07] {2390} INFO - task = regression
[flaml.automl: 09-19 01:06:07] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:06:07] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:06:07] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:06:07] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:06:07] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:06:08] {3025} INFO - Estimated sufficient time budget=12145s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:06:08] {3072} INFO -  at 1.3s,	estimator xgboost's best error=7.1402,	best estimator xgboost's best error=7.1402
[flaml.automl: 09-19 01:06:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:06:11] {3072} INFO -  at 4.1s,	estimator xgboost's best error=3.7647,	best estimator xgboost's best error=3.7647
[flaml.automl: 09-19 01:06:11] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:06:13] {3072} INFO -  at 5.9s,	estimator xgboost's best error=3.7647,	best estimator xgboost's best error=3.7647
[flaml.automl: 09-19 01:06:13] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:06:31] {3072} INFO -  at 24.1s,	estimator xgboost's best error=3.7647,	best estimator xgboost's best error=3.7647
[flaml.automl: 09-19 01:06:31] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:06:33] {3072} INFO -  at 26.5s,	estimator xgboost's best error=3.1745,	best estimator xgboost's best error=3.1745
[flaml.automl: 09-19 01:06:33] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:06:37] {3072} INFO -  at 30.5s,	estimator xgboost's best error=3.0745,	best estimator xgboost's best error=3.0745
[flaml.automl: 09-19 01:06:37] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:06:41] {3072} INFO -  at 34.4s,	estimator xgboost's best error=2.9610,	best estimator xgboost's best error=2.9610
[flaml.automl: 09-19 01:06:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:06:48] {3072} INFO -  at 40.6s,	estimator xgboost's best error=2.9610,	best estimator xgboost's best error=2.9610
[flaml.automl: 09-19 01:06:48] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:06:51] {3072} INFO -  at 44.5s,	estimator xgboost's best error=2.9610,	best estimator xgboost's best error=2.9610
[flaml.automl: 09-19 01:06:51] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:06:58] {3072} INFO -  at 51.6s,	estimator xgboost's best error=2.7661,	best estimator xgboost's best error=2.7661
[flaml.automl: 09-19 01:06:58] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:07:04] {3072} INFO -  at 57.5s,	estimator xgboost's best error=2.7661,	best estimator xgboost's best error=2.7661
[flaml.automl: 09-19 01:07:13] {3335} INFO - retrain xgboost for 8.5s
[flaml.automl: 09-19 01:07:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 01:07:13] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:07:13] {2637} INFO - Time taken to find the best model: 51.55301380157471
[flaml.automl: 09-19 01:07:13] {2648} WARNING - Time taken to find the best model is 86% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-1.7661280071149914
SO2(0)最好结果：{'pred_time': 4.1366656654096566e-05, 'wall_clock_time': 51.55301380157471, 'metric_for_logging': {'pred_time': 4.1366656654096566e-05}, 'val_loss': 2.7661280071149914, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 7.058060884475708}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.16665831183796898
SO2(0)的mse=27.369710211610172
SO2(0)的mae=2.9940046107432465
SO2(0)的mar=0.28724497670544125
总共花费的时间为：66.47
中卫市
2680A
3650A
[flaml.automl: 09-19 01:14:47] {2390} INFO - task = regression
[flaml.automl: 09-19 01:14:47] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:14:47] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:14:47] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:14:47] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:14:47] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:14:48] {3025} INFO - Estimated sufficient time budget=12157s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 01:14:48] {3072} INFO -  at 1.3s,	estimator xgboost's best error=6.1717,	best estimator xgboost's best error=6.1717
[flaml.automl: 09-19 01:14:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:14:50] {3072} INFO -  at 3.2s,	estimator xgboost's best error=3.4680,	best estimator xgboost's best error=3.4680
[flaml.automl: 09-19 01:14:50] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:14:51] {3072} INFO -  at 4.4s,	estimator xgboost's best error=3.4680,	best estimator xgboost's best error=3.4680
[flaml.automl: 09-19 01:14:51] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:15:06] {3072} INFO -  at 19.3s,	estimator xgboost's best error=3.4680,	best estimator xgboost's best error=3.4680
[flaml.automl: 09-19 01:15:06] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:15:09] {3072} INFO -  at 21.9s,	estimator xgboost's best error=2.3163,	best estimator xgboost's best error=2.3163
[flaml.automl: 09-19 01:15:09] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:15:12] {3072} INFO -  at 25.6s,	estimator xgboost's best error=2.2434,	best estimator xgboost's best error=2.2434
[flaml.automl: 09-19 01:15:12] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:15:16] {3072} INFO -  at 29.4s,	estimator xgboost's best error=2.1586,	best estimator xgboost's best error=2.1586
[flaml.automl: 09-19 01:15:16] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:15:21] {3072} INFO -  at 34.7s,	estimator xgboost's best error=2.1586,	best estimator xgboost's best error=2.1586
[flaml.automl: 09-19 01:15:21] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:15:25] {3072} INFO -  at 38.5s,	estimator xgboost's best error=2.1586,	best estimator xgboost's best error=2.1586
[flaml.automl: 09-19 01:15:25] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:15:32] {3072} INFO -  at 45.3s,	estimator xgboost's best error=2.1586,	best estimator xgboost's best error=2.1586
[flaml.automl: 09-19 01:15:32] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:15:36] {3072} INFO -  at 49.1s,	estimator xgboost's best error=2.1564,	best estimator xgboost's best error=2.1564
[flaml.automl: 09-19 01:15:36] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:15:39] {3072} INFO -  at 51.8s,	estimator xgboost's best error=2.1564,	best estimator xgboost's best error=2.1564
[flaml.automl: 09-19 01:15:39] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:15:46] {3072} INFO -  at 59.5s,	estimator xgboost's best error=2.0934,	best estimator xgboost's best error=2.0934
[flaml.automl: 09-19 01:15:58] {3335} INFO - retrain xgboost for 11.6s
[flaml.automl: 09-19 01:15:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:15:58] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:15:58] {2637} INFO - Time taken to find the best model: 59.485355377197266
[flaml.automl: 09-19 01:15:58] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}
SO2(0)最佳损失：-1.0934020232546584
SO2(0)最好结果：{'pred_time': 4.527967085771036e-05, 'wall_clock_time': 59.485355377197266, 'metric_for_logging': {'pred_time': 4.527967085771036e-05}, 'val_loss': 2.0934020232546584, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'experiment_tag': 'exp', 'time_total_s': 7.69032621383667}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.5246219613878762
SO2(0)的mse=12.801916985987258
SO2(0)的mae=1.9963310753307788
SO2(0)的mar=0.20950748153083298
总共花费的时间为：71.47
固原市
2683A
2684A
2685A
3522A
[flaml.automl: 09-19 01:30:06] {2390} INFO - task = regression
[flaml.automl: 09-19 01:30:06] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:30:06] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:30:06] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:30:06] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:30:06] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:30:08] {3025} INFO - Estimated sufficient time budget=90467s. Estimated necessary time budget=90s.
[flaml.automl: 09-19 01:30:08] {3072} INFO -  at 2.4s,	estimator xgboost's best error=2.9038,	best estimator xgboost's best error=2.9038
[flaml.automl: 09-19 01:30:08] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:30:12] {3072} INFO -  at 6.2s,	estimator xgboost's best error=1.3642,	best estimator xgboost's best error=1.3642
[flaml.automl: 09-19 01:30:12] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:30:14] {3072} INFO -  at 8.5s,	estimator xgboost's best error=1.3642,	best estimator xgboost's best error=1.3642
[flaml.automl: 09-19 01:30:14] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:30:20] {3072} INFO -  at 13.9s,	estimator xgboost's best error=1.3642,	best estimator xgboost's best error=1.3642
[flaml.automl: 09-19 01:30:20] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:30:22] {3072} INFO -  at 16.1s,	estimator xgboost's best error=0.8642,	best estimator xgboost's best error=0.8642
[flaml.automl: 09-19 01:30:22] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:30:25] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.7713,	best estimator xgboost's best error=0.7713
[flaml.automl: 09-19 01:30:25] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:30:29] {3072} INFO -  at 23.6s,	estimator xgboost's best error=0.7216,	best estimator xgboost's best error=0.7216
[flaml.automl: 09-19 01:30:29] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:30:33] {3072} INFO -  at 27.5s,	estimator xgboost's best error=0.7216,	best estimator xgboost's best error=0.7216
[flaml.automl: 09-19 01:30:33] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:30:37] {3072} INFO -  at 31.1s,	estimator xgboost's best error=0.7216,	best estimator xgboost's best error=0.7216
[flaml.automl: 09-19 01:30:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:30:40] {3072} INFO -  at 33.8s,	estimator xgboost's best error=0.7216,	best estimator xgboost's best error=0.7216
[flaml.automl: 09-19 01:30:40] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:30:42] {3072} INFO -  at 36.4s,	estimator xgboost's best error=0.7216,	best estimator xgboost's best error=0.7216
[flaml.automl: 09-19 01:30:42] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:30:48] {3072} INFO -  at 42.1s,	estimator xgboost's best error=0.7216,	best estimator xgboost's best error=0.7216
[flaml.automl: 09-19 01:30:48] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:30:51] {3072} INFO -  at 44.8s,	estimator xgboost's best error=0.7216,	best estimator xgboost's best error=0.7216
[flaml.automl: 09-19 01:30:51] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 01:31:05] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.6638,	best estimator xgboost's best error=0.6638
[flaml.automl: 09-19 01:31:20] {3335} INFO - retrain xgboost for 15.5s
[flaml.automl: 09-19 01:31:20] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 01:31:20] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:31:20] {2637} INFO - Time taken to find the best model: 59.23403549194336
[flaml.automl: 09-19 01:31:20] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 41794}
SO2(0)最佳损失：0.33621403290011775
SO2(0)最好结果：{'pred_time': 2.522549682604045e-05, 'wall_clock_time': 59.23403549194336, 'metric_for_logging': {'pred_time': 2.522549682604045e-05}, 'val_loss': 0.6637859670998822, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.5485438732200603, 'learning_rate': 1.0, 'subsample': 0.9994631883864484, 'colsample_bylevel': 0.9328307686903516, 'colsample_bytree': 0.8363596999612937, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.32276354487840386, 'FLAML_sample_size': 41794}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.5485438732200603, 'config/learning_rate': 1.0, 'config/subsample': 0.9994631883864484, 'config/colsample_bylevel': 0.9328307686903516, 'config/colsample_bytree': 0.8363596999612937, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.32276354487840386, 'config/FLAML_sample_size': 41794, 'experiment_tag': 'exp', 'time_total_s': 14.413553953170776}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.9328307686903516, colsample_bynode=1,
             colsample_bytree=0.8363596999612937, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.5485438732200603,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.32276354487840386,
             scale_pos_weight=1, subsample=0.9994631883864484,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6534694372644494
SO2(0)的mse=1.3742598461102022
SO2(0)的mae=0.6713774627493333
SO2(0)的mar=0.14259888926425165
总共花费的时间为：75.88
吐鲁番地区
2686A
2687A
[flaml.automl: 09-19 01:38:27] {2390} INFO - task = regression
[flaml.automl: 09-19 01:38:27] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:38:27] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:38:27] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:38:27] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:38:27] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:38:29] {3025} INFO - Estimated sufficient time budget=21436s. Estimated necessary time budget=21s.
[flaml.automl: 09-19 01:38:29] {3072} INFO -  at 2.2s,	estimator xgboost's best error=4.1172,	best estimator xgboost's best error=4.1172
[flaml.automl: 09-19 01:38:29] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:38:31] {3072} INFO -  at 4.3s,	estimator xgboost's best error=1.8791,	best estimator xgboost's best error=1.8791
[flaml.automl: 09-19 01:38:31] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:38:32] {3072} INFO -  at 5.5s,	estimator xgboost's best error=1.8791,	best estimator xgboost's best error=1.8791
[flaml.automl: 09-19 01:38:32] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:38:49] {3072} INFO -  at 22.7s,	estimator xgboost's best error=1.8791,	best estimator xgboost's best error=1.8791
[flaml.automl: 09-19 01:38:49] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:38:52] {3072} INFO -  at 25.1s,	estimator xgboost's best error=0.9992,	best estimator xgboost's best error=0.9992
[flaml.automl: 09-19 01:38:52] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:38:55] {3072} INFO -  at 28.7s,	estimator xgboost's best error=0.9626,	best estimator xgboost's best error=0.9626
[flaml.automl: 09-19 01:38:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:38:59] {3072} INFO -  at 32.4s,	estimator xgboost's best error=0.8533,	best estimator xgboost's best error=0.8533
[flaml.automl: 09-19 01:38:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:39:05] {3072} INFO -  at 38.4s,	estimator xgboost's best error=0.8533,	best estimator xgboost's best error=0.8533
[flaml.automl: 09-19 01:39:05] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:39:09] {3072} INFO -  at 42.0s,	estimator xgboost's best error=0.8533,	best estimator xgboost's best error=0.8533
[flaml.automl: 09-19 01:39:09] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:39:16] {3072} INFO -  at 49.0s,	estimator xgboost's best error=0.8516,	best estimator xgboost's best error=0.8516
[flaml.automl: 09-19 01:39:16] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:39:20] {3072} INFO -  at 53.4s,	estimator xgboost's best error=0.8516,	best estimator xgboost's best error=0.8516
[flaml.automl: 09-19 01:39:27] {3335} INFO - retrain xgboost for 7.4s
[flaml.automl: 09-19 01:39:27] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 01:39:27] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:39:27] {2637} INFO - Time taken to find the best model: 48.96726608276367
[flaml.automl: 09-19 01:39:27] {2648} WARNING - Time taken to find the best model is 82% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：0.14841153369207272
SO2(0)最好结果：{'pred_time': 6.09870206489453e-05, 'wall_clock_time': 48.96726608276367, 'metric_for_logging': {'pred_time': 6.09870206489453e-05}, 'val_loss': 0.8515884663079273, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 6.994854688644409}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.6297711093355103
SO2(0)的mse=1.9592514323415104
SO2(0)的mae=0.8614064034683686
SO2(0)的mar=0.12546423803910253
总共花费的时间为：61.37
哈密地区
2688A
2689A
[flaml.automl: 09-19 01:46:11] {2390} INFO - task = regression
[flaml.automl: 09-19 01:46:11] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:46:11] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:46:11] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:46:11] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:46:11] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:46:14] {3025} INFO - Estimated sufficient time budget=22391s. Estimated necessary time budget=22s.
[flaml.automl: 09-19 01:46:14] {3072} INFO -  at 2.3s,	estimator xgboost's best error=3.9890,	best estimator xgboost's best error=3.9890
[flaml.automl: 09-19 01:46:14] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:46:19] {3072} INFO -  at 7.6s,	estimator xgboost's best error=2.1443,	best estimator xgboost's best error=2.1443
[flaml.automl: 09-19 01:46:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:46:22] {3072} INFO -  at 10.9s,	estimator xgboost's best error=2.1443,	best estimator xgboost's best error=2.1443
[flaml.automl: 09-19 01:46:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:46:52] {3072} INFO -  at 40.4s,	estimator xgboost's best error=2.1443,	best estimator xgboost's best error=2.1443
[flaml.automl: 09-19 01:46:52] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:46:56] {3072} INFO -  at 44.4s,	estimator xgboost's best error=1.6952,	best estimator xgboost's best error=1.6952
[flaml.automl: 09-19 01:46:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:47:01] {3072} INFO -  at 50.1s,	estimator xgboost's best error=1.6952,	best estimator xgboost's best error=1.6952
[flaml.automl: 09-19 01:47:01] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:47:07] {3072} INFO -  at 56.0s,	estimator xgboost's best error=1.6042,	best estimator xgboost's best error=1.6042
[flaml.automl: 09-19 01:47:13] {3335} INFO - retrain xgboost for 6.2s
[flaml.automl: 09-19 01:47:13] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 01:47:13] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:47:13] {2637} INFO - Time taken to find the best model: 55.982529163360596
[flaml.automl: 09-19 01:47:13] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-0.604224192561656
SO2(0)最好结果：{'pred_time': 6.328154767506018e-05, 'wall_clock_time': 55.982529163360596, 'metric_for_logging': {'pred_time': 6.328154767506018e-05}, 'val_loss': 1.604224192561656, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 5.926103115081787}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.19730716223930422
SO2(0)的mse=13.43096076917544
SO2(0)的mae=1.6326300479093776
SO2(0)的mar=0.2517488427080388
总共花费的时间为：62.73
昌吉州
2690A
3613A
[flaml.automl: 09-19 01:54:23] {2390} INFO - task = regression
[flaml.automl: 09-19 01:54:23] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 01:54:23] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 01:54:23] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 01:54:23] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 01:54:23] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 01:54:24] {3025} INFO - Estimated sufficient time budget=15838s. Estimated necessary time budget=16s.
[flaml.automl: 09-19 01:54:24] {3072} INFO -  at 1.7s,	estimator xgboost's best error=4.9041,	best estimator xgboost's best error=4.9041
[flaml.automl: 09-19 01:54:24] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 01:54:27] {3072} INFO -  at 4.6s,	estimator xgboost's best error=2.6195,	best estimator xgboost's best error=2.6195
[flaml.automl: 09-19 01:54:27] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 01:54:29] {3072} INFO -  at 6.6s,	estimator xgboost's best error=2.6195,	best estimator xgboost's best error=2.6195
[flaml.automl: 09-19 01:54:29] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 01:54:47] {3072} INFO -  at 24.2s,	estimator xgboost's best error=2.6195,	best estimator xgboost's best error=2.6195
[flaml.automl: 09-19 01:54:47] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 01:54:49] {3072} INFO -  at 26.9s,	estimator xgboost's best error=1.1936,	best estimator xgboost's best error=1.1936
[flaml.automl: 09-19 01:54:49] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 01:54:53] {3072} INFO -  at 30.4s,	estimator xgboost's best error=1.0829,	best estimator xgboost's best error=1.0829
[flaml.automl: 09-19 01:54:53] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 01:54:56] {3072} INFO -  at 34.0s,	estimator xgboost's best error=0.9517,	best estimator xgboost's best error=0.9517
[flaml.automl: 09-19 01:54:56] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 01:55:02] {3072} INFO -  at 39.2s,	estimator xgboost's best error=0.9517,	best estimator xgboost's best error=0.9517
[flaml.automl: 09-19 01:55:02] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 01:55:05] {3072} INFO -  at 42.9s,	estimator xgboost's best error=0.9517,	best estimator xgboost's best error=0.9517
[flaml.automl: 09-19 01:55:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 01:55:12] {3072} INFO -  at 49.9s,	estimator xgboost's best error=0.9517,	best estimator xgboost's best error=0.9517
[flaml.automl: 09-19 01:55:12] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 01:55:16] {3072} INFO -  at 53.6s,	estimator xgboost's best error=0.9061,	best estimator xgboost's best error=0.9061
[flaml.automl: 09-19 01:55:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 01:55:19] {3072} INFO -  at 56.1s,	estimator xgboost's best error=0.9061,	best estimator xgboost's best error=0.9061
[flaml.automl: 09-19 01:55:19] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 01:55:22] {3072} INFO -  at 59.2s,	estimator xgboost's best error=0.8565,	best estimator xgboost's best error=0.8565
[flaml.automl: 09-19 01:55:33] {3335} INFO - retrain xgboost for 11.0s
[flaml.automl: 09-19 01:55:33] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 01:55:33] {2636} INFO - fit succeeded
[flaml.automl: 09-19 01:55:33] {2637} INFO - Time taken to find the best model: 59.24980545043945
[flaml.automl: 09-19 01:55:33] {2648} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}
SO2(0)最佳损失：0.1435059725425587
SO2(0)最好结果：{'pred_time': 3.283719922322648e-05, 'wall_clock_time': 59.24980545043945, 'metric_for_logging': {'pred_time': 3.283719922322648e-05}, 'val_loss': 0.8564940274574413, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 11.341713451089943, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8018237505253438, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.5101694158699024}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 11.341713451089943, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8018237505253438, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.5101694158699024, 'experiment_tag': 'exp', 'time_total_s': 3.1777684688568115}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8018237505253438, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=11.341713451089943,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.5101694158699024,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7800347455899391
SO2(0)的mse=2.114065683740743
SO2(0)的mae=0.8030980437641014
SO2(0)的mar=0.10350180202645089
总共花费的时间为：70.75
博尔塔拉蒙古自治州
2693A
2694A
[flaml.automl: 09-19 02:02:13] {2390} INFO - task = regression
[flaml.automl: 09-19 02:02:13] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:02:13] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:02:13] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:02:13] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:02:13] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:02:16] {3025} INFO - Estimated sufficient time budget=23650s. Estimated necessary time budget=24s.
[flaml.automl: 09-19 02:02:16] {3072} INFO -  at 2.5s,	estimator xgboost's best error=4.5926,	best estimator xgboost's best error=4.5926
[flaml.automl: 09-19 02:02:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:02:19] {3072} INFO -  at 6.2s,	estimator xgboost's best error=2.1772,	best estimator xgboost's best error=2.1772
[flaml.automl: 09-19 02:02:19] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:02:22] {3072} INFO -  at 8.4s,	estimator xgboost's best error=2.1772,	best estimator xgboost's best error=2.1772
[flaml.automl: 09-19 02:02:22] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:02:41] {3072} INFO -  at 27.5s,	estimator xgboost's best error=2.1772,	best estimator xgboost's best error=2.1772
[flaml.automl: 09-19 02:02:41] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:02:43] {3072} INFO -  at 29.5s,	estimator xgboost's best error=1.4311,	best estimator xgboost's best error=1.4311
[flaml.automl: 09-19 02:02:43] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:02:45] {3072} INFO -  at 32.2s,	estimator xgboost's best error=1.3027,	best estimator xgboost's best error=1.3027
[flaml.automl: 09-19 02:02:45] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:02:48] {3072} INFO -  at 35.0s,	estimator xgboost's best error=1.2024,	best estimator xgboost's best error=1.2024
[flaml.automl: 09-19 02:02:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:02:53] {3072} INFO -  at 39.9s,	estimator xgboost's best error=1.2024,	best estimator xgboost's best error=1.2024
[flaml.automl: 09-19 02:02:53] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:02:56] {3072} INFO -  at 42.7s,	estimator xgboost's best error=1.1830,	best estimator xgboost's best error=1.1830
[flaml.automl: 09-19 02:02:56] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:03:04] {3072} INFO -  at 50.6s,	estimator xgboost's best error=1.1830,	best estimator xgboost's best error=1.1830
[flaml.automl: 09-19 02:03:04] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:03:08] {3072} INFO -  at 55.3s,	estimator xgboost's best error=1.1830,	best estimator xgboost's best error=1.1830
[flaml.automl: 09-19 02:03:08] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:03:12] {3072} INFO -  at 58.5s,	estimator xgboost's best error=1.1830,	best estimator xgboost's best error=1.1830
[flaml.automl: 09-19 02:03:16] {3335} INFO - retrain xgboost for 4.5s
[flaml.automl: 09-19 02:03:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:03:16] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:03:16] {2637} INFO - Time taken to find the best model: 42.743823528289795
[flaml.automl: 09-19 02:03:16] {2648} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}
SO2(0)最佳损失：-0.18301940167182806
SO2(0)最好结果：{'pred_time': 2.5027517759735628e-05, 'wall_clock_time': 42.743823528289795, 'metric_for_logging': {'pred_time': 2.5027517759735628e-05}, 'val_loss': 1.183019401671828, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 1.1765273389893034, 'learning_rate': 0.8338182546732649, 'subsample': 0.9114052575858262, 'colsample_bylevel': 0.7232930776213671, 'colsample_bytree': 0.8046603913292237, 'reg_alpha': 0.00473280911734172, 'reg_lambda': 15.090081382165623}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 1.1765273389893034, 'config/learning_rate': 0.8338182546732649, 'config/subsample': 0.9114052575858262, 'config/colsample_bylevel': 0.7232930776213671, 'config/colsample_bytree': 0.8046603913292237, 'config/reg_alpha': 0.00473280911734172, 'config/reg_lambda': 15.090081382165623, 'experiment_tag': 'exp', 'time_total_s': 2.8889403343200684}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7232930776213671, colsample_bynode=1,
             colsample_bytree=0.8046603913292237, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8338182546732649,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=1.1765273389893034, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.00473280911734172,
             reg_lambda=15.090081382165623, scale_pos_weight=1,
             subsample=0.9114052575858262, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.8188298100104212
SO2(0)的mse=3.283702495115799
SO2(0)的mae=1.1686312824565614
SO2(0)的mar=0.18543492576815315
总共花费的时间为：63.59
阿克苏地区
2695A
2696A
[flaml.automl: 09-19 02:09:46] {2390} INFO - task = regression
[flaml.automl: 09-19 02:09:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:09:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:09:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:09:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:09:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:09:48] {3025} INFO - Estimated sufficient time budget=20569s. Estimated necessary time budget=21s.
[flaml.automl: 09-19 02:09:48] {3072} INFO -  at 2.2s,	estimator xgboost's best error=3.7044,	best estimator xgboost's best error=3.7044
[flaml.automl: 09-19 02:09:48] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:09:51] {3072} INFO -  at 5.3s,	estimator xgboost's best error=1.7748,	best estimator xgboost's best error=1.7748
[flaml.automl: 09-19 02:09:51] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:09:54] {3072} INFO -  at 8.1s,	estimator xgboost's best error=1.7748,	best estimator xgboost's best error=1.7748
[flaml.automl: 09-19 02:09:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:10:12] {3072} INFO -  at 25.7s,	estimator xgboost's best error=1.7748,	best estimator xgboost's best error=1.7748
[flaml.automl: 09-19 02:10:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:10:14] {3072} INFO -  at 27.9s,	estimator xgboost's best error=1.0674,	best estimator xgboost's best error=1.0674
[flaml.automl: 09-19 02:10:14] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:10:17] {3072} INFO -  at 30.7s,	estimator xgboost's best error=0.9582,	best estimator xgboost's best error=0.9582
[flaml.automl: 09-19 02:10:17] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:10:20] {3072} INFO -  at 33.8s,	estimator xgboost's best error=0.9297,	best estimator xgboost's best error=0.9297
[flaml.automl: 09-19 02:10:20] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:10:24] {3072} INFO -  at 38.6s,	estimator xgboost's best error=0.9297,	best estimator xgboost's best error=0.9297
[flaml.automl: 09-19 02:10:24] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:10:27] {3072} INFO -  at 41.6s,	estimator xgboost's best error=0.9297,	best estimator xgboost's best error=0.9297
[flaml.automl: 09-19 02:10:27] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:10:33] {3072} INFO -  at 46.8s,	estimator xgboost's best error=0.8955,	best estimator xgboost's best error=0.8955
[flaml.automl: 09-19 02:10:33] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:10:35] {3072} INFO -  at 49.4s,	estimator xgboost's best error=0.8955,	best estimator xgboost's best error=0.8955
[flaml.automl: 09-19 02:10:35] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:10:37] {3072} INFO -  at 51.6s,	estimator xgboost's best error=0.8955,	best estimator xgboost's best error=0.8955
[flaml.automl: 09-19 02:10:37] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:10:45] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.8555,	best estimator xgboost's best error=0.8555
[flaml.automl: 09-19 02:11:03] {3335} INFO - retrain xgboost for 18.3s
[flaml.automl: 09-19 02:11:03] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:11:03] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:11:03] {2637} INFO - Time taken to find the best model: 59.03416585922241
[flaml.automl: 09-19 02:11:03] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}
SO2(0)最佳损失：0.14449224450921416
SO2(0)最好结果：{'pred_time': 3.3921404226720214e-05, 'wall_clock_time': 59.03416585922241, 'metric_for_logging': {'pred_time': 3.3921404226720214e-05}, 'val_loss': 0.8555077554907858, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 15, 'min_child_weight': 0.026530416426483502, 'learning_rate': 0.8192721968561636, 'subsample': 0.9099932448156185, 'colsample_bylevel': 0.8647803489281933, 'colsample_bytree': 0.8708956493972435, 'reg_alpha': 0.003048836975765305, 'reg_lambda': 0.20419943387794007}, 'config/n_estimators': 14, 'config/max_leaves': 15, 'config/min_child_weight': 0.026530416426483502, 'config/learning_rate': 0.8192721968561636, 'config/subsample': 0.9099932448156185, 'config/colsample_bylevel': 0.8647803489281933, 'config/colsample_bytree': 0.8708956493972435, 'config/reg_alpha': 0.003048836975765305, 'config/reg_lambda': 0.20419943387794007, 'experiment_tag': 'exp', 'time_total_s': 7.458439826965332}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8647803489281933, colsample_bynode=1,
             colsample_bytree=0.8708956493972435, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.8192721968561636,
             max_delta_step=0, max_depth=0, max_leaves=15,
             min_child_weight=0.026530416426483502, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.003048836975765305, reg_lambda=0.20419943387794007,
             scale_pos_weight=1, subsample=0.9099932448156185,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7249272434360927
SO2(0)的mse=1.9209012617045376
SO2(0)的mae=0.8452284237793193
SO2(0)的mar=0.1498812523508871
总共花费的时间为：77.80
克孜勒苏柯尔克孜自治州
2697A
3665A
[flaml.automl: 09-19 02:17:41] {2390} INFO - task = regression
[flaml.automl: 09-19 02:17:41] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:17:41] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:17:41] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:17:41] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:17:41] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:17:42] {3025} INFO - Estimated sufficient time budget=19289s. Estimated necessary time budget=19s.
[flaml.automl: 09-19 02:17:42] {3072} INFO -  at 2.0s,	estimator xgboost's best error=3.3511,	best estimator xgboost's best error=3.3511
[flaml.automl: 09-19 02:17:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:17:46] {3072} INFO -  at 5.8s,	estimator xgboost's best error=1.5398,	best estimator xgboost's best error=1.5398
[flaml.automl: 09-19 02:17:46] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:17:49] {3072} INFO -  at 8.1s,	estimator xgboost's best error=1.5398,	best estimator xgboost's best error=1.5398
[flaml.automl: 09-19 02:17:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:18:05] {3072} INFO -  at 24.5s,	estimator xgboost's best error=1.5398,	best estimator xgboost's best error=1.5398
[flaml.automl: 09-19 02:18:05] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:18:07] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.7079,	best estimator xgboost's best error=0.7079
[flaml.automl: 09-19 02:18:07] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:18:09] {3072} INFO -  at 28.9s,	estimator xgboost's best error=0.6014,	best estimator xgboost's best error=0.6014
[flaml.automl: 09-19 02:18:09] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:18:12] {3072} INFO -  at 31.9s,	estimator xgboost's best error=0.5183,	best estimator xgboost's best error=0.5183
[flaml.automl: 09-19 02:18:12] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:18:16] {3072} INFO -  at 36.0s,	estimator xgboost's best error=0.5183,	best estimator xgboost's best error=0.5183
[flaml.automl: 09-19 02:18:16] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:18:19] {3072} INFO -  at 38.9s,	estimator xgboost's best error=0.5065,	best estimator xgboost's best error=0.5065
[flaml.automl: 09-19 02:18:19] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:18:24] {3072} INFO -  at 44.0s,	estimator xgboost's best error=0.5065,	best estimator xgboost's best error=0.5065
[flaml.automl: 09-19 02:18:24] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:18:27] {3072} INFO -  at 46.6s,	estimator xgboost's best error=0.5065,	best estimator xgboost's best error=0.5065
[flaml.automl: 09-19 02:18:27] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:18:29] {3072} INFO -  at 48.3s,	estimator xgboost's best error=0.5065,	best estimator xgboost's best error=0.5065
[flaml.automl: 09-19 02:18:29] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:18:39] {3072} INFO -  at 58.7s,	estimator xgboost's best error=0.4304,	best estimator xgboost's best error=0.4304
[flaml.automl: 09-19 02:18:50] {3335} INFO - retrain xgboost for 11.1s
[flaml.automl: 09-19 02:18:50] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466852,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:18:50] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:18:50] {2637} INFO - Time taken to find the best model: 58.685746908187866
[flaml.automl: 09-19 02:18:50] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466852}
SO2(0)最佳损失：0.5696169804082348
SO2(0)最好结果：{'pred_time': 3.112146416333815e-05, 'wall_clock_time': 58.685746908187866, 'metric_for_logging': {'pred_time': 3.112146416333815e-05}, 'val_loss': 0.43038301959176517, 'training_iteration': 1, 'config': {'n_estimators': 12, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466852}, 'config/n_estimators': 12, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609401, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466852, 'experiment_tag': 'exp', 'time_total_s': 10.374558687210083}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=12, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466852,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.9039645217829002
SO2(0)的mse=0.5029439950167403
SO2(0)的mae=0.4178130987128363
SO2(0)的mar=0.09161393134041142
总共花费的时间为：70.33
喀什地区
2698A
2699A
2700A
[flaml.automl: 09-19 02:28:59] {2390} INFO - task = regression
[flaml.automl: 09-19 02:28:59] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:28:59] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:28:59] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:28:59] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:28:59] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:29:00] {3025} INFO - Estimated sufficient time budget=12114s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 02:29:00] {3072} INFO -  at 1.3s,	estimator xgboost's best error=4.3823,	best estimator xgboost's best error=4.3823
[flaml.automl: 09-19 02:29:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:29:03] {3072} INFO -  at 3.9s,	estimator xgboost's best error=2.0285,	best estimator xgboost's best error=2.0285
[flaml.automl: 09-19 02:29:03] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:29:05] {3072} INFO -  at 5.9s,	estimator xgboost's best error=2.0285,	best estimator xgboost's best error=2.0285
[flaml.automl: 09-19 02:29:05] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:29:22] {3072} INFO -  at 23.3s,	estimator xgboost's best error=2.0285,	best estimator xgboost's best error=2.0285
[flaml.automl: 09-19 02:29:22] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:29:24] {3072} INFO -  at 25.4s,	estimator xgboost's best error=1.1072,	best estimator xgboost's best error=1.1072
[flaml.automl: 09-19 02:29:24] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:29:27] {3072} INFO -  at 28.0s,	estimator xgboost's best error=1.0457,	best estimator xgboost's best error=1.0457
[flaml.automl: 09-19 02:29:27] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:29:30] {3072} INFO -  at 30.9s,	estimator xgboost's best error=0.9207,	best estimator xgboost's best error=0.9207
[flaml.automl: 09-19 02:29:30] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:29:34] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.9207,	best estimator xgboost's best error=0.9207
[flaml.automl: 09-19 02:29:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:29:37] {3072} INFO -  at 37.7s,	estimator xgboost's best error=0.9116,	best estimator xgboost's best error=0.9116
[flaml.automl: 09-19 02:29:37] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:29:42] {3072} INFO -  at 43.0s,	estimator xgboost's best error=0.9116,	best estimator xgboost's best error=0.9116
[flaml.automl: 09-19 02:29:42] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:29:44] {3072} INFO -  at 45.0s,	estimator xgboost's best error=0.9116,	best estimator xgboost's best error=0.9116
[flaml.automl: 09-19 02:29:44] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:29:46] {3072} INFO -  at 47.2s,	estimator xgboost's best error=0.9116,	best estimator xgboost's best error=0.9116
[flaml.automl: 09-19 02:29:46] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:29:58] {3072} INFO -  at 59.0s,	estimator xgboost's best error=0.8218,	best estimator xgboost's best error=0.8218
[flaml.automl: 09-19 02:30:10] {3335} INFO - retrain xgboost for 11.7s
[flaml.automl: 09-19 02:30:10] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466852,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:30:10] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:30:10] {2637} INFO - Time taken to find the best model: 58.95066690444946
[flaml.automl: 09-19 02:30:10] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466852}
SO2(0)最佳损失：0.17817943362594135
SO2(0)最好结果：{'pred_time': 1.7898372022199878e-05, 'wall_clock_time': 58.95066690444946, 'metric_for_logging': {'pred_time': 1.7898372022199878e-05}, 'val_loss': 0.8218205663740586, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466852}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609401, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466852, 'experiment_tag': 'exp', 'time_total_s': 11.758175611495972}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466852,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7976230066713698
SO2(0)的mse=1.6340955590864217
SO2(0)的mae=0.7996540106015749
SO2(0)的mar=0.11738790863409393
总共花费的时间为：71.23
和田地区
3614A
3615A
[flaml.automl: 09-19 02:37:10] {2390} INFO - task = regression
[flaml.automl: 09-19 02:37:10] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:37:10] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:37:10] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:37:10] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:37:10] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:37:12] {3025} INFO - Estimated sufficient time budget=13607s. Estimated necessary time budget=14s.
[flaml.automl: 09-19 02:37:12] {3072} INFO -  at 1.4s,	estimator xgboost's best error=6.3267,	best estimator xgboost's best error=6.3267
[flaml.automl: 09-19 02:37:12] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:37:15] {3072} INFO -  at 4.7s,	estimator xgboost's best error=3.6848,	best estimator xgboost's best error=3.6848
[flaml.automl: 09-19 02:37:15] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:37:17] {3072} INFO -  at 6.6s,	estimator xgboost's best error=3.6848,	best estimator xgboost's best error=3.6848
[flaml.automl: 09-19 02:37:17] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:37:33] {3072} INFO -  at 22.7s,	estimator xgboost's best error=3.6848,	best estimator xgboost's best error=3.6848
[flaml.automl: 09-19 02:37:33] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:37:35] {3072} INFO -  at 24.7s,	estimator xgboost's best error=3.2486,	best estimator xgboost's best error=3.2486
[flaml.automl: 09-19 02:37:35] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:37:38] {3072} INFO -  at 27.7s,	estimator xgboost's best error=3.2486,	best estimator xgboost's best error=3.2486
[flaml.automl: 09-19 02:37:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:37:41] {3072} INFO -  at 30.5s,	estimator xgboost's best error=3.1276,	best estimator xgboost's best error=3.1276
[flaml.automl: 09-19 02:37:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:37:44] {3072} INFO -  at 33.9s,	estimator xgboost's best error=3.1276,	best estimator xgboost's best error=3.1276
[flaml.automl: 09-19 02:37:44] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:37:47] {3072} INFO -  at 36.4s,	estimator xgboost's best error=3.1276,	best estimator xgboost's best error=3.1276
[flaml.automl: 09-19 02:37:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:37:52] {3072} INFO -  at 41.7s,	estimator xgboost's best error=3.0191,	best estimator xgboost's best error=3.0191
[flaml.automl: 09-19 02:37:52] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:37:55] {3072} INFO -  at 44.6s,	estimator xgboost's best error=3.0191,	best estimator xgboost's best error=3.0191
[flaml.automl: 09-19 02:37:55] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:37:57] {3072} INFO -  at 46.8s,	estimator xgboost's best error=3.0191,	best estimator xgboost's best error=3.0191
[flaml.automl: 09-19 02:37:57] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:38:10] {3072} INFO -  at 59.3s,	estimator xgboost's best error=3.0191,	best estimator xgboost's best error=3.0191
[flaml.automl: 09-19 02:38:16] {3335} INFO - retrain xgboost for 6.7s
[flaml.automl: 09-19 02:38:16] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:38:16] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:38:16] {2637} INFO - Time taken to find the best model: 41.681530237197876
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}
SO2(0)最佳损失：-2.0190946544417336
SO2(0)最好结果：{'pred_time': 3.7021391812729224e-05, 'wall_clock_time': 41.681530237197876, 'metric_for_logging': {'pred_time': 3.7021391812729224e-05}, 'val_loss': 3.0190946544417336, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'experiment_tag': 'exp', 'time_total_s': 5.270949840545654}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.09072318482523811
SO2(0)的mse=28.92165357647371
SO2(0)的mae=2.9949225855000234
SO2(0)的mar=0.4315422923809786
总共花费的时间为：66.54
伊犁哈萨克州
2703A
2704A
2705A
[flaml.automl: 09-19 02:48:14] {2390} INFO - task = regression
[flaml.automl: 09-19 02:48:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:48:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:48:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:48:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:48:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:48:16] {3025} INFO - Estimated sufficient time budget=12116s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 02:48:16] {3072} INFO -  at 1.3s,	estimator xgboost's best error=5.9839,	best estimator xgboost's best error=5.9839
[flaml.automl: 09-19 02:48:16] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:48:18] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.7660,	best estimator xgboost's best error=2.7660
[flaml.automl: 09-19 02:48:18] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:48:19] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.7660,	best estimator xgboost's best error=2.7660
[flaml.automl: 09-19 02:48:19] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:48:34] {3072} INFO -  at 19.6s,	estimator xgboost's best error=2.7660,	best estimator xgboost's best error=2.7660
[flaml.automl: 09-19 02:48:34] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:48:36] {3072} INFO -  at 21.6s,	estimator xgboost's best error=1.7950,	best estimator xgboost's best error=1.7950
[flaml.automl: 09-19 02:48:36] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:48:38] {3072} INFO -  at 23.9s,	estimator xgboost's best error=1.7950,	best estimator xgboost's best error=1.7950
[flaml.automl: 09-19 02:48:38] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:48:41] {3072} INFO -  at 26.8s,	estimator xgboost's best error=1.5016,	best estimator xgboost's best error=1.5016
[flaml.automl: 09-19 02:48:41] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:48:45] {3072} INFO -  at 31.1s,	estimator xgboost's best error=1.5016,	best estimator xgboost's best error=1.5016
[flaml.automl: 09-19 02:48:45] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:48:48] {3072} INFO -  at 33.8s,	estimator xgboost's best error=1.5016,	best estimator xgboost's best error=1.5016
[flaml.automl: 09-19 02:48:48] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:48:54] {3072} INFO -  at 39.5s,	estimator xgboost's best error=1.5016,	best estimator xgboost's best error=1.5016
[flaml.automl: 09-19 02:48:54] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:48:56] {3072} INFO -  at 42.2s,	estimator xgboost's best error=1.4883,	best estimator xgboost's best error=1.4883
[flaml.automl: 09-19 02:48:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:48:59] {3072} INFO -  at 44.3s,	estimator xgboost's best error=1.4883,	best estimator xgboost's best error=1.4883
[flaml.automl: 09-19 02:48:59] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:49:08] {3072} INFO -  at 53.8s,	estimator xgboost's best error=1.4378,	best estimator xgboost's best error=1.4378
[flaml.automl: 09-19 02:49:18] {3335} INFO - retrain xgboost for 9.4s
[flaml.automl: 09-19 02:49:18] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 02:49:18] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:49:18] {2637} INFO - Time taken to find the best model: 53.84861183166504
[flaml.automl: 09-19 02:49:18] {2648} WARNING - Time taken to find the best model is 90% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}
SO2(0)最佳损失：-0.4378286190819012
SO2(0)最好结果：{'pred_time': 1.5448534853073186e-05, 'wall_clock_time': 53.84861183166504, 'metric_for_logging': {'pred_time': 1.5448534853073186e-05}, 'val_loss': 1.4378286190819012, 'training_iteration': 1, 'config': {'n_estimators': 11, 'max_leaves': 9, 'min_child_weight': 22.20894762206578, 'learning_rate': 1.0, 'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.8668816728674211, 'reg_alpha': 0.0019639563593375044, 'reg_lambda': 0.09231698378821403}, 'config/n_estimators': 11, 'config/max_leaves': 9, 'config/min_child_weight': 22.20894762206578, 'config/learning_rate': 1.0, 'config/subsample': 1.0, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.8668816728674211, 'config/reg_alpha': 0.0019639563593375044, 'config/reg_lambda': 0.09231698378821403, 'experiment_tag': 'exp', 'time_total_s': 9.508604526519775}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.8668816728674211, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=22.20894762206578,
             missing=nan, monotone_constraints='()', n_estimators=11, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0019639563593375044, reg_lambda=0.09231698378821403,
             scale_pos_weight=1, subsample=1.0, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.7178211644491928
SO2(0)的mse=5.605693878492864
SO2(0)的mae=1.4581672261546619
SO2(0)的mar=0.14413012894541358
总共花费的时间为：63.82
塔城地区
2706A
[flaml.automl: 09-19 02:52:26] {2390} INFO - task = regression
[flaml.automl: 09-19 02:52:26] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 02:52:26] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 02:52:26] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 02:52:26] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 02:52:26] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 02:52:28] {3025} INFO - Estimated sufficient time budget=22968s. Estimated necessary time budget=23s.
[flaml.automl: 09-19 02:52:28] {3072} INFO -  at 2.4s,	estimator xgboost's best error=2.2762,	best estimator xgboost's best error=2.2762
[flaml.automl: 09-19 02:52:28] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 02:52:32] {3072} INFO -  at 5.8s,	estimator xgboost's best error=1.3340,	best estimator xgboost's best error=1.3340
[flaml.automl: 09-19 02:52:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 02:52:35] {3072} INFO -  at 8.6s,	estimator xgboost's best error=1.3340,	best estimator xgboost's best error=1.3340
[flaml.automl: 09-19 02:52:35] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 02:52:54] {3072} INFO -  at 28.4s,	estimator xgboost's best error=1.3340,	best estimator xgboost's best error=1.3340
[flaml.automl: 09-19 02:52:54] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 02:52:56] {3072} INFO -  at 30.2s,	estimator xgboost's best error=0.7285,	best estimator xgboost's best error=0.7285
[flaml.automl: 09-19 02:52:56] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 02:52:59] {3072} INFO -  at 32.8s,	estimator xgboost's best error=0.6099,	best estimator xgboost's best error=0.6099
[flaml.automl: 09-19 02:52:59] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 02:53:01] {3072} INFO -  at 35.2s,	estimator xgboost's best error=0.6099,	best estimator xgboost's best error=0.6099
[flaml.automl: 09-19 02:53:01] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 02:53:04] {3072} INFO -  at 38.5s,	estimator xgboost's best error=0.6099,	best estimator xgboost's best error=0.6099
[flaml.automl: 09-19 02:53:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 02:53:06] {3072} INFO -  at 40.4s,	estimator xgboost's best error=0.6099,	best estimator xgboost's best error=0.6099
[flaml.automl: 09-19 02:53:06] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 02:53:10] {3072} INFO -  at 44.4s,	estimator xgboost's best error=0.6099,	best estimator xgboost's best error=0.6099
[flaml.automl: 09-19 02:53:10] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 02:53:12] {3072} INFO -  at 46.4s,	estimator xgboost's best error=0.6099,	best estimator xgboost's best error=0.6099
[flaml.automl: 09-19 02:53:12] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 02:53:14] {3072} INFO -  at 48.5s,	estimator xgboost's best error=0.6099,	best estimator xgboost's best error=0.6099
[flaml.automl: 09-19 02:53:14] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 02:53:24] {3072} INFO -  at 58.3s,	estimator xgboost's best error=0.5366,	best estimator xgboost's best error=0.5366
[flaml.automl: 09-19 02:53:34] {3335} INFO - retrain xgboost for 9.8s
[flaml.automl: 09-19 02:53:34] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 02:53:34] {2636} INFO - fit succeeded
[flaml.automl: 09-19 02:53:34] {2637} INFO - Time taken to find the best model: 58.28296399116516
[flaml.automl: 09-19 02:53:34] {2648} WARNING - Time taken to find the best model is 97% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}
SO2(0)最佳损失：0.4634121312411056
SO2(0)最好结果：{'pred_time': 6.418669591653933e-05, 'wall_clock_time': 58.28296399116516, 'metric_for_logging': {'pred_time': 6.418669591653933e-05}, 'val_loss': 0.5365878687588944, 'training_iteration': 1, 'config': {'n_estimators': 8, 'max_leaves': 12, 'min_child_weight': 0.28013157270142836, 'learning_rate': 1.0, 'subsample': 0.8978611264064174, 'colsample_bylevel': 0.8749096038556152, 'colsample_bytree': 0.6189807684654242, 'reg_alpha': 0.0035201437229812524, 'reg_lambda': 1.783681424562932}, 'config/n_estimators': 8, 'config/max_leaves': 12, 'config/min_child_weight': 0.28013157270142836, 'config/learning_rate': 1.0, 'config/subsample': 0.8978611264064174, 'config/colsample_bylevel': 0.8749096038556152, 'config/colsample_bytree': 0.6189807684654242, 'config/reg_alpha': 0.0035201437229812524, 'config/reg_lambda': 1.783681424562932, 'experiment_tag': 'exp', 'time_total_s': 9.74568510055542}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8749096038556152, colsample_bynode=1,
             colsample_bytree=0.6189807684654242, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=12, min_child_weight=0.28013157270142836,
             missing=nan, monotone_constraints='()', n_estimators=8, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0035201437229812524, reg_lambda=1.783681424562932,
             scale_pos_weight=1, subsample=0.8978611264064174,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8598269134752834
SO2(0)的mse=0.6266429937419933
SO2(0)的mae=0.48213918887459123
SO2(0)的mar=0.18148306306562073
总共花费的时间为：68.33
阿勒泰地区
阿勒泰地区没有数据
石河子市
2709A
2710A
3442A
[flaml.automl: 09-19 03:03:57] {2390} INFO - task = regression
[flaml.automl: 09-19 03:03:57] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:03:57] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:03:57] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:03:57] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:03:57] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:04:00] {3025} INFO - Estimated sufficient time budget=23641s. Estimated necessary time budget=24s.
[flaml.automl: 09-19 03:04:00] {3072} INFO -  at 2.5s,	estimator xgboost's best error=5.3258,	best estimator xgboost's best error=5.3258
[flaml.automl: 09-19 03:04:00] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:04:04] {3072} INFO -  at 6.6s,	estimator xgboost's best error=2.4640,	best estimator xgboost's best error=2.4640
[flaml.automl: 09-19 03:04:04] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:04:06] {3072} INFO -  at 8.8s,	estimator xgboost's best error=2.4640,	best estimator xgboost's best error=2.4640
[flaml.automl: 09-19 03:04:06] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:04:23] {3072} INFO -  at 25.8s,	estimator xgboost's best error=2.4640,	best estimator xgboost's best error=2.4640
[flaml.automl: 09-19 03:04:23] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:04:25] {3072} INFO -  at 27.9s,	estimator xgboost's best error=1.6399,	best estimator xgboost's best error=1.6399
[flaml.automl: 09-19 03:04:25] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:04:28] {3072} INFO -  at 30.8s,	estimator xgboost's best error=1.6399,	best estimator xgboost's best error=1.6399
[flaml.automl: 09-19 03:04:28] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:04:31] {3072} INFO -  at 33.8s,	estimator xgboost's best error=1.3585,	best estimator xgboost's best error=1.3585
[flaml.automl: 09-19 03:04:31] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:04:35] {3072} INFO -  at 37.3s,	estimator xgboost's best error=1.3585,	best estimator xgboost's best error=1.3585
[flaml.automl: 09-19 03:04:35] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:04:36] {3072} INFO -  at 38.9s,	estimator xgboost's best error=1.3585,	best estimator xgboost's best error=1.3585
[flaml.automl: 09-19 03:04:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:04:39] {3072} INFO -  at 41.9s,	estimator xgboost's best error=1.3585,	best estimator xgboost's best error=1.3585
[flaml.automl: 09-19 03:04:39] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:04:41] {3072} INFO -  at 43.4s,	estimator xgboost's best error=1.3585,	best estimator xgboost's best error=1.3585
[flaml.automl: 09-19 03:04:41] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:04:42] {3072} INFO -  at 44.5s,	estimator xgboost's best error=1.3585,	best estimator xgboost's best error=1.3585
[flaml.automl: 09-19 03:04:42] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:04:49] {3072} INFO -  at 51.6s,	estimator xgboost's best error=1.3585,	best estimator xgboost's best error=1.3585
[flaml.automl: 09-19 03:04:49] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:04:52] {3072} INFO -  at 55.1s,	estimator xgboost's best error=1.3139,	best estimator xgboost's best error=1.3139
[flaml.automl: 09-19 03:04:52] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 03:04:55] {3072} INFO -  at 57.2s,	estimator xgboost's best error=1.3139,	best estimator xgboost's best error=1.3139
[flaml.automl: 09-19 03:04:58] {3335} INFO - retrain xgboost for 3.5s
[flaml.automl: 09-19 03:04:58] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 03:04:58] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:04:58] {2637} INFO - Time taken to find the best model: 55.14095878601074
[flaml.automl: 09-19 03:04:58] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445}
SO2(0)最佳损失：-0.313884699862359
SO2(0)最好结果：{'pred_time': 1.1095557287169458e-05, 'wall_clock_time': 55.14095878601074, 'metric_for_logging': {'pred_time': 1.1095557287169458e-05}, 'val_loss': 1.313884699862359, 'training_iteration': 1, 'config': {'n_estimators': 14, 'max_leaves': 4, 'min_child_weight': 2.909605268359626, 'learning_rate': 0.1976415421521206, 'subsample': 0.8921566499079494, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.040166475000702445}, 'config/n_estimators': 14, 'config/max_leaves': 4, 'config/min_child_weight': 2.909605268359626, 'config/learning_rate': 0.1976415421521206, 'config/subsample': 0.8921566499079494, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.040166475000702445, 'experiment_tag': 'exp', 'time_total_s': 3.5702457427978516}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.1976415421521206,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=2.909605268359626, missing=nan,
             monotone_constraints='()', n_estimators=14, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.040166475000702445, scale_pos_weight=1,
             subsample=0.8921566499079494, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.4582264474107062
SO2(0)的mse=6.128735936183802
SO2(0)的mae=1.3895216480409014
SO2(0)的mar=0.1477992053645084
总共花费的时间为：61.30
五家渠市
2711A
3441A
[flaml.automl: 09-19 03:11:14] {2390} INFO - task = regression
[flaml.automl: 09-19 03:11:14] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:11:14] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:11:14] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:11:14] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:11:14] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:11:17] {3025} INFO - Estimated sufficient time budget=28194s. Estimated necessary time budget=28s.
[flaml.automl: 09-19 03:11:17] {3072} INFO -  at 3.0s,	estimator xgboost's best error=4.3467,	best estimator xgboost's best error=4.3467
[flaml.automl: 09-19 03:11:17] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:11:22] {3072} INFO -  at 8.0s,	estimator xgboost's best error=2.1631,	best estimator xgboost's best error=2.1631
[flaml.automl: 09-19 03:11:22] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:11:25] {3072} INFO -  at 10.9s,	estimator xgboost's best error=2.1631,	best estimator xgboost's best error=2.1631
[flaml.automl: 09-19 03:11:25] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:11:48] {3072} INFO -  at 34.0s,	estimator xgboost's best error=2.1631,	best estimator xgboost's best error=2.1631
[flaml.automl: 09-19 03:11:48] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:11:51] {3072} INFO -  at 36.7s,	estimator xgboost's best error=1.6347,	best estimator xgboost's best error=1.6347
[flaml.automl: 09-19 03:11:51] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:11:55] {3072} INFO -  at 40.7s,	estimator xgboost's best error=1.6347,	best estimator xgboost's best error=1.6347
[flaml.automl: 09-19 03:11:55] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:11:59] {3072} INFO -  at 45.3s,	estimator xgboost's best error=1.3691,	best estimator xgboost's best error=1.3691
[flaml.automl: 09-19 03:11:59] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:12:04] {3072} INFO -  at 49.7s,	estimator xgboost's best error=1.3691,	best estimator xgboost's best error=1.3691
[flaml.automl: 09-19 03:12:04] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:12:05] {3072} INFO -  at 51.3s,	estimator xgboost's best error=1.3691,	best estimator xgboost's best error=1.3691
[flaml.automl: 09-19 03:12:05] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:12:08] {3072} INFO -  at 54.4s,	estimator xgboost's best error=1.3691,	best estimator xgboost's best error=1.3691
[flaml.automl: 09-19 03:12:08] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:12:10] {3072} INFO -  at 56.1s,	estimator xgboost's best error=1.3691,	best estimator xgboost's best error=1.3691
[flaml.automl: 09-19 03:12:12] {3335} INFO - retrain xgboost for 1.7s
[flaml.automl: 09-19 03:12:12] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 03:12:12] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:12:12] {2637} INFO - Time taken to find the best model: 45.2990984916687
[flaml.automl: 09-19 03:12:12] {2648} WARNING - Time taken to find the best model is 75% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}
SO2(0)最佳损失：-0.36908652431590094
SO2(0)最好结果：{'pred_time': 4.150038180143937e-05, 'wall_clock_time': 45.2990984916687, 'metric_for_logging': {'pred_time': 4.150038180143937e-05}, 'val_loss': 1.369086524315901, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 5.0520043140230495, 'learning_rate': 0.365452415156215, 'subsample': 1.0, 'colsample_bylevel': 0.8559256777087657, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.20358706520158557}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 5.0520043140230495, 'config/learning_rate': 0.365452415156215, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8559256777087657, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.20358706520158557, 'experiment_tag': 'exp', 'time_total_s': 4.583828449249268}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8559256777087657, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.365452415156215, max_delta_step=0, max_depth=0,
             max_leaves=4, min_child_weight=5.0520043140230495, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.20358706520158557, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.536255883178869
SO2(0)的mse=5.269450270877564
SO2(0)的mae=1.3895343518777392
SO2(0)的mar=0.25719571350227366
总共花费的时间为：58.24
三沙市
三沙市没有数据
兰州新区
3245A
3246A
[flaml.automl: 09-19 03:19:39] {2390} INFO - task = regression
[flaml.automl: 09-19 03:19:39] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:19:39] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:19:39] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:19:39] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:19:39] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:19:42] {3025} INFO - Estimated sufficient time budget=29810s. Estimated necessary time budget=30s.
[flaml.automl: 09-19 03:19:42] {3072} INFO -  at 3.1s,	estimator xgboost's best error=8.7777,	best estimator xgboost's best error=8.7777
[flaml.automl: 09-19 03:19:42] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:19:47] {3072} INFO -  at 8.0s,	estimator xgboost's best error=4.4334,	best estimator xgboost's best error=4.4334
[flaml.automl: 09-19 03:19:47] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:19:49] {3072} INFO -  at 10.8s,	estimator xgboost's best error=4.4334,	best estimator xgboost's best error=4.4334
[flaml.automl: 09-19 03:19:49] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:20:12] {3072} INFO -  at 33.9s,	estimator xgboost's best error=4.4334,	best estimator xgboost's best error=4.4334
[flaml.automl: 09-19 03:20:12] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:20:15] {3072} INFO -  at 36.8s,	estimator xgboost's best error=3.4561,	best estimator xgboost's best error=3.4561
[flaml.automl: 09-19 03:20:15] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:20:19] {3072} INFO -  at 40.8s,	estimator xgboost's best error=3.2926,	best estimator xgboost's best error=3.2926
[flaml.automl: 09-19 03:20:19] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:20:23] {3072} INFO -  at 44.4s,	estimator xgboost's best error=3.2352,	best estimator xgboost's best error=3.2352
[flaml.automl: 09-19 03:20:23] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:20:26] {3072} INFO -  at 47.6s,	estimator xgboost's best error=3.2352,	best estimator xgboost's best error=3.2352
[flaml.automl: 09-19 03:20:26] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:20:28] {3072} INFO -  at 49.2s,	estimator xgboost's best error=3.2352,	best estimator xgboost's best error=3.2352
[flaml.automl: 09-19 03:20:28] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:20:31] {3072} INFO -  at 52.2s,	estimator xgboost's best error=3.1566,	best estimator xgboost's best error=3.1566
[flaml.automl: 09-19 03:20:31] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:20:32] {3072} INFO -  at 53.8s,	estimator xgboost's best error=3.1566,	best estimator xgboost's best error=3.1566
[flaml.automl: 09-19 03:20:32] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:20:33] {3072} INFO -  at 54.9s,	estimator xgboost's best error=3.1566,	best estimator xgboost's best error=3.1566
[flaml.automl: 09-19 03:20:33] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:20:38] {3072} INFO -  at 59.2s,	estimator xgboost's best error=3.1566,	best estimator xgboost's best error=3.1566
[flaml.automl: 09-19 03:20:41] {3335} INFO - retrain xgboost for 2.9s
[flaml.automl: 09-19 03:20:41] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 03:20:41] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:20:41] {2637} INFO - Time taken to find the best model: 52.187888383865356
[flaml.automl: 09-19 03:20:41] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}
SO2(0)最佳损失：-2.1565911322085807
SO2(0)最好结果：{'pred_time': 1.6329881543906184e-05, 'wall_clock_time': 52.187888383865356, 'metric_for_logging': {'pred_time': 1.6329881543906184e-05}, 'val_loss': 3.1565911322085807, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.124780609198487, 'learning_rate': 0.33555895600495617, 'subsample': 0.872014371732895, 'colsample_bylevel': 0.7299540931118711, 'colsample_bytree': 0.9694780270938725, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.7117919169304978}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.124780609198487, 'config/learning_rate': 0.33555895600495617, 'config/subsample': 0.872014371732895, 'config/colsample_bylevel': 0.7299540931118711, 'config/colsample_bytree': 0.9694780270938725, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.7117919169304978, 'experiment_tag': 'exp', 'time_total_s': 2.9821763038635254}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7299540931118711, colsample_bynode=1,
             colsample_bytree=0.9694780270938725, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.33555895600495617,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.124780609198487, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.7117919169304978,
             scale_pos_weight=1, subsample=0.872014371732895,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.31622396355939986
SO2(0)的mse=25.955343904707206
SO2(0)的mae=2.798365575408649
SO2(0)的mar=0.22379195697022866
总共花费的时间为：62.77
赣江新区
3414A
[flaml.automl: 09-19 03:24:01] {2390} INFO - task = regression
[flaml.automl: 09-19 03:24:01] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:24:01] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:24:01] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:24:01] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:24:01] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:24:03] {3025} INFO - Estimated sufficient time budget=28758s. Estimated necessary time budget=29s.
[flaml.automl: 09-19 03:24:03] {3072} INFO -  at 2.9s,	estimator xgboost's best error=4.4784,	best estimator xgboost's best error=4.4784
[flaml.automl: 09-19 03:24:03] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:24:07] {3072} INFO -  at 6.9s,	estimator xgboost's best error=2.3527,	best estimator xgboost's best error=2.3527
[flaml.automl: 09-19 03:24:07] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:24:10] {3072} INFO -  at 9.7s,	estimator xgboost's best error=2.3527,	best estimator xgboost's best error=2.3527
[flaml.automl: 09-19 03:24:10] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:24:27] {3072} INFO -  at 26.3s,	estimator xgboost's best error=2.3527,	best estimator xgboost's best error=2.3527
[flaml.automl: 09-19 03:24:27] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:24:30] {3072} INFO -  at 29.1s,	estimator xgboost's best error=1.0418,	best estimator xgboost's best error=1.0418
[flaml.automl: 09-19 03:24:30] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:24:33] {3072} INFO -  at 32.9s,	estimator xgboost's best error=0.9498,	best estimator xgboost's best error=0.9498
[flaml.automl: 09-19 03:24:33] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:24:37] {3072} INFO -  at 36.6s,	estimator xgboost's best error=0.8927,	best estimator xgboost's best error=0.8927
[flaml.automl: 09-19 03:24:37] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:24:43] {3072} INFO -  at 42.3s,	estimator xgboost's best error=0.8927,	best estimator xgboost's best error=0.8927
[flaml.automl: 09-19 03:24:43] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:24:47] {3072} INFO -  at 46.0s,	estimator xgboost's best error=0.8927,	best estimator xgboost's best error=0.8927
[flaml.automl: 09-19 03:24:47] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:24:53] {3072} INFO -  at 52.5s,	estimator xgboost's best error=0.8927,	best estimator xgboost's best error=0.8927
[flaml.automl: 09-19 03:24:53] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:24:56] {3072} INFO -  at 55.3s,	estimator xgboost's best error=0.8625,	best estimator xgboost's best error=0.8625
[flaml.automl: 09-19 03:24:56] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:24:58] {3072} INFO -  at 57.4s,	estimator xgboost's best error=0.8625,	best estimator xgboost's best error=0.8625
[flaml.automl: 09-19 03:25:00] {3335} INFO - retrain xgboost for 1.6s
[flaml.automl: 09-19 03:25:00] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8660549326361876, colsample_bynode=1,
             colsample_bytree=0.9004061282219729, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.590319080116618,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=53.34352431682733, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.778332386065348, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 03:25:00] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:25:00] {2637} INFO - Time taken to find the best model: 55.34585785865784
[flaml.automl: 09-19 03:25:00] {2648} WARNING - Time taken to find the best model is 92% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 53.34352431682733, 'learning_rate': 0.590319080116618, 'subsample': 1.0, 'colsample_bylevel': 0.8660549326361876, 'colsample_bytree': 0.9004061282219729, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.778332386065348}
SO2(0)最佳损失：0.13749497407437528
SO2(0)最好结果：{'pred_time': 7.100341573757045e-05, 'wall_clock_time': 55.34585785865784, 'metric_for_logging': {'pred_time': 7.100341573757045e-05}, 'val_loss': 0.8625050259256247, 'training_iteration': 1, 'config': {'n_estimators': 6, 'max_leaves': 4, 'min_child_weight': 53.34352431682733, 'learning_rate': 0.590319080116618, 'subsample': 1.0, 'colsample_bylevel': 0.8660549326361876, 'colsample_bytree': 0.9004061282219729, 'reg_alpha': 0.0009765625, 'reg_lambda': 1.778332386065348}, 'config/n_estimators': 6, 'config/max_leaves': 4, 'config/min_child_weight': 53.34352431682733, 'config/learning_rate': 0.590319080116618, 'config/subsample': 1.0, 'config/colsample_bylevel': 0.8660549326361876, 'config/colsample_bytree': 0.9004061282219729, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 1.778332386065348, 'experiment_tag': 'exp', 'time_total_s': 2.8838489055633545}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8660549326361876, colsample_bynode=1,
             colsample_bytree=0.9004061282219729, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.590319080116618,
             max_delta_step=0, max_depth=0, max_leaves=4,
             min_child_weight=53.34352431682733, missing=nan,
             monotone_constraints='()', n_estimators=6, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=1.778332386065348, scale_pos_weight=1, subsample=1.0,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.7647530758929996
SO2(0)的mse=1.7751263083390743
SO2(0)的mae=0.8894513750390232
SO2(0)的mar=0.1331946404003282
总共花费的时间为：59.28
儋州市
3541A
3542A
[flaml.automl: 09-19 03:31:46] {2390} INFO - task = regression
[flaml.automl: 09-19 03:31:46] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:31:46] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:31:46] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:31:46] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:31:46] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:31:49] {3025} INFO - Estimated sufficient time budget=28671s. Estimated necessary time budget=29s.
[flaml.automl: 09-19 03:31:49] {3072} INFO -  at 3.0s,	estimator xgboost's best error=3.2876,	best estimator xgboost's best error=3.2876
[flaml.automl: 09-19 03:31:49] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:31:53] {3072} INFO -  at 7.5s,	estimator xgboost's best error=2.0446,	best estimator xgboost's best error=2.0446
[flaml.automl: 09-19 03:31:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:31:57] {3072} INFO -  at 10.7s,	estimator xgboost's best error=2.0446,	best estimator xgboost's best error=2.0446
[flaml.automl: 09-19 03:31:57] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:32:18] {3072} INFO -  at 32.1s,	estimator xgboost's best error=2.0446,	best estimator xgboost's best error=2.0446
[flaml.automl: 09-19 03:32:18] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:32:21] {3072} INFO -  at 34.9s,	estimator xgboost's best error=1.9581,	best estimator xgboost's best error=1.9581
[flaml.automl: 09-19 03:32:21] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:32:24] {3072} INFO -  at 38.7s,	estimator xgboost's best error=1.9581,	best estimator xgboost's best error=1.9581
[flaml.automl: 09-19 03:32:24] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:32:28] {3072} INFO -  at 42.6s,	estimator xgboost's best error=1.8990,	best estimator xgboost's best error=1.8990
[flaml.automl: 09-19 03:32:28] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:32:34] {3072} INFO -  at 47.8s,	estimator xgboost's best error=1.8990,	best estimator xgboost's best error=1.8990
[flaml.automl: 09-19 03:32:34] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:32:36] {3072} INFO -  at 50.6s,	estimator xgboost's best error=1.8990,	best estimator xgboost's best error=1.8990
[flaml.automl: 09-19 03:32:36] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:32:42] {3072} INFO -  at 55.9s,	estimator xgboost's best error=1.8607,	best estimator xgboost's best error=1.8607
[flaml.automl: 09-19 03:32:47] {3335} INFO - retrain xgboost for 5.1s
[flaml.automl: 09-19 03:32:47] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 03:32:47] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:32:47] {2637} INFO - Time taken to find the best model: 55.85375475883484
[flaml.automl: 09-19 03:32:47] {2648} WARNING - Time taken to find the best model is 93% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}
SO2(0)最佳损失：-0.8607487310553881
SO2(0)最好结果：{'pred_time': 3.211668486674661e-05, 'wall_clock_time': 55.85375475883484, 'metric_for_logging': {'pred_time': 3.211668486674661e-05}, 'val_loss': 1.860748731055388, 'training_iteration': 1, 'config': {'n_estimators': 7, 'max_leaves': 7, 'min_child_weight': 0.24434103593688827, 'learning_rate': 0.2755318722843116, 'subsample': 0.9105300564291702, 'colsample_bylevel': 0.7878752579466075, 'colsample_bytree': 1.0, 'reg_alpha': 0.0015160112112420647, 'reg_lambda': 0.12880129778812768}, 'config/n_estimators': 7, 'config/max_leaves': 7, 'config/min_child_weight': 0.24434103593688827, 'config/learning_rate': 0.2755318722843116, 'config/subsample': 0.9105300564291702, 'config/colsample_bylevel': 0.7878752579466075, 'config/colsample_bytree': 1.0, 'config/reg_alpha': 0.0015160112112420647, 'config/reg_lambda': 0.12880129778812768, 'experiment_tag': 'exp', 'time_total_s': 5.268816232681274}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.7878752579466075, colsample_bynode=1,
             colsample_bytree=1.0, gamma=0, gpu_id=-1, grow_policy='lossguide',
             importance_type='gain', interaction_constraints='',
             learning_rate=0.2755318722843116, max_delta_step=0, max_depth=0,
             max_leaves=7, min_child_weight=0.24434103593688827, missing=nan,
             monotone_constraints='()', n_estimators=7, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.0015160112112420647, reg_lambda=0.12880129778812768,
             scale_pos_weight=1, subsample=0.9105300564291702,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=-1.0214510682372056
SO2(0)的mse=11.490023152781095
SO2(0)的mae=1.8295334176220954
SO2(0)的mar=0.33930941757782634
总共花费的时间为：61.40
雄安新区
3584A
3585A
3586A
3587A
3588A
3589A
[flaml.automl: 09-19 03:53:49] {2390} INFO - task = regression
[flaml.automl: 09-19 03:53:49] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 03:53:49] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 03:53:49] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 03:53:49] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 03:53:49] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 03:53:51] {3025} INFO - Estimated sufficient time budget=79146s. Estimated necessary time budget=79s.
[flaml.automl: 09-19 03:53:51] {3072} INFO -  at 1.5s,	estimator xgboost's best error=5.6082,	best estimator xgboost's best error=5.6082
[flaml.automl: 09-19 03:53:51] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 03:53:53] {3072} INFO -  at 3.6s,	estimator xgboost's best error=2.5548,	best estimator xgboost's best error=2.5548
[flaml.automl: 09-19 03:53:53] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 03:53:54] {3072} INFO -  at 4.8s,	estimator xgboost's best error=2.5548,	best estimator xgboost's best error=2.5548
[flaml.automl: 09-19 03:53:54] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 03:53:58] {3072} INFO -  at 8.5s,	estimator xgboost's best error=2.5548,	best estimator xgboost's best error=2.5548
[flaml.automl: 09-19 03:53:58] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 03:54:00] {3072} INFO -  at 10.6s,	estimator xgboost's best error=1.4273,	best estimator xgboost's best error=1.4273
[flaml.automl: 09-19 03:54:00] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 03:54:02] {3072} INFO -  at 13.2s,	estimator xgboost's best error=1.1625,	best estimator xgboost's best error=1.1625
[flaml.automl: 09-19 03:54:02] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 03:54:05] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.0925,	best estimator xgboost's best error=1.0925
[flaml.automl: 09-19 03:54:05] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 03:54:08] {3072} INFO -  at 18.6s,	estimator xgboost's best error=1.0925,	best estimator xgboost's best error=1.0925
[flaml.automl: 09-19 03:54:08] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 03:54:11] {3072} INFO -  at 21.7s,	estimator xgboost's best error=1.0922,	best estimator xgboost's best error=1.0922
[flaml.automl: 09-19 03:54:11] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 03:54:13] {3072} INFO -  at 23.9s,	estimator xgboost's best error=1.0922,	best estimator xgboost's best error=1.0922
[flaml.automl: 09-19 03:54:13] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 03:54:16] {3072} INFO -  at 26.4s,	estimator xgboost's best error=1.0922,	best estimator xgboost's best error=1.0922
[flaml.automl: 09-19 03:54:16] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 03:54:18] {3072} INFO -  at 28.4s,	estimator xgboost's best error=1.0922,	best estimator xgboost's best error=1.0922
[flaml.automl: 09-19 03:54:18] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 03:54:19] {3072} INFO -  at 30.0s,	estimator xgboost's best error=0.9941,	best estimator xgboost's best error=0.9941
[flaml.automl: 09-19 03:54:19] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 03:54:20] {3072} INFO -  at 31.2s,	estimator xgboost's best error=0.9941,	best estimator xgboost's best error=0.9941
[flaml.automl: 09-19 03:54:20] {2897} INFO - iteration 14, current learner xgboost
[flaml.automl: 09-19 03:54:22] {3072} INFO -  at 33.3s,	estimator xgboost's best error=0.9941,	best estimator xgboost's best error=0.9941
[flaml.automl: 09-19 03:54:22] {2897} INFO - iteration 15, current learner xgboost
[flaml.automl: 09-19 03:54:24] {3072} INFO -  at 35.1s,	estimator xgboost's best error=0.9941,	best estimator xgboost's best error=0.9941
[flaml.automl: 09-19 03:54:24] {2897} INFO - iteration 16, current learner xgboost
[flaml.automl: 09-19 03:54:25] {3072} INFO -  at 36.3s,	estimator xgboost's best error=0.9941,	best estimator xgboost's best error=0.9941
[flaml.automl: 09-19 03:54:25] {2897} INFO - iteration 17, current learner xgboost
[flaml.automl: 09-19 03:54:41] {3072} INFO -  at 52.2s,	estimator xgboost's best error=0.8345,	best estimator xgboost's best error=0.8345
[flaml.automl: 09-19 03:55:07] {3335} INFO - retrain xgboost for 25.3s
[flaml.automl: 09-19 03:55:07] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466852,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
[flaml.automl: 09-19 03:55:07] {2636} INFO - fit succeeded
[flaml.automl: 09-19 03:55:07] {2637} INFO - Time taken to find the best model: 52.24943399429321
[flaml.automl: 09-19 03:55:07] {2648} WARNING - Time taken to find the best model is 87% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466852, 'FLAML_sample_size': 66139}
SO2(0)最佳损失：0.16545484850789138
SO2(0)最好结果：{'pred_time': 1.78137836464085e-05, 'wall_clock_time': 52.24943399429321, 'metric_for_logging': {'pred_time': 1.78137836464085e-05}, 'val_loss': 0.8345451514921086, 'training_iteration': 1, 'config': {'n_estimators': 13, 'max_leaves': 9, 'min_child_weight': 0.2501491252609401, 'learning_rate': 1.0, 'subsample': 0.9493841306685498, 'colsample_bylevel': 0.8581193334376893, 'colsample_bytree': 0.7060780136325947, 'reg_alpha': 0.009518111297058601, 'reg_lambda': 4.329054604466852, 'FLAML_sample_size': 66139}, 'config/n_estimators': 13, 'config/max_leaves': 9, 'config/min_child_weight': 0.2501491252609401, 'config/learning_rate': 1.0, 'config/subsample': 0.9493841306685498, 'config/colsample_bylevel': 0.8581193334376893, 'config/colsample_bytree': 0.7060780136325947, 'config/reg_alpha': 0.009518111297058601, 'config/reg_lambda': 4.329054604466852, 'config/FLAML_sample_size': 66139, 'experiment_tag': 'exp', 'time_total_s': 15.946101188659668}
XGBRegressor(base_score=0.5, booster='gbtree',
             colsample_bylevel=0.8581193334376893, colsample_bynode=1,
             colsample_bytree=0.7060780136325947, gamma=0, gpu_id=-1,
             grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=1.0, max_delta_step=0,
             max_depth=0, max_leaves=9, min_child_weight=0.2501491252609401,
             missing=nan, monotone_constraints='()', n_estimators=13, n_jobs=-1,
             num_parallel_tree=1, random_state=0,
             reg_alpha=0.009518111297058601, reg_lambda=4.329054604466852,
             scale_pos_weight=1, subsample=0.9493841306685498,
             tree_method='hist', use_label_encoder=False, validate_parameters=1,
             verbosity=0)
SO2(0)的R2=0.8812363682874802
SO2(0)的mse=1.7171453679764224
SO2(0)的mae=0.8293663716377536
SO2(0)的mar=0.10666692465527651
总共花费的时间为：78.73
西咸新区
3606A
3607A
3608A
[flaml.automl: 09-19 04:05:29] {2390} INFO - task = regression
[flaml.automl: 09-19 04:05:29] {2392} INFO - Data split method: uniform
[flaml.automl: 09-19 04:05:29] {2396} INFO - Evaluation method: holdout
[flaml.automl: 09-19 04:05:29] {2465} INFO - Minimizing error metric: mae
[flaml.automl: 09-19 04:05:29] {2605} INFO - List of ML learners in AutoML Run: ['xgboost']
[flaml.automl: 09-19 04:05:29] {2897} INFO - iteration 0, current learner xgboost
[flaml.automl: 09-19 04:05:30] {3025} INFO - Estimated sufficient time budget=11928s. Estimated necessary time budget=12s.
[flaml.automl: 09-19 04:05:30] {3072} INFO -  at 1.4s,	estimator xgboost's best error=4.6435,	best estimator xgboost's best error=4.6435
[flaml.automl: 09-19 04:05:30] {2897} INFO - iteration 1, current learner xgboost
[flaml.automl: 09-19 04:05:32] {3072} INFO -  at 3.5s,	estimator xgboost's best error=2.0951,	best estimator xgboost's best error=2.0951
[flaml.automl: 09-19 04:05:32] {2897} INFO - iteration 2, current learner xgboost
[flaml.automl: 09-19 04:05:33] {3072} INFO -  at 4.7s,	estimator xgboost's best error=2.0951,	best estimator xgboost's best error=2.0951
[flaml.automl: 09-19 04:05:33] {2897} INFO - iteration 3, current learner xgboost
[flaml.automl: 09-19 04:05:43] {3072} INFO -  at 14.7s,	estimator xgboost's best error=2.0951,	best estimator xgboost's best error=2.0951
[flaml.automl: 09-19 04:05:43] {2897} INFO - iteration 4, current learner xgboost
[flaml.automl: 09-19 04:05:45] {3072} INFO -  at 15.8s,	estimator xgboost's best error=1.0030,	best estimator xgboost's best error=1.0030
[flaml.automl: 09-19 04:05:45] {2897} INFO - iteration 5, current learner xgboost
[flaml.automl: 09-19 04:05:46] {3072} INFO -  at 17.4s,	estimator xgboost's best error=0.9090,	best estimator xgboost's best error=0.9090
[flaml.automl: 09-19 04:05:46] {2897} INFO - iteration 6, current learner xgboost
[flaml.automl: 09-19 04:05:48] {3072} INFO -  at 19.1s,	estimator xgboost's best error=0.7053,	best estimator xgboost's best error=0.7053
[flaml.automl: 09-19 04:05:48] {2897} INFO - iteration 7, current learner xgboost
[flaml.automl: 09-19 04:05:50] {3072} INFO -  at 21.8s,	estimator xgboost's best error=0.7053,	best estimator xgboost's best error=0.7053
[flaml.automl: 09-19 04:05:50] {2897} INFO - iteration 8, current learner xgboost
[flaml.automl: 09-19 04:05:52] {3072} INFO -  at 23.4s,	estimator xgboost's best error=0.7053,	best estimator xgboost's best error=0.7053
[flaml.automl: 09-19 04:05:52] {2897} INFO - iteration 9, current learner xgboost
[flaml.automl: 09-19 04:05:55] {3072} INFO -  at 26.4s,	estimator xgboost's best error=0.7053,	best estimator xgboost's best error=0.7053
[flaml.automl: 09-19 04:05:55] {2897} INFO - iteration 10, current learner xgboost
[flaml.automl: 09-19 04:05:57] {3072} INFO -  at 27.9s,	estimator xgboost's best error=0.7053,	best estimator xgboost's best error=0.7053
[flaml.automl: 09-19 04:05:57] {2897} INFO - iteration 11, current learner xgboost
[flaml.automl: 09-19 04:05:58] {3072} INFO -  at 29.0s,	estimator xgboost's best error=0.7053,	best estimator xgboost's best error=0.7053
[flaml.automl: 09-19 04:05:58] {2897} INFO - iteration 12, current learner xgboost
[flaml.automl: 09-19 04:06:08] {3072} INFO -  at 39.4s,	estimator xgboost's best error=0.5789,	best estimator xgboost's best error=0.5789
[flaml.automl: 09-19 04:06:08] {2897} INFO - iteration 13, current learner xgboost
[flaml.automl: 09-19 04:06:28] {3072} INFO -  at 58.8s,	estimator xgboost's best error=0.5784,	best estimator xgboost's best error=0.5784
[flaml.automl: 09-19 04:06:57] {3335} INFO - retrain xgboost for 29.1s
[flaml.automl: 09-19 04:06:57] {3342} INFO - retrained model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
[flaml.automl: 09-19 04:06:57] {2636} INFO - fit succeeded
[flaml.automl: 09-19 04:06:57] {2637} INFO - Time taken to find the best model: 58.80031442642212
[flaml.automl: 09-19 04:06:57] {2648} WARNING - Time taken to find the best model is 98% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.
SO2(0)最佳参数：{'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767}
SO2(0)最佳损失：0.4216235890294181
SO2(0)最好结果：{'pred_time': 2.4124717557375363e-05, 'wall_clock_time': 58.80031442642212, 'metric_for_logging': {'pred_time': 2.4124717557375363e-05}, 'val_loss': 0.5783764109705819, 'training_iteration': 1, 'config': {'n_estimators': 31, 'max_leaves': 7, 'min_child_weight': 0.315923353235719, 'learning_rate': 0.5408133424638543, 'subsample': 0.8916198382943978, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.918062466998774, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.06367926097691767}, 'config/n_estimators': 31, 'config/max_leaves': 7, 'config/min_child_weight': 0.315923353235719, 'config/learning_rate': 0.5408133424638543, 'config/subsample': 0.8916198382943978, 'config/colsample_bylevel': 1.0, 'config/colsample_bytree': 0.918062466998774, 'config/reg_alpha': 0.0009765625, 'config/reg_lambda': 0.06367926097691767, 'experiment_tag': 'exp', 'time_total_s': 19.387895584106445}
XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,
             colsample_bynode=1, colsample_bytree=0.918062466998774, gamma=0,
             gpu_id=-1, grow_policy='lossguide', importance_type='gain',
             interaction_constraints='', learning_rate=0.5408133424638543,
             max_delta_step=0, max_depth=0, max_leaves=7,
             min_child_weight=0.315923353235719, missing=nan,
             monotone_constraints='()', n_estimators=31, n_jobs=-1,
             num_parallel_tree=1, random_state=0, reg_alpha=0.0009765625,
             reg_lambda=0.06367926097691767, scale_pos_weight=1,
             subsample=0.8916198382943978, tree_method='hist',
             use_label_encoder=False, validate_parameters=1, verbosity=0)
SO2(0)的R2=0.840085828403994
SO2(0)的mse=0.8679707167134135
SO2(0)的mae=0.584691839286741
SO2(0)的mar=0.08053755025872406
总共花费的时间为：88.54
